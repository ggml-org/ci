### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.68 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.65 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.31 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.94 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.27 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.91 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.63 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   27.09 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 223.81 sec*proc (28 tests)

Total Test time (real) = 223.82 sec

real	3m43.851s
user	7m36.065s
sys	0m6.513s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.58 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.23 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.57 sec*proc (28 tests)

Total Test time (real) =  51.58 sec

real	0m51.593s
user	1m11.421s
sys	0m5.682s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.094 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.633 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.650 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.652 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.653 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.653 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.655 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.656 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.656 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.657 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.658 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.661 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.662 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.663 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.663 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.664 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.665 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.665 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.306 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.310 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.311 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.312 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.312 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.313 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.032.314 I llama_model_loader: - type  f32:  124 tensors
0.00.032.314 I llama_model_loader: - type  f16:   73 tensors
0.00.032.315 I print_info: file format = GGUF V3 (latest)
0.00.032.316 I print_info: file type   = F16
0.00.032.318 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.495 I load: special tokens cache size = 5
0.00.039.313 I load: token to piece cache size = 0.2032 MB
0.00.039.333 I print_info: arch             = bert
0.00.039.334 I print_info: vocab_only       = 0
0.00.039.334 I print_info: n_ctx_train      = 512
0.00.039.334 I print_info: n_embd           = 384
0.00.039.334 I print_info: n_layer          = 12
0.00.039.339 I print_info: n_head           = 12
0.00.039.340 I print_info: n_head_kv        = 12
0.00.039.340 I print_info: n_rot            = 32
0.00.039.340 I print_info: n_swa            = 0
0.00.039.341 I print_info: n_embd_head_k    = 32
0.00.039.341 I print_info: n_embd_head_v    = 32
0.00.039.341 I print_info: n_gqa            = 1
0.00.039.342 I print_info: n_embd_k_gqa     = 384
0.00.039.342 I print_info: n_embd_v_gqa     = 384
0.00.039.343 I print_info: f_norm_eps       = 1.0e-12
0.00.039.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.344 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.344 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.344 I print_info: f_logit_scale    = 0.0e+00
0.00.039.346 I print_info: n_ff             = 1536
0.00.039.346 I print_info: n_expert         = 0
0.00.039.346 I print_info: n_expert_used    = 0
0.00.039.346 I print_info: causal attn      = 0
0.00.039.347 I print_info: pooling type     = 2
0.00.039.347 I print_info: rope type        = 2
0.00.039.347 I print_info: rope scaling     = linear
0.00.039.347 I print_info: freq_base_train  = 10000.0
0.00.039.347 I print_info: freq_scale_train = 1
0.00.039.348 I print_info: n_ctx_orig_yarn  = 512
0.00.039.348 I print_info: rope_finetuned   = unknown
0.00.039.348 I print_info: ssm_d_conv       = 0
0.00.039.348 I print_info: ssm_d_inner      = 0
0.00.039.348 I print_info: ssm_d_state      = 0
0.00.039.348 I print_info: ssm_dt_rank      = 0
0.00.039.348 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.349 I print_info: model type       = 33M
0.00.039.349 I print_info: model params     = 33.21 M
0.00.039.349 I print_info: general.name     = Bge Small
0.00.039.349 I print_info: vocab type       = WPM
0.00.039.350 I print_info: n_vocab          = 30522
0.00.039.350 I print_info: n_merges         = 0
0.00.039.350 I print_info: BOS token        = 101 '[CLS]'
0.00.039.350 I print_info: UNK token        = 100 '[UNK]'
0.00.039.350 I print_info: SEP token        = 102 '[SEP]'
0.00.039.351 I print_info: PAD token        = 0 '[PAD]'
0.00.039.351 I print_info: MASK token       = 103 '[MASK]'
0.00.039.351 I print_info: LF token         = 0 '[PAD]'
0.00.039.351 I print_info: max token length = 21
0.00.040.659 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.659 I load_tensors: offloading output layer to GPU
0.00.040.660 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.681 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.682 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.040.829 I llama_init_from_model: n_seq_max     = 1
0.00.040.830 I llama_init_from_model: n_ctx         = 512
0.00.040.830 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.831 I llama_init_from_model: n_batch       = 2048
0.00.040.831 I llama_init_from_model: n_ubatch      = 2048
0.00.040.831 I llama_init_from_model: flash_attn    = 0
0.00.040.831 I llama_init_from_model: freq_base     = 10000.0
0.00.040.833 I llama_init_from_model: freq_scale    = 1
0.00.040.833 I ggml_metal_init: allocating
0.00.040.836 I ggml_metal_init: found device: Apple M4
0.00.040.839 I ggml_metal_init: picking default device: Apple M4
0.00.041.498 I ggml_metal_init: using embedded metal library
0.00.044.195 I ggml_metal_init: GPU name:   Apple M4
0.00.044.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.198 I ggml_metal_init: simdgroup reduction   = true
0.00.044.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.198 I ggml_metal_init: has bfloat            = true
0.00.044.198 I ggml_metal_init: use bfloat            = true
0.00.044.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.463 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.946 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.948 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.949 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.054.592 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.054.593 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.054.594 I llama_init_from_model: graph nodes  = 429
0.00.054.594 I llama_init_from_model: graph splits = 2
0.00.054.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.612 I 
0.00.059.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.168 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.363 I llama_perf_context_print:        load time =      37.82 ms
0.00.064.364 I llama_perf_context_print: prompt eval time =       4.08 ms /     9 tokens (    0.45 ms per token,  2206.42 tokens per second)
0.00.064.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.365 I llama_perf_context_print:       total time =       4.75 ms /    10 tokens
0.00.064.581 I ggml_metal_free: deallocating

real	0m0.264s
user	0m0.043s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.316 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.320 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.321 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.321 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.323 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.324 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.324 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.324 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.325 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.325 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.327 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.327 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.328 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.328 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.328 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.329 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.684 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.278 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.279 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.280 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.280 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.282 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.282 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.282 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.283 I llama_model_loader: - type  f32:  124 tensors
0.00.014.283 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.284 I print_info: file format = GGUF V3 (latest)
0.00.014.284 I print_info: file type   = Q8_0
0.00.014.285 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.619 I load: special tokens cache size = 5
0.00.017.805 I load: token to piece cache size = 0.2032 MB
0.00.017.808 I print_info: arch             = bert
0.00.017.808 I print_info: vocab_only       = 0
0.00.017.808 I print_info: n_ctx_train      = 512
0.00.017.808 I print_info: n_embd           = 384
0.00.017.808 I print_info: n_layer          = 12
0.00.017.811 I print_info: n_head           = 12
0.00.017.812 I print_info: n_head_kv        = 12
0.00.017.812 I print_info: n_rot            = 32
0.00.017.813 I print_info: n_swa            = 0
0.00.017.813 I print_info: n_embd_head_k    = 32
0.00.017.813 I print_info: n_embd_head_v    = 32
0.00.017.814 I print_info: n_gqa            = 1
0.00.017.814 I print_info: n_embd_k_gqa     = 384
0.00.017.815 I print_info: n_embd_v_gqa     = 384
0.00.017.815 I print_info: f_norm_eps       = 1.0e-12
0.00.017.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.817 I print_info: f_logit_scale    = 0.0e+00
0.00.017.817 I print_info: n_ff             = 1536
0.00.017.817 I print_info: n_expert         = 0
0.00.017.819 I print_info: n_expert_used    = 0
0.00.017.819 I print_info: causal attn      = 0
0.00.017.820 I print_info: pooling type     = 2
0.00.017.820 I print_info: rope type        = 2
0.00.017.820 I print_info: rope scaling     = linear
0.00.017.821 I print_info: freq_base_train  = 10000.0
0.00.017.822 I print_info: freq_scale_train = 1
0.00.017.822 I print_info: n_ctx_orig_yarn  = 512
0.00.017.823 I print_info: rope_finetuned   = unknown
0.00.017.823 I print_info: ssm_d_conv       = 0
0.00.017.823 I print_info: ssm_d_inner      = 0
0.00.017.823 I print_info: ssm_d_state      = 0
0.00.017.823 I print_info: ssm_dt_rank      = 0
0.00.017.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.824 I print_info: model type       = 33M
0.00.017.824 I print_info: model params     = 33.21 M
0.00.017.824 I print_info: general.name     = Bge Small
0.00.017.825 I print_info: vocab type       = WPM
0.00.017.825 I print_info: n_vocab          = 30522
0.00.017.825 I print_info: n_merges         = 0
0.00.017.826 I print_info: BOS token        = 101 '[CLS]'
0.00.017.826 I print_info: UNK token        = 100 '[UNK]'
0.00.017.826 I print_info: SEP token        = 102 '[SEP]'
0.00.017.826 I print_info: PAD token        = 0 '[PAD]'
0.00.017.827 I print_info: MASK token       = 103 '[MASK]'
0.00.017.827 I print_info: LF token         = 0 '[PAD]'
0.00.017.827 I print_info: max token length = 21
0.00.018.988 I load_tensors: offloading 12 repeating layers to GPU
0.00.018.988 I load_tensors: offloading output layer to GPU
0.00.018.990 I load_tensors: offloaded 13/13 layers to GPU
0.00.018.998 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.999 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.138 I llama_init_from_model: n_seq_max     = 1
0.00.019.139 I llama_init_from_model: n_ctx         = 512
0.00.019.140 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.140 I llama_init_from_model: n_batch       = 2048
0.00.019.140 I llama_init_from_model: n_ubatch      = 2048
0.00.019.140 I llama_init_from_model: flash_attn    = 0
0.00.019.140 I llama_init_from_model: freq_base     = 10000.0
0.00.019.141 I llama_init_from_model: freq_scale    = 1
0.00.019.141 I ggml_metal_init: allocating
0.00.019.144 I ggml_metal_init: found device: Apple M4
0.00.019.146 I ggml_metal_init: picking default device: Apple M4
0.00.019.755 I ggml_metal_init: using embedded metal library
0.00.022.096 I ggml_metal_init: GPU name:   Apple M4
0.00.022.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.099 I ggml_metal_init: simdgroup reduction   = true
0.00.022.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.099 I ggml_metal_init: has bfloat            = true
0.00.022.099 I ggml_metal_init: use bfloat            = true
0.00.022.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.506 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.031.990 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.992 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.994 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.032.598 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.032.599 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.032.599 I llama_init_from_model: graph nodes  = 429
0.00.032.599 I llama_init_from_model: graph splits = 2
0.00.032.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.192 I 
0.00.037.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.037.756 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.077 I llama_perf_context_print:        load time =      28.27 ms
0.00.042.078 I llama_perf_context_print: prompt eval time =       4.20 ms /     9 tokens (    0.47 ms per token,  2143.88 tokens per second)
0.00.042.079 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.079 I llama_perf_context_print:       total time =       4.89 ms /    10 tokens
0.00.042.288 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.028s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.183 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.350 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.530 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.537 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.539 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.540 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.540 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.542 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.543 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.547 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.548 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.548 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.551 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.552 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.552 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.021 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.022 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.022 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.022 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.023 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.023 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.023 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.024 I llama_model_loader: - type  f32:   40 tensors
0.00.049.024 I llama_model_loader: - type  f16:   30 tensors
0.00.049.031 I print_info: file format = GGUF V3 (latest)
0.00.049.032 I print_info: file type   = F16
0.00.049.033 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.065.164 W load: empty token at index 5
0.00.069.390 W load: model vocab missing newline token, using special_pad_id instead
0.00.070.661 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.691 I load: special tokens cache size = 5
0.00.332.838 I load: token to piece cache size = 1.5060 MB
0.00.332.843 I print_info: arch             = jina-bert-v2
0.00.332.844 I print_info: vocab_only       = 0
0.00.332.844 I print_info: n_ctx_train      = 8192
0.00.332.848 I print_info: n_embd           = 384
0.00.332.848 I print_info: n_layer          = 4
0.00.332.854 I print_info: n_head           = 12
0.00.332.855 I print_info: n_head_kv        = 12
0.00.332.855 I print_info: n_rot            = 32
0.00.332.858 I print_info: n_swa            = 0
0.00.332.858 I print_info: n_embd_head_k    = 32
0.00.332.858 I print_info: n_embd_head_v    = 32
0.00.332.858 I print_info: n_gqa            = 1
0.00.332.863 I print_info: n_embd_k_gqa     = 384
0.00.332.863 I print_info: n_embd_v_gqa     = 384
0.00.332.864 I print_info: f_norm_eps       = 1.0e-12
0.00.332.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.332.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.332.865 I print_info: f_max_alibi_bias = 8.0e+00
0.00.332.865 I print_info: f_logit_scale    = 0.0e+00
0.00.332.866 I print_info: n_ff             = 1536
0.00.332.866 I print_info: n_expert         = 0
0.00.332.867 I print_info: n_expert_used    = 0
0.00.332.867 I print_info: causal attn      = 0
0.00.332.867 I print_info: pooling type     = -1
0.00.332.867 I print_info: rope type        = -1
0.00.332.867 I print_info: rope scaling     = linear
0.00.332.868 I print_info: freq_base_train  = 10000.0
0.00.332.869 I print_info: freq_scale_train = 1
0.00.332.869 I print_info: n_ctx_orig_yarn  = 8192
0.00.332.869 I print_info: rope_finetuned   = unknown
0.00.332.870 I print_info: ssm_d_conv       = 0
0.00.332.870 I print_info: ssm_d_inner      = 0
0.00.332.870 I print_info: ssm_d_state      = 0
0.00.332.870 I print_info: ssm_dt_rank      = 0
0.00.332.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.332.870 I print_info: model type       = 33M
0.00.332.871 I print_info: model params     = 32.90 M
0.00.332.871 I print_info: general.name     = Jina Bert Implementation
0.00.332.872 I print_info: vocab type       = BPE
0.00.332.873 I print_info: n_vocab          = 61056
0.00.332.873 I print_info: n_merges         = 39382
0.00.332.873 I print_info: BOS token        = 0 '<s>'
0.00.332.873 I print_info: EOS token        = 2 '</s>'
0.00.332.873 I print_info: UNK token        = 3 '<unk>'
0.00.332.874 I print_info: SEP token        = 2 '</s>'
0.00.332.874 I print_info: PAD token        = 1 '<pad>'
0.00.332.874 I print_info: MASK token       = 4 '<mask>'
0.00.332.875 I print_info: EOG token        = 2 '</s>'
0.00.332.875 I print_info: max token length = 45
0.00.334.111 I load_tensors: offloading 4 repeating layers to GPU
0.00.334.111 I load_tensors: offloading output layer to GPU
0.00.334.112 I load_tensors: offloaded 5/5 layers to GPU
0.00.334.136 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.334.137 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.334.431 I llama_init_from_model: n_seq_max     = 1
0.00.334.432 I llama_init_from_model: n_ctx         = 8192
0.00.334.432 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.334.433 I llama_init_from_model: n_batch       = 2048
0.00.334.433 I llama_init_from_model: n_ubatch      = 2048
0.00.334.433 I llama_init_from_model: flash_attn    = 0
0.00.334.433 I llama_init_from_model: freq_base     = 10000.0
0.00.334.433 I llama_init_from_model: freq_scale    = 1
0.00.334.434 I ggml_metal_init: allocating
0.00.334.437 I ggml_metal_init: found device: Apple M4
0.00.334.439 I ggml_metal_init: picking default device: Apple M4
0.00.335.461 I ggml_metal_init: using embedded metal library
0.00.338.446 I ggml_metal_init: GPU name:   Apple M4
0.00.338.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.448 I ggml_metal_init: simdgroup reduction   = true
0.00.338.448 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.448 I ggml_metal_init: has bfloat            = true
0.00.338.448 I ggml_metal_init: use bfloat            = true
0.00.338.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.861 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.350.246 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.248 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.249 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.004 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.006 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.006 I llama_init_from_model: graph nodes  = 154
0.00.351.006 I llama_init_from_model: graph splits = 2
0.00.351.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.614 I 
0.00.365.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.802 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.803 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.806 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.806 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.811 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.811 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.346 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.804 I llama_perf_context_print:        load time =     342.26 ms
0.00.369.805 I llama_perf_context_print: prompt eval time =       3.45 ms /    62 tokens (    0.06 ms per token, 17976.22 tokens per second)
0.00.369.810 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.812 I llama_perf_context_print:       total time =       4.19 ms /    63 tokens
0.00.370.060 I ggml_metal_free: deallocating

real	0m1.097s
user	0m0.351s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.228 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.348 I main: llama backend init
0.00.000.356 I main: load the model and apply lora adapter, if any
0.00.045.364 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.058.207 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.058.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.058.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.058.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.058.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.058.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.058.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.058.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.058.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.058.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.058.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.058.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.058.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.058.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.058.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.058.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.058.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.325 I llama_model_loader: - type  f32:  194 tensors
0.00.077.326 I llama_model_loader: - type  f16:   98 tensors
0.00.077.336 I print_info: file format = GGUF V3 (latest)
0.00.077.338 I print_info: file type   = all F32 (guessed)
0.00.077.340 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.110.535 I load: special tokens cache size = 25
0.00.118.315 I load: token to piece cache size = 0.2984 MB
0.00.118.318 I print_info: arch             = gptneox
0.00.118.318 I print_info: vocab_only       = 0
0.00.118.319 I print_info: n_ctx_train      = 2048
0.00.118.319 I print_info: n_embd           = 2048
0.00.118.319 I print_info: n_layer          = 24
0.00.118.323 I print_info: n_head           = 16
0.00.118.324 I print_info: n_head_kv        = 16
0.00.118.324 I print_info: n_rot            = 32
0.00.118.324 I print_info: n_swa            = 0
0.00.118.324 I print_info: n_embd_head_k    = 128
0.00.118.327 I print_info: n_embd_head_v    = 128
0.00.118.327 I print_info: n_gqa            = 1
0.00.118.328 I print_info: n_embd_k_gqa     = 2048
0.00.118.329 I print_info: n_embd_v_gqa     = 2048
0.00.118.329 I print_info: f_norm_eps       = 1.0e-05
0.00.118.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.118.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.118.331 I print_info: f_max_alibi_bias = 0.0e+00
0.00.118.331 I print_info: f_logit_scale    = 0.0e+00
0.00.118.332 I print_info: n_ff             = 8192
0.00.118.333 I print_info: n_expert         = 0
0.00.118.333 I print_info: n_expert_used    = 0
0.00.118.333 I print_info: causal attn      = 1
0.00.118.333 I print_info: pooling type     = 0
0.00.118.333 I print_info: rope type        = 2
0.00.118.333 I print_info: rope scaling     = linear
0.00.118.335 I print_info: freq_base_train  = 10000.0
0.00.118.335 I print_info: freq_scale_train = 1
0.00.118.335 I print_info: n_ctx_orig_yarn  = 2048
0.00.118.336 I print_info: rope_finetuned   = unknown
0.00.118.336 I print_info: ssm_d_conv       = 0
0.00.118.336 I print_info: ssm_d_inner      = 0
0.00.118.336 I print_info: ssm_d_state      = 0
0.00.118.336 I print_info: ssm_dt_rank      = 0
0.00.118.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.118.336 I print_info: model type       = 1.4B
0.00.118.337 I print_info: model params     = 1.41 B
0.00.118.337 I print_info: general.name     = 1.4B
0.00.118.343 I print_info: vocab type       = BPE
0.00.118.345 I print_info: n_vocab          = 50304
0.00.118.346 I print_info: n_merges         = 50009
0.00.118.346 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.118.346 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.118.346 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.118.347 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.118.347 I print_info: LF token         = 128 'Ä'
0.00.118.347 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.118.347 I print_info: max token length = 1024
0.00.121.052 I load_tensors: offloading 24 repeating layers to GPU
0.00.121.052 I load_tensors: offloading output layer to GPU
0.00.121.052 I load_tensors: offloaded 25/25 layers to GPU
0.00.121.071 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.121.072 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.121.383 I llama_init_from_model: n_seq_max     = 1
0.00.121.384 I llama_init_from_model: n_ctx         = 2048
0.00.121.384 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.121.384 I llama_init_from_model: n_batch       = 2048
0.00.121.385 I llama_init_from_model: n_ubatch      = 512
0.00.121.385 I llama_init_from_model: flash_attn    = 0
0.00.121.385 I llama_init_from_model: freq_base     = 10000.0
0.00.121.386 I llama_init_from_model: freq_scale    = 1
0.00.121.386 I ggml_metal_init: allocating
0.00.121.389 I ggml_metal_init: found device: Apple M4
0.00.121.392 I ggml_metal_init: picking default device: Apple M4
0.00.122.109 I ggml_metal_init: using embedded metal library
0.00.156.072 I ggml_metal_init: GPU name:   Apple M4
0.00.156.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.075 I ggml_metal_init: simdgroup reduction   = true
0.00.156.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.076 I ggml_metal_init: has bfloat            = true
0.00.156.076 I ggml_metal_init: use bfloat            = true
0.00.156.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.181.591 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.205.667 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.205.672 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.205.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.206.704 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.206.706 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.206.706 I llama_init_from_model: graph nodes  = 967
0.00.206.707 I llama_init_from_model: graph splits = 2
0.00.206.709 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.206.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.206.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.284.511 I main: llama threadpool init, n_threads = 4
0.00.284.551 I 
0.00.284.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.284.589 I 
0.00.284.657 I sampler seed: 1234
0.00.284.662 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.284.688 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.284.690 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.284.690 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.111.799 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.02.111.800 I llama_perf_context_print:        load time =     239.13 ms
0.02.111.800 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.62 tokens per second)
0.02.111.801 I llama_perf_context_print:        eval time =    1780.71 ms /    63 runs   (   28.27 ms per token,    35.38 tokens per second)
0.02.111.801 I llama_perf_context_print:       total time =    1827.29 ms /    70 tokens
0.02.112.065 I ggml_metal_free: deallocating

real	0m2.420s
user	0m0.154s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.585 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.586 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.374 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.393 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.394 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.025 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.026 I llama_model_loader: - type  f32:  194 tensors
0.00.055.026 I llama_model_loader: - type  f16:   98 tensors
0.00.055.027 I print_info: file format = GGUF V3 (latest)
0.00.055.028 I print_info: file type   = all F32 (guessed)
0.00.055.029 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.420 I load: special tokens cache size = 25
0.00.090.521 I load: token to piece cache size = 0.2984 MB
0.00.090.525 I print_info: arch             = gptneox
0.00.090.525 I print_info: vocab_only       = 0
0.00.090.525 I print_info: n_ctx_train      = 2048
0.00.090.525 I print_info: n_embd           = 2048
0.00.090.525 I print_info: n_layer          = 24
0.00.090.529 I print_info: n_head           = 16
0.00.090.530 I print_info: n_head_kv        = 16
0.00.090.530 I print_info: n_rot            = 32
0.00.090.530 I print_info: n_swa            = 0
0.00.090.530 I print_info: n_embd_head_k    = 128
0.00.090.531 I print_info: n_embd_head_v    = 128
0.00.090.531 I print_info: n_gqa            = 1
0.00.090.532 I print_info: n_embd_k_gqa     = 2048
0.00.090.532 I print_info: n_embd_v_gqa     = 2048
0.00.090.533 I print_info: f_norm_eps       = 1.0e-05
0.00.090.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.540 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.541 I print_info: f_logit_scale    = 0.0e+00
0.00.090.541 I print_info: n_ff             = 8192
0.00.090.542 I print_info: n_expert         = 0
0.00.090.542 I print_info: n_expert_used    = 0
0.00.090.542 I print_info: causal attn      = 1
0.00.090.542 I print_info: pooling type     = 0
0.00.090.544 I print_info: rope type        = 2
0.00.090.544 I print_info: rope scaling     = linear
0.00.090.544 I print_info: freq_base_train  = 10000.0
0.00.090.545 I print_info: freq_scale_train = 1
0.00.090.545 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.545 I print_info: rope_finetuned   = unknown
0.00.090.546 I print_info: ssm_d_conv       = 0
0.00.090.546 I print_info: ssm_d_inner      = 0
0.00.090.546 I print_info: ssm_d_state      = 0
0.00.090.547 I print_info: ssm_dt_rank      = 0
0.00.090.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.547 I print_info: model type       = 1.4B
0.00.090.547 I print_info: model params     = 1.41 B
0.00.090.547 I print_info: general.name     = 1.4B
0.00.090.548 I print_info: vocab type       = BPE
0.00.090.548 I print_info: n_vocab          = 50304
0.00.090.548 I print_info: n_merges         = 50009
0.00.090.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.549 I print_info: LF token         = 128 'Ä'
0.00.090.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.550 I print_info: max token length = 1024
0.00.093.306 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.307 I load_tensors: offloading output layer to GPU
0.00.093.307 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.318 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.319 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.571 I llama_init_from_model: n_seq_max     = 1
0.00.093.572 I llama_init_from_model: n_ctx         = 128
0.00.093.572 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.572 I llama_init_from_model: n_batch       = 128
0.00.093.572 I llama_init_from_model: n_ubatch      = 128
0.00.093.572 I llama_init_from_model: flash_attn    = 0
0.00.093.573 I llama_init_from_model: freq_base     = 10000.0
0.00.093.573 I llama_init_from_model: freq_scale    = 1
0.00.093.573 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.574 I ggml_metal_init: allocating
0.00.093.577 I ggml_metal_init: found device: Apple M4
0.00.093.578 I ggml_metal_init: picking default device: Apple M4
0.00.094.246 I ggml_metal_init: using embedded metal library
0.00.096.973 I ggml_metal_init: GPU name:   Apple M4
0.00.096.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.976 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.976 I ggml_metal_init: simdgroup reduction   = true
0.00.096.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.976 I ggml_metal_init: has bfloat            = true
0.00.096.976 I ggml_metal_init: use bfloat            = true
0.00.096.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.986 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.230 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.246 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.083 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.084 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.085 I llama_init_from_model: graph nodes  = 967
0.00.108.085 I llama_init_from_model: graph splits = 2
0.00.108.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.056.508 I 
0.01.056.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.056.544 I perplexity: tokenizing the input ..
0.01.070.024 I perplexity: tokenization took 13.476 ms
0.01.070.030 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.192.302 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.194.176 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.194.230 I llama_perf_context_print:        load time =    1032.91 ms
0.01.194.232 I llama_perf_context_print: prompt eval time =     121.38 ms /   128 tokens (    0.95 ms per token,  1054.56 tokens per second)
0.01.194.233 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.194.236 I llama_perf_context_print:       total time =     137.72 ms /   129 tokens
0.01.195.039 I ggml_metal_free: deallocating

real	0m1.382s
user	0m0.124s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.349 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.349 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.350 I llama_model_loader: - type  f32:  194 tensors
0.00.031.351 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.352 I print_info: file format = GGUF V3 (latest)
0.00.031.352 I print_info: file type   = Q8_0
0.00.031.354 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.050.751 I load: special tokens cache size = 25
0.00.056.840 I load: token to piece cache size = 0.2984 MB
0.00.056.845 I print_info: arch             = gptneox
0.00.056.845 I print_info: vocab_only       = 0
0.00.056.847 I print_info: n_ctx_train      = 2048
0.00.056.847 I print_info: n_embd           = 2048
0.00.056.847 I print_info: n_layer          = 24
0.00.056.853 I print_info: n_head           = 16
0.00.056.854 I print_info: n_head_kv        = 16
0.00.056.854 I print_info: n_rot            = 32
0.00.056.854 I print_info: n_swa            = 0
0.00.056.854 I print_info: n_embd_head_k    = 128
0.00.056.855 I print_info: n_embd_head_v    = 128
0.00.056.855 I print_info: n_gqa            = 1
0.00.056.856 I print_info: n_embd_k_gqa     = 2048
0.00.056.857 I print_info: n_embd_v_gqa     = 2048
0.00.056.857 I print_info: f_norm_eps       = 1.0e-05
0.00.056.858 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.858 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.859 I print_info: f_logit_scale    = 0.0e+00
0.00.056.860 I print_info: n_ff             = 8192
0.00.056.860 I print_info: n_expert         = 0
0.00.056.860 I print_info: n_expert_used    = 0
0.00.056.861 I print_info: causal attn      = 1
0.00.056.861 I print_info: pooling type     = 0
0.00.056.861 I print_info: rope type        = 2
0.00.056.862 I print_info: rope scaling     = linear
0.00.056.864 I print_info: freq_base_train  = 10000.0
0.00.056.865 I print_info: freq_scale_train = 1
0.00.056.865 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.865 I print_info: rope_finetuned   = unknown
0.00.056.865 I print_info: ssm_d_conv       = 0
0.00.056.865 I print_info: ssm_d_inner      = 0
0.00.056.866 I print_info: ssm_d_state      = 0
0.00.056.866 I print_info: ssm_dt_rank      = 0
0.00.056.866 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.866 I print_info: model type       = 1.4B
0.00.056.866 I print_info: model params     = 1.41 B
0.00.056.868 I print_info: general.name     = 1.4B
0.00.056.868 I print_info: vocab type       = BPE
0.00.056.869 I print_info: n_vocab          = 50304
0.00.056.869 I print_info: n_merges         = 50009
0.00.056.869 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.869 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.869 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.869 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.870 I print_info: LF token         = 128 'Ä'
0.00.056.870 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.870 I print_info: max token length = 1024
0.00.059.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.258 I load_tensors: offloading output layer to GPU
0.00.059.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.269 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.271 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.059.594 I llama_init_from_model: n_seq_max     = 1
0.00.059.595 I llama_init_from_model: n_ctx         = 2048
0.00.059.595 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.059.596 I llama_init_from_model: n_batch       = 2048
0.00.059.596 I llama_init_from_model: n_ubatch      = 512
0.00.059.596 I llama_init_from_model: flash_attn    = 0
0.00.059.596 I llama_init_from_model: freq_base     = 10000.0
0.00.059.597 I llama_init_from_model: freq_scale    = 1
0.00.059.597 I ggml_metal_init: allocating
0.00.059.602 I ggml_metal_init: found device: Apple M4
0.00.059.605 I ggml_metal_init: picking default device: Apple M4
0.00.060.332 I ggml_metal_init: using embedded metal library
0.00.062.907 I ggml_metal_init: GPU name:   Apple M4
0.00.062.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.909 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.910 I ggml_metal_init: simdgroup reduction   = true
0.00.062.910 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.910 I ggml_metal_init: has bfloat            = true
0.00.062.910 I ggml_metal_init: use bfloat            = true
0.00.062.911 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.912 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.247 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.318 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.326 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.353 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.610 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.613 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.613 I llama_init_from_model: graph nodes  = 967
0.00.098.613 I llama_init_from_model: graph splits = 2
0.00.098.617 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.856.021 I main: llama threadpool init, n_threads = 4
0.01.856.085 I 
0.01.856.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.856.131 I 
0.01.856.422 I sampler seed: 1234
0.01.856.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.856.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.856.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.856.439 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.966.036 I llama_perf_sampler_print:    sampling time =       1.64 ms /    71 runs   (    0.02 ms per token, 43266.30 tokens per second)
0.02.966.037 I llama_perf_context_print:        load time =    1846.13 ms
0.02.966.038 I llama_perf_context_print: prompt eval time =      51.09 ms /     7 tokens (    7.30 ms per token,   137.03 tokens per second)
0.02.966.039 I llama_perf_context_print:        eval time =    1055.50 ms /    63 runs   (   16.75 ms per token,    59.69 tokens per second)
0.02.966.039 I llama_perf_context_print:       total time =    1110.02 ms /    70 tokens
0.02.966.276 I ggml_metal_free: deallocating

real	0m2.984s
user	0m0.125s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.646 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.929 I llama_model_loader: - type  f32:  194 tensors
0.00.033.930 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.932 I print_info: file format = GGUF V3 (latest)
0.00.033.933 I print_info: file type   = Q8_0
0.00.033.935 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.208 I load: special tokens cache size = 25
0.00.064.527 I load: token to piece cache size = 0.2984 MB
0.00.064.530 I print_info: arch             = gptneox
0.00.064.531 I print_info: vocab_only       = 0
0.00.064.531 I print_info: n_ctx_train      = 2048
0.00.064.531 I print_info: n_embd           = 2048
0.00.064.531 I print_info: n_layer          = 24
0.00.064.536 I print_info: n_head           = 16
0.00.064.537 I print_info: n_head_kv        = 16
0.00.064.537 I print_info: n_rot            = 32
0.00.064.537 I print_info: n_swa            = 0
0.00.064.537 I print_info: n_embd_head_k    = 128
0.00.064.537 I print_info: n_embd_head_v    = 128
0.00.064.538 I print_info: n_gqa            = 1
0.00.064.539 I print_info: n_embd_k_gqa     = 2048
0.00.064.539 I print_info: n_embd_v_gqa     = 2048
0.00.064.540 I print_info: f_norm_eps       = 1.0e-05
0.00.064.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.541 I print_info: f_logit_scale    = 0.0e+00
0.00.064.542 I print_info: n_ff             = 8192
0.00.064.542 I print_info: n_expert         = 0
0.00.064.543 I print_info: n_expert_used    = 0
0.00.064.543 I print_info: causal attn      = 1
0.00.064.543 I print_info: pooling type     = 0
0.00.064.544 I print_info: rope type        = 2
0.00.064.544 I print_info: rope scaling     = linear
0.00.064.545 I print_info: freq_base_train  = 10000.0
0.00.064.545 I print_info: freq_scale_train = 1
0.00.064.545 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.545 I print_info: rope_finetuned   = unknown
0.00.064.546 I print_info: ssm_d_conv       = 0
0.00.064.546 I print_info: ssm_d_inner      = 0
0.00.064.546 I print_info: ssm_d_state      = 0
0.00.064.546 I print_info: ssm_dt_rank      = 0
0.00.064.548 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.548 I print_info: model type       = 1.4B
0.00.064.549 I print_info: model params     = 1.41 B
0.00.064.549 I print_info: general.name     = 1.4B
0.00.064.550 I print_info: vocab type       = BPE
0.00.064.550 I print_info: n_vocab          = 50304
0.00.064.550 I print_info: n_merges         = 50009
0.00.064.550 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.550 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.550 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.552 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.552 I print_info: LF token         = 128 'Ä'
0.00.064.553 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.553 I print_info: max token length = 1024
0.00.066.976 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.976 I load_tensors: offloading output layer to GPU
0.00.066.976 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.988 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.989 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.332 I llama_init_from_model: n_seq_max     = 1
0.00.067.333 I llama_init_from_model: n_ctx         = 128
0.00.067.333 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.333 I llama_init_from_model: n_batch       = 128
0.00.067.333 I llama_init_from_model: n_ubatch      = 128
0.00.067.334 I llama_init_from_model: flash_attn    = 0
0.00.067.334 I llama_init_from_model: freq_base     = 10000.0
0.00.067.334 I llama_init_from_model: freq_scale    = 1
0.00.067.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.335 I ggml_metal_init: allocating
0.00.067.338 I ggml_metal_init: found device: Apple M4
0.00.067.340 I ggml_metal_init: picking default device: Apple M4
0.00.068.055 I ggml_metal_init: using embedded metal library
0.00.070.751 I ggml_metal_init: GPU name:   Apple M4
0.00.070.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.754 I ggml_metal_init: simdgroup reduction   = true
0.00.070.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.754 I ggml_metal_init: has bfloat            = true
0.00.070.754 I ggml_metal_init: use bfloat            = true
0.00.070.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.070 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.512 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.518 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.676 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.677 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.677 I llama_init_from_model: graph nodes  = 967
0.00.083.678 I llama_init_from_model: graph splits = 2
0.00.083.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.950.880 I 
0.00.950.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.950.919 I perplexity: tokenizing the input ..
0.00.958.307 I perplexity: tokenization took 7.386 ms
0.00.958.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.081.651 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.083.095 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.083.117 I llama_perf_context_print:        load time =     939.19 ms
0.01.083.119 I llama_perf_context_print: prompt eval time =     123.11 ms /   128 tokens (    0.96 ms per token,  1039.73 tokens per second)
0.01.083.120 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.083.120 I llama_perf_context_print:       total time =     132.24 ms /   129 tokens
0.01.083.428 I ggml_metal_free: deallocating

real	0m1.100s
user	0m0.093s
sys	0m0.155s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.013.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.528 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.531 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.531 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.532 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.842 I llama_model_loader: - type  f32:  194 tensors
0.00.041.842 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.842 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.843 I print_info: file format = GGUF V3 (latest)
0.00.041.843 I print_info: file type   = Q4_0
0.00.041.844 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.065.496 I load: special tokens cache size = 25
0.00.073.419 I load: token to piece cache size = 0.2984 MB
0.00.073.422 I print_info: arch             = gptneox
0.00.073.423 I print_info: vocab_only       = 0
0.00.073.423 I print_info: n_ctx_train      = 2048
0.00.073.423 I print_info: n_embd           = 2048
0.00.073.423 I print_info: n_layer          = 24
0.00.073.427 I print_info: n_head           = 16
0.00.073.428 I print_info: n_head_kv        = 16
0.00.073.428 I print_info: n_rot            = 32
0.00.073.428 I print_info: n_swa            = 0
0.00.073.428 I print_info: n_embd_head_k    = 128
0.00.073.431 I print_info: n_embd_head_v    = 128
0.00.073.432 I print_info: n_gqa            = 1
0.00.073.433 I print_info: n_embd_k_gqa     = 2048
0.00.073.433 I print_info: n_embd_v_gqa     = 2048
0.00.073.434 I print_info: f_norm_eps       = 1.0e-05
0.00.073.435 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.435 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.435 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.435 I print_info: f_logit_scale    = 0.0e+00
0.00.073.436 I print_info: n_ff             = 8192
0.00.073.436 I print_info: n_expert         = 0
0.00.073.436 I print_info: n_expert_used    = 0
0.00.073.436 I print_info: causal attn      = 1
0.00.073.436 I print_info: pooling type     = 0
0.00.073.437 I print_info: rope type        = 2
0.00.073.437 I print_info: rope scaling     = linear
0.00.073.437 I print_info: freq_base_train  = 10000.0
0.00.073.438 I print_info: freq_scale_train = 1
0.00.073.438 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.438 I print_info: rope_finetuned   = unknown
0.00.073.438 I print_info: ssm_d_conv       = 0
0.00.073.439 I print_info: ssm_d_inner      = 0
0.00.073.439 I print_info: ssm_d_state      = 0
0.00.073.439 I print_info: ssm_dt_rank      = 0
0.00.073.439 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.439 I print_info: model type       = 1.4B
0.00.073.440 I print_info: model params     = 1.41 B
0.00.073.440 I print_info: general.name     = 1.4B
0.00.073.440 I print_info: vocab type       = BPE
0.00.073.441 I print_info: n_vocab          = 50304
0.00.073.441 I print_info: n_merges         = 50009
0.00.073.441 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.441 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.443 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.443 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.443 I print_info: LF token         = 128 'Ä'
0.00.073.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.444 I print_info: max token length = 1024
0.00.075.652 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.652 I load_tensors: offloading output layer to GPU
0.00.075.653 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.659 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.075.660 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.075.967 I llama_init_from_model: n_seq_max     = 1
0.00.075.969 I llama_init_from_model: n_ctx         = 2048
0.00.075.969 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.075.969 I llama_init_from_model: n_batch       = 2048
0.00.075.969 I llama_init_from_model: n_ubatch      = 512
0.00.075.970 I llama_init_from_model: flash_attn    = 0
0.00.075.970 I llama_init_from_model: freq_base     = 10000.0
0.00.075.971 I llama_init_from_model: freq_scale    = 1
0.00.075.971 I ggml_metal_init: allocating
0.00.075.975 I ggml_metal_init: found device: Apple M4
0.00.075.977 I ggml_metal_init: picking default device: Apple M4
0.00.076.798 I ggml_metal_init: using embedded metal library
0.00.080.478 I ggml_metal_init: GPU name:   Apple M4
0.00.080.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.481 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.482 I ggml_metal_init: simdgroup reduction   = true
0.00.080.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.482 I ggml_metal_init: has bfloat            = true
0.00.080.482 I ggml_metal_init: use bfloat            = true
0.00.080.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.119 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.736 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.745 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.121.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.121.852 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.121.852 I llama_init_from_model: graph nodes  = 967
0.00.121.853 I llama_init_from_model: graph splits = 2
0.00.121.856 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.794 I main: llama threadpool init, n_threads = 4
0.00.797.837 I 
0.00.797.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.875 I 
0.00.798.109 I sampler seed: 1234
0.00.798.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.172 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.176 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.476.372 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.476.372 I llama_perf_context_print:        load time =     784.02 ms
0.01.476.374 I llama_perf_context_print: prompt eval time =      44.74 ms /     7 tokens (    6.39 ms per token,   156.44 tokens per second)
0.01.476.374 I llama_perf_context_print:        eval time =     630.44 ms /    63 runs   (   10.01 ms per token,    99.93 tokens per second)
0.01.476.375 I llama_perf_context_print:       total time =     678.58 ms /    70 tokens
0.01.476.640 I ggml_metal_free: deallocating

real	0m1.494s
user	0m0.124s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.532 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.449 I llama_model_loader: - type  f32:  194 tensors
0.00.025.449 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.450 I print_info: file format = GGUF V3 (latest)
0.00.025.450 I print_info: file type   = Q4_0
0.00.025.452 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.401 I load: special tokens cache size = 25
0.00.050.239 I load: token to piece cache size = 0.2984 MB
0.00.050.242 I print_info: arch             = gptneox
0.00.050.242 I print_info: vocab_only       = 0
0.00.050.242 I print_info: n_ctx_train      = 2048
0.00.050.242 I print_info: n_embd           = 2048
0.00.050.243 I print_info: n_layer          = 24
0.00.050.247 I print_info: n_head           = 16
0.00.050.247 I print_info: n_head_kv        = 16
0.00.050.248 I print_info: n_rot            = 32
0.00.050.248 I print_info: n_swa            = 0
0.00.050.248 I print_info: n_embd_head_k    = 128
0.00.050.248 I print_info: n_embd_head_v    = 128
0.00.050.249 I print_info: n_gqa            = 1
0.00.050.249 I print_info: n_embd_k_gqa     = 2048
0.00.050.250 I print_info: n_embd_v_gqa     = 2048
0.00.050.251 I print_info: f_norm_eps       = 1.0e-05
0.00.050.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.253 I print_info: f_logit_scale    = 0.0e+00
0.00.050.253 I print_info: n_ff             = 8192
0.00.050.254 I print_info: n_expert         = 0
0.00.050.254 I print_info: n_expert_used    = 0
0.00.050.254 I print_info: causal attn      = 1
0.00.050.254 I print_info: pooling type     = 0
0.00.050.254 I print_info: rope type        = 2
0.00.050.254 I print_info: rope scaling     = linear
0.00.050.255 I print_info: freq_base_train  = 10000.0
0.00.050.255 I print_info: freq_scale_train = 1
0.00.050.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.255 I print_info: rope_finetuned   = unknown
0.00.050.257 I print_info: ssm_d_conv       = 0
0.00.050.257 I print_info: ssm_d_inner      = 0
0.00.050.257 I print_info: ssm_d_state      = 0
0.00.050.257 I print_info: ssm_dt_rank      = 0
0.00.050.257 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.257 I print_info: model type       = 1.4B
0.00.050.258 I print_info: model params     = 1.41 B
0.00.050.258 I print_info: general.name     = 1.4B
0.00.050.258 I print_info: vocab type       = BPE
0.00.050.259 I print_info: n_vocab          = 50304
0.00.050.259 I print_info: n_merges         = 50009
0.00.050.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: LF token         = 128 'Ä'
0.00.050.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.264 I print_info: max token length = 1024
0.00.052.349 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.350 I load_tensors: offloading output layer to GPU
0.00.052.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.361 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.361 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.643 I llama_init_from_model: n_seq_max     = 1
0.00.052.645 I llama_init_from_model: n_ctx         = 128
0.00.052.645 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.645 I llama_init_from_model: n_batch       = 128
0.00.052.645 I llama_init_from_model: n_ubatch      = 128
0.00.052.646 I llama_init_from_model: flash_attn    = 0
0.00.052.646 I llama_init_from_model: freq_base     = 10000.0
0.00.052.646 I llama_init_from_model: freq_scale    = 1
0.00.052.646 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.647 I ggml_metal_init: allocating
0.00.052.650 I ggml_metal_init: found device: Apple M4
0.00.052.651 I ggml_metal_init: picking default device: Apple M4
0.00.053.370 I ggml_metal_init: using embedded metal library
0.00.055.870 I ggml_metal_init: GPU name:   Apple M4
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.872 I ggml_metal_init: simdgroup reduction   = true
0.00.055.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.873 I ggml_metal_init: has bfloat            = true
0.00.055.873 I ggml_metal_init: use bfloat            = true
0.00.055.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.386 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.611 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.616 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.633 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.507 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.508 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.509 I llama_init_from_model: graph nodes  = 967
0.00.068.509 I llama_init_from_model: graph splits = 2
0.00.068.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.631 I 
0.00.629.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.672 I perplexity: tokenizing the input ..
0.00.637.144 I perplexity: tokenization took 7.471 ms
0.00.637.150 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.089 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.760.565 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.760.588 I llama_perf_context_print:        load time =     620.10 ms
0.00.760.588 I llama_perf_context_print: prompt eval time =     121.70 ms /   128 tokens (    0.95 ms per token,  1051.74 tokens per second)
0.00.760.592 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.592 I llama_perf_context_print:       total time =     130.96 ms /   129 tokens
0.00.760.952 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.078s
sys	0m0.079s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.143 I llama_model_loader: - type  f32:  194 tensors
0.00.037.143 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.143 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.144 I print_info: file format = GGUF V3 (latest)
0.00.037.144 I print_info: file type   = Q4_1
0.00.037.145 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.059.224 I load: special tokens cache size = 25
0.00.065.365 I load: token to piece cache size = 0.2984 MB
0.00.065.368 I print_info: arch             = gptneox
0.00.065.368 I print_info: vocab_only       = 0
0.00.065.369 I print_info: n_ctx_train      = 2048
0.00.065.369 I print_info: n_embd           = 2048
0.00.065.369 I print_info: n_layer          = 24
0.00.065.371 I print_info: n_head           = 16
0.00.065.372 I print_info: n_head_kv        = 16
0.00.065.373 I print_info: n_rot            = 32
0.00.065.373 I print_info: n_swa            = 0
0.00.065.373 I print_info: n_embd_head_k    = 128
0.00.065.373 I print_info: n_embd_head_v    = 128
0.00.065.374 I print_info: n_gqa            = 1
0.00.065.374 I print_info: n_embd_k_gqa     = 2048
0.00.065.375 I print_info: n_embd_v_gqa     = 2048
0.00.065.376 I print_info: f_norm_eps       = 1.0e-05
0.00.065.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.376 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.377 I print_info: f_logit_scale    = 0.0e+00
0.00.065.377 I print_info: n_ff             = 8192
0.00.065.377 I print_info: n_expert         = 0
0.00.065.378 I print_info: n_expert_used    = 0
0.00.065.378 I print_info: causal attn      = 1
0.00.065.378 I print_info: pooling type     = 0
0.00.065.378 I print_info: rope type        = 2
0.00.065.378 I print_info: rope scaling     = linear
0.00.065.379 I print_info: freq_base_train  = 10000.0
0.00.065.379 I print_info: freq_scale_train = 1
0.00.065.379 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.379 I print_info: rope_finetuned   = unknown
0.00.065.380 I print_info: ssm_d_conv       = 0
0.00.065.380 I print_info: ssm_d_inner      = 0
0.00.065.382 I print_info: ssm_d_state      = 0
0.00.065.382 I print_info: ssm_dt_rank      = 0
0.00.065.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.382 I print_info: model type       = 1.4B
0.00.065.383 I print_info: model params     = 1.41 B
0.00.065.383 I print_info: general.name     = 1.4B
0.00.065.383 I print_info: vocab type       = BPE
0.00.065.384 I print_info: n_vocab          = 50304
0.00.065.384 I print_info: n_merges         = 50009
0.00.065.384 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.385 I print_info: LF token         = 128 'Ä'
0.00.065.385 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.385 I print_info: max token length = 1024
0.00.067.436 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.437 I load_tensors: offloading output layer to GPU
0.00.067.437 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.447 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.067.449 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.067.755 I llama_init_from_model: n_seq_max     = 1
0.00.067.756 I llama_init_from_model: n_ctx         = 2048
0.00.067.756 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.756 I llama_init_from_model: n_batch       = 2048
0.00.067.756 I llama_init_from_model: n_ubatch      = 512
0.00.067.756 I llama_init_from_model: flash_attn    = 0
0.00.067.757 I llama_init_from_model: freq_base     = 10000.0
0.00.067.757 I llama_init_from_model: freq_scale    = 1
0.00.067.757 I ggml_metal_init: allocating
0.00.067.760 I ggml_metal_init: found device: Apple M4
0.00.067.762 I ggml_metal_init: picking default device: Apple M4
0.00.068.369 I ggml_metal_init: using embedded metal library
0.00.070.923 I ggml_metal_init: GPU name:   Apple M4
0.00.070.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.926 I ggml_metal_init: simdgroup reduction   = true
0.00.070.926 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.926 I ggml_metal_init: has bfloat            = true
0.00.070.926 I ggml_metal_init: use bfloat            = true
0.00.070.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.615 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.213 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.219 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.238 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.375 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.377 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.377 I llama_init_from_model: graph nodes  = 967
0.00.104.378 I llama_init_from_model: graph splits = 2
0.00.104.382 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.431 I main: llama threadpool init, n_threads = 4
0.00.841.473 I 
0.00.841.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.508 I 
0.00.841.726 I sampler seed: 1234
0.00.841.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.841.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.841.772 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.841.772 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.562.237 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66231.34 tokens per second)
0.01.562.237 I llama_perf_context_print:        load time =     832.37 ms
0.01.562.238 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.73 tokens per second)
0.01.562.239 I llama_perf_context_print:        eval time =     678.05 ms /    63 runs   (   10.76 ms per token,    92.91 tokens per second)
0.01.562.239 I llama_perf_context_print:       total time =     720.81 ms /    70 tokens
0.01.562.474 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.113s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.638 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.640 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.641 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.642 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.643 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.644 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.644 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.644 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.645 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.648 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.648 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.563 I llama_model_loader: - type  f32:  194 tensors
0.00.024.563 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.564 I print_info: file format = GGUF V3 (latest)
0.00.024.567 I print_info: file type   = Q4_1
0.00.024.568 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.316 I load: special tokens cache size = 25
0.00.051.661 I load: token to piece cache size = 0.2984 MB
0.00.051.666 I print_info: arch             = gptneox
0.00.051.666 I print_info: vocab_only       = 0
0.00.051.666 I print_info: n_ctx_train      = 2048
0.00.051.667 I print_info: n_embd           = 2048
0.00.051.667 I print_info: n_layer          = 24
0.00.051.671 I print_info: n_head           = 16
0.00.051.671 I print_info: n_head_kv        = 16
0.00.051.673 I print_info: n_rot            = 32
0.00.051.673 I print_info: n_swa            = 0
0.00.051.673 I print_info: n_embd_head_k    = 128
0.00.051.674 I print_info: n_embd_head_v    = 128
0.00.051.674 I print_info: n_gqa            = 1
0.00.051.675 I print_info: n_embd_k_gqa     = 2048
0.00.051.676 I print_info: n_embd_v_gqa     = 2048
0.00.051.676 I print_info: f_norm_eps       = 1.0e-05
0.00.051.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.676 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.677 I print_info: f_logit_scale    = 0.0e+00
0.00.051.679 I print_info: n_ff             = 8192
0.00.051.680 I print_info: n_expert         = 0
0.00.051.681 I print_info: n_expert_used    = 0
0.00.051.681 I print_info: causal attn      = 1
0.00.051.681 I print_info: pooling type     = 0
0.00.051.681 I print_info: rope type        = 2
0.00.051.682 I print_info: rope scaling     = linear
0.00.051.682 I print_info: freq_base_train  = 10000.0
0.00.051.682 I print_info: freq_scale_train = 1
0.00.051.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.705 I print_info: rope_finetuned   = unknown
0.00.051.707 I print_info: ssm_d_conv       = 0
0.00.051.707 I print_info: ssm_d_inner      = 0
0.00.051.707 I print_info: ssm_d_state      = 0
0.00.051.707 I print_info: ssm_dt_rank      = 0
0.00.051.707 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.708 I print_info: model type       = 1.4B
0.00.051.709 I print_info: model params     = 1.41 B
0.00.051.709 I print_info: general.name     = 1.4B
0.00.051.710 I print_info: vocab type       = BPE
0.00.051.710 I print_info: n_vocab          = 50304
0.00.051.710 I print_info: n_merges         = 50009
0.00.051.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.711 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.711 I print_info: LF token         = 128 'Ä'
0.00.051.714 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.714 I print_info: max token length = 1024
0.00.053.638 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.638 I load_tensors: offloading output layer to GPU
0.00.053.638 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.649 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.651 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.938 I llama_init_from_model: n_seq_max     = 1
0.00.053.939 I llama_init_from_model: n_ctx         = 128
0.00.053.939 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.939 I llama_init_from_model: n_batch       = 128
0.00.053.939 I llama_init_from_model: n_ubatch      = 128
0.00.053.939 I llama_init_from_model: flash_attn    = 0
0.00.053.940 I llama_init_from_model: freq_base     = 10000.0
0.00.053.941 I llama_init_from_model: freq_scale    = 1
0.00.053.941 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.942 I ggml_metal_init: allocating
0.00.053.945 I ggml_metal_init: found device: Apple M4
0.00.053.946 I ggml_metal_init: picking default device: Apple M4
0.00.054.583 I ggml_metal_init: using embedded metal library
0.00.057.047 I ggml_metal_init: GPU name:   Apple M4
0.00.057.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.050 I ggml_metal_init: simdgroup reduction   = true
0.00.057.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.050 I ggml_metal_init: has bfloat            = true
0.00.057.051 I ggml_metal_init: use bfloat            = true
0.00.057.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.052 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.225 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.439 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.303 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.304 I llama_init_from_model: graph nodes  = 967
0.00.069.304 I llama_init_from_model: graph splits = 2
0.00.069.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.714 I 
0.00.688.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.744 I perplexity: tokenizing the input ..
0.00.696.373 I perplexity: tokenization took 7.627 ms
0.00.696.377 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.073 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.227 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.820.253 I llama_perf_context_print:        load time =     679.92 ms
0.00.820.254 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.29 tokens per second)
0.00.820.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.256 I llama_perf_context_print:       total time =     131.54 ms /   129 tokens
0.00.820.745 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.103s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.012.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.143 I llama_model_loader: - type  f32:  194 tensors
0.00.029.144 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.144 I print_info: file format = GGUF V3 (latest)
0.00.029.145 I print_info: file type   = Q5_0
0.00.029.146 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.874 I load: special tokens cache size = 25
0.00.053.745 I load: token to piece cache size = 0.2984 MB
0.00.053.748 I print_info: arch             = gptneox
0.00.053.749 I print_info: vocab_only       = 0
0.00.053.749 I print_info: n_ctx_train      = 2048
0.00.053.749 I print_info: n_embd           = 2048
0.00.053.749 I print_info: n_layer          = 24
0.00.053.752 I print_info: n_head           = 16
0.00.053.753 I print_info: n_head_kv        = 16
0.00.053.753 I print_info: n_rot            = 32
0.00.053.753 I print_info: n_swa            = 0
0.00.053.754 I print_info: n_embd_head_k    = 128
0.00.053.754 I print_info: n_embd_head_v    = 128
0.00.053.755 I print_info: n_gqa            = 1
0.00.053.755 I print_info: n_embd_k_gqa     = 2048
0.00.053.756 I print_info: n_embd_v_gqa     = 2048
0.00.053.757 I print_info: f_norm_eps       = 1.0e-05
0.00.053.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.757 I print_info: f_logit_scale    = 0.0e+00
0.00.053.758 I print_info: n_ff             = 8192
0.00.053.758 I print_info: n_expert         = 0
0.00.053.758 I print_info: n_expert_used    = 0
0.00.053.758 I print_info: causal attn      = 1
0.00.053.758 I print_info: pooling type     = 0
0.00.053.760 I print_info: rope type        = 2
0.00.053.762 I print_info: rope scaling     = linear
0.00.053.762 I print_info: freq_base_train  = 10000.0
0.00.053.763 I print_info: freq_scale_train = 1
0.00.053.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.763 I print_info: rope_finetuned   = unknown
0.00.053.765 I print_info: ssm_d_conv       = 0
0.00.053.765 I print_info: ssm_d_inner      = 0
0.00.053.765 I print_info: ssm_d_state      = 0
0.00.053.765 I print_info: ssm_dt_rank      = 0
0.00.053.765 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.765 I print_info: model type       = 1.4B
0.00.053.766 I print_info: model params     = 1.41 B
0.00.053.766 I print_info: general.name     = 1.4B
0.00.053.766 I print_info: vocab type       = BPE
0.00.053.766 I print_info: n_vocab          = 50304
0.00.053.767 I print_info: n_merges         = 50009
0.00.053.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: LF token         = 128 'Ä'
0.00.053.773 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.773 I print_info: max token length = 1024
0.00.055.758 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.758 I load_tensors: offloading output layer to GPU
0.00.055.759 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.769 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.770 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.049 I llama_init_from_model: n_seq_max     = 1
0.00.056.050 I llama_init_from_model: n_ctx         = 2048
0.00.056.050 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.050 I llama_init_from_model: n_batch       = 2048
0.00.056.050 I llama_init_from_model: n_ubatch      = 512
0.00.056.050 I llama_init_from_model: flash_attn    = 0
0.00.056.051 I llama_init_from_model: freq_base     = 10000.0
0.00.056.051 I llama_init_from_model: freq_scale    = 1
0.00.056.051 I ggml_metal_init: allocating
0.00.056.054 I ggml_metal_init: found device: Apple M4
0.00.056.056 I ggml_metal_init: picking default device: Apple M4
0.00.056.659 I ggml_metal_init: using embedded metal library
0.00.059.011 I ggml_metal_init: GPU name:   Apple M4
0.00.059.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.013 I ggml_metal_init: simdgroup reduction   = true
0.00.059.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.013 I ggml_metal_init: has bfloat            = true
0.00.059.014 I ggml_metal_init: use bfloat            = true
0.00.059.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.650 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.538 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.704 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.705 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.705 I llama_init_from_model: graph nodes  = 967
0.00.089.706 I llama_init_from_model: graph splits = 2
0.00.089.709 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.855 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.275 I main: llama threadpool init, n_threads = 4
0.00.779.326 I 
0.00.779.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.361 I 
0.00.779.593 I sampler seed: 1234
0.00.779.597 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.649 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.649 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.572.426 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.572.426 I llama_perf_context_print:        load time =     766.53 ms
0.01.572.427 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.76 tokens per second)
0.01.572.428 I llama_perf_context_print:        eval time =     742.69 ms /    63 runs   (   11.79 ms per token,    84.83 tokens per second)
0.01.572.428 I llama_perf_context_print:       total time =     793.15 ms /    70 tokens
0.01.572.637 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.822 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.823 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.824 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.825 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.537 I llama_model_loader: - type  f32:  194 tensors
0.00.026.538 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.538 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.539 I print_info: file format = GGUF V3 (latest)
0.00.026.539 I print_info: file type   = Q5_0
0.00.026.540 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.225 I load: special tokens cache size = 25
0.00.051.217 I load: token to piece cache size = 0.2984 MB
0.00.051.221 I print_info: arch             = gptneox
0.00.051.221 I print_info: vocab_only       = 0
0.00.051.221 I print_info: n_ctx_train      = 2048
0.00.051.221 I print_info: n_embd           = 2048
0.00.051.222 I print_info: n_layer          = 24
0.00.051.225 I print_info: n_head           = 16
0.00.051.225 I print_info: n_head_kv        = 16
0.00.051.226 I print_info: n_rot            = 32
0.00.051.226 I print_info: n_swa            = 0
0.00.051.226 I print_info: n_embd_head_k    = 128
0.00.051.228 I print_info: n_embd_head_v    = 128
0.00.051.229 I print_info: n_gqa            = 1
0.00.051.230 I print_info: n_embd_k_gqa     = 2048
0.00.051.231 I print_info: n_embd_v_gqa     = 2048
0.00.051.231 I print_info: f_norm_eps       = 1.0e-05
0.00.051.232 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.232 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.232 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.232 I print_info: f_logit_scale    = 0.0e+00
0.00.051.234 I print_info: n_ff             = 8192
0.00.051.234 I print_info: n_expert         = 0
0.00.051.234 I print_info: n_expert_used    = 0
0.00.051.234 I print_info: causal attn      = 1
0.00.051.234 I print_info: pooling type     = 0
0.00.051.234 I print_info: rope type        = 2
0.00.051.235 I print_info: rope scaling     = linear
0.00.051.238 I print_info: freq_base_train  = 10000.0
0.00.051.239 I print_info: freq_scale_train = 1
0.00.051.240 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.240 I print_info: rope_finetuned   = unknown
0.00.051.240 I print_info: ssm_d_conv       = 0
0.00.051.240 I print_info: ssm_d_inner      = 0
0.00.051.240 I print_info: ssm_d_state      = 0
0.00.051.240 I print_info: ssm_dt_rank      = 0
0.00.051.241 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.241 I print_info: model type       = 1.4B
0.00.051.241 I print_info: model params     = 1.41 B
0.00.051.241 I print_info: general.name     = 1.4B
0.00.051.245 I print_info: vocab type       = BPE
0.00.051.245 I print_info: n_vocab          = 50304
0.00.051.246 I print_info: n_merges         = 50009
0.00.051.246 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.246 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.246 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.246 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.246 I print_info: LF token         = 128 'Ä'
0.00.051.247 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.248 I print_info: max token length = 1024
0.00.053.177 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.178 I load_tensors: offloading output layer to GPU
0.00.053.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.188 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.189 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.462 I llama_init_from_model: n_seq_max     = 1
0.00.053.463 I llama_init_from_model: n_ctx         = 128
0.00.053.463 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.463 I llama_init_from_model: n_batch       = 128
0.00.053.463 I llama_init_from_model: n_ubatch      = 128
0.00.053.463 I llama_init_from_model: flash_attn    = 0
0.00.053.464 I llama_init_from_model: freq_base     = 10000.0
0.00.053.464 I llama_init_from_model: freq_scale    = 1
0.00.053.464 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.465 I ggml_metal_init: allocating
0.00.053.468 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.056.388 I ggml_metal_init: GPU name:   Apple M4
0.00.056.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.390 I ggml_metal_init: simdgroup reduction   = true
0.00.056.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.391 I ggml_metal_init: has bfloat            = true
0.00.056.391 I ggml_metal_init: use bfloat            = true
0.00.056.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.933 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.247 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.178 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.180 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.180 I llama_init_from_model: graph nodes  = 967
0.00.068.180 I llama_init_from_model: graph splits = 2
0.00.068.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.093 I 
0.00.736.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.152 I perplexity: tokenizing the input ..
0.00.744.553 I perplexity: tokenization took 8.4 ms
0.00.744.557 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.484 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.880.651 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.880.682 I llama_perf_context_print:        load time =     725.29 ms
0.00.880.683 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.26 tokens per second)
0.00.880.684 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.684 I llama_perf_context_print:       total time =     144.59 ms /   129 tokens
0.00.881.179 I ggml_metal_free: deallocating

real	0m0.897s
user	0m0.077s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.560 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.790 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.473 I llama_model_loader: - type  f32:  194 tensors
0.00.025.473 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.474 I print_info: file format = GGUF V3 (latest)
0.00.025.474 I print_info: file type   = Q5_1
0.00.025.475 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.204 I load: special tokens cache size = 25
0.00.050.310 I load: token to piece cache size = 0.2984 MB
0.00.050.313 I print_info: arch             = gptneox
0.00.050.313 I print_info: vocab_only       = 0
0.00.050.313 I print_info: n_ctx_train      = 2048
0.00.050.313 I print_info: n_embd           = 2048
0.00.050.313 I print_info: n_layer          = 24
0.00.050.316 I print_info: n_head           = 16
0.00.050.317 I print_info: n_head_kv        = 16
0.00.050.317 I print_info: n_rot            = 32
0.00.050.317 I print_info: n_swa            = 0
0.00.050.318 I print_info: n_embd_head_k    = 128
0.00.050.318 I print_info: n_embd_head_v    = 128
0.00.050.318 I print_info: n_gqa            = 1
0.00.050.319 I print_info: n_embd_k_gqa     = 2048
0.00.050.320 I print_info: n_embd_v_gqa     = 2048
0.00.050.320 I print_info: f_norm_eps       = 1.0e-05
0.00.050.321 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.321 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.321 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.321 I print_info: f_logit_scale    = 0.0e+00
0.00.050.322 I print_info: n_ff             = 8192
0.00.050.322 I print_info: n_expert         = 0
0.00.050.322 I print_info: n_expert_used    = 0
0.00.050.322 I print_info: causal attn      = 1
0.00.050.322 I print_info: pooling type     = 0
0.00.050.324 I print_info: rope type        = 2
0.00.050.326 I print_info: rope scaling     = linear
0.00.050.326 I print_info: freq_base_train  = 10000.0
0.00.050.327 I print_info: freq_scale_train = 1
0.00.050.327 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.327 I print_info: rope_finetuned   = unknown
0.00.050.327 I print_info: ssm_d_conv       = 0
0.00.050.327 I print_info: ssm_d_inner      = 0
0.00.050.328 I print_info: ssm_d_state      = 0
0.00.050.328 I print_info: ssm_dt_rank      = 0
0.00.050.328 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.328 I print_info: model type       = 1.4B
0.00.050.328 I print_info: model params     = 1.41 B
0.00.050.330 I print_info: general.name     = 1.4B
0.00.050.331 I print_info: vocab type       = BPE
0.00.050.331 I print_info: n_vocab          = 50304
0.00.050.331 I print_info: n_merges         = 50009
0.00.050.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: LF token         = 128 'Ä'
0.00.050.333 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.333 I print_info: max token length = 1024
0.00.052.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.278 I load_tensors: offloading output layer to GPU
0.00.052.278 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.289 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.290 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.552 I llama_init_from_model: n_seq_max     = 1
0.00.052.553 I llama_init_from_model: n_ctx         = 2048
0.00.052.553 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.553 I llama_init_from_model: n_batch       = 2048
0.00.052.553 I llama_init_from_model: n_ubatch      = 512
0.00.052.553 I llama_init_from_model: flash_attn    = 0
0.00.052.554 I llama_init_from_model: freq_base     = 10000.0
0.00.052.554 I llama_init_from_model: freq_scale    = 1
0.00.052.554 I ggml_metal_init: allocating
0.00.052.557 I ggml_metal_init: found device: Apple M4
0.00.052.559 I ggml_metal_init: picking default device: Apple M4
0.00.053.177 I ggml_metal_init: using embedded metal library
0.00.055.498 I ggml_metal_init: GPU name:   Apple M4
0.00.055.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.500 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.500 I ggml_metal_init: simdgroup reduction   = true
0.00.055.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.500 I ggml_metal_init: has bfloat            = true
0.00.055.500 I ggml_metal_init: use bfloat            = true
0.00.055.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.066 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.198 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.216 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.240 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.242 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.242 I llama_init_from_model: graph nodes  = 967
0.00.084.242 I llama_init_from_model: graph splits = 2
0.00.084.245 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.675 I main: llama threadpool init, n_threads = 4
0.00.692.723 I 
0.00.692.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.764 I 
0.00.692.990 I sampler seed: 1234
0.00.692.995 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.036 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.039 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.040 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.535.013 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.535.014 I llama_perf_context_print:        load time =     684.11 ms
0.01.535.015 I llama_perf_context_print: prompt eval time =      45.33 ms /     7 tokens (    6.48 ms per token,   154.43 tokens per second)
0.01.535.015 I llama_perf_context_print:        eval time =     793.56 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.535.016 I llama_perf_context_print:       total time =     842.34 ms /    70 tokens
0.01.535.195 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.108s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.015 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.544 I llama_model_loader: - type  f32:  194 tensors
0.00.024.544 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.545 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.545 I print_info: file format = GGUF V3 (latest)
0.00.024.546 I print_info: file type   = Q5_1
0.00.024.547 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.216 I load: special tokens cache size = 25
0.00.049.141 I load: token to piece cache size = 0.2984 MB
0.00.049.144 I print_info: arch             = gptneox
0.00.049.145 I print_info: vocab_only       = 0
0.00.049.145 I print_info: n_ctx_train      = 2048
0.00.049.145 I print_info: n_embd           = 2048
0.00.049.145 I print_info: n_layer          = 24
0.00.049.148 I print_info: n_head           = 16
0.00.049.149 I print_info: n_head_kv        = 16
0.00.049.149 I print_info: n_rot            = 32
0.00.049.149 I print_info: n_swa            = 0
0.00.049.151 I print_info: n_embd_head_k    = 128
0.00.049.152 I print_info: n_embd_head_v    = 128
0.00.049.152 I print_info: n_gqa            = 1
0.00.049.153 I print_info: n_embd_k_gqa     = 2048
0.00.049.154 I print_info: n_embd_v_gqa     = 2048
0.00.049.154 I print_info: f_norm_eps       = 1.0e-05
0.00.049.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.155 I print_info: f_logit_scale    = 0.0e+00
0.00.049.156 I print_info: n_ff             = 8192
0.00.049.156 I print_info: n_expert         = 0
0.00.049.156 I print_info: n_expert_used    = 0
0.00.049.156 I print_info: causal attn      = 1
0.00.049.156 I print_info: pooling type     = 0
0.00.049.156 I print_info: rope type        = 2
0.00.049.157 I print_info: rope scaling     = linear
0.00.049.161 I print_info: freq_base_train  = 10000.0
0.00.049.161 I print_info: freq_scale_train = 1
0.00.049.161 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.162 I print_info: rope_finetuned   = unknown
0.00.049.162 I print_info: ssm_d_conv       = 0
0.00.049.162 I print_info: ssm_d_inner      = 0
0.00.049.162 I print_info: ssm_d_state      = 0
0.00.049.162 I print_info: ssm_dt_rank      = 0
0.00.049.162 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.163 I print_info: model type       = 1.4B
0.00.049.163 I print_info: model params     = 1.41 B
0.00.049.163 I print_info: general.name     = 1.4B
0.00.049.164 I print_info: vocab type       = BPE
0.00.049.164 I print_info: n_vocab          = 50304
0.00.049.164 I print_info: n_merges         = 50009
0.00.049.164 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.166 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.166 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.166 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.166 I print_info: LF token         = 128 'Ä'
0.00.049.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.167 I print_info: max token length = 1024
0.00.051.150 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.150 I load_tensors: offloading output layer to GPU
0.00.051.150 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.161 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.162 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.419 I llama_init_from_model: n_seq_max     = 1
0.00.051.419 I llama_init_from_model: n_ctx         = 128
0.00.051.420 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.420 I llama_init_from_model: n_batch       = 128
0.00.051.420 I llama_init_from_model: n_ubatch      = 128
0.00.051.420 I llama_init_from_model: flash_attn    = 0
0.00.051.421 I llama_init_from_model: freq_base     = 10000.0
0.00.051.421 I llama_init_from_model: freq_scale    = 1
0.00.051.421 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.422 I ggml_metal_init: allocating
0.00.051.425 I ggml_metal_init: found device: Apple M4
0.00.051.427 I ggml_metal_init: picking default device: Apple M4
0.00.052.016 I ggml_metal_init: using embedded metal library
0.00.054.317 I ggml_metal_init: GPU name:   Apple M4
0.00.054.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.319 I ggml_metal_init: simdgroup reduction   = true
0.00.054.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.320 I ggml_metal_init: has bfloat            = true
0.00.054.320 I ggml_metal_init: use bfloat            = true
0.00.054.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.869 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.100 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.114 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.969 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.970 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.970 I llama_init_from_model: graph nodes  = 967
0.00.065.970 I llama_init_from_model: graph splits = 2
0.00.065.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.430 I 
0.00.641.466 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.469 I perplexity: tokenizing the input ..
0.00.649.613 I perplexity: tokenization took 8.142 ms
0.00.649.617 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.715 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.785.888 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.785.919 I llama_perf_context_print:        load time =     632.51 ms
0.00.785.920 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.08 tokens per second)
0.00.785.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.921 I llama_perf_context_print:       total time =     144.49 ms /   129 tokens
0.00.786.407 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.077s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.169 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.678 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.689 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.690 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.692 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.693 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.694 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.209 I llama_model_loader: - type  f32:  194 tensors
0.00.024.209 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.209 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.210 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.210 I print_info: file format = GGUF V3 (latest)
0.00.024.211 I print_info: file type   = Q2_K - Medium
0.00.024.212 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.761 I load: special tokens cache size = 25
0.00.048.622 I load: token to piece cache size = 0.2984 MB
0.00.048.625 I print_info: arch             = gptneox
0.00.048.625 I print_info: vocab_only       = 0
0.00.048.625 I print_info: n_ctx_train      = 2048
0.00.048.625 I print_info: n_embd           = 2048
0.00.048.626 I print_info: n_layer          = 24
0.00.048.628 I print_info: n_head           = 16
0.00.048.629 I print_info: n_head_kv        = 16
0.00.048.630 I print_info: n_rot            = 32
0.00.048.630 I print_info: n_swa            = 0
0.00.048.630 I print_info: n_embd_head_k    = 128
0.00.048.630 I print_info: n_embd_head_v    = 128
0.00.048.631 I print_info: n_gqa            = 1
0.00.048.632 I print_info: n_embd_k_gqa     = 2048
0.00.048.632 I print_info: n_embd_v_gqa     = 2048
0.00.048.633 I print_info: f_norm_eps       = 1.0e-05
0.00.048.633 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.634 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.634 I print_info: f_logit_scale    = 0.0e+00
0.00.048.634 I print_info: n_ff             = 8192
0.00.048.635 I print_info: n_expert         = 0
0.00.048.635 I print_info: n_expert_used    = 0
0.00.048.635 I print_info: causal attn      = 1
0.00.048.637 I print_info: pooling type     = 0
0.00.048.637 I print_info: rope type        = 2
0.00.048.637 I print_info: rope scaling     = linear
0.00.048.638 I print_info: freq_base_train  = 10000.0
0.00.048.638 I print_info: freq_scale_train = 1
0.00.048.638 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.638 I print_info: rope_finetuned   = unknown
0.00.048.639 I print_info: ssm_d_conv       = 0
0.00.048.639 I print_info: ssm_d_inner      = 0
0.00.048.639 I print_info: ssm_d_state      = 0
0.00.048.639 I print_info: ssm_dt_rank      = 0
0.00.048.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.639 I print_info: model type       = 1.4B
0.00.048.640 I print_info: model params     = 1.41 B
0.00.048.640 I print_info: general.name     = 1.4B
0.00.048.641 I print_info: vocab type       = BPE
0.00.048.641 I print_info: n_vocab          = 50304
0.00.048.641 I print_info: n_merges         = 50009
0.00.048.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.643 I print_info: LF token         = 128 'Ä'
0.00.048.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.643 I print_info: max token length = 1024
0.00.050.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.451 I load_tensors: offloading output layer to GPU
0.00.050.451 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.462 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.463 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.050.786 I llama_init_from_model: n_seq_max     = 1
0.00.050.787 I llama_init_from_model: n_ctx         = 2048
0.00.050.787 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.787 I llama_init_from_model: n_batch       = 2048
0.00.050.788 I llama_init_from_model: n_ubatch      = 512
0.00.050.788 I llama_init_from_model: flash_attn    = 0
0.00.050.788 I llama_init_from_model: freq_base     = 10000.0
0.00.050.788 I llama_init_from_model: freq_scale    = 1
0.00.050.789 I ggml_metal_init: allocating
0.00.050.792 I ggml_metal_init: found device: Apple M4
0.00.050.794 I ggml_metal_init: picking default device: Apple M4
0.00.051.408 I ggml_metal_init: using embedded metal library
0.00.053.722 I ggml_metal_init: GPU name:   Apple M4
0.00.053.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.725 I ggml_metal_init: simdgroup reduction   = true
0.00.053.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.725 I ggml_metal_init: has bfloat            = true
0.00.053.725 I ggml_metal_init: use bfloat            = true
0.00.053.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.259 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.331 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.342 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.471 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.473 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.473 I llama_init_from_model: graph nodes  = 967
0.00.084.473 I llama_init_from_model: graph splits = 2
0.00.084.476 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.622 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.622 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.981 I main: llama threadpool init, n_threads = 4
0.00.430.033 I 
0.00.430.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.430.067 I 
0.00.430.295 I sampler seed: 1234
0.00.430.300 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.430.335 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.430.338 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.430.339 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.107.686 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.107.687 I llama_perf_context_print:        load time =     420.81 ms
0.01.107.687 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.69 tokens per second)
0.01.107.688 I llama_perf_context_print:        eval time =     638.83 ms /    63 runs   (   10.14 ms per token,    98.62 tokens per second)
0.01.107.689 I llama_perf_context_print:       total time =     677.71 ms /    70 tokens
0.01.107.988 I ggml_metal_free: deallocating

real	0m1.127s
user	0m0.108s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.529 I llama_model_loader: - type  f32:  194 tensors
0.00.025.530 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.530 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.530 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.531 I print_info: file format = GGUF V3 (latest)
0.00.025.531 I print_info: file type   = Q2_K - Medium
0.00.025.532 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.860 I load: special tokens cache size = 25
0.00.050.609 I load: token to piece cache size = 0.2984 MB
0.00.050.612 I print_info: arch             = gptneox
0.00.050.612 I print_info: vocab_only       = 0
0.00.050.613 I print_info: n_ctx_train      = 2048
0.00.050.613 I print_info: n_embd           = 2048
0.00.050.613 I print_info: n_layer          = 24
0.00.050.616 I print_info: n_head           = 16
0.00.050.617 I print_info: n_head_kv        = 16
0.00.050.617 I print_info: n_rot            = 32
0.00.050.617 I print_info: n_swa            = 0
0.00.050.617 I print_info: n_embd_head_k    = 128
0.00.050.617 I print_info: n_embd_head_v    = 128
0.00.050.618 I print_info: n_gqa            = 1
0.00.050.619 I print_info: n_embd_k_gqa     = 2048
0.00.050.620 I print_info: n_embd_v_gqa     = 2048
0.00.050.620 I print_info: f_norm_eps       = 1.0e-05
0.00.050.621 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.621 I print_info: f_logit_scale    = 0.0e+00
0.00.050.622 I print_info: n_ff             = 8192
0.00.050.622 I print_info: n_expert         = 0
0.00.050.622 I print_info: n_expert_used    = 0
0.00.050.622 I print_info: causal attn      = 1
0.00.050.622 I print_info: pooling type     = 0
0.00.050.622 I print_info: rope type        = 2
0.00.050.623 I print_info: rope scaling     = linear
0.00.050.623 I print_info: freq_base_train  = 10000.0
0.00.050.624 I print_info: freq_scale_train = 1
0.00.050.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.624 I print_info: rope_finetuned   = unknown
0.00.050.624 I print_info: ssm_d_conv       = 0
0.00.050.624 I print_info: ssm_d_inner      = 0
0.00.050.624 I print_info: ssm_d_state      = 0
0.00.050.625 I print_info: ssm_dt_rank      = 0
0.00.050.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.625 I print_info: model type       = 1.4B
0.00.050.625 I print_info: model params     = 1.41 B
0.00.050.626 I print_info: general.name     = 1.4B
0.00.050.626 I print_info: vocab type       = BPE
0.00.050.626 I print_info: n_vocab          = 50304
0.00.050.629 I print_info: n_merges         = 50009
0.00.050.629 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.630 I print_info: LF token         = 128 'Ä'
0.00.050.630 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: max token length = 1024
0.00.052.527 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.527 I load_tensors: offloading output layer to GPU
0.00.052.528 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.538 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.539 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.795 I llama_init_from_model: n_seq_max     = 1
0.00.052.795 I llama_init_from_model: n_ctx         = 128
0.00.052.795 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.796 I llama_init_from_model: n_batch       = 128
0.00.052.796 I llama_init_from_model: n_ubatch      = 128
0.00.052.796 I llama_init_from_model: flash_attn    = 0
0.00.052.796 I llama_init_from_model: freq_base     = 10000.0
0.00.052.796 I llama_init_from_model: freq_scale    = 1
0.00.052.797 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.797 I ggml_metal_init: allocating
0.00.052.800 I ggml_metal_init: found device: Apple M4
0.00.052.802 I ggml_metal_init: picking default device: Apple M4
0.00.053.363 I ggml_metal_init: using embedded metal library
0.00.055.649 I ggml_metal_init: GPU name:   Apple M4
0.00.055.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.651 I ggml_metal_init: simdgroup reduction   = true
0.00.055.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.651 I ggml_metal_init: has bfloat            = true
0.00.055.651 I ggml_metal_init: use bfloat            = true
0.00.055.652 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.188 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.460 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.465 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.481 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.399 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.400 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.401 I llama_init_from_model: graph nodes  = 967
0.00.067.401 I llama_init_from_model: graph splits = 2
0.00.067.402 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.140 I 
0.00.387.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.182 I perplexity: tokenizing the input ..
0.00.394.776 I perplexity: tokenization took 7.592 ms
0.00.394.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.527.437 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.597 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.528.628 I llama_perf_context_print:        load time =     377.26 ms
0.00.528.629 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.86 tokens per second)
0.00.528.630 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.528.631 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.529.137 I ggml_metal_free: deallocating

real	0m0.545s
user	0m0.078s
sys	0m0.071s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.445 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.618 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.618 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.618 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.619 I print_info: file format = GGUF V3 (latest)
0.00.025.619 I print_info: file type   = Q3_K - Medium
0.00.025.620 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.359 I load: special tokens cache size = 25
0.00.051.264 I load: token to piece cache size = 0.2984 MB
0.00.051.269 I print_info: arch             = gptneox
0.00.051.269 I print_info: vocab_only       = 0
0.00.051.269 I print_info: n_ctx_train      = 2048
0.00.051.269 I print_info: n_embd           = 2048
0.00.051.269 I print_info: n_layer          = 24
0.00.051.273 I print_info: n_head           = 16
0.00.051.274 I print_info: n_head_kv        = 16
0.00.051.274 I print_info: n_rot            = 32
0.00.051.274 I print_info: n_swa            = 0
0.00.051.274 I print_info: n_embd_head_k    = 128
0.00.051.275 I print_info: n_embd_head_v    = 128
0.00.051.275 I print_info: n_gqa            = 1
0.00.051.276 I print_info: n_embd_k_gqa     = 2048
0.00.051.276 I print_info: n_embd_v_gqa     = 2048
0.00.051.277 I print_info: f_norm_eps       = 1.0e-05
0.00.051.279 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.279 I print_info: f_logit_scale    = 0.0e+00
0.00.051.280 I print_info: n_ff             = 8192
0.00.051.280 I print_info: n_expert         = 0
0.00.051.280 I print_info: n_expert_used    = 0
0.00.051.280 I print_info: causal attn      = 1
0.00.051.280 I print_info: pooling type     = 0
0.00.051.280 I print_info: rope type        = 2
0.00.051.281 I print_info: rope scaling     = linear
0.00.051.281 I print_info: freq_base_train  = 10000.0
0.00.051.281 I print_info: freq_scale_train = 1
0.00.051.281 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.281 I print_info: rope_finetuned   = unknown
0.00.051.282 I print_info: ssm_d_conv       = 0
0.00.051.282 I print_info: ssm_d_inner      = 0
0.00.051.282 I print_info: ssm_d_state      = 0
0.00.051.282 I print_info: ssm_dt_rank      = 0
0.00.051.282 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.282 I print_info: model type       = 1.4B
0.00.051.283 I print_info: model params     = 1.41 B
0.00.051.283 I print_info: general.name     = 1.4B
0.00.051.283 I print_info: vocab type       = BPE
0.00.051.283 I print_info: n_vocab          = 50304
0.00.051.284 I print_info: n_merges         = 50009
0.00.051.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: LF token         = 128 'Ä'
0.00.051.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.285 I print_info: max token length = 1024
0.00.053.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.335 I load_tensors: offloading output layer to GPU
0.00.053.335 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.346 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.347 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.709 I llama_init_from_model: n_seq_max     = 1
0.00.053.710 I llama_init_from_model: n_ctx         = 2048
0.00.053.710 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.710 I llama_init_from_model: n_batch       = 2048
0.00.053.710 I llama_init_from_model: n_ubatch      = 512
0.00.053.710 I llama_init_from_model: flash_attn    = 0
0.00.053.711 I llama_init_from_model: freq_base     = 10000.0
0.00.053.711 I llama_init_from_model: freq_scale    = 1
0.00.053.712 I ggml_metal_init: allocating
0.00.053.716 I ggml_metal_init: found device: Apple M4
0.00.053.718 I ggml_metal_init: picking default device: Apple M4
0.00.054.375 I ggml_metal_init: using embedded metal library
0.00.056.812 I ggml_metal_init: GPU name:   Apple M4
0.00.056.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.815 I ggml_metal_init: simdgroup reduction   = true
0.00.056.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.815 I ggml_metal_init: has bfloat            = true
0.00.056.815 I ggml_metal_init: use bfloat            = true
0.00.056.816 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.817 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.644 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.715 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.716 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.716 I llama_init_from_model: graph nodes  = 967
0.00.086.717 I llama_init_from_model: graph splits = 2
0.00.086.720 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.855 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.774 I main: llama threadpool init, n_threads = 4
0.00.531.817 I 
0.00.531.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.859 I 
0.00.532.086 I sampler seed: 1234
0.00.532.090 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.110 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.110 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.110 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.285.414 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62610.23 tokens per second)
0.01.285.415 I llama_perf_context_print:        load time =     522.96 ms
0.01.285.416 I llama_perf_context_print: prompt eval time =      46.98 ms /     7 tokens (    6.71 ms per token,   149.02 tokens per second)
0.01.285.417 I llama_perf_context_print:        eval time =     703.48 ms /    63 runs   (   11.17 ms per token,    89.56 tokens per second)
0.01.285.418 I llama_perf_context_print:       total time =     753.64 ms /    70 tokens
0.01.285.685 I ggml_metal_free: deallocating

real	0m1.302s
user	0m0.112s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.941 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.943 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.943 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.944 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.948 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.749 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.495 I llama_model_loader: - type  f32:  194 tensors
0.00.024.496 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.496 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.496 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.496 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.497 I print_info: file format = GGUF V3 (latest)
0.00.024.497 I print_info: file type   = Q3_K - Medium
0.00.024.498 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.992 I load: special tokens cache size = 25
0.00.049.850 I load: token to piece cache size = 0.2984 MB
0.00.049.853 I print_info: arch             = gptneox
0.00.049.853 I print_info: vocab_only       = 0
0.00.049.853 I print_info: n_ctx_train      = 2048
0.00.049.853 I print_info: n_embd           = 2048
0.00.049.853 I print_info: n_layer          = 24
0.00.049.856 I print_info: n_head           = 16
0.00.049.857 I print_info: n_head_kv        = 16
0.00.049.857 I print_info: n_rot            = 32
0.00.049.857 I print_info: n_swa            = 0
0.00.049.858 I print_info: n_embd_head_k    = 128
0.00.049.858 I print_info: n_embd_head_v    = 128
0.00.049.860 I print_info: n_gqa            = 1
0.00.049.861 I print_info: n_embd_k_gqa     = 2048
0.00.049.862 I print_info: n_embd_v_gqa     = 2048
0.00.049.863 I print_info: f_norm_eps       = 1.0e-05
0.00.049.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.865 I print_info: f_logit_scale    = 0.0e+00
0.00.049.866 I print_info: n_ff             = 8192
0.00.049.866 I print_info: n_expert         = 0
0.00.049.866 I print_info: n_expert_used    = 0
0.00.049.867 I print_info: causal attn      = 1
0.00.049.867 I print_info: pooling type     = 0
0.00.049.867 I print_info: rope type        = 2
0.00.049.868 I print_info: rope scaling     = linear
0.00.049.868 I print_info: freq_base_train  = 10000.0
0.00.049.869 I print_info: freq_scale_train = 1
0.00.049.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.869 I print_info: rope_finetuned   = unknown
0.00.049.869 I print_info: ssm_d_conv       = 0
0.00.049.869 I print_info: ssm_d_inner      = 0
0.00.049.869 I print_info: ssm_d_state      = 0
0.00.049.869 I print_info: ssm_dt_rank      = 0
0.00.049.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.870 I print_info: model type       = 1.4B
0.00.049.871 I print_info: model params     = 1.41 B
0.00.049.871 I print_info: general.name     = 1.4B
0.00.049.872 I print_info: vocab type       = BPE
0.00.049.872 I print_info: n_vocab          = 50304
0.00.049.872 I print_info: n_merges         = 50009
0.00.049.872 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.873 I print_info: LF token         = 128 'Ä'
0.00.049.875 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.875 I print_info: max token length = 1024
0.00.051.769 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.770 I load_tensors: offloading output layer to GPU
0.00.051.770 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.780 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.781 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.034 I llama_init_from_model: n_seq_max     = 1
0.00.052.035 I llama_init_from_model: n_ctx         = 128
0.00.052.035 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.035 I llama_init_from_model: n_batch       = 128
0.00.052.035 I llama_init_from_model: n_ubatch      = 128
0.00.052.036 I llama_init_from_model: flash_attn    = 0
0.00.052.036 I llama_init_from_model: freq_base     = 10000.0
0.00.052.036 I llama_init_from_model: freq_scale    = 1
0.00.052.037 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.037 I ggml_metal_init: allocating
0.00.052.040 I ggml_metal_init: found device: Apple M4
0.00.052.042 I ggml_metal_init: picking default device: Apple M4
0.00.052.607 I ggml_metal_init: using embedded metal library
0.00.054.921 I ggml_metal_init: GPU name:   Apple M4
0.00.054.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.923 I ggml_metal_init: simdgroup reduction   = true
0.00.054.923 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.923 I ggml_metal_init: has bfloat            = true
0.00.054.924 I ggml_metal_init: use bfloat            = true
0.00.054.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.212 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.431 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.435 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.450 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.364 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.365 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.366 I llama_init_from_model: graph nodes  = 967
0.00.066.366 I llama_init_from_model: graph splits = 2
0.00.066.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.466.328 I 
0.00.466.372 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.466.376 I perplexity: tokenizing the input ..
0.00.474.484 I perplexity: tokenization took 8.105 ms
0.00.474.489 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.606.679 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.607.845 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.607.869 I llama_perf_context_print:        load time =     457.52 ms
0.00.607.875 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   969.97 tokens per second)
0.00.607.876 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.607.876 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.608.448 I ggml_metal_free: deallocating

real	0m0.621s
user	0m0.077s
sys	0m0.081s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.322 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.327 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.329 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.173 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.173 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.174 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.174 I print_info: file format = GGUF V3 (latest)
0.00.025.175 I print_info: file type   = Q4_K - Medium
0.00.025.176 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.777 I load: special tokens cache size = 25
0.00.050.950 I load: token to piece cache size = 0.2984 MB
0.00.050.953 I print_info: arch             = gptneox
0.00.050.953 I print_info: vocab_only       = 0
0.00.050.953 I print_info: n_ctx_train      = 2048
0.00.050.953 I print_info: n_embd           = 2048
0.00.050.954 I print_info: n_layer          = 24
0.00.050.957 I print_info: n_head           = 16
0.00.050.957 I print_info: n_head_kv        = 16
0.00.050.959 I print_info: n_rot            = 32
0.00.050.960 I print_info: n_swa            = 0
0.00.050.960 I print_info: n_embd_head_k    = 128
0.00.050.960 I print_info: n_embd_head_v    = 128
0.00.050.961 I print_info: n_gqa            = 1
0.00.050.961 I print_info: n_embd_k_gqa     = 2048
0.00.050.966 I print_info: n_embd_v_gqa     = 2048
0.00.050.967 I print_info: f_norm_eps       = 1.0e-05
0.00.050.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.968 I print_info: f_logit_scale    = 0.0e+00
0.00.050.969 I print_info: n_ff             = 8192
0.00.050.972 I print_info: n_expert         = 0
0.00.050.972 I print_info: n_expert_used    = 0
0.00.050.972 I print_info: causal attn      = 1
0.00.050.974 I print_info: pooling type     = 0
0.00.050.975 I print_info: rope type        = 2
0.00.050.975 I print_info: rope scaling     = linear
0.00.050.976 I print_info: freq_base_train  = 10000.0
0.00.050.976 I print_info: freq_scale_train = 1
0.00.050.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.976 I print_info: rope_finetuned   = unknown
0.00.050.977 I print_info: ssm_d_conv       = 0
0.00.050.977 I print_info: ssm_d_inner      = 0
0.00.050.977 I print_info: ssm_d_state      = 0
0.00.050.977 I print_info: ssm_dt_rank      = 0
0.00.050.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.977 I print_info: model type       = 1.4B
0.00.050.978 I print_info: model params     = 1.41 B
0.00.050.978 I print_info: general.name     = 1.4B
0.00.050.978 I print_info: vocab type       = BPE
0.00.050.978 I print_info: n_vocab          = 50304
0.00.050.978 I print_info: n_merges         = 50009
0.00.050.979 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.979 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.979 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.979 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.980 I print_info: LF token         = 128 'Ä'
0.00.050.981 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.981 I print_info: max token length = 1024
0.00.052.916 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.916 I load_tensors: offloading output layer to GPU
0.00.052.916 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.927 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.928 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.213 I llama_init_from_model: n_seq_max     = 1
0.00.053.213 I llama_init_from_model: n_ctx         = 2048
0.00.053.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.214 I llama_init_from_model: n_batch       = 2048
0.00.053.214 I llama_init_from_model: n_ubatch      = 512
0.00.053.214 I llama_init_from_model: flash_attn    = 0
0.00.053.214 I llama_init_from_model: freq_base     = 10000.0
0.00.053.215 I llama_init_from_model: freq_scale    = 1
0.00.053.215 I ggml_metal_init: allocating
0.00.053.218 I ggml_metal_init: found device: Apple M4
0.00.053.220 I ggml_metal_init: picking default device: Apple M4
0.00.053.836 I ggml_metal_init: using embedded metal library
0.00.056.234 I ggml_metal_init: GPU name:   Apple M4
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.236 I ggml_metal_init: simdgroup reduction   = true
0.00.056.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.236 I ggml_metal_init: has bfloat            = true
0.00.056.236 I ggml_metal_init: use bfloat            = true
0.00.056.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.131 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.275 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.281 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.301 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.243 I llama_init_from_model: graph nodes  = 967
0.00.086.243 I llama_init_from_model: graph splits = 2
0.00.086.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.245 I main: llama threadpool init, n_threads = 4
0.00.601.292 I 
0.00.601.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.317 I 
0.00.601.455 I sampler seed: 1234
0.00.601.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.491 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.491 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.361.943 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.361.944 I llama_perf_context_print:        load time =     592.42 ms
0.01.361.945 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.66 tokens per second)
0.01.361.945 I llama_perf_context_print:        eval time =     710.36 ms /    63 runs   (   11.28 ms per token,    88.69 tokens per second)
0.01.361.946 I llama_perf_context_print:       total time =     760.70 ms /    70 tokens
0.01.362.153 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.110s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.985 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.617 I llama_model_loader: - type  f32:  194 tensors
0.00.024.617 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.617 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.617 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.618 I print_info: file format = GGUF V3 (latest)
0.00.024.618 I print_info: file type   = Q4_K - Medium
0.00.024.619 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.996 I load: special tokens cache size = 25
0.00.050.027 I load: token to piece cache size = 0.2984 MB
0.00.050.030 I print_info: arch             = gptneox
0.00.050.030 I print_info: vocab_only       = 0
0.00.050.030 I print_info: n_ctx_train      = 2048
0.00.050.030 I print_info: n_embd           = 2048
0.00.050.030 I print_info: n_layer          = 24
0.00.050.033 I print_info: n_head           = 16
0.00.050.034 I print_info: n_head_kv        = 16
0.00.050.034 I print_info: n_rot            = 32
0.00.050.034 I print_info: n_swa            = 0
0.00.050.035 I print_info: n_embd_head_k    = 128
0.00.050.035 I print_info: n_embd_head_v    = 128
0.00.050.035 I print_info: n_gqa            = 1
0.00.050.036 I print_info: n_embd_k_gqa     = 2048
0.00.050.037 I print_info: n_embd_v_gqa     = 2048
0.00.050.038 I print_info: f_norm_eps       = 1.0e-05
0.00.050.038 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.038 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.038 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.038 I print_info: f_logit_scale    = 0.0e+00
0.00.050.039 I print_info: n_ff             = 8192
0.00.050.039 I print_info: n_expert         = 0
0.00.050.039 I print_info: n_expert_used    = 0
0.00.050.039 I print_info: causal attn      = 1
0.00.050.040 I print_info: pooling type     = 0
0.00.050.040 I print_info: rope type        = 2
0.00.050.040 I print_info: rope scaling     = linear
0.00.050.040 I print_info: freq_base_train  = 10000.0
0.00.050.041 I print_info: freq_scale_train = 1
0.00.050.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.041 I print_info: rope_finetuned   = unknown
0.00.050.041 I print_info: ssm_d_conv       = 0
0.00.050.041 I print_info: ssm_d_inner      = 0
0.00.050.042 I print_info: ssm_d_state      = 0
0.00.050.044 I print_info: ssm_dt_rank      = 0
0.00.050.044 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.044 I print_info: model type       = 1.4B
0.00.050.044 I print_info: model params     = 1.41 B
0.00.050.045 I print_info: general.name     = 1.4B
0.00.050.045 I print_info: vocab type       = BPE
0.00.050.045 I print_info: n_vocab          = 50304
0.00.050.045 I print_info: n_merges         = 50009
0.00.050.046 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.046 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.050 I print_info: LF token         = 128 'Ä'
0.00.050.051 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.052 I print_info: max token length = 1024
0.00.052.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.027 I load_tensors: offloading output layer to GPU
0.00.052.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.037 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.038 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.303 I llama_init_from_model: n_seq_max     = 1
0.00.052.304 I llama_init_from_model: n_ctx         = 128
0.00.052.304 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.304 I llama_init_from_model: n_batch       = 128
0.00.052.304 I llama_init_from_model: n_ubatch      = 128
0.00.052.305 I llama_init_from_model: flash_attn    = 0
0.00.052.305 I llama_init_from_model: freq_base     = 10000.0
0.00.052.305 I llama_init_from_model: freq_scale    = 1
0.00.052.305 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.306 I ggml_metal_init: allocating
0.00.052.309 I ggml_metal_init: found device: Apple M4
0.00.052.311 I ggml_metal_init: picking default device: Apple M4
0.00.052.889 I ggml_metal_init: using embedded metal library
0.00.055.204 I ggml_metal_init: GPU name:   Apple M4
0.00.055.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.206 I ggml_metal_init: simdgroup reduction   = true
0.00.055.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.206 I ggml_metal_init: has bfloat            = true
0.00.055.207 I ggml_metal_init: use bfloat            = true
0.00.055.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.994 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.240 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.244 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.260 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.107 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.108 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.108 I llama_init_from_model: graph nodes  = 967
0.00.067.108 I llama_init_from_model: graph splits = 2
0.00.067.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.748 I 
0.00.565.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.781 I perplexity: tokenizing the input ..
0.00.573.505 I perplexity: tokenization took 7.724 ms
0.00.573.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.984 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.709.156 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.709.187 I llama_perf_context_print:        load time =     556.85 ms
0.00.709.188 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.52 tokens per second)
0.00.709.188 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.709.189 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.709.696 I ggml_metal_free: deallocating

real	0m0.723s
user	0m0.078s
sys	0m0.106s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.011.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.206 I llama_model_loader: - type  f32:  194 tensors
0.00.027.206 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.206 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.207 I print_info: file format = GGUF V3 (latest)
0.00.027.208 I print_info: file type   = Q5_K - Medium
0.00.027.208 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.759 I load: special tokens cache size = 25
0.00.052.573 I load: token to piece cache size = 0.2984 MB
0.00.052.576 I print_info: arch             = gptneox
0.00.052.576 I print_info: vocab_only       = 0
0.00.052.577 I print_info: n_ctx_train      = 2048
0.00.052.577 I print_info: n_embd           = 2048
0.00.052.577 I print_info: n_layer          = 24
0.00.052.580 I print_info: n_head           = 16
0.00.052.581 I print_info: n_head_kv        = 16
0.00.052.581 I print_info: n_rot            = 32
0.00.052.581 I print_info: n_swa            = 0
0.00.052.581 I print_info: n_embd_head_k    = 128
0.00.052.581 I print_info: n_embd_head_v    = 128
0.00.052.582 I print_info: n_gqa            = 1
0.00.052.583 I print_info: n_embd_k_gqa     = 2048
0.00.052.584 I print_info: n_embd_v_gqa     = 2048
0.00.052.589 I print_info: f_norm_eps       = 1.0e-05
0.00.052.589 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.589 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.589 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.589 I print_info: f_logit_scale    = 0.0e+00
0.00.052.590 I print_info: n_ff             = 8192
0.00.052.591 I print_info: n_expert         = 0
0.00.052.591 I print_info: n_expert_used    = 0
0.00.052.591 I print_info: causal attn      = 1
0.00.052.591 I print_info: pooling type     = 0
0.00.052.591 I print_info: rope type        = 2
0.00.052.592 I print_info: rope scaling     = linear
0.00.052.592 I print_info: freq_base_train  = 10000.0
0.00.052.592 I print_info: freq_scale_train = 1
0.00.052.593 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.593 I print_info: rope_finetuned   = unknown
0.00.052.593 I print_info: ssm_d_conv       = 0
0.00.052.593 I print_info: ssm_d_inner      = 0
0.00.052.594 I print_info: ssm_d_state      = 0
0.00.052.594 I print_info: ssm_dt_rank      = 0
0.00.052.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.594 I print_info: model type       = 1.4B
0.00.052.594 I print_info: model params     = 1.41 B
0.00.052.595 I print_info: general.name     = 1.4B
0.00.052.595 I print_info: vocab type       = BPE
0.00.052.595 I print_info: n_vocab          = 50304
0.00.052.595 I print_info: n_merges         = 50009
0.00.052.597 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.598 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.598 I print_info: LF token         = 128 'Ä'
0.00.052.598 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.598 I print_info: max token length = 1024
0.00.054.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.629 I load_tensors: offloading output layer to GPU
0.00.054.629 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.639 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.640 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.911 I llama_init_from_model: n_seq_max     = 1
0.00.054.912 I llama_init_from_model: n_ctx         = 2048
0.00.054.912 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.912 I llama_init_from_model: n_batch       = 2048
0.00.054.912 I llama_init_from_model: n_ubatch      = 512
0.00.054.913 I llama_init_from_model: flash_attn    = 0
0.00.054.913 I llama_init_from_model: freq_base     = 10000.0
0.00.054.913 I llama_init_from_model: freq_scale    = 1
0.00.054.914 I ggml_metal_init: allocating
0.00.054.917 I ggml_metal_init: found device: Apple M4
0.00.054.919 I ggml_metal_init: picking default device: Apple M4
0.00.055.535 I ggml_metal_init: using embedded metal library
0.00.057.906 I ggml_metal_init: GPU name:   Apple M4
0.00.057.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.909 I ggml_metal_init: simdgroup reduction   = true
0.00.057.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.909 I ggml_metal_init: has bfloat            = true
0.00.057.909 I ggml_metal_init: use bfloat            = true
0.00.057.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.809 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.882 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.888 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.905 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.037 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.039 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.039 I llama_init_from_model: graph nodes  = 967
0.00.088.039 I llama_init_from_model: graph splits = 2
0.00.088.042 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.466 I main: llama threadpool init, n_threads = 4
0.00.685.505 I 
0.00.685.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.552 I 
0.00.685.773 I sampler seed: 1234
0.00.685.776 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.788 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.789 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.529.817 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.01.529.817 I llama_perf_context_print:        load time =     674.39 ms
0.01.529.818 I llama_perf_context_print: prompt eval time =      51.72 ms /     7 tokens (    7.39 ms per token,   135.35 tokens per second)
0.01.529.822 I llama_perf_context_print:        eval time =     789.36 ms /    63 runs   (   12.53 ms per token,    79.81 tokens per second)
0.01.529.823 I llama_perf_context_print:       total time =     844.35 ms /    70 tokens
0.01.530.049 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.063 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.067 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.067 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.069 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.967 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.783 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.784 I llama_model_loader: - type  f32:  194 tensors
0.00.027.784 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.785 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.785 I print_info: file format = GGUF V3 (latest)
0.00.027.786 I print_info: file type   = Q5_K - Medium
0.00.027.787 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.263 I load: special tokens cache size = 25
0.00.053.028 I load: token to piece cache size = 0.2984 MB
0.00.053.032 I print_info: arch             = gptneox
0.00.053.032 I print_info: vocab_only       = 0
0.00.053.032 I print_info: n_ctx_train      = 2048
0.00.053.032 I print_info: n_embd           = 2048
0.00.053.033 I print_info: n_layer          = 24
0.00.053.036 I print_info: n_head           = 16
0.00.053.037 I print_info: n_head_kv        = 16
0.00.053.037 I print_info: n_rot            = 32
0.00.053.037 I print_info: n_swa            = 0
0.00.053.040 I print_info: n_embd_head_k    = 128
0.00.053.040 I print_info: n_embd_head_v    = 128
0.00.053.041 I print_info: n_gqa            = 1
0.00.053.041 I print_info: n_embd_k_gqa     = 2048
0.00.053.042 I print_info: n_embd_v_gqa     = 2048
0.00.053.043 I print_info: f_norm_eps       = 1.0e-05
0.00.053.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.044 I print_info: f_logit_scale    = 0.0e+00
0.00.053.044 I print_info: n_ff             = 8192
0.00.053.044 I print_info: n_expert         = 0
0.00.053.045 I print_info: n_expert_used    = 0
0.00.053.045 I print_info: causal attn      = 1
0.00.053.045 I print_info: pooling type     = 0
0.00.053.045 I print_info: rope type        = 2
0.00.053.045 I print_info: rope scaling     = linear
0.00.053.046 I print_info: freq_base_train  = 10000.0
0.00.053.046 I print_info: freq_scale_train = 1
0.00.053.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.046 I print_info: rope_finetuned   = unknown
0.00.053.047 I print_info: ssm_d_conv       = 0
0.00.053.047 I print_info: ssm_d_inner      = 0
0.00.053.047 I print_info: ssm_d_state      = 0
0.00.053.047 I print_info: ssm_dt_rank      = 0
0.00.053.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.048 I print_info: model type       = 1.4B
0.00.053.048 I print_info: model params     = 1.41 B
0.00.053.048 I print_info: general.name     = 1.4B
0.00.053.049 I print_info: vocab type       = BPE
0.00.053.049 I print_info: n_vocab          = 50304
0.00.053.049 I print_info: n_merges         = 50009
0.00.053.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.050 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.050 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.050 I print_info: LF token         = 128 'Ä'
0.00.053.051 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.051 I print_info: max token length = 1024
0.00.055.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.012 I load_tensors: offloading output layer to GPU
0.00.055.012 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.023 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.024 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.288 I llama_init_from_model: n_seq_max     = 1
0.00.055.289 I llama_init_from_model: n_ctx         = 128
0.00.055.289 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.289 I llama_init_from_model: n_batch       = 128
0.00.055.290 I llama_init_from_model: n_ubatch      = 128
0.00.055.290 I llama_init_from_model: flash_attn    = 0
0.00.055.290 I llama_init_from_model: freq_base     = 10000.0
0.00.055.290 I llama_init_from_model: freq_scale    = 1
0.00.055.291 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.291 I ggml_metal_init: allocating
0.00.055.294 I ggml_metal_init: found device: Apple M4
0.00.055.296 I ggml_metal_init: picking default device: Apple M4
0.00.055.856 I ggml_metal_init: using embedded metal library
0.00.058.171 I ggml_metal_init: GPU name:   Apple M4
0.00.058.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.174 I ggml_metal_init: simdgroup reduction   = true
0.00.058.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.174 I ggml_metal_init: has bfloat            = true
0.00.058.174 I ggml_metal_init: use bfloat            = true
0.00.058.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.640 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.643 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.658 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.532 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.533 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.533 I llama_init_from_model: graph nodes  = 967
0.00.069.533 I llama_init_from_model: graph splits = 2
0.00.069.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.250 I 
0.00.618.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.301 I perplexity: tokenizing the input ..
0.00.626.277 I perplexity: tokenization took 7.974 ms
0.00.626.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.073 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.768.286 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.768.312 I llama_perf_context_print:        load time =     606.51 ms
0.00.768.313 I llama_perf_context_print: prompt eval time =     140.56 ms /   128 tokens (    1.10 ms per token,   910.64 tokens per second)
0.00.768.314 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.314 I llama_perf_context_print:       total time =     150.06 ms /   129 tokens
0.00.768.802 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.077s
sys	0m0.103s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.669 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.506 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.016 I llama_model_loader: - type  f32:  194 tensors
0.00.025.017 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.017 I print_info: file format = GGUF V3 (latest)
0.00.025.017 I print_info: file type   = Q6_K
0.00.025.018 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.825 I load: special tokens cache size = 25
0.00.049.956 I load: token to piece cache size = 0.2984 MB
0.00.049.959 I print_info: arch             = gptneox
0.00.049.960 I print_info: vocab_only       = 0
0.00.049.960 I print_info: n_ctx_train      = 2048
0.00.049.960 I print_info: n_embd           = 2048
0.00.049.960 I print_info: n_layer          = 24
0.00.049.964 I print_info: n_head           = 16
0.00.049.964 I print_info: n_head_kv        = 16
0.00.049.965 I print_info: n_rot            = 32
0.00.049.965 I print_info: n_swa            = 0
0.00.049.965 I print_info: n_embd_head_k    = 128
0.00.049.965 I print_info: n_embd_head_v    = 128
0.00.049.966 I print_info: n_gqa            = 1
0.00.049.967 I print_info: n_embd_k_gqa     = 2048
0.00.049.967 I print_info: n_embd_v_gqa     = 2048
0.00.049.968 I print_info: f_norm_eps       = 1.0e-05
0.00.049.969 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.969 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.969 I print_info: f_logit_scale    = 0.0e+00
0.00.049.970 I print_info: n_ff             = 8192
0.00.049.971 I print_info: n_expert         = 0
0.00.049.971 I print_info: n_expert_used    = 0
0.00.049.971 I print_info: causal attn      = 1
0.00.049.971 I print_info: pooling type     = 0
0.00.049.971 I print_info: rope type        = 2
0.00.049.971 I print_info: rope scaling     = linear
0.00.049.972 I print_info: freq_base_train  = 10000.0
0.00.049.972 I print_info: freq_scale_train = 1
0.00.049.972 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.972 I print_info: rope_finetuned   = unknown
0.00.049.973 I print_info: ssm_d_conv       = 0
0.00.049.973 I print_info: ssm_d_inner      = 0
0.00.049.973 I print_info: ssm_d_state      = 0
0.00.049.973 I print_info: ssm_dt_rank      = 0
0.00.049.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.975 I print_info: model type       = 1.4B
0.00.049.975 I print_info: model params     = 1.41 B
0.00.049.976 I print_info: general.name     = 1.4B
0.00.049.976 I print_info: vocab type       = BPE
0.00.049.976 I print_info: n_vocab          = 50304
0.00.049.976 I print_info: n_merges         = 50009
0.00.049.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.977 I print_info: LF token         = 128 'Ä'
0.00.049.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.978 I print_info: max token length = 1024
0.00.051.966 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.966 I load_tensors: offloading output layer to GPU
0.00.051.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.976 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.978 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.238 I llama_init_from_model: n_seq_max     = 1
0.00.052.239 I llama_init_from_model: n_ctx         = 2048
0.00.052.239 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.239 I llama_init_from_model: n_batch       = 2048
0.00.052.240 I llama_init_from_model: n_ubatch      = 512
0.00.052.240 I llama_init_from_model: flash_attn    = 0
0.00.052.240 I llama_init_from_model: freq_base     = 10000.0
0.00.052.240 I llama_init_from_model: freq_scale    = 1
0.00.052.241 I ggml_metal_init: allocating
0.00.052.244 I ggml_metal_init: found device: Apple M4
0.00.052.246 I ggml_metal_init: picking default device: Apple M4
0.00.052.816 I ggml_metal_init: using embedded metal library
0.00.055.136 I ggml_metal_init: GPU name:   Apple M4
0.00.055.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.139 I ggml_metal_init: simdgroup reduction   = true
0.00.055.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.139 I ggml_metal_init: has bfloat            = true
0.00.055.139 I ggml_metal_init: use bfloat            = true
0.00.055.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.807 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.505 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.512 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.443 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.445 I llama_init_from_model: graph nodes  = 967
0.00.084.445 I llama_init_from_model: graph splits = 2
0.00.084.448 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.443 I main: llama threadpool init, n_threads = 4
0.00.743.492 I 
0.00.743.523 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.523 I 
0.00.743.762 I sampler seed: 1234
0.00.743.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.814 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.816 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.816 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.618.964 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.618.965 I llama_perf_context_print:        load time =     734.77 ms
0.01.618.965 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.74 tokens per second)
0.01.618.966 I llama_perf_context_print:        eval time =     817.72 ms /    63 runs   (   12.98 ms per token,    77.04 tokens per second)
0.01.618.966 I llama_perf_context_print:       total time =     875.53 ms /    70 tokens
0.01.619.228 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.107s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4496 (466300fe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.536 I llama_model_loader: - type  f32:  194 tensors
0.00.024.537 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.537 I print_info: file format = GGUF V3 (latest)
0.00.024.538 I print_info: file type   = Q6_K
0.00.024.539 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.039 I load: special tokens cache size = 25
0.00.049.797 I load: token to piece cache size = 0.2984 MB
0.00.049.800 I print_info: arch             = gptneox
0.00.049.800 I print_info: vocab_only       = 0
0.00.049.801 I print_info: n_ctx_train      = 2048
0.00.049.801 I print_info: n_embd           = 2048
0.00.049.801 I print_info: n_layer          = 24
0.00.049.804 I print_info: n_head           = 16
0.00.049.804 I print_info: n_head_kv        = 16
0.00.049.805 I print_info: n_rot            = 32
0.00.049.805 I print_info: n_swa            = 0
0.00.049.805 I print_info: n_embd_head_k    = 128
0.00.049.805 I print_info: n_embd_head_v    = 128
0.00.049.806 I print_info: n_gqa            = 1
0.00.049.807 I print_info: n_embd_k_gqa     = 2048
0.00.049.807 I print_info: n_embd_v_gqa     = 2048
0.00.049.808 I print_info: f_norm_eps       = 1.0e-05
0.00.049.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.813 I print_info: f_logit_scale    = 0.0e+00
0.00.049.814 I print_info: n_ff             = 8192
0.00.049.814 I print_info: n_expert         = 0
0.00.049.814 I print_info: n_expert_used    = 0
0.00.049.815 I print_info: causal attn      = 1
0.00.049.815 I print_info: pooling type     = 0
0.00.049.815 I print_info: rope type        = 2
0.00.049.816 I print_info: rope scaling     = linear
0.00.049.816 I print_info: freq_base_train  = 10000.0
0.00.049.817 I print_info: freq_scale_train = 1
0.00.049.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.817 I print_info: rope_finetuned   = unknown
0.00.049.819 I print_info: ssm_d_conv       = 0
0.00.049.819 I print_info: ssm_d_inner      = 0
0.00.049.819 I print_info: ssm_d_state      = 0
0.00.049.819 I print_info: ssm_dt_rank      = 0
0.00.049.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.819 I print_info: model type       = 1.4B
0.00.049.820 I print_info: model params     = 1.41 B
0.00.049.820 I print_info: general.name     = 1.4B
0.00.049.821 I print_info: vocab type       = BPE
0.00.049.821 I print_info: n_vocab          = 50304
0.00.049.821 I print_info: n_merges         = 50009
0.00.049.821 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.822 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.822 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.822 I print_info: LF token         = 128 'Ä'
0.00.049.822 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.822 I print_info: max token length = 1024
0.00.051.803 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.803 I load_tensors: offloading output layer to GPU
0.00.051.803 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.813 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.814 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.067 I llama_init_from_model: n_seq_max     = 1
0.00.052.068 I llama_init_from_model: n_ctx         = 128
0.00.052.068 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.069 I llama_init_from_model: n_batch       = 128
0.00.052.069 I llama_init_from_model: n_ubatch      = 128
0.00.052.069 I llama_init_from_model: flash_attn    = 0
0.00.052.069 I llama_init_from_model: freq_base     = 10000.0
0.00.052.069 I llama_init_from_model: freq_scale    = 1
0.00.052.070 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.070 I ggml_metal_init: allocating
0.00.052.073 I ggml_metal_init: found device: Apple M4
0.00.052.075 I ggml_metal_init: picking default device: Apple M4
0.00.052.639 I ggml_metal_init: using embedded metal library
0.00.054.941 I ggml_metal_init: GPU name:   Apple M4
0.00.054.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.943 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.944 I ggml_metal_init: simdgroup reduction   = true
0.00.054.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.944 I ggml_metal_init: has bfloat            = true
0.00.054.944 I ggml_metal_init: use bfloat            = true
0.00.054.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.945 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.497 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.501 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.415 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.417 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.417 I llama_init_from_model: graph nodes  = 967
0.00.066.417 I llama_init_from_model: graph splits = 2
0.00.066.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.322.453 I 
0.00.322.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.322.504 I perplexity: tokenizing the input ..
0.00.329.857 I perplexity: tokenization took 7.352 ms
0.00.329.866 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.470.210 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.471.410 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.471.446 I llama_perf_context_print:        load time =     313.52 ms
0.00.471.448 I llama_perf_context_print: prompt eval time =     140.10 ms /   128 tokens (    1.09 ms per token,   913.60 tokens per second)
0.00.471.448 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.471.449 I llama_perf_context_print:       total time =     148.99 ms /   129 tokens
0.00.471.925 I ggml_metal_free: deallocating

real	0m0.485s
user	0m0.077s
sys	0m0.059s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4496 (466300fe)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14310a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14310ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14310b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14310b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14310bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14310c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14310c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14310cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14310d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14310d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14310dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14310e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14310ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14310f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14310fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143110430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143110b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143111270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143111990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143112160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143112880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143112fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1431136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143113f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143114680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143114940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143114f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143115bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143116100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1431163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143116860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143116b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1431173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1431178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143117bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143118050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1431184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143118990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143118e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1431192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143119770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143119c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14311a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14311a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14311a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14311ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14311b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14311bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14311c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14311c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14311cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14311d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14311dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14311e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14311e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14311ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14311f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14311f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14311fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1431203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143120660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143120b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143120fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143121440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1431218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143121d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143122220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1431226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143122b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143123000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1431234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143123940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143123de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143124330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143124880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143124dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143125320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143125870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143125dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143126310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143126860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143126db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143127300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143127850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143127da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1431282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143128840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143128d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1431292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143129830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143129d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14312a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14312a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14312ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14312b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14312b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14312bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14311ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14312c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14312c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14312ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14312d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14312d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14312dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14312e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14312e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14312eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14312f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14312f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14312fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1431303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143130940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143130e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143131330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1431317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143131c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143132110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1431325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143132a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143132ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143133390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143133830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143133cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143134170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143134610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143134ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143134f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1431353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143135890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143135d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1431361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143136670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143136b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143136fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143137450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1431378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143137d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143138230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1431386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143138b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143139010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1431394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143139950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143139df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14313a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14313a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14313abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14313b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14313b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14313b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14313be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14313c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14313c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14313cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14313d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14313d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14313da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14313deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14313e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14313e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14313ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14313f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14313f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14313fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14313ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1431403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143140850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143140cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143141190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143141630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143141ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143141f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143142410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1431428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143142d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1431431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143143690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143143b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143143fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143144470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143144910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143144db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143145250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1431456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143145b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143146030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1431464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143146970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143146e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1431472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143147750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143147bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143148090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1431485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143148b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143149080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1431495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143149890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143149ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14314a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14314aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14314b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14314b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14314ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14314c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14314c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14314ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14314d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14314d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14314dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14314e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14314e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14314ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14314f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14314f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14314fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143150390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1431508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143150e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143151380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1431518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143151e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143152370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1431528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143152e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143153360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1431538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143153e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143154350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1431548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143154df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143155340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143155890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143155de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143156330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143156880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143156dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143157320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143157870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143157dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143158310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143158860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143158db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143159300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143159850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143159da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14315a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14315a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14315ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14315b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14315b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14315bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14315c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14315c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14315cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14315d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14315d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14315dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14315e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14315e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14315ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14315f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14315f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14315fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143160290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1431607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143160d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1431611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143161670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143161b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143161fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143162450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1431628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143162d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143163230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1431636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143163b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143164010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1431644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143164950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143164df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143165290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1431657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143165f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143166620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143166d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143167460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143167720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143167f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1431681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1431687e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143168490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14314a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143149b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14314a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14311d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14311d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14311f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14314c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143114c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14311b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14311c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14311c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14311aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14311cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143113c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14311fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14312c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1431679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143116de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1431170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14314c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14314ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143115210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1431154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143115790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143168c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143168f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1431691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143169480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143169740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143169a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143169cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143169f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14316a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14316a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14316a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14316aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14316ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14316b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14316b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14316b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14316b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14316bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14316bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14316c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14316c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14316c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14316c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14316cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14316ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14316d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14316d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14316d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14316d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14316dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14316dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14316e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14316e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14316e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14316e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14316ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14316ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14316f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14316f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14316f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14316fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14316fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14316ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143170280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143170540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143170800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143170ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143170d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143171040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143171300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1431715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143171880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143171b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143171e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1431720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143172380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143172640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143172900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143172bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143172e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143173140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143173400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1431736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143173980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143173c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143173f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1431741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143174480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143174740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143174a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143174cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143174f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143175240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143175500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1431757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143175a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143175d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143176000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1431762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143176580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143176840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143176b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143176dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143177080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143177340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143177600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1431778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143177b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143177e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143178100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1431783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143178680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143178940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143178c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143178ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143179180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143179440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143179700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1431799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143179c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143179f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14317a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14317a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14317a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14317aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14317ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14317afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14317b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14317b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14317b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14317bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14317bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14317c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14317c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14317c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14317c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14317cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14317ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14317d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14317d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14317d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14317d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14317dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14317de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14317e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14317e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14317e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14317e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14317ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14317ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14317f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14317f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14317f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14317fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14317fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14317ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143180240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143180500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1431807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143180a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143180d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143181000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1431812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143181580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143181840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143181b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143181dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143182080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143182340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143182600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1431828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143182b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143182e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143183100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1431833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143183680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143183940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143183c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143183ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143184180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143184440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143184700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1431849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143184c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143184f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143185200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1431854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143185780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143185a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143185d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143185fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143186280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143186540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143186800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143186ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143186d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143187040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143187300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1431875c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143187880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143187b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143187e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1431880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143188380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143188820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143188fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143189290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143189550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1431899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143189e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14318a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14318a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14318ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14318aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14318b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14318b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14318bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14318c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14318c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14318ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14318cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14318d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14318d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14318dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14318e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14318e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14318e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14318ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14318f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14318f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14318fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14318ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143190440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1431908b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143190d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143191190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143191600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143191a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143191ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143192350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1431927c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143192c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1431930a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143193510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143193980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143193df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143194260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1431946d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143194b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143194fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143195420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143195890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143195d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143196170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1431965e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143196a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143196ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143197330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1431977a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143197c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143198080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1431984f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143198960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143198dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143199240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1431996b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143199b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143199f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14319a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14319a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14319ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14319b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14319b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14319ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14319bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14319c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14319c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14319cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14319d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14319dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14319e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14319ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14319ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14319f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14319f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14319ff40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14319ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14319fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14319f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1431a03a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1431a0660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1431a0920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1431a0be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1431a0ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1431a1160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1431a1420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1431a16e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1431a19a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1431a1f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1431a2540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1431a2b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1431a2e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1431a30f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1431a33b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1431a3670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1431a3930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1431a3bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1431a3eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1431a4170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1431a4430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1431a46f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1431a49b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1431a4c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1431a4f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1431a51f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1431a54b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1431a5770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1431a5a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1431a5cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1431a5fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1431a6270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1431a6530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1431a67f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1431a6ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1431a6d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1431a7030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1431a72f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1431a75b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1431a7870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1431a7b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1431a7df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1431a80b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1431a8370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1431a8630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1431a88f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1431a8bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1431a8e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1431a9130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1431a93f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1431a96b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1431a9970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1431a9c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1431a9ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1431aa1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1431aa470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1431aa730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1431aa9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1431aacb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1431aaf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1431ab230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1431ab4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1431ab7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1431aba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1431abd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1431abff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1431ac2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1431ac570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1431ac830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1431acaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1431acdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1431ad070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1431ad330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1431ad5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1431ad8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1431adb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1431ade30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1431ae0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1431ae3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1431ae670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1431ae930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1431aebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1431aeeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1431af170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1431af430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1431af6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1431af9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1431afc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1431aff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1431b01f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1431b04b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1431b0770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1431b0a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1431b0cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1431b0fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1431b1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1431b1530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1431b17f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1431b1ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1431b1d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1431b2030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1431b22f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1431b25b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1431b2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1431b2b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1431b2df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1431b30b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1431b3370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1431b3630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1431b38f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1431b3bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1431b3e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1431b4130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1431b43f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1431b46b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1431b4970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1431b4c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1431b4ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1431b51b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1431b5470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1431b5730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1431b59f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1431b5cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1431b5f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1431b6230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1431b64f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1431b67b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1431b6a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1431b6d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1431b6ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1431b72b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1431b7570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1431b7830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1431b7af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1431b7db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1431b8070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1431b8330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1431b85f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1431b88b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1431b8b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1431b8e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1431b90f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1431b93b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1431b9670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1431b9930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1431b9bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1431b9eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1431ba170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1431ba430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1431ba6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1431ba9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1431bac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1431baf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1431bb1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1431bb4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1431bb770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1431bba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1431bbcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1431bbfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1431bc270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1431bc530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1431bc7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1431bcab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1431bcd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1431bd030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1431bd2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1431bd5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1431bd870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1431bdb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1431bddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1431be0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1431be370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1431be630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1431be8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1431bebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1431bee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1431bf130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1431bf3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1431bf6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1431bf970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1431bfc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1431bfef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1431c01b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1431c0470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1431c0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1431c09f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1431c0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1431c0f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1431c1230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1431c14f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1431c17b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1431c1a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1431c1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1431c1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1431c22b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1431c2570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1431c2830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1431c2af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1431c2db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1431c3070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1431c3330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1431c35f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1431c38b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1431c3b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1431c3e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1431c40f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1431c43b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1431c4980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1431c4c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1431c4f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1431c51c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1431c5480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1431c5740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1431c5a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1431c5cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1431c5f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1431c6240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1431c6500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1431c67c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1431c6a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1431c6d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1431c7000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1431c72c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1431c7580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1431c7840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1431c7b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1431c7dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1431c8080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1431c8340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1431c8600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1431c88c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1431c8b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1431c8e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1431c9100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1431c93c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1431c9680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1431c9940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1431c9c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1431c9ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1431ca180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1431ca440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1431ca700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1431ca9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1431cac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1431caf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1431cb200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1431cb4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1431cb780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1431cba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1431cbd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1431cbfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1431cc280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1431cc540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1431cc800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1431ccac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1431ccd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1431cd040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1431cd300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1431cd5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1431cd880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1431cdb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1431cde00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1431ce0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1431ce380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1431ce640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1431ce900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1431cebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1431cee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1431cf140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1431cf540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1431cf800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1431cfac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1431cff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1431d03a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1431d0810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1431d0c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1431d10f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1431d1560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1431d19d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1431d1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1431d29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1431d30d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1431d37f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1431d3f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1431d41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1431d4490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1431d49c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1431d4e30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.862s
user	0m0.299s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4496 (466300fe)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f0a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f0ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f0b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f0b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f0bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f0c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f0cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f0d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f0f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f11990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f12160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f12fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f14680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f14940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f16860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f17bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f1a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f1cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f1fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f29d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f2a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f2ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f2b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f2bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f2c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f34610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f36fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f39010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f3b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f3d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f3ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f41190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f43b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f44db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f45b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f46030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f46970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f48b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f49890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f49ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f4ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f4c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f4fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f51380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f53360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f5a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f5ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f5b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f5b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f5bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f5c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f5cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f5d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f5dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f5e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f5ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f5f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f5fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f60290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f61670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f62d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f63b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f64010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f64950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f64df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f65290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f65f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f66620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f66d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f67460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f687e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.277 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123904f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123905370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1239057e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123905c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1239060c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123906530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1239069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123906e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123907280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1239076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123907b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1239081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123908d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1239094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123909cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12390a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12390ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12390b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12390b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12390c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12390c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12390cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12390d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12390dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12390e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12390e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12390ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12390eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12390f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12390f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12390fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123910120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123910590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123910850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123910cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123911130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1239115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123911a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123911e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1239122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123912760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123912bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123913040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1239134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123913920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123913d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123914200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123914670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123914ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123914f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1239153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123915830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123915ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123916110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123916580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1239169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123916f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123917460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1239178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123917d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1239181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123918620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123918a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123918f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123919370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1239197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123919c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12391a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12391a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12391a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12391ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12391b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12391b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12391bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12391bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12391c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12391c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12391cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12391d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12391d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12391da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12391dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12391e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12391e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12391ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12391f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12391f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12391f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12391fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123920260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1239206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123920b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123920fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123921420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123921890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123921d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123922170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1239225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123922a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123922ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123923330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1239237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123923c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123924080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1239244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123924960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123924dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123925240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1239256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123925b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123925f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123926400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123926870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123926ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123927150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1239275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123927a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123927ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123928310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123928780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123928bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123929060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1239294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123929940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123929db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12392a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12392a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12392ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12392af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12392b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12392b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12392bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12392c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12392c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12392ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12392ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12392d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12392d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12392dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12392e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12392e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12392e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12392ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12392f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12392f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12392fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12392ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1239303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123930830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123930ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123931110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123931580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1239319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123931e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1239322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123932740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123932bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123933020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123933490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123933900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123933d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1239341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123934650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123934ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123934f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1239353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123935fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123936290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123936550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1239369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123936e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1239372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123937710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123937b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123937ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123938460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1239388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123938d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1239391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123939620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123939a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123939f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12393a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12393a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12393ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12393b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12393b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12393b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12393be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12393c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12393c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12393cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12393cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12393d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12393d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12393dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12393e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12393e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12393ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12393eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12393f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12393f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12393fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123940230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1239406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123940b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123940f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1239413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123941910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123941e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123942990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123942c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123943210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1239437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123943d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123944350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123944910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123944ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123945490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123945a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123946010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1239465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123946b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123947150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123947710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123947cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123948290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123948850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123948e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1239493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123949990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123949f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12394a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12394aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12394b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12394b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12394bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12394c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12394c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12394cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12394d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12394d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12394de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12394e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12394ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12394efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12394f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12394fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123950110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1239506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123950c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123951250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123951810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123951dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123952390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123952950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123952f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1239534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123953a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123954050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123954610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123954bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123955190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123955750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123955d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1239562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123956890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123956e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123957350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123957850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123957d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123958250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123958750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123958c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123959150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123959650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123959b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12395a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12395a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12395aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12395af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12395b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12395b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12395c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12395ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12395d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12395d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12395db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12395e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12395e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12395ec40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f68490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f49b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f1d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f14c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f16de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f68c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f68f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f69480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f69740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f69cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f69f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f6a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f6a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f6a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f6aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f6ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f6b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f6b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f6b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f6b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f6bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f6bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f6c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f6c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f6c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f6c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f6cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f6ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f6d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f6d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f6d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f6d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f6dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f6dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f6e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f6e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f6e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f6e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f6ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f6ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f6f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f6f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f6f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f6fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f6fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f6ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f70280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f70540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f70800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f70ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f70d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f71040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f71300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f71880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f71b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f71e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f72380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f72640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f72900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f72bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f72e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f73140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f73400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f73980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f73c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f73f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f74480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f74740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f74a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f74cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f74f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f75240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f75500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f75a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f75d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f76000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f76580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f76840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f76b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f76dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f77080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f77340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f77600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f77b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f77e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f78100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f78680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f78940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f78c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f78ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f79180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f79440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f79700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f79c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f79f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f7a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f7a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f7a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f7aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f7ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f7afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f7b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f7b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f7b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f7bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f7bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f7c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f7c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f7c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f7c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f7cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f7ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f7d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f7d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f7d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f7d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f7dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f7de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f7e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f7e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f7e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f7e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f7ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f7ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f7f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f7f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f7f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f7fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f7fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f7ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f80240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f80500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f80a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f80d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f81000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f81580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f81840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f81b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f81dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f82080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f82340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f82600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f82b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f82e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f83100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f83680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f83940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f83c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f83ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f84180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f84440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f84700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f84c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f84f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f85200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f85780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f85a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f85d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f85fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f86280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f86540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f86800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f86ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f86d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f87040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f87300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f875c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f87880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f87b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f87e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f88380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f88640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f88c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f88ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f89190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f89450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f899a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f89ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f8a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f8a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f8aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f8b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f8b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f8bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f8c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f8c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f8cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f8d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f8d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f8deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f8e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f8e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f8eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f8f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f8f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f8fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f903e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f90930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f90e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f913d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f91920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f91e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f923c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f92910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f92e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f93900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f93e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f943a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f94e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f95390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f958e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f95e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f96380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f968d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f96e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f97370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f978c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f97e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f98360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f988b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f98e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f99350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f998a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f99df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f9a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f9a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f9ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f9b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f9b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f9b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f9ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f9bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f9c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f9c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f9cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f9d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f9d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f9d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f9de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f9e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f9e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f9eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f9efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f9f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121fa0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121fa0850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121fa0f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121fa1230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121fa16a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121fa1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121fa22b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.911s
user	0m0.244s
sys	0m0.129s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.59 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.54 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.16 real         0.69 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
