Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.520s
user	0m0.891s
sys	0m1.218s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-log
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Built target test-arg-parser
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-chat-template
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-gguf-split
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-quantize
[ 84%] Built target llama-parallel
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-passkey
[ 84%] Built target llama-lookup
[ 84%] Built target llama-cli
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-run
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.109s
user	0m6.222s
sys	0m9.566s

main: quantize time =  4429.68 ms
main:    total time =  4429.68 ms

main: quantize time =  3354.20 ms
main:    total time =  3354.20 ms

main: quantize time =  2186.86 ms
main:    total time =  2186.86 ms

main: quantize time =  3447.67 ms
main:    total time =  3447.67 ms

main: quantize time =  2483.16 ms
main:    total time =  2483.16 ms

main: quantize time =  5257.25 ms
main:    total time =  5257.25 ms

main: quantize time =  5859.39 ms
main:    total time =  5859.39 ms

main: quantize time =  6896.68 ms
main:    total time =  6896.68 ms

main: quantize time =  5927.09 ms
main:    total time =  5927.09 ms

main: quantize time =  4486.75 ms
main:    total time =  4486.75 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.139 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.318 I main: llama backend init
0.00.000.326 I main: load the model and apply lora adapter, if any
0.00.043.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.057.005 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.033 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.033 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.034 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.603 I llama_model_loader: - type  f32:  194 tensors
0.00.076.603 I llama_model_loader: - type  f16:   98 tensors
0.00.076.604 I print_info: file format = GGUF V3 (latest)
0.00.076.607 I print_info: file type   = all F32 (guessed)
0.00.076.608 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.417 I load: special tokens cache size = 25
0.00.113.612 I load: token to piece cache size = 0.2984 MB
0.00.113.615 I print_info: arch             = gptneox
0.00.113.616 I print_info: vocab_only       = 0
0.00.113.616 I print_info: n_ctx_train      = 2048
0.00.113.616 I print_info: n_embd           = 2048
0.00.113.616 I print_info: n_layer          = 24
0.00.113.619 I print_info: n_head           = 16
0.00.113.620 I print_info: n_head_kv        = 16
0.00.113.620 I print_info: n_rot            = 32
0.00.113.620 I print_info: n_swa            = 0
0.00.113.621 I print_info: n_embd_head_k    = 128
0.00.113.622 I print_info: n_embd_head_v    = 128
0.00.113.622 I print_info: n_gqa            = 1
0.00.113.623 I print_info: n_embd_k_gqa     = 2048
0.00.113.624 I print_info: n_embd_v_gqa     = 2048
0.00.113.624 I print_info: f_norm_eps       = 1.0e-05
0.00.113.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.625 I print_info: f_logit_scale    = 0.0e+00
0.00.113.625 I print_info: n_ff             = 8192
0.00.113.626 I print_info: n_expert         = 0
0.00.113.626 I print_info: n_expert_used    = 0
0.00.113.626 I print_info: causal attn      = 1
0.00.113.626 I print_info: pooling type     = 0
0.00.113.626 I print_info: rope type        = 2
0.00.113.627 I print_info: rope scaling     = linear
0.00.113.627 I print_info: freq_base_train  = 10000.0
0.00.113.627 I print_info: freq_scale_train = 1
0.00.113.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.629 I print_info: rope_finetuned   = unknown
0.00.113.629 I print_info: ssm_d_conv       = 0
0.00.113.629 I print_info: ssm_d_inner      = 0
0.00.113.629 I print_info: ssm_d_state      = 0
0.00.113.629 I print_info: ssm_dt_rank      = 0
0.00.113.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.629 I print_info: model type       = 1.4B
0.00.113.630 I print_info: model params     = 1.41 B
0.00.113.630 I print_info: general.name     = 1.4B
0.00.113.630 I print_info: vocab type       = BPE
0.00.113.631 I print_info: n_vocab          = 50304
0.00.113.632 I print_info: n_merges         = 50009
0.00.113.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.633 I print_info: LF token         = 128 'Ä'
0.00.113.633 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.633 I print_info: max token length = 1024
0.00.116.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.116.298 I load_tensors: offloading output layer to GPU
0.00.116.298 I load_tensors: offloaded 25/25 layers to GPU
0.00.116.317 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.116.318 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.116.618 I llama_init_from_model: n_seq_max     = 1
0.00.116.619 I llama_init_from_model: n_ctx         = 2048
0.00.116.619 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.116.619 I llama_init_from_model: n_batch       = 2048
0.00.116.620 I llama_init_from_model: n_ubatch      = 512
0.00.116.620 I llama_init_from_model: flash_attn    = 0
0.00.116.620 I llama_init_from_model: freq_base     = 10000.0
0.00.116.620 I llama_init_from_model: freq_scale    = 1
0.00.116.621 I ggml_metal_init: allocating
0.00.116.624 I ggml_metal_init: found device: Apple M4
0.00.116.626 I ggml_metal_init: picking default device: Apple M4
0.00.117.192 I ggml_metal_init: using embedded metal library
0.00.170.527 I ggml_metal_init: GPU name:   Apple M4
0.00.170.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.170.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.170.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.170.534 I ggml_metal_init: simdgroup reduction   = true
0.00.170.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.170.534 I ggml_metal_init: has bfloat            = true
0.00.170.534 I ggml_metal_init: use bfloat            = true
0.00.170.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.170.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.287.925 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.310.444 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.310.450 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.310.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.311.387 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.311.388 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.311.388 I llama_init_from_model: graph nodes  = 967
0.00.311.388 I llama_init_from_model: graph splits = 2
0.00.311.391 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.311.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.311.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.773 I main: llama threadpool init, n_threads = 4
0.00.391.809 I 
0.00.391.841 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.841 I 
0.00.391.903 I sampler seed: 1234
0.00.391.909 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.391.936 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.391.938 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.391.938 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.218.890 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.02.218.890 I llama_perf_context_print:        load time =     347.26 ms
0.02.218.891 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.83 tokens per second)
0.02.218.892 I llama_perf_context_print:        eval time =    1780.26 ms /    63 runs   (   28.26 ms per token,    35.39 tokens per second)
0.02.218.892 I llama_perf_context_print:       total time =    1828.46 ms /    70 tokens
0.02.219.157 I ggml_metal_free: deallocating

real	0m2.534s
user	0m0.157s
sys	0m0.108s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.084 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.319 I llama_model_loader: - type  f32:  194 tensors
0.00.036.319 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.320 I print_info: file format = GGUF V3 (latest)
0.00.036.320 I print_info: file type   = Q8_0
0.00.036.322 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.249 I load: special tokens cache size = 25
0.00.063.448 I load: token to piece cache size = 0.2984 MB
0.00.063.452 I print_info: arch             = gptneox
0.00.063.453 I print_info: vocab_only       = 0
0.00.063.453 I print_info: n_ctx_train      = 2048
0.00.063.455 I print_info: n_embd           = 2048
0.00.063.455 I print_info: n_layer          = 24
0.00.063.460 I print_info: n_head           = 16
0.00.063.461 I print_info: n_head_kv        = 16
0.00.063.461 I print_info: n_rot            = 32
0.00.063.462 I print_info: n_swa            = 0
0.00.063.462 I print_info: n_embd_head_k    = 128
0.00.063.462 I print_info: n_embd_head_v    = 128
0.00.063.463 I print_info: n_gqa            = 1
0.00.063.463 I print_info: n_embd_k_gqa     = 2048
0.00.063.464 I print_info: n_embd_v_gqa     = 2048
0.00.063.465 I print_info: f_norm_eps       = 1.0e-05
0.00.063.466 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.468 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.468 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.468 I print_info: f_logit_scale    = 0.0e+00
0.00.063.469 I print_info: n_ff             = 8192
0.00.063.469 I print_info: n_expert         = 0
0.00.063.469 I print_info: n_expert_used    = 0
0.00.063.469 I print_info: causal attn      = 1
0.00.063.469 I print_info: pooling type     = 0
0.00.063.470 I print_info: rope type        = 2
0.00.063.471 I print_info: rope scaling     = linear
0.00.063.472 I print_info: freq_base_train  = 10000.0
0.00.063.472 I print_info: freq_scale_train = 1
0.00.063.472 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.472 I print_info: rope_finetuned   = unknown
0.00.063.473 I print_info: ssm_d_conv       = 0
0.00.063.473 I print_info: ssm_d_inner      = 0
0.00.063.473 I print_info: ssm_d_state      = 0
0.00.063.473 I print_info: ssm_dt_rank      = 0
0.00.063.473 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.473 I print_info: model type       = 1.4B
0.00.063.474 I print_info: model params     = 1.41 B
0.00.063.479 I print_info: general.name     = 1.4B
0.00.063.482 I print_info: vocab type       = BPE
0.00.063.482 I print_info: n_vocab          = 50304
0.00.063.482 I print_info: n_merges         = 50009
0.00.063.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.483 I print_info: LF token         = 128 'Ä'
0.00.063.483 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.484 I print_info: max token length = 1024
0.00.065.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.979 I load_tensors: offloading output layer to GPU
0.00.065.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.991 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.992 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.326 I llama_init_from_model: n_seq_max     = 1
0.00.066.327 I llama_init_from_model: n_ctx         = 2048
0.00.066.328 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.066.328 I llama_init_from_model: n_batch       = 2048
0.00.066.328 I llama_init_from_model: n_ubatch      = 512
0.00.066.328 I llama_init_from_model: flash_attn    = 0
0.00.066.329 I llama_init_from_model: freq_base     = 10000.0
0.00.066.329 I llama_init_from_model: freq_scale    = 1
0.00.066.329 I ggml_metal_init: allocating
0.00.066.333 I ggml_metal_init: found device: Apple M4
0.00.066.335 I ggml_metal_init: picking default device: Apple M4
0.00.066.992 I ggml_metal_init: using embedded metal library
0.00.069.628 I ggml_metal_init: GPU name:   Apple M4
0.00.069.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.631 I ggml_metal_init: simdgroup reduction   = true
0.00.069.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.631 I ggml_metal_init: has bfloat            = true
0.00.069.631 I ggml_metal_init: use bfloat            = true
0.00.069.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.160 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.459 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.468 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.496 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.587 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.108.589 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.108.590 I llama_init_from_model: graph nodes  = 967
0.00.108.590 I llama_init_from_model: graph splits = 2
0.00.108.594 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.578.516 I main: llama threadpool init, n_threads = 4
0.01.578.551 I 
0.01.578.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.578.576 I 
0.01.578.784 I sampler seed: 1234
0.01.578.788 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.578.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.578.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.578.845 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.677.004 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46679.82 tokens per second)
0.02.677.005 I llama_perf_context_print:        load time =    1567.68 ms
0.02.677.006 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.73 tokens per second)
0.02.677.007 I llama_perf_context_print:        eval time =    1051.98 ms /    63 runs   (   16.70 ms per token,    59.89 tokens per second)
0.02.677.007 I llama_perf_context_print:       total time =    1099.47 ms /    70 tokens
0.02.677.275 I ggml_metal_free: deallocating

real	0m2.696s
user	0m0.115s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.648 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.258 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.272 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.277 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.327 I llama_model_loader: - type  f32:  194 tensors
0.00.027.327 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.328 I print_info: file format = GGUF V3 (latest)
0.00.027.329 I print_info: file type   = Q4_0
0.00.027.329 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.703 I load: special tokens cache size = 25
0.00.052.623 I load: token to piece cache size = 0.2984 MB
0.00.052.626 I print_info: arch             = gptneox
0.00.052.626 I print_info: vocab_only       = 0
0.00.052.627 I print_info: n_ctx_train      = 2048
0.00.052.627 I print_info: n_embd           = 2048
0.00.052.627 I print_info: n_layer          = 24
0.00.052.630 I print_info: n_head           = 16
0.00.052.634 I print_info: n_head_kv        = 16
0.00.052.634 I print_info: n_rot            = 32
0.00.052.634 I print_info: n_swa            = 0
0.00.052.634 I print_info: n_embd_head_k    = 128
0.00.052.634 I print_info: n_embd_head_v    = 128
0.00.052.635 I print_info: n_gqa            = 1
0.00.052.636 I print_info: n_embd_k_gqa     = 2048
0.00.052.637 I print_info: n_embd_v_gqa     = 2048
0.00.052.637 I print_info: f_norm_eps       = 1.0e-05
0.00.052.638 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.638 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.638 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.638 I print_info: f_logit_scale    = 0.0e+00
0.00.052.639 I print_info: n_ff             = 8192
0.00.052.639 I print_info: n_expert         = 0
0.00.052.639 I print_info: n_expert_used    = 0
0.00.052.639 I print_info: causal attn      = 1
0.00.052.640 I print_info: pooling type     = 0
0.00.052.640 I print_info: rope type        = 2
0.00.052.640 I print_info: rope scaling     = linear
0.00.052.641 I print_info: freq_base_train  = 10000.0
0.00.052.641 I print_info: freq_scale_train = 1
0.00.052.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.641 I print_info: rope_finetuned   = unknown
0.00.052.642 I print_info: ssm_d_conv       = 0
0.00.052.644 I print_info: ssm_d_inner      = 0
0.00.052.644 I print_info: ssm_d_state      = 0
0.00.052.644 I print_info: ssm_dt_rank      = 0
0.00.052.644 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.645 I print_info: model type       = 1.4B
0.00.052.645 I print_info: model params     = 1.41 B
0.00.052.645 I print_info: general.name     = 1.4B
0.00.052.646 I print_info: vocab type       = BPE
0.00.052.646 I print_info: n_vocab          = 50304
0.00.052.646 I print_info: n_merges         = 50009
0.00.052.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.647 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.647 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.647 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.647 I print_info: LF token         = 128 'Ä'
0.00.052.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.648 I print_info: max token length = 1024
0.00.054.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.623 I load_tensors: offloading output layer to GPU
0.00.054.623 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.633 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.635 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.962 I llama_init_from_model: n_seq_max     = 1
0.00.054.962 I llama_init_from_model: n_ctx         = 2048
0.00.054.962 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.963 I llama_init_from_model: n_batch       = 2048
0.00.054.963 I llama_init_from_model: n_ubatch      = 512
0.00.054.963 I llama_init_from_model: flash_attn    = 0
0.00.054.963 I llama_init_from_model: freq_base     = 10000.0
0.00.054.964 I llama_init_from_model: freq_scale    = 1
0.00.054.964 I ggml_metal_init: allocating
0.00.054.967 I ggml_metal_init: found device: Apple M4
0.00.054.970 I ggml_metal_init: picking default device: Apple M4
0.00.055.468 I ggml_metal_init: using embedded metal library
0.00.057.883 I ggml_metal_init: GPU name:   Apple M4
0.00.057.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.885 I ggml_metal_init: simdgroup reduction   = true
0.00.057.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.886 I ggml_metal_init: has bfloat            = true
0.00.057.886 I ggml_metal_init: use bfloat            = true
0.00.057.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.660 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.606 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.612 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.632 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.715 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.717 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.718 I llama_init_from_model: graph nodes  = 967
0.00.088.718 I llama_init_from_model: graph splits = 2
0.00.088.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.142 I main: llama threadpool init, n_threads = 4
0.00.651.185 I 
0.00.651.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.205 I 
0.00.651.434 I sampler seed: 1234
0.00.651.438 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.651.449 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.651.449 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.651.449 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.324.383 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.324.383 I llama_perf_context_print:        load time =     639.62 ms
0.01.324.384 I llama_perf_context_print: prompt eval time =      42.55 ms /     7 tokens (    6.08 ms per token,   164.52 tokens per second)
0.01.324.385 I llama_perf_context_print:        eval time =     627.36 ms /    63 runs   (    9.96 ms per token,   100.42 tokens per second)
0.01.324.385 I llama_perf_context_print:       total time =     674.11 ms /    70 tokens
0.01.324.611 I ggml_metal_free: deallocating

real	0m1.344s
user	0m0.111s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.464 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.154 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.877 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.878 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.879 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.879 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.879 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.880 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.880 I llama_model_loader: - type  f32:  194 tensors
0.00.025.881 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.881 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.881 I print_info: file format = GGUF V3 (latest)
0.00.025.882 I print_info: file type   = Q4_1
0.00.025.883 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.495 I load: special tokens cache size = 25
0.00.050.614 I load: token to piece cache size = 0.2984 MB
0.00.050.617 I print_info: arch             = gptneox
0.00.050.617 I print_info: vocab_only       = 0
0.00.050.617 I print_info: n_ctx_train      = 2048
0.00.050.617 I print_info: n_embd           = 2048
0.00.050.618 I print_info: n_layer          = 24
0.00.050.621 I print_info: n_head           = 16
0.00.050.622 I print_info: n_head_kv        = 16
0.00.050.622 I print_info: n_rot            = 32
0.00.050.622 I print_info: n_swa            = 0
0.00.050.622 I print_info: n_embd_head_k    = 128
0.00.050.622 I print_info: n_embd_head_v    = 128
0.00.050.624 I print_info: n_gqa            = 1
0.00.050.625 I print_info: n_embd_k_gqa     = 2048
0.00.050.626 I print_info: n_embd_v_gqa     = 2048
0.00.050.626 I print_info: f_norm_eps       = 1.0e-05
0.00.050.627 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.627 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.627 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.627 I print_info: f_logit_scale    = 0.0e+00
0.00.050.628 I print_info: n_ff             = 8192
0.00.050.628 I print_info: n_expert         = 0
0.00.050.628 I print_info: n_expert_used    = 0
0.00.050.628 I print_info: causal attn      = 1
0.00.050.629 I print_info: pooling type     = 0
0.00.050.629 I print_info: rope type        = 2
0.00.050.629 I print_info: rope scaling     = linear
0.00.050.629 I print_info: freq_base_train  = 10000.0
0.00.050.630 I print_info: freq_scale_train = 1
0.00.050.630 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.630 I print_info: rope_finetuned   = unknown
0.00.050.632 I print_info: ssm_d_conv       = 0
0.00.050.633 I print_info: ssm_d_inner      = 0
0.00.050.633 I print_info: ssm_d_state      = 0
0.00.050.633 I print_info: ssm_dt_rank      = 0
0.00.050.633 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.633 I print_info: model type       = 1.4B
0.00.050.633 I print_info: model params     = 1.41 B
0.00.050.634 I print_info: general.name     = 1.4B
0.00.050.634 I print_info: vocab type       = BPE
0.00.050.634 I print_info: n_vocab          = 50304
0.00.050.635 I print_info: n_merges         = 50009
0.00.050.635 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.636 I print_info: LF token         = 128 'Ä'
0.00.050.636 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.636 I print_info: max token length = 1024
0.00.052.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.576 I load_tensors: offloading output layer to GPU
0.00.052.577 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.587 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.588 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.892 I llama_init_from_model: n_seq_max     = 1
0.00.052.893 I llama_init_from_model: n_ctx         = 2048
0.00.052.893 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.893 I llama_init_from_model: n_batch       = 2048
0.00.052.893 I llama_init_from_model: n_ubatch      = 512
0.00.052.893 I llama_init_from_model: flash_attn    = 0
0.00.052.894 I llama_init_from_model: freq_base     = 10000.0
0.00.052.894 I llama_init_from_model: freq_scale    = 1
0.00.052.895 I ggml_metal_init: allocating
0.00.052.898 I ggml_metal_init: found device: Apple M4
0.00.052.900 I ggml_metal_init: picking default device: Apple M4
0.00.053.413 I ggml_metal_init: using embedded metal library
0.00.055.746 I ggml_metal_init: GPU name:   Apple M4
0.00.055.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.748 I ggml_metal_init: simdgroup reduction   = true
0.00.055.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.749 I ggml_metal_init: has bfloat            = true
0.00.055.749 I ggml_metal_init: use bfloat            = true
0.00.055.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.648 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.240 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.311 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.312 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.313 I llama_init_from_model: graph nodes  = 967
0.00.085.313 I llama_init_from_model: graph splits = 2
0.00.085.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.951 I main: llama threadpool init, n_threads = 4
0.00.738.994 I 
0.00.739.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.019 I 
0.00.739.247 I sampler seed: 1234
0.00.739.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.300 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.302 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.303 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.460.759 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63734.29 tokens per second)
0.01.460.760 I llama_perf_context_print:        load time =     728.59 ms
0.01.460.761 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.44 tokens per second)
0.01.460.761 I llama_perf_context_print:        eval time =     675.46 ms /    63 runs   (   10.72 ms per token,    93.27 tokens per second)
0.01.460.762 I llama_perf_context_print:       total time =     722.71 ms /    70 tokens
0.01.461.016 I ggml_metal_free: deallocating

real	0m1.478s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.853 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.860 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.864 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.866 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.742 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.475 I llama_model_loader: - type  f32:  194 tensors
0.00.025.475 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.475 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.476 I print_info: file format = GGUF V3 (latest)
0.00.025.476 I print_info: file type   = Q5_0
0.00.025.477 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.990 I load: special tokens cache size = 25
0.00.050.078 I load: token to piece cache size = 0.2984 MB
0.00.050.081 I print_info: arch             = gptneox
0.00.050.082 I print_info: vocab_only       = 0
0.00.050.082 I print_info: n_ctx_train      = 2048
0.00.050.082 I print_info: n_embd           = 2048
0.00.050.082 I print_info: n_layer          = 24
0.00.050.085 I print_info: n_head           = 16
0.00.050.086 I print_info: n_head_kv        = 16
0.00.050.086 I print_info: n_rot            = 32
0.00.050.086 I print_info: n_swa            = 0
0.00.050.086 I print_info: n_embd_head_k    = 128
0.00.050.086 I print_info: n_embd_head_v    = 128
0.00.050.087 I print_info: n_gqa            = 1
0.00.050.088 I print_info: n_embd_k_gqa     = 2048
0.00.050.088 I print_info: n_embd_v_gqa     = 2048
0.00.050.089 I print_info: f_norm_eps       = 1.0e-05
0.00.050.089 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.092 I print_info: f_logit_scale    = 0.0e+00
0.00.050.093 I print_info: n_ff             = 8192
0.00.050.093 I print_info: n_expert         = 0
0.00.050.093 I print_info: n_expert_used    = 0
0.00.050.094 I print_info: causal attn      = 1
0.00.050.094 I print_info: pooling type     = 0
0.00.050.095 I print_info: rope type        = 2
0.00.050.097 I print_info: rope scaling     = linear
0.00.050.097 I print_info: freq_base_train  = 10000.0
0.00.050.097 I print_info: freq_scale_train = 1
0.00.050.098 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.098 I print_info: rope_finetuned   = unknown
0.00.050.098 I print_info: ssm_d_conv       = 0
0.00.050.098 I print_info: ssm_d_inner      = 0
0.00.050.098 I print_info: ssm_d_state      = 0
0.00.050.098 I print_info: ssm_dt_rank      = 0
0.00.050.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.099 I print_info: model type       = 1.4B
0.00.050.099 I print_info: model params     = 1.41 B
0.00.050.100 I print_info: general.name     = 1.4B
0.00.050.100 I print_info: vocab type       = BPE
0.00.050.100 I print_info: n_vocab          = 50304
0.00.050.100 I print_info: n_merges         = 50009
0.00.050.101 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.107 I print_info: LF token         = 128 'Ä'
0.00.050.109 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.109 I print_info: max token length = 1024
0.00.052.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.071 I load_tensors: offloading output layer to GPU
0.00.052.071 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.082 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.083 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.356 I llama_init_from_model: n_seq_max     = 1
0.00.052.357 I llama_init_from_model: n_ctx         = 2048
0.00.052.357 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.357 I llama_init_from_model: n_batch       = 2048
0.00.052.357 I llama_init_from_model: n_ubatch      = 512
0.00.052.357 I llama_init_from_model: flash_attn    = 0
0.00.052.358 I llama_init_from_model: freq_base     = 10000.0
0.00.052.358 I llama_init_from_model: freq_scale    = 1
0.00.052.358 I ggml_metal_init: allocating
0.00.052.361 I ggml_metal_init: found device: Apple M4
0.00.052.363 I ggml_metal_init: picking default device: Apple M4
0.00.052.896 I ggml_metal_init: using embedded metal library
0.00.055.247 I ggml_metal_init: GPU name:   Apple M4
0.00.055.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.250 I ggml_metal_init: simdgroup reduction   = true
0.00.055.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.250 I ggml_metal_init: has bfloat            = true
0.00.055.250 I ggml_metal_init: use bfloat            = true
0.00.055.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.808 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.578 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.585 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.612 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.564 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.566 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.566 I llama_init_from_model: graph nodes  = 967
0.00.085.566 I llama_init_from_model: graph splits = 2
0.00.085.569 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.021 I main: llama threadpool init, n_threads = 4
0.00.799.055 I 
0.00.799.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.104 I 
0.00.799.330 I sampler seed: 1234
0.00.799.334 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.345 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.346 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.580.796 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.580.797 I llama_perf_context_print:        load time =     789.34 ms
0.01.580.798 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.71 tokens per second)
0.01.580.798 I llama_perf_context_print:        eval time =     731.48 ms /    63 runs   (   11.61 ms per token,    86.13 tokens per second)
0.01.580.799 I llama_perf_context_print:       total time =     782.66 ms /    70 tokens
0.01.581.059 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.108s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.479 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.480 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.481 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.485 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.236 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.238 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.239 I llama_model_loader: - type  f32:  194 tensors
0.00.026.239 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.240 I print_info: file format = GGUF V3 (latest)
0.00.026.240 I print_info: file type   = Q5_1
0.00.026.241 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.661 I load: special tokens cache size = 25
0.00.050.731 I load: token to piece cache size = 0.2984 MB
0.00.050.733 I print_info: arch             = gptneox
0.00.050.734 I print_info: vocab_only       = 0
0.00.050.734 I print_info: n_ctx_train      = 2048
0.00.050.734 I print_info: n_embd           = 2048
0.00.050.734 I print_info: n_layer          = 24
0.00.050.737 I print_info: n_head           = 16
0.00.050.738 I print_info: n_head_kv        = 16
0.00.050.738 I print_info: n_rot            = 32
0.00.050.738 I print_info: n_swa            = 0
0.00.050.738 I print_info: n_embd_head_k    = 128
0.00.050.738 I print_info: n_embd_head_v    = 128
0.00.050.739 I print_info: n_gqa            = 1
0.00.050.740 I print_info: n_embd_k_gqa     = 2048
0.00.050.740 I print_info: n_embd_v_gqa     = 2048
0.00.050.743 I print_info: f_norm_eps       = 1.0e-05
0.00.050.744 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.744 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.744 I print_info: f_logit_scale    = 0.0e+00
0.00.050.746 I print_info: n_ff             = 8192
0.00.050.746 I print_info: n_expert         = 0
0.00.050.747 I print_info: n_expert_used    = 0
0.00.050.747 I print_info: causal attn      = 1
0.00.050.747 I print_info: pooling type     = 0
0.00.050.748 I print_info: rope type        = 2
0.00.050.754 I print_info: rope scaling     = linear
0.00.050.756 I print_info: freq_base_train  = 10000.0
0.00.050.756 I print_info: freq_scale_train = 1
0.00.050.757 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.758 I print_info: rope_finetuned   = unknown
0.00.050.758 I print_info: ssm_d_conv       = 0
0.00.050.758 I print_info: ssm_d_inner      = 0
0.00.050.758 I print_info: ssm_d_state      = 0
0.00.050.758 I print_info: ssm_dt_rank      = 0
0.00.050.758 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.759 I print_info: model type       = 1.4B
0.00.050.759 I print_info: model params     = 1.41 B
0.00.050.759 I print_info: general.name     = 1.4B
0.00.050.760 I print_info: vocab type       = BPE
0.00.050.761 I print_info: n_vocab          = 50304
0.00.050.761 I print_info: n_merges         = 50009
0.00.050.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.762 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.762 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.763 I print_info: LF token         = 128 'Ä'
0.00.050.764 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I print_info: max token length = 1024
0.00.052.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.319 I load_tensors: offloading output layer to GPU
0.00.052.319 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.329 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.330 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.594 I llama_init_from_model: n_seq_max     = 1
0.00.052.595 I llama_init_from_model: n_ctx         = 2048
0.00.052.595 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.595 I llama_init_from_model: n_batch       = 2048
0.00.052.595 I llama_init_from_model: n_ubatch      = 512
0.00.052.595 I llama_init_from_model: flash_attn    = 0
0.00.052.596 I llama_init_from_model: freq_base     = 10000.0
0.00.052.596 I llama_init_from_model: freq_scale    = 1
0.00.052.596 I ggml_metal_init: allocating
0.00.052.599 I ggml_metal_init: found device: Apple M4
0.00.052.601 I ggml_metal_init: picking default device: Apple M4
0.00.053.094 I ggml_metal_init: using embedded metal library
0.00.055.401 I ggml_metal_init: GPU name:   Apple M4
0.00.055.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.403 I ggml_metal_init: simdgroup reduction   = true
0.00.055.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.404 I ggml_metal_init: has bfloat            = true
0.00.055.404 I ggml_metal_init: use bfloat            = true
0.00.055.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.057 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.067 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.090 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.067 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.069 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.069 I llama_init_from_model: graph nodes  = 967
0.00.086.070 I llama_init_from_model: graph splits = 2
0.00.086.072 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.199 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.200 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.846.445 I main: llama threadpool init, n_threads = 4
0.00.846.480 I 
0.00.846.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.846.504 I 
0.00.846.733 I sampler seed: 1234
0.00.846.737 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.846.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.846.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.846.797 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.690.162 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.690.163 I llama_perf_context_print:        load time =     835.64 ms
0.01.690.163 I llama_perf_context_print: prompt eval time =      45.11 ms /     7 tokens (    6.44 ms per token,   155.18 tokens per second)
0.01.690.164 I llama_perf_context_print:        eval time =     795.26 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.690.164 I llama_perf_context_print:       total time =     844.60 ms /    70 tokens
0.01.690.434 I ggml_metal_free: deallocating

real	0m1.708s
user	0m0.110s
sys	0m0.162s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.399 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.193 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.239 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.053 I llama_model_loader: - type  f32:  194 tensors
0.00.026.053 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.053 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.054 I print_info: file format = GGUF V3 (latest)
0.00.026.055 I print_info: file type   = Q2_K - Medium
0.00.026.056 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.518 I load: special tokens cache size = 25
0.00.050.486 I load: token to piece cache size = 0.2984 MB
0.00.050.489 I print_info: arch             = gptneox
0.00.050.489 I print_info: vocab_only       = 0
0.00.050.490 I print_info: n_ctx_train      = 2048
0.00.050.490 I print_info: n_embd           = 2048
0.00.050.490 I print_info: n_layer          = 24
0.00.050.493 I print_info: n_head           = 16
0.00.050.494 I print_info: n_head_kv        = 16
0.00.050.494 I print_info: n_rot            = 32
0.00.050.494 I print_info: n_swa            = 0
0.00.050.495 I print_info: n_embd_head_k    = 128
0.00.050.495 I print_info: n_embd_head_v    = 128
0.00.050.496 I print_info: n_gqa            = 1
0.00.050.496 I print_info: n_embd_k_gqa     = 2048
0.00.050.497 I print_info: n_embd_v_gqa     = 2048
0.00.050.498 I print_info: f_norm_eps       = 1.0e-05
0.00.050.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.499 I print_info: f_logit_scale    = 0.0e+00
0.00.050.499 I print_info: n_ff             = 8192
0.00.050.499 I print_info: n_expert         = 0
0.00.050.500 I print_info: n_expert_used    = 0
0.00.050.500 I print_info: causal attn      = 1
0.00.050.500 I print_info: pooling type     = 0
0.00.050.500 I print_info: rope type        = 2
0.00.050.500 I print_info: rope scaling     = linear
0.00.050.501 I print_info: freq_base_train  = 10000.0
0.00.050.501 I print_info: freq_scale_train = 1
0.00.050.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.501 I print_info: rope_finetuned   = unknown
0.00.050.501 I print_info: ssm_d_conv       = 0
0.00.050.502 I print_info: ssm_d_inner      = 0
0.00.050.502 I print_info: ssm_d_state      = 0
0.00.050.502 I print_info: ssm_dt_rank      = 0
0.00.050.502 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.502 I print_info: model type       = 1.4B
0.00.050.503 I print_info: model params     = 1.41 B
0.00.050.503 I print_info: general.name     = 1.4B
0.00.050.503 I print_info: vocab type       = BPE
0.00.050.504 I print_info: n_vocab          = 50304
0.00.050.504 I print_info: n_merges         = 50009
0.00.050.504 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.504 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.504 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.505 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.505 I print_info: LF token         = 128 'Ä'
0.00.050.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.505 I print_info: max token length = 1024
0.00.052.097 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.097 I load_tensors: offloading output layer to GPU
0.00.052.097 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.107 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.108 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.370 I llama_init_from_model: n_seq_max     = 1
0.00.052.370 I llama_init_from_model: n_ctx         = 2048
0.00.052.370 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.371 I llama_init_from_model: n_batch       = 2048
0.00.052.371 I llama_init_from_model: n_ubatch      = 512
0.00.052.371 I llama_init_from_model: flash_attn    = 0
0.00.052.371 I llama_init_from_model: freq_base     = 10000.0
0.00.052.372 I llama_init_from_model: freq_scale    = 1
0.00.052.372 I ggml_metal_init: allocating
0.00.052.375 I ggml_metal_init: found device: Apple M4
0.00.052.377 I ggml_metal_init: picking default device: Apple M4
0.00.052.881 I ggml_metal_init: using embedded metal library
0.00.055.238 I ggml_metal_init: GPU name:   Apple M4
0.00.055.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.240 I ggml_metal_init: simdgroup reduction   = true
0.00.055.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.240 I ggml_metal_init: has bfloat            = true
0.00.055.241 I ggml_metal_init: use bfloat            = true
0.00.055.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.004 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.991 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.973 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.975 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.976 I llama_init_from_model: graph nodes  = 967
0.00.086.976 I llama_init_from_model: graph splits = 2
0.00.086.979 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.759 I main: llama threadpool init, n_threads = 4
0.00.503.807 I 
0.00.503.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.846 I 
0.00.504.012 I sampler seed: 1234
0.00.504.017 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.045 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.046 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.047 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.189.834 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.189.835 I llama_perf_context_print:        load time =     492.49 ms
0.01.189.836 I llama_perf_context_print: prompt eval time =      35.92 ms /     7 tokens (    5.13 ms per token,   194.88 tokens per second)
0.01.189.836 I llama_perf_context_print:        eval time =     646.86 ms /    63 runs   (   10.27 ms per token,    97.39 tokens per second)
0.01.189.837 I llama_perf_context_print:       total time =     686.95 ms /    70 tokens
0.01.190.038 I ggml_metal_free: deallocating

real	0m1.206s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.012.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.028.996 I llama_model_loader: - type  f32:  194 tensors
0.00.028.996 I llama_model_loader: - type q3_K:   25 tensors
0.00.028.996 I llama_model_loader: - type q4_K:   71 tensors
0.00.028.997 I llama_model_loader: - type q5_K:    1 tensors
0.00.028.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.998 I print_info: file format = GGUF V3 (latest)
0.00.028.998 I print_info: file type   = Q3_K - Medium
0.00.028.999 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.047.329 I load: special tokens cache size = 25
0.00.053.062 I load: token to piece cache size = 0.2984 MB
0.00.053.065 I print_info: arch             = gptneox
0.00.053.065 I print_info: vocab_only       = 0
0.00.053.065 I print_info: n_ctx_train      = 2048
0.00.053.066 I print_info: n_embd           = 2048
0.00.053.066 I print_info: n_layer          = 24
0.00.053.069 I print_info: n_head           = 16
0.00.053.070 I print_info: n_head_kv        = 16
0.00.053.072 I print_info: n_rot            = 32
0.00.053.072 I print_info: n_swa            = 0
0.00.053.072 I print_info: n_embd_head_k    = 128
0.00.053.072 I print_info: n_embd_head_v    = 128
0.00.053.073 I print_info: n_gqa            = 1
0.00.053.074 I print_info: n_embd_k_gqa     = 2048
0.00.053.074 I print_info: n_embd_v_gqa     = 2048
0.00.053.075 I print_info: f_norm_eps       = 1.0e-05
0.00.053.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.076 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.076 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.076 I print_info: f_logit_scale    = 0.0e+00
0.00.053.077 I print_info: n_ff             = 8192
0.00.053.077 I print_info: n_expert         = 0
0.00.053.077 I print_info: n_expert_used    = 0
0.00.053.078 I print_info: causal attn      = 1
0.00.053.078 I print_info: pooling type     = 0
0.00.053.078 I print_info: rope type        = 2
0.00.053.079 I print_info: rope scaling     = linear
0.00.053.079 I print_info: freq_base_train  = 10000.0
0.00.053.079 I print_info: freq_scale_train = 1
0.00.053.079 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.080 I print_info: rope_finetuned   = unknown
0.00.053.080 I print_info: ssm_d_conv       = 0
0.00.053.080 I print_info: ssm_d_inner      = 0
0.00.053.080 I print_info: ssm_d_state      = 0
0.00.053.080 I print_info: ssm_dt_rank      = 0
0.00.053.080 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.080 I print_info: model type       = 1.4B
0.00.053.081 I print_info: model params     = 1.41 B
0.00.053.081 I print_info: general.name     = 1.4B
0.00.053.081 I print_info: vocab type       = BPE
0.00.053.082 I print_info: n_vocab          = 50304
0.00.053.082 I print_info: n_merges         = 50009
0.00.053.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.082 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.082 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.083 I print_info: LF token         = 128 'Ä'
0.00.053.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.084 I print_info: max token length = 1024
0.00.054.700 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.700 I load_tensors: offloading output layer to GPU
0.00.054.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.711 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.712 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.968 I llama_init_from_model: n_seq_max     = 1
0.00.054.969 I llama_init_from_model: n_ctx         = 2048
0.00.054.969 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.969 I llama_init_from_model: n_batch       = 2048
0.00.054.969 I llama_init_from_model: n_ubatch      = 512
0.00.054.970 I llama_init_from_model: flash_attn    = 0
0.00.054.970 I llama_init_from_model: freq_base     = 10000.0
0.00.054.970 I llama_init_from_model: freq_scale    = 1
0.00.054.971 I ggml_metal_init: allocating
0.00.054.974 I ggml_metal_init: found device: Apple M4
0.00.054.976 I ggml_metal_init: picking default device: Apple M4
0.00.055.486 I ggml_metal_init: using embedded metal library
0.00.057.838 I ggml_metal_init: GPU name:   Apple M4
0.00.057.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.840 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.841 I ggml_metal_init: simdgroup reduction   = true
0.00.057.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.841 I ggml_metal_init: has bfloat            = true
0.00.057.841 I ggml_metal_init: use bfloat            = true
0.00.057.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.453 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.737 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.756 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.859 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.861 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.861 I llama_init_from_model: graph nodes  = 967
0.00.088.862 I llama_init_from_model: graph splits = 2
0.00.088.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.724 I main: llama threadpool init, n_threads = 4
0.00.546.762 I 
0.00.546.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.806 I 
0.00.546.962 I sampler seed: 1234
0.00.546.966 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.977 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.977 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.977 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.314.303 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.314.304 I llama_perf_context_print:        load time =     533.18 ms
0.01.314.305 I llama_perf_context_print: prompt eval time =      44.38 ms /     7 tokens (    6.34 ms per token,   157.73 tokens per second)
0.01.314.305 I llama_perf_context_print:        eval time =     719.89 ms /    63 runs   (   11.43 ms per token,    87.51 tokens per second)
0.01.314.306 I llama_perf_context_print:       total time =     768.43 ms /    70 tokens
0.01.314.540 I ggml_metal_free: deallocating

real	0m1.332s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.012.236 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.368 I llama_model_loader: - type  f32:  194 tensors
0.00.028.369 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.369 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.369 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.370 I print_info: file format = GGUF V3 (latest)
0.00.028.370 I print_info: file type   = Q4_K - Medium
0.00.028.373 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.764 I load: special tokens cache size = 25
0.00.053.693 I load: token to piece cache size = 0.2984 MB
0.00.053.696 I print_info: arch             = gptneox
0.00.053.696 I print_info: vocab_only       = 0
0.00.053.696 I print_info: n_ctx_train      = 2048
0.00.053.696 I print_info: n_embd           = 2048
0.00.053.697 I print_info: n_layer          = 24
0.00.053.699 I print_info: n_head           = 16
0.00.053.700 I print_info: n_head_kv        = 16
0.00.053.700 I print_info: n_rot            = 32
0.00.053.701 I print_info: n_swa            = 0
0.00.053.701 I print_info: n_embd_head_k    = 128
0.00.053.701 I print_info: n_embd_head_v    = 128
0.00.053.702 I print_info: n_gqa            = 1
0.00.053.702 I print_info: n_embd_k_gqa     = 2048
0.00.053.703 I print_info: n_embd_v_gqa     = 2048
0.00.053.703 I print_info: f_norm_eps       = 1.0e-05
0.00.053.704 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.704 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.704 I print_info: f_logit_scale    = 0.0e+00
0.00.053.705 I print_info: n_ff             = 8192
0.00.053.705 I print_info: n_expert         = 0
0.00.053.705 I print_info: n_expert_used    = 0
0.00.053.706 I print_info: causal attn      = 1
0.00.053.706 I print_info: pooling type     = 0
0.00.053.708 I print_info: rope type        = 2
0.00.053.708 I print_info: rope scaling     = linear
0.00.053.708 I print_info: freq_base_train  = 10000.0
0.00.053.709 I print_info: freq_scale_train = 1
0.00.053.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.709 I print_info: rope_finetuned   = unknown
0.00.053.709 I print_info: ssm_d_conv       = 0
0.00.053.710 I print_info: ssm_d_inner      = 0
0.00.053.710 I print_info: ssm_d_state      = 0
0.00.053.710 I print_info: ssm_dt_rank      = 0
0.00.053.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.710 I print_info: model type       = 1.4B
0.00.053.711 I print_info: model params     = 1.41 B
0.00.053.712 I print_info: general.name     = 1.4B
0.00.053.712 I print_info: vocab type       = BPE
0.00.053.713 I print_info: n_vocab          = 50304
0.00.053.713 I print_info: n_merges         = 50009
0.00.053.714 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.714 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.715 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.715 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.715 I print_info: LF token         = 128 'Ä'
0.00.053.715 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.715 I print_info: max token length = 1024
0.00.055.368 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.368 I load_tensors: offloading output layer to GPU
0.00.055.368 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.379 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.380 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.657 I llama_init_from_model: n_seq_max     = 1
0.00.055.658 I llama_init_from_model: n_ctx         = 2048
0.00.055.658 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.658 I llama_init_from_model: n_batch       = 2048
0.00.055.658 I llama_init_from_model: n_ubatch      = 512
0.00.055.658 I llama_init_from_model: flash_attn    = 0
0.00.055.659 I llama_init_from_model: freq_base     = 10000.0
0.00.055.659 I llama_init_from_model: freq_scale    = 1
0.00.055.659 I ggml_metal_init: allocating
0.00.055.663 I ggml_metal_init: found device: Apple M4
0.00.055.665 I ggml_metal_init: picking default device: Apple M4
0.00.056.188 I ggml_metal_init: using embedded metal library
0.00.058.590 I ggml_metal_init: GPU name:   Apple M4
0.00.058.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.593 I ggml_metal_init: simdgroup reduction   = true
0.00.058.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.593 I ggml_metal_init: has bfloat            = true
0.00.058.593 I ggml_metal_init: use bfloat            = true
0.00.058.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.401 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.419 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.425 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.528 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.529 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.529 I llama_init_from_model: graph nodes  = 967
0.00.090.530 I llama_init_from_model: graph splits = 2
0.00.090.532 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.049.428 I main: llama threadpool init, n_threads = 4
0.01.049.468 I 
0.01.049.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.049.494 I 
0.01.049.652 I sampler seed: 1234
0.01.049.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.049.698 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.049.700 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.049.700 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.841.901 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.841.901 I llama_perf_context_print:        load time =    1036.32 ms
0.01.841.902 I llama_perf_context_print: prompt eval time =      47.22 ms /     7 tokens (    6.75 ms per token,   148.25 tokens per second)
0.01.841.903 I llama_perf_context_print:        eval time =     741.78 ms /    63 runs   (   11.77 ms per token,    84.93 tokens per second)
0.01.841.903 I llama_perf_context_print:       total time =     793.34 ms /    70 tokens
0.01.842.127 I ggml_metal_free: deallocating

real	0m1.860s
user	0m0.111s
sys	0m0.550s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.577 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.578 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.384 I llama_model_loader: - type  f32:  194 tensors
0.00.026.385 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.385 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.386 I print_info: file format = GGUF V3 (latest)
0.00.026.386 I print_info: file type   = Q5_K - Medium
0.00.026.387 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.014 I load: special tokens cache size = 25
0.00.051.142 I load: token to piece cache size = 0.2984 MB
0.00.051.145 I print_info: arch             = gptneox
0.00.051.145 I print_info: vocab_only       = 0
0.00.051.145 I print_info: n_ctx_train      = 2048
0.00.051.145 I print_info: n_embd           = 2048
0.00.051.146 I print_info: n_layer          = 24
0.00.051.149 I print_info: n_head           = 16
0.00.051.149 I print_info: n_head_kv        = 16
0.00.051.151 I print_info: n_rot            = 32
0.00.051.152 I print_info: n_swa            = 0
0.00.051.152 I print_info: n_embd_head_k    = 128
0.00.051.152 I print_info: n_embd_head_v    = 128
0.00.051.152 I print_info: n_gqa            = 1
0.00.051.153 I print_info: n_embd_k_gqa     = 2048
0.00.051.158 I print_info: n_embd_v_gqa     = 2048
0.00.051.159 I print_info: f_norm_eps       = 1.0e-05
0.00.051.159 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.160 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.160 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.161 I print_info: f_logit_scale    = 0.0e+00
0.00.051.162 I print_info: n_ff             = 8192
0.00.051.162 I print_info: n_expert         = 0
0.00.051.162 I print_info: n_expert_used    = 0
0.00.051.162 I print_info: causal attn      = 1
0.00.051.162 I print_info: pooling type     = 0
0.00.051.163 I print_info: rope type        = 2
0.00.051.165 I print_info: rope scaling     = linear
0.00.051.165 I print_info: freq_base_train  = 10000.0
0.00.051.165 I print_info: freq_scale_train = 1
0.00.051.166 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.166 I print_info: rope_finetuned   = unknown
0.00.051.166 I print_info: ssm_d_conv       = 0
0.00.051.167 I print_info: ssm_d_inner      = 0
0.00.051.167 I print_info: ssm_d_state      = 0
0.00.051.167 I print_info: ssm_dt_rank      = 0
0.00.051.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.168 I print_info: model type       = 1.4B
0.00.051.168 I print_info: model params     = 1.41 B
0.00.051.168 I print_info: general.name     = 1.4B
0.00.051.169 I print_info: vocab type       = BPE
0.00.051.169 I print_info: n_vocab          = 50304
0.00.051.169 I print_info: n_merges         = 50009
0.00.051.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.169 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.170 I print_info: LF token         = 128 'Ä'
0.00.051.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.170 I print_info: max token length = 1024
0.00.052.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.764 I load_tensors: offloading output layer to GPU
0.00.052.765 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.775 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.776 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.049 I llama_init_from_model: n_seq_max     = 1
0.00.053.049 I llama_init_from_model: n_ctx         = 2048
0.00.053.050 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.050 I llama_init_from_model: n_batch       = 2048
0.00.053.050 I llama_init_from_model: n_ubatch      = 512
0.00.053.050 I llama_init_from_model: flash_attn    = 0
0.00.053.051 I llama_init_from_model: freq_base     = 10000.0
0.00.053.051 I llama_init_from_model: freq_scale    = 1
0.00.053.051 I ggml_metal_init: allocating
0.00.053.054 I ggml_metal_init: found device: Apple M4
0.00.053.056 I ggml_metal_init: picking default device: Apple M4
0.00.053.560 I ggml_metal_init: using embedded metal library
0.00.055.876 I ggml_metal_init: GPU name:   Apple M4
0.00.055.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.879 I ggml_metal_init: simdgroup reduction   = true
0.00.055.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.879 I ggml_metal_init: has bfloat            = true
0.00.055.879 I ggml_metal_init: use bfloat            = true
0.00.055.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.849 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.871 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.011 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.013 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.013 I llama_init_from_model: graph nodes  = 967
0.00.087.014 I llama_init_from_model: graph splits = 2
0.00.087.017 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.862 I main: llama threadpool init, n_threads = 4
0.00.736.896 I 
0.00.736.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.933 I 
0.00.737.159 I sampler seed: 1234
0.00.737.163 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.175 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.176 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.578.617 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.578.618 I llama_perf_context_print:        load time =     726.12 ms
0.01.578.618 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.61 tokens per second)
0.01.578.619 I llama_perf_context_print:        eval time =     786.85 ms /    63 runs   (   12.49 ms per token,    80.07 tokens per second)
0.01.578.620 I llama_perf_context_print:       total time =     842.62 ms /    70 tokens
0.01.578.839 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.974 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.978 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.979 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.981 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.982 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.834 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.742 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.743 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.743 I llama_model_loader: - type  f32:  194 tensors
0.00.026.744 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.744 I print_info: file format = GGUF V3 (latest)
0.00.026.744 I print_info: file type   = Q6_K
0.00.026.745 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.995 I load: special tokens cache size = 25
0.00.052.220 I load: token to piece cache size = 0.2984 MB
0.00.052.223 I print_info: arch             = gptneox
0.00.052.223 I print_info: vocab_only       = 0
0.00.052.224 I print_info: n_ctx_train      = 2048
0.00.052.224 I print_info: n_embd           = 2048
0.00.052.224 I print_info: n_layer          = 24
0.00.052.227 I print_info: n_head           = 16
0.00.052.228 I print_info: n_head_kv        = 16
0.00.052.228 I print_info: n_rot            = 32
0.00.052.228 I print_info: n_swa            = 0
0.00.052.229 I print_info: n_embd_head_k    = 128
0.00.052.229 I print_info: n_embd_head_v    = 128
0.00.052.232 I print_info: n_gqa            = 1
0.00.052.232 I print_info: n_embd_k_gqa     = 2048
0.00.052.233 I print_info: n_embd_v_gqa     = 2048
0.00.052.234 I print_info: f_norm_eps       = 1.0e-05
0.00.052.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.236 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.236 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.237 I print_info: f_logit_scale    = 0.0e+00
0.00.052.237 I print_info: n_ff             = 8192
0.00.052.238 I print_info: n_expert         = 0
0.00.052.238 I print_info: n_expert_used    = 0
0.00.052.238 I print_info: causal attn      = 1
0.00.052.238 I print_info: pooling type     = 0
0.00.052.238 I print_info: rope type        = 2
0.00.052.238 I print_info: rope scaling     = linear
0.00.052.239 I print_info: freq_base_train  = 10000.0
0.00.052.239 I print_info: freq_scale_train = 1
0.00.052.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.239 I print_info: rope_finetuned   = unknown
0.00.052.240 I print_info: ssm_d_conv       = 0
0.00.052.240 I print_info: ssm_d_inner      = 0
0.00.052.240 I print_info: ssm_d_state      = 0
0.00.052.240 I print_info: ssm_dt_rank      = 0
0.00.052.240 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.241 I print_info: model type       = 1.4B
0.00.052.241 I print_info: model params     = 1.41 B
0.00.052.241 I print_info: general.name     = 1.4B
0.00.052.242 I print_info: vocab type       = BPE
0.00.052.242 I print_info: n_vocab          = 50304
0.00.052.242 I print_info: n_merges         = 50009
0.00.052.242 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.243 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.243 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.243 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.243 I print_info: LF token         = 128 'Ä'
0.00.052.244 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.244 I print_info: max token length = 1024
0.00.053.871 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.871 I load_tensors: offloading output layer to GPU
0.00.053.871 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.882 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.883 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.161 I llama_init_from_model: n_seq_max     = 1
0.00.054.162 I llama_init_from_model: n_ctx         = 2048
0.00.054.162 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.163 I llama_init_from_model: n_batch       = 2048
0.00.054.163 I llama_init_from_model: n_ubatch      = 512
0.00.054.163 I llama_init_from_model: flash_attn    = 0
0.00.054.163 I llama_init_from_model: freq_base     = 10000.0
0.00.054.164 I llama_init_from_model: freq_scale    = 1
0.00.054.164 I ggml_metal_init: allocating
0.00.054.167 I ggml_metal_init: found device: Apple M4
0.00.054.169 I ggml_metal_init: picking default device: Apple M4
0.00.054.671 I ggml_metal_init: using embedded metal library
0.00.057.030 I ggml_metal_init: GPU name:   Apple M4
0.00.057.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.033 I ggml_metal_init: simdgroup reduction   = true
0.00.057.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.033 I ggml_metal_init: has bfloat            = true
0.00.057.034 I ggml_metal_init: use bfloat            = true
0.00.057.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.031 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.280 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.290 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.310 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.360 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.361 I llama_init_from_model: graph nodes  = 967
0.00.087.362 I llama_init_from_model: graph splits = 2
0.00.087.365 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.495 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.037 I main: llama threadpool init, n_threads = 4
0.00.766.080 I 
0.00.766.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.106 I 
0.00.766.347 I sampler seed: 1234
0.00.766.353 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.414 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.416 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.416 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.643.759 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.643.760 I llama_perf_context_print:        load time =     755.06 ms
0.01.643.760 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.64 tokens per second)
0.01.643.761 I llama_perf_context_print:        eval time =     820.18 ms /    63 runs   (   13.02 ms per token,    76.81 tokens per second)
0.01.643.761 I llama_perf_context_print:       total time =     878.62 ms /    70 tokens
0.01.644.043 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.525 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.484 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.198 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.219 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.224 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.589 I llama_model_loader: - type  f32:  194 tensors
0.00.057.589 I llama_model_loader: - type  f16:   98 tensors
0.00.057.591 I print_info: file format = GGUF V3 (latest)
0.00.057.594 I print_info: file type   = all F32 (guessed)
0.00.057.596 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.086.827 I load: special tokens cache size = 25
0.00.093.978 I load: token to piece cache size = 0.2984 MB
0.00.093.981 I print_info: arch             = gptneox
0.00.093.982 I print_info: vocab_only       = 0
0.00.093.982 I print_info: n_ctx_train      = 2048
0.00.093.982 I print_info: n_embd           = 2048
0.00.093.982 I print_info: n_layer          = 24
0.00.093.985 I print_info: n_head           = 16
0.00.093.986 I print_info: n_head_kv        = 16
0.00.093.986 I print_info: n_rot            = 32
0.00.093.986 I print_info: n_swa            = 0
0.00.093.986 I print_info: n_embd_head_k    = 128
0.00.093.989 I print_info: n_embd_head_v    = 128
0.00.093.989 I print_info: n_gqa            = 1
0.00.093.990 I print_info: n_embd_k_gqa     = 2048
0.00.093.991 I print_info: n_embd_v_gqa     = 2048
0.00.093.991 I print_info: f_norm_eps       = 1.0e-05
0.00.093.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.093.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.093.992 I print_info: f_max_alibi_bias = 0.0e+00
0.00.093.992 I print_info: f_logit_scale    = 0.0e+00
0.00.093.992 I print_info: n_ff             = 8192
0.00.093.994 I print_info: n_expert         = 0
0.00.093.994 I print_info: n_expert_used    = 0
0.00.093.994 I print_info: causal attn      = 1
0.00.093.994 I print_info: pooling type     = 0
0.00.093.994 I print_info: rope type        = 2
0.00.093.994 I print_info: rope scaling     = linear
0.00.093.995 I print_info: freq_base_train  = 10000.0
0.00.093.995 I print_info: freq_scale_train = 1
0.00.093.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.093.996 I print_info: rope_finetuned   = unknown
0.00.093.996 I print_info: ssm_d_conv       = 0
0.00.093.996 I print_info: ssm_d_inner      = 0
0.00.093.996 I print_info: ssm_d_state      = 0
0.00.093.996 I print_info: ssm_dt_rank      = 0
0.00.093.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.000 I print_info: model type       = 1.4B
0.00.094.000 I print_info: model params     = 1.41 B
0.00.094.007 I print_info: general.name     = 1.4B
0.00.094.010 I print_info: vocab type       = BPE
0.00.094.010 I print_info: n_vocab          = 50304
0.00.094.010 I print_info: n_merges         = 50009
0.00.094.010 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.011 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.011 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.011 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.011 I print_info: LF token         = 128 'Ä'
0.00.094.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.013 I print_info: max token length = 1024
0.00.096.615 I load_tensors: offloading 24 repeating layers to GPU
0.00.096.615 I load_tensors: offloading output layer to GPU
0.00.096.615 I load_tensors: offloaded 25/25 layers to GPU
0.00.096.626 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.627 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.096.940 I llama_init_from_model: n_seq_max     = 1
0.00.096.941 I llama_init_from_model: n_ctx         = 128
0.00.096.941 I llama_init_from_model: n_ctx_per_seq = 128
0.00.096.941 I llama_init_from_model: n_batch       = 128
0.00.096.941 I llama_init_from_model: n_ubatch      = 128
0.00.096.941 I llama_init_from_model: flash_attn    = 0
0.00.096.942 I llama_init_from_model: freq_base     = 10000.0
0.00.096.942 I llama_init_from_model: freq_scale    = 1
0.00.096.942 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.943 I ggml_metal_init: allocating
0.00.096.946 I ggml_metal_init: found device: Apple M4
0.00.096.948 I ggml_metal_init: picking default device: Apple M4
0.00.097.481 I ggml_metal_init: using embedded metal library
0.00.100.220 I ggml_metal_init: GPU name:   Apple M4
0.00.100.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.223 I ggml_metal_init: simdgroup reduction   = true
0.00.100.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.223 I ggml_metal_init: has bfloat            = true
0.00.100.223 I ggml_metal_init: use bfloat            = true
0.00.100.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.636 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.112.017 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.112.033 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.112.993 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.112.994 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.112.995 I llama_init_from_model: graph nodes  = 967
0.00.112.995 I llama_init_from_model: graph splits = 2
0.00.112.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.112.996 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.035.453 I 
0.01.035.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.035.537 I perplexity: tokenizing the input ..
0.01.048.518 I perplexity: tokenization took 12.974 ms
0.01.048.546 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.171.196 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.174.098 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.174.138 I llama_perf_context_print:        load time =    1009.95 ms
0.01.174.142 I llama_perf_context_print: prompt eval time =     121.71 ms /   128 tokens (    0.95 ms per token,  1051.66 tokens per second)
0.01.174.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.174.144 I llama_perf_context_print:       total time =     138.69 ms /   129 tokens
0.01.175.017 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.130s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.153 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.455 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.595 I llama_model_loader: - type  f32:  194 tensors
0.00.028.595 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.596 I print_info: file format = GGUF V3 (latest)
0.00.028.597 I print_info: file type   = Q8_0
0.00.028.598 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.827 I load: special tokens cache size = 25
0.00.053.781 I load: token to piece cache size = 0.2984 MB
0.00.053.785 I print_info: arch             = gptneox
0.00.053.786 I print_info: vocab_only       = 0
0.00.053.786 I print_info: n_ctx_train      = 2048
0.00.053.786 I print_info: n_embd           = 2048
0.00.053.786 I print_info: n_layer          = 24
0.00.053.791 I print_info: n_head           = 16
0.00.053.792 I print_info: n_head_kv        = 16
0.00.053.792 I print_info: n_rot            = 32
0.00.053.792 I print_info: n_swa            = 0
0.00.053.792 I print_info: n_embd_head_k    = 128
0.00.053.792 I print_info: n_embd_head_v    = 128
0.00.053.795 I print_info: n_gqa            = 1
0.00.053.795 I print_info: n_embd_k_gqa     = 2048
0.00.053.796 I print_info: n_embd_v_gqa     = 2048
0.00.053.796 I print_info: f_norm_eps       = 1.0e-05
0.00.053.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.797 I print_info: f_logit_scale    = 0.0e+00
0.00.053.798 I print_info: n_ff             = 8192
0.00.053.798 I print_info: n_expert         = 0
0.00.053.798 I print_info: n_expert_used    = 0
0.00.053.799 I print_info: causal attn      = 1
0.00.053.799 I print_info: pooling type     = 0
0.00.053.799 I print_info: rope type        = 2
0.00.053.799 I print_info: rope scaling     = linear
0.00.053.799 I print_info: freq_base_train  = 10000.0
0.00.053.799 I print_info: freq_scale_train = 1
0.00.053.800 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.800 I print_info: rope_finetuned   = unknown
0.00.053.800 I print_info: ssm_d_conv       = 0
0.00.053.800 I print_info: ssm_d_inner      = 0
0.00.053.800 I print_info: ssm_d_state      = 0
0.00.053.800 I print_info: ssm_dt_rank      = 0
0.00.053.801 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.801 I print_info: model type       = 1.4B
0.00.053.801 I print_info: model params     = 1.41 B
0.00.053.801 I print_info: general.name     = 1.4B
0.00.053.802 I print_info: vocab type       = BPE
0.00.053.802 I print_info: n_vocab          = 50304
0.00.053.802 I print_info: n_merges         = 50009
0.00.053.804 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.805 I print_info: LF token         = 128 'Ä'
0.00.053.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.805 I print_info: max token length = 1024
0.00.055.959 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.959 I load_tensors: offloading output layer to GPU
0.00.055.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.970 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.971 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.056.253 I llama_init_from_model: n_seq_max     = 1
0.00.056.254 I llama_init_from_model: n_ctx         = 128
0.00.056.254 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.254 I llama_init_from_model: n_batch       = 128
0.00.056.254 I llama_init_from_model: n_ubatch      = 128
0.00.056.254 I llama_init_from_model: flash_attn    = 0
0.00.056.255 I llama_init_from_model: freq_base     = 10000.0
0.00.056.255 I llama_init_from_model: freq_scale    = 1
0.00.056.255 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.256 I ggml_metal_init: allocating
0.00.056.259 I ggml_metal_init: found device: Apple M4
0.00.056.261 I ggml_metal_init: picking default device: Apple M4
0.00.056.787 I ggml_metal_init: using embedded metal library
0.00.059.231 I ggml_metal_init: GPU name:   Apple M4
0.00.059.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.234 I ggml_metal_init: simdgroup reduction   = true
0.00.059.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.234 I ggml_metal_init: has bfloat            = true
0.00.059.234 I ggml_metal_init: use bfloat            = true
0.00.059.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.235 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.352 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.567 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.572 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.591 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.463 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.464 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.464 I llama_init_from_model: graph nodes  = 967
0.00.071.464 I llama_init_from_model: graph splits = 2
0.00.071.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.356 I 
0.00.876.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.876.399 I perplexity: tokenizing the input ..
0.00.884.201 I perplexity: tokenization took 7.799 ms
0.00.884.211 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.008.607 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.009.800 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.009.817 I llama_perf_context_print:        load time =     865.37 ms
0.01.009.818 I llama_perf_context_print: prompt eval time =     124.17 ms /   128 tokens (    0.97 ms per token,  1030.84 tokens per second)
0.01.009.819 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.009.819 I llama_perf_context_print:       total time =     133.46 ms /   129 tokens
0.01.010.338 I ggml_metal_free: deallocating

real	0m1.030s
user	0m0.083s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.293 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.345 I llama_model_loader: - type  f32:  194 tensors
0.00.027.346 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.346 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.346 I print_info: file format = GGUF V3 (latest)
0.00.027.347 I print_info: file type   = Q4_0
0.00.027.347 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.544 I load: special tokens cache size = 25
0.00.052.624 I load: token to piece cache size = 0.2984 MB
0.00.052.627 I print_info: arch             = gptneox
0.00.052.627 I print_info: vocab_only       = 0
0.00.052.627 I print_info: n_ctx_train      = 2048
0.00.052.627 I print_info: n_embd           = 2048
0.00.052.627 I print_info: n_layer          = 24
0.00.052.630 I print_info: n_head           = 16
0.00.052.633 I print_info: n_head_kv        = 16
0.00.052.633 I print_info: n_rot            = 32
0.00.052.633 I print_info: n_swa            = 0
0.00.052.633 I print_info: n_embd_head_k    = 128
0.00.052.633 I print_info: n_embd_head_v    = 128
0.00.052.636 I print_info: n_gqa            = 1
0.00.052.637 I print_info: n_embd_k_gqa     = 2048
0.00.052.637 I print_info: n_embd_v_gqa     = 2048
0.00.052.638 I print_info: f_norm_eps       = 1.0e-05
0.00.052.638 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.639 I print_info: f_logit_scale    = 0.0e+00
0.00.052.639 I print_info: n_ff             = 8192
0.00.052.640 I print_info: n_expert         = 0
0.00.052.640 I print_info: n_expert_used    = 0
0.00.052.640 I print_info: causal attn      = 1
0.00.052.640 I print_info: pooling type     = 0
0.00.052.640 I print_info: rope type        = 2
0.00.052.640 I print_info: rope scaling     = linear
0.00.052.641 I print_info: freq_base_train  = 10000.0
0.00.052.641 I print_info: freq_scale_train = 1
0.00.052.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.641 I print_info: rope_finetuned   = unknown
0.00.052.642 I print_info: ssm_d_conv       = 0
0.00.052.642 I print_info: ssm_d_inner      = 0
0.00.052.642 I print_info: ssm_d_state      = 0
0.00.052.642 I print_info: ssm_dt_rank      = 0
0.00.052.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.642 I print_info: model type       = 1.4B
0.00.052.643 I print_info: model params     = 1.41 B
0.00.052.643 I print_info: general.name     = 1.4B
0.00.052.643 I print_info: vocab type       = BPE
0.00.052.644 I print_info: n_vocab          = 50304
0.00.052.644 I print_info: n_merges         = 50009
0.00.052.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.645 I print_info: LF token         = 128 'Ä'
0.00.052.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.645 I print_info: max token length = 1024
0.00.054.596 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.597 I load_tensors: offloading output layer to GPU
0.00.054.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.608 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.609 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.888 I llama_init_from_model: n_seq_max     = 1
0.00.054.889 I llama_init_from_model: n_ctx         = 128
0.00.054.889 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.889 I llama_init_from_model: n_batch       = 128
0.00.054.889 I llama_init_from_model: n_ubatch      = 128
0.00.054.889 I llama_init_from_model: flash_attn    = 0
0.00.054.890 I llama_init_from_model: freq_base     = 10000.0
0.00.054.890 I llama_init_from_model: freq_scale    = 1
0.00.054.890 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.891 I ggml_metal_init: allocating
0.00.054.894 I ggml_metal_init: found device: Apple M4
0.00.054.896 I ggml_metal_init: picking default device: Apple M4
0.00.055.366 I ggml_metal_init: using embedded metal library
0.00.057.732 I ggml_metal_init: GPU name:   Apple M4
0.00.057.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.735 I ggml_metal_init: simdgroup reduction   = true
0.00.057.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.735 I ggml_metal_init: has bfloat            = true
0.00.057.736 I ggml_metal_init: use bfloat            = true
0.00.057.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.800 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.803 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.765 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.766 I llama_init_from_model: graph nodes  = 967
0.00.069.767 I llama_init_from_model: graph splits = 2
0.00.069.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.535 I 
0.00.567.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.592 I perplexity: tokenizing the input ..
0.00.575.618 I perplexity: tokenization took 8.025 ms
0.00.575.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.797 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.698.960 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.698.976 I llama_perf_context_print:        load time =     556.37 ms
0.00.698.977 I llama_perf_context_print: prompt eval time =     121.94 ms /   128 tokens (    0.95 ms per token,  1049.69 tokens per second)
0.00.698.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.978 I llama_perf_context_print:       total time =     131.44 ms /   129 tokens
0.00.699.482 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.153 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.156 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.159 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.060 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.061 I print_info: file format = GGUF V3 (latest)
0.00.025.062 I print_info: file type   = Q4_1
0.00.025.063 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.309 I load: special tokens cache size = 25
0.00.050.474 I load: token to piece cache size = 0.2984 MB
0.00.050.477 I print_info: arch             = gptneox
0.00.050.477 I print_info: vocab_only       = 0
0.00.050.478 I print_info: n_ctx_train      = 2048
0.00.050.478 I print_info: n_embd           = 2048
0.00.050.478 I print_info: n_layer          = 24
0.00.050.481 I print_info: n_head           = 16
0.00.050.481 I print_info: n_head_kv        = 16
0.00.050.482 I print_info: n_rot            = 32
0.00.050.482 I print_info: n_swa            = 0
0.00.050.482 I print_info: n_embd_head_k    = 128
0.00.050.482 I print_info: n_embd_head_v    = 128
0.00.050.483 I print_info: n_gqa            = 1
0.00.050.484 I print_info: n_embd_k_gqa     = 2048
0.00.050.485 I print_info: n_embd_v_gqa     = 2048
0.00.050.485 I print_info: f_norm_eps       = 1.0e-05
0.00.050.486 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.486 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.486 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.486 I print_info: f_logit_scale    = 0.0e+00
0.00.050.488 I print_info: n_ff             = 8192
0.00.050.488 I print_info: n_expert         = 0
0.00.050.488 I print_info: n_expert_used    = 0
0.00.050.488 I print_info: causal attn      = 1
0.00.050.488 I print_info: pooling type     = 0
0.00.050.489 I print_info: rope type        = 2
0.00.050.489 I print_info: rope scaling     = linear
0.00.050.491 I print_info: freq_base_train  = 10000.0
0.00.050.491 I print_info: freq_scale_train = 1
0.00.050.491 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.491 I print_info: rope_finetuned   = unknown
0.00.050.492 I print_info: ssm_d_conv       = 0
0.00.050.492 I print_info: ssm_d_inner      = 0
0.00.050.492 I print_info: ssm_d_state      = 0
0.00.050.492 I print_info: ssm_dt_rank      = 0
0.00.050.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.492 I print_info: model type       = 1.4B
0.00.050.493 I print_info: model params     = 1.41 B
0.00.050.493 I print_info: general.name     = 1.4B
0.00.050.493 I print_info: vocab type       = BPE
0.00.050.498 I print_info: n_vocab          = 50304
0.00.050.498 I print_info: n_merges         = 50009
0.00.050.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.499 I print_info: LF token         = 128 'Ä'
0.00.050.499 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.499 I print_info: max token length = 1024
0.00.052.463 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.463 I load_tensors: offloading output layer to GPU
0.00.052.464 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.474 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.475 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.747 I llama_init_from_model: n_seq_max     = 1
0.00.052.748 I llama_init_from_model: n_ctx         = 128
0.00.052.748 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.748 I llama_init_from_model: n_batch       = 128
0.00.052.749 I llama_init_from_model: n_ubatch      = 128
0.00.052.749 I llama_init_from_model: flash_attn    = 0
0.00.052.749 I llama_init_from_model: freq_base     = 10000.0
0.00.052.749 I llama_init_from_model: freq_scale    = 1
0.00.052.750 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.750 I ggml_metal_init: allocating
0.00.052.753 I ggml_metal_init: found device: Apple M4
0.00.052.755 I ggml_metal_init: picking default device: Apple M4
0.00.053.227 I ggml_metal_init: using embedded metal library
0.00.055.597 I ggml_metal_init: GPU name:   Apple M4
0.00.055.599 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.600 I ggml_metal_init: simdgroup reduction   = true
0.00.055.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.600 I ggml_metal_init: has bfloat            = true
0.00.055.600 I ggml_metal_init: use bfloat            = true
0.00.055.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.431 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.713 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.715 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.728 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.699 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.700 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.701 I llama_init_from_model: graph nodes  = 967
0.00.067.701 I llama_init_from_model: graph splits = 2
0.00.067.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.387 I 
0.00.693.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.528 I perplexity: tokenizing the input ..
0.00.701.618 I perplexity: tokenization took 8.088 ms
0.00.701.628 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.341 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.825.508 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.825.525 I llama_perf_context_print:        load time =     684.46 ms
0.00.825.526 I llama_perf_context_print: prompt eval time =     122.47 ms /   128 tokens (    0.96 ms per token,  1045.18 tokens per second)
0.00.825.527 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.528 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.826.057 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.609 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.662 I llama_model_loader: - type  f32:  194 tensors
0.00.025.662 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.662 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.663 I print_info: file format = GGUF V3 (latest)
0.00.025.663 I print_info: file type   = Q5_0
0.00.025.664 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.947 I load: special tokens cache size = 25
0.00.050.957 I load: token to piece cache size = 0.2984 MB
0.00.050.960 I print_info: arch             = gptneox
0.00.050.960 I print_info: vocab_only       = 0
0.00.050.960 I print_info: n_ctx_train      = 2048
0.00.050.960 I print_info: n_embd           = 2048
0.00.050.960 I print_info: n_layer          = 24
0.00.050.963 I print_info: n_head           = 16
0.00.050.964 I print_info: n_head_kv        = 16
0.00.050.964 I print_info: n_rot            = 32
0.00.050.964 I print_info: n_swa            = 0
0.00.050.965 I print_info: n_embd_head_k    = 128
0.00.050.965 I print_info: n_embd_head_v    = 128
0.00.050.966 I print_info: n_gqa            = 1
0.00.050.966 I print_info: n_embd_k_gqa     = 2048
0.00.050.967 I print_info: n_embd_v_gqa     = 2048
0.00.050.968 I print_info: f_norm_eps       = 1.0e-05
0.00.050.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.969 I print_info: f_logit_scale    = 0.0e+00
0.00.050.969 I print_info: n_ff             = 8192
0.00.050.969 I print_info: n_expert         = 0
0.00.050.970 I print_info: n_expert_used    = 0
0.00.050.970 I print_info: causal attn      = 1
0.00.050.970 I print_info: pooling type     = 0
0.00.050.970 I print_info: rope type        = 2
0.00.050.970 I print_info: rope scaling     = linear
0.00.050.971 I print_info: freq_base_train  = 10000.0
0.00.050.971 I print_info: freq_scale_train = 1
0.00.050.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.972 I print_info: rope_finetuned   = unknown
0.00.050.972 I print_info: ssm_d_conv       = 0
0.00.050.972 I print_info: ssm_d_inner      = 0
0.00.050.972 I print_info: ssm_d_state      = 0
0.00.050.972 I print_info: ssm_dt_rank      = 0
0.00.050.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.973 I print_info: model type       = 1.4B
0.00.050.974 I print_info: model params     = 1.41 B
0.00.050.974 I print_info: general.name     = 1.4B
0.00.050.976 I print_info: vocab type       = BPE
0.00.050.976 I print_info: n_vocab          = 50304
0.00.050.977 I print_info: n_merges         = 50009
0.00.050.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.978 I print_info: LF token         = 128 'Ä'
0.00.050.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.978 I print_info: max token length = 1024
0.00.052.934 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.934 I load_tensors: offloading output layer to GPU
0.00.052.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.945 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.946 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.244 I llama_init_from_model: n_seq_max     = 1
0.00.053.245 I llama_init_from_model: n_ctx         = 128
0.00.053.245 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.246 I llama_init_from_model: n_batch       = 128
0.00.053.246 I llama_init_from_model: n_ubatch      = 128
0.00.053.246 I llama_init_from_model: flash_attn    = 0
0.00.053.246 I llama_init_from_model: freq_base     = 10000.0
0.00.053.246 I llama_init_from_model: freq_scale    = 1
0.00.053.247 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.247 I ggml_metal_init: allocating
0.00.053.251 I ggml_metal_init: found device: Apple M4
0.00.053.253 I ggml_metal_init: picking default device: Apple M4
0.00.053.728 I ggml_metal_init: using embedded metal library
0.00.056.087 I ggml_metal_init: GPU name:   Apple M4
0.00.056.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.089 I ggml_metal_init: simdgroup reduction   = true
0.00.056.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.090 I ggml_metal_init: has bfloat            = true
0.00.056.090 I ggml_metal_init: use bfloat            = true
0.00.056.090 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.091 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.140 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.143 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.157 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.097 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.098 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.099 I llama_init_from_model: graph nodes  = 967
0.00.068.099 I llama_init_from_model: graph splits = 2
0.00.068.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.747 I 
0.00.690.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.789 I perplexity: tokenizing the input ..
0.00.698.492 I perplexity: tokenization took 7.701 ms
0.00.698.503 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.815 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.834.964 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.834.986 I llama_perf_context_print:        load time =     681.10 ms
0.00.834.988 I llama_perf_context_print: prompt eval time =     135.08 ms /   128 tokens (    1.06 ms per token,   947.56 tokens per second)
0.00.834.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.989 I llama_perf_context_print:       total time =     144.24 ms /   129 tokens
0.00.835.530 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.596 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.598 I llama_model_loader: - type  f32:  194 tensors
0.00.024.598 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.598 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.599 I print_info: file format = GGUF V3 (latest)
0.00.024.599 I print_info: file type   = Q5_1
0.00.024.600 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.070 I load: special tokens cache size = 25
0.00.048.966 I load: token to piece cache size = 0.2984 MB
0.00.048.969 I print_info: arch             = gptneox
0.00.048.969 I print_info: vocab_only       = 0
0.00.048.970 I print_info: n_ctx_train      = 2048
0.00.048.970 I print_info: n_embd           = 2048
0.00.048.970 I print_info: n_layer          = 24
0.00.048.973 I print_info: n_head           = 16
0.00.048.974 I print_info: n_head_kv        = 16
0.00.048.979 I print_info: n_rot            = 32
0.00.048.980 I print_info: n_swa            = 0
0.00.048.980 I print_info: n_embd_head_k    = 128
0.00.048.980 I print_info: n_embd_head_v    = 128
0.00.048.988 I print_info: n_gqa            = 1
0.00.048.989 I print_info: n_embd_k_gqa     = 2048
0.00.048.990 I print_info: n_embd_v_gqa     = 2048
0.00.048.990 I print_info: f_norm_eps       = 1.0e-05
0.00.048.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.991 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.991 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.991 I print_info: f_logit_scale    = 0.0e+00
0.00.048.992 I print_info: n_ff             = 8192
0.00.048.993 I print_info: n_expert         = 0
0.00.048.993 I print_info: n_expert_used    = 0
0.00.048.993 I print_info: causal attn      = 1
0.00.048.993 I print_info: pooling type     = 0
0.00.048.993 I print_info: rope type        = 2
0.00.048.993 I print_info: rope scaling     = linear
0.00.048.995 I print_info: freq_base_train  = 10000.0
0.00.048.995 I print_info: freq_scale_train = 1
0.00.048.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.995 I print_info: rope_finetuned   = unknown
0.00.048.995 I print_info: ssm_d_conv       = 0
0.00.048.996 I print_info: ssm_d_inner      = 0
0.00.048.997 I print_info: ssm_d_state      = 0
0.00.048.997 I print_info: ssm_dt_rank      = 0
0.00.048.997 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.998 I print_info: model type       = 1.4B
0.00.048.998 I print_info: model params     = 1.41 B
0.00.048.998 I print_info: general.name     = 1.4B
0.00.048.999 I print_info: vocab type       = BPE
0.00.049.000 I print_info: n_vocab          = 50304
0.00.049.000 I print_info: n_merges         = 50009
0.00.049.000 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.000 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.001 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.001 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.001 I print_info: LF token         = 128 'Ä'
0.00.049.001 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.001 I print_info: max token length = 1024
0.00.050.945 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.945 I load_tensors: offloading output layer to GPU
0.00.050.945 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.956 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.957 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.220 I llama_init_from_model: n_seq_max     = 1
0.00.051.221 I llama_init_from_model: n_ctx         = 128
0.00.051.221 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.221 I llama_init_from_model: n_batch       = 128
0.00.051.221 I llama_init_from_model: n_ubatch      = 128
0.00.051.221 I llama_init_from_model: flash_attn    = 0
0.00.051.222 I llama_init_from_model: freq_base     = 10000.0
0.00.051.222 I llama_init_from_model: freq_scale    = 1
0.00.051.222 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.223 I ggml_metal_init: allocating
0.00.051.225 I ggml_metal_init: found device: Apple M4
0.00.051.227 I ggml_metal_init: picking default device: Apple M4
0.00.051.681 I ggml_metal_init: using embedded metal library
0.00.054.099 I ggml_metal_init: GPU name:   Apple M4
0.00.054.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.101 I ggml_metal_init: simdgroup reduction   = true
0.00.054.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.101 I ggml_metal_init: has bfloat            = true
0.00.054.102 I ggml_metal_init: use bfloat            = true
0.00.054.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.431 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.724 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.728 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.744 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.578 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.579 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.580 I llama_init_from_model: graph nodes  = 967
0.00.065.580 I llama_init_from_model: graph splits = 2
0.00.065.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.846 I 
0.00.692.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.904 I perplexity: tokenizing the input ..
0.00.700.790 I perplexity: tokenization took 7.885 ms
0.00.700.800 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.800 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.836.953 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.836.973 I llama_perf_context_print:        load time =     683.86 ms
0.00.836.974 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.75 tokens per second)
0.00.836.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.975 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.837.425 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.076s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.612 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.613 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.578 I llama_model_loader: - type  f32:  194 tensors
0.00.026.578 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.578 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.579 I print_info: file format = GGUF V3 (latest)
0.00.026.579 I print_info: file type   = Q2_K - Medium
0.00.026.580 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.054 I load: special tokens cache size = 25
0.00.051.022 I load: token to piece cache size = 0.2984 MB
0.00.051.025 I print_info: arch             = gptneox
0.00.051.025 I print_info: vocab_only       = 0
0.00.051.026 I print_info: n_ctx_train      = 2048
0.00.051.026 I print_info: n_embd           = 2048
0.00.051.026 I print_info: n_layer          = 24
0.00.051.030 I print_info: n_head           = 16
0.00.051.031 I print_info: n_head_kv        = 16
0.00.051.031 I print_info: n_rot            = 32
0.00.051.031 I print_info: n_swa            = 0
0.00.051.031 I print_info: n_embd_head_k    = 128
0.00.051.032 I print_info: n_embd_head_v    = 128
0.00.051.032 I print_info: n_gqa            = 1
0.00.051.033 I print_info: n_embd_k_gqa     = 2048
0.00.051.034 I print_info: n_embd_v_gqa     = 2048
0.00.051.035 I print_info: f_norm_eps       = 1.0e-05
0.00.051.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.036 I print_info: f_logit_scale    = 0.0e+00
0.00.051.037 I print_info: n_ff             = 8192
0.00.051.037 I print_info: n_expert         = 0
0.00.051.037 I print_info: n_expert_used    = 0
0.00.051.037 I print_info: causal attn      = 1
0.00.051.037 I print_info: pooling type     = 0
0.00.051.037 I print_info: rope type        = 2
0.00.051.038 I print_info: rope scaling     = linear
0.00.051.038 I print_info: freq_base_train  = 10000.0
0.00.051.038 I print_info: freq_scale_train = 1
0.00.051.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.039 I print_info: rope_finetuned   = unknown
0.00.051.039 I print_info: ssm_d_conv       = 0
0.00.051.039 I print_info: ssm_d_inner      = 0
0.00.051.039 I print_info: ssm_d_state      = 0
0.00.051.039 I print_info: ssm_dt_rank      = 0
0.00.051.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.040 I print_info: model type       = 1.4B
0.00.051.040 I print_info: model params     = 1.41 B
0.00.051.040 I print_info: general.name     = 1.4B
0.00.051.041 I print_info: vocab type       = BPE
0.00.051.041 I print_info: n_vocab          = 50304
0.00.051.041 I print_info: n_merges         = 50009
0.00.051.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.042 I print_info: LF token         = 128 'Ä'
0.00.051.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.043 I print_info: max token length = 1024
0.00.052.892 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.892 I load_tensors: offloading output layer to GPU
0.00.052.892 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.902 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.904 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.171 I llama_init_from_model: n_seq_max     = 1
0.00.053.172 I llama_init_from_model: n_ctx         = 128
0.00.053.172 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.172 I llama_init_from_model: n_batch       = 128
0.00.053.172 I llama_init_from_model: n_ubatch      = 128
0.00.053.173 I llama_init_from_model: flash_attn    = 0
0.00.053.173 I llama_init_from_model: freq_base     = 10000.0
0.00.053.173 I llama_init_from_model: freq_scale    = 1
0.00.053.173 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.174 I ggml_metal_init: allocating
0.00.053.177 I ggml_metal_init: found device: Apple M4
0.00.053.179 I ggml_metal_init: picking default device: Apple M4
0.00.053.636 I ggml_metal_init: using embedded metal library
0.00.055.960 I ggml_metal_init: GPU name:   Apple M4
0.00.055.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.962 I ggml_metal_init: simdgroup reduction   = true
0.00.055.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.962 I ggml_metal_init: has bfloat            = true
0.00.055.962 I ggml_metal_init: use bfloat            = true
0.00.055.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.441 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.731 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.733 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.748 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.628 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.629 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.630 I llama_init_from_model: graph nodes  = 967
0.00.067.630 I llama_init_from_model: graph splits = 2
0.00.067.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.424 I 
0.00.437.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.474 I perplexity: tokenizing the input ..
0.00.445.768 I perplexity: tokenization took 8.291 ms
0.00.445.779 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.578.329 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.528 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.550 I llama_perf_context_print:        load time =     426.52 ms
0.00.579.551 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.34 tokens per second)
0.00.579.552 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.552 I llama_perf_context_print:       total time =     142.13 ms /   129 tokens
0.00.580.073 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.916 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.916 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.925 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.926 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.024 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.934 I llama_model_loader: - type  f32:  194 tensors
0.00.024.935 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.935 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.935 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.935 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.936 I print_info: file format = GGUF V3 (latest)
0.00.024.936 I print_info: file type   = Q3_K - Medium
0.00.024.937 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.120 I load: special tokens cache size = 25
0.00.050.246 I load: token to piece cache size = 0.2984 MB
0.00.050.249 I print_info: arch             = gptneox
0.00.050.249 I print_info: vocab_only       = 0
0.00.050.249 I print_info: n_ctx_train      = 2048
0.00.050.249 I print_info: n_embd           = 2048
0.00.050.250 I print_info: n_layer          = 24
0.00.050.252 I print_info: n_head           = 16
0.00.050.253 I print_info: n_head_kv        = 16
0.00.050.255 I print_info: n_rot            = 32
0.00.050.255 I print_info: n_swa            = 0
0.00.050.255 I print_info: n_embd_head_k    = 128
0.00.050.255 I print_info: n_embd_head_v    = 128
0.00.050.256 I print_info: n_gqa            = 1
0.00.050.257 I print_info: n_embd_k_gqa     = 2048
0.00.050.258 I print_info: n_embd_v_gqa     = 2048
0.00.050.258 I print_info: f_norm_eps       = 1.0e-05
0.00.050.259 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.259 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.259 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.259 I print_info: f_logit_scale    = 0.0e+00
0.00.050.260 I print_info: n_ff             = 8192
0.00.050.260 I print_info: n_expert         = 0
0.00.050.260 I print_info: n_expert_used    = 0
0.00.050.260 I print_info: causal attn      = 1
0.00.050.260 I print_info: pooling type     = 0
0.00.050.263 I print_info: rope type        = 2
0.00.050.264 I print_info: rope scaling     = linear
0.00.050.264 I print_info: freq_base_train  = 10000.0
0.00.050.264 I print_info: freq_scale_train = 1
0.00.050.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.265 I print_info: rope_finetuned   = unknown
0.00.050.265 I print_info: ssm_d_conv       = 0
0.00.050.265 I print_info: ssm_d_inner      = 0
0.00.050.266 I print_info: ssm_d_state      = 0
0.00.050.266 I print_info: ssm_dt_rank      = 0
0.00.050.266 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.267 I print_info: model type       = 1.4B
0.00.050.267 I print_info: model params     = 1.41 B
0.00.050.267 I print_info: general.name     = 1.4B
0.00.050.268 I print_info: vocab type       = BPE
0.00.050.268 I print_info: n_vocab          = 50304
0.00.050.268 I print_info: n_merges         = 50009
0.00.050.270 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: LF token         = 128 'Ä'
0.00.050.271 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.271 I print_info: max token length = 1024
0.00.052.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.183 I load_tensors: offloading output layer to GPU
0.00.052.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.194 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.195 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.468 I llama_init_from_model: n_seq_max     = 1
0.00.052.469 I llama_init_from_model: n_ctx         = 128
0.00.052.469 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.469 I llama_init_from_model: n_batch       = 128
0.00.052.469 I llama_init_from_model: n_ubatch      = 128
0.00.052.469 I llama_init_from_model: flash_attn    = 0
0.00.052.470 I llama_init_from_model: freq_base     = 10000.0
0.00.052.470 I llama_init_from_model: freq_scale    = 1
0.00.052.470 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.471 I ggml_metal_init: allocating
0.00.052.474 I ggml_metal_init: found device: Apple M4
0.00.052.476 I ggml_metal_init: picking default device: Apple M4
0.00.052.948 I ggml_metal_init: using embedded metal library
0.00.055.314 I ggml_metal_init: GPU name:   Apple M4
0.00.055.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.316 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.316 I ggml_metal_init: simdgroup reduction   = true
0.00.055.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.317 I ggml_metal_init: has bfloat            = true
0.00.055.317 I ggml_metal_init: use bfloat            = true
0.00.055.317 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.318 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.942 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.276 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.279 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.292 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.146 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.147 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.147 I llama_init_from_model: graph nodes  = 967
0.00.067.147 I llama_init_from_model: graph splits = 2
0.00.067.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.391 I 
0.00.480.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.449 I perplexity: tokenizing the input ..
0.00.488.497 I perplexity: tokenization took 8.046 ms
0.00.488.507 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.697 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.620.857 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.620.886 I llama_perf_context_print:        load time =     471.60 ms
0.00.620.887 I llama_perf_context_print: prompt eval time =     130.96 ms /   128 tokens (    1.02 ms per token,   977.38 tokens per second)
0.00.620.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.620.888 I llama_perf_context_print:       total time =     140.50 ms /   129 tokens
0.00.621.321 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.318 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.330 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.296 I llama_model_loader: - type  f32:  194 tensors
0.00.026.297 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.297 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.297 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.298 I print_info: file format = GGUF V3 (latest)
0.00.026.298 I print_info: file type   = Q4_K - Medium
0.00.026.299 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.706 I load: special tokens cache size = 25
0.00.050.457 I load: token to piece cache size = 0.2984 MB
0.00.050.459 I print_info: arch             = gptneox
0.00.050.460 I print_info: vocab_only       = 0
0.00.050.460 I print_info: n_ctx_train      = 2048
0.00.050.460 I print_info: n_embd           = 2048
0.00.050.460 I print_info: n_layer          = 24
0.00.050.463 I print_info: n_head           = 16
0.00.050.464 I print_info: n_head_kv        = 16
0.00.050.464 I print_info: n_rot            = 32
0.00.050.464 I print_info: n_swa            = 0
0.00.050.465 I print_info: n_embd_head_k    = 128
0.00.050.465 I print_info: n_embd_head_v    = 128
0.00.050.465 I print_info: n_gqa            = 1
0.00.050.466 I print_info: n_embd_k_gqa     = 2048
0.00.050.467 I print_info: n_embd_v_gqa     = 2048
0.00.050.467 I print_info: f_norm_eps       = 1.0e-05
0.00.050.468 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.468 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.468 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.468 I print_info: f_logit_scale    = 0.0e+00
0.00.050.469 I print_info: n_ff             = 8192
0.00.050.469 I print_info: n_expert         = 0
0.00.050.469 I print_info: n_expert_used    = 0
0.00.050.469 I print_info: causal attn      = 1
0.00.050.470 I print_info: pooling type     = 0
0.00.050.470 I print_info: rope type        = 2
0.00.050.471 I print_info: rope scaling     = linear
0.00.050.473 I print_info: freq_base_train  = 10000.0
0.00.050.474 I print_info: freq_scale_train = 1
0.00.050.474 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.474 I print_info: rope_finetuned   = unknown
0.00.050.474 I print_info: ssm_d_conv       = 0
0.00.050.474 I print_info: ssm_d_inner      = 0
0.00.050.475 I print_info: ssm_d_state      = 0
0.00.050.475 I print_info: ssm_dt_rank      = 0
0.00.050.475 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.475 I print_info: model type       = 1.4B
0.00.050.475 I print_info: model params     = 1.41 B
0.00.050.476 I print_info: general.name     = 1.4B
0.00.050.476 I print_info: vocab type       = BPE
0.00.050.476 I print_info: n_vocab          = 50304
0.00.050.476 I print_info: n_merges         = 50009
0.00.050.477 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.481 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.481 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.481 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.482 I print_info: LF token         = 128 'Ä'
0.00.050.482 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.482 I print_info: max token length = 1024
0.00.052.410 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.410 I load_tensors: offloading output layer to GPU
0.00.052.411 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.421 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.422 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.702 I llama_init_from_model: n_seq_max     = 1
0.00.052.703 I llama_init_from_model: n_ctx         = 128
0.00.052.703 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.703 I llama_init_from_model: n_batch       = 128
0.00.052.703 I llama_init_from_model: n_ubatch      = 128
0.00.052.703 I llama_init_from_model: flash_attn    = 0
0.00.052.704 I llama_init_from_model: freq_base     = 10000.0
0.00.052.704 I llama_init_from_model: freq_scale    = 1
0.00.052.704 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.705 I ggml_metal_init: allocating
0.00.052.707 I ggml_metal_init: found device: Apple M4
0.00.052.709 I ggml_metal_init: picking default device: Apple M4
0.00.053.199 I ggml_metal_init: using embedded metal library
0.00.055.531 I ggml_metal_init: GPU name:   Apple M4
0.00.055.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.533 I ggml_metal_init: simdgroup reduction   = true
0.00.055.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.533 I ggml_metal_init: has bfloat            = true
0.00.055.533 I ggml_metal_init: use bfloat            = true
0.00.055.534 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.025 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.271 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.285 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.178 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.179 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.180 I llama_init_from_model: graph nodes  = 967
0.00.067.180 I llama_init_from_model: graph splits = 2
0.00.067.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.066 I 
0.00.520.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.116 I perplexity: tokenizing the input ..
0.00.527.745 I perplexity: tokenization took 7.627 ms
0.00.527.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.662.264 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.663.421 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.663.438 I llama_perf_context_print:        load time =     510.00 ms
0.00.663.440 I llama_perf_context_print: prompt eval time =     134.28 ms /   128 tokens (    1.05 ms per token,   953.25 tokens per second)
0.00.663.450 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.663.451 I llama_perf_context_print:       total time =     143.37 ms /   129 tokens
0.00.663.956 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.077s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.020 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.024 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.024 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.025 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.851 I llama_model_loader: - type  f32:  194 tensors
0.00.024.852 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.852 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.852 I print_info: file format = GGUF V3 (latest)
0.00.024.853 I print_info: file type   = Q5_K - Medium
0.00.024.854 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.014 I load: special tokens cache size = 25
0.00.050.172 I load: token to piece cache size = 0.2984 MB
0.00.050.175 I print_info: arch             = gptneox
0.00.050.175 I print_info: vocab_only       = 0
0.00.050.175 I print_info: n_ctx_train      = 2048
0.00.050.175 I print_info: n_embd           = 2048
0.00.050.175 I print_info: n_layer          = 24
0.00.050.178 I print_info: n_head           = 16
0.00.050.179 I print_info: n_head_kv        = 16
0.00.050.179 I print_info: n_rot            = 32
0.00.050.179 I print_info: n_swa            = 0
0.00.050.180 I print_info: n_embd_head_k    = 128
0.00.050.180 I print_info: n_embd_head_v    = 128
0.00.050.181 I print_info: n_gqa            = 1
0.00.050.181 I print_info: n_embd_k_gqa     = 2048
0.00.050.182 I print_info: n_embd_v_gqa     = 2048
0.00.050.183 I print_info: f_norm_eps       = 1.0e-05
0.00.050.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.183 I print_info: f_logit_scale    = 0.0e+00
0.00.050.184 I print_info: n_ff             = 8192
0.00.050.184 I print_info: n_expert         = 0
0.00.050.185 I print_info: n_expert_used    = 0
0.00.050.185 I print_info: causal attn      = 1
0.00.050.185 I print_info: pooling type     = 0
0.00.050.185 I print_info: rope type        = 2
0.00.050.185 I print_info: rope scaling     = linear
0.00.050.186 I print_info: freq_base_train  = 10000.0
0.00.050.186 I print_info: freq_scale_train = 1
0.00.050.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.187 I print_info: rope_finetuned   = unknown
0.00.050.187 I print_info: ssm_d_conv       = 0
0.00.050.187 I print_info: ssm_d_inner      = 0
0.00.050.187 I print_info: ssm_d_state      = 0
0.00.050.187 I print_info: ssm_dt_rank      = 0
0.00.050.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.187 I print_info: model type       = 1.4B
0.00.050.188 I print_info: model params     = 1.41 B
0.00.050.188 I print_info: general.name     = 1.4B
0.00.050.188 I print_info: vocab type       = BPE
0.00.050.189 I print_info: n_vocab          = 50304
0.00.050.189 I print_info: n_merges         = 50009
0.00.050.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: LF token         = 128 'Ä'
0.00.050.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: max token length = 1024
0.00.052.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.119 I load_tensors: offloading output layer to GPU
0.00.052.119 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.130 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.131 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.402 I llama_init_from_model: n_seq_max     = 1
0.00.052.403 I llama_init_from_model: n_ctx         = 128
0.00.052.403 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.403 I llama_init_from_model: n_batch       = 128
0.00.052.404 I llama_init_from_model: n_ubatch      = 128
0.00.052.404 I llama_init_from_model: flash_attn    = 0
0.00.052.404 I llama_init_from_model: freq_base     = 10000.0
0.00.052.404 I llama_init_from_model: freq_scale    = 1
0.00.052.405 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.405 I ggml_metal_init: allocating
0.00.052.408 I ggml_metal_init: found device: Apple M4
0.00.052.410 I ggml_metal_init: picking default device: Apple M4
0.00.052.878 I ggml_metal_init: using embedded metal library
0.00.055.225 I ggml_metal_init: GPU name:   Apple M4
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.228 I ggml_metal_init: simdgroup reduction   = true
0.00.055.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.228 I ggml_metal_init: has bfloat            = true
0.00.055.228 I ggml_metal_init: use bfloat            = true
0.00.055.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.539 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.749 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.751 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.766 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.608 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.609 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.609 I llama_init_from_model: graph nodes  = 967
0.00.066.609 I llama_init_from_model: graph splits = 2
0.00.066.610 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.025 I 
0.00.644.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.090 I perplexity: tokenizing the input ..
0.00.651.826 I perplexity: tokenization took 7.734 ms
0.00.651.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.588 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.136 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.151 I llama_perf_context_print:        load time =     635.01 ms
0.00.793.152 I llama_perf_context_print: prompt eval time =     139.50 ms /   128 tokens (    1.09 ms per token,   917.58 tokens per second)
0.00.793.153 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.154 I llama_perf_context_print:       total time =     149.13 ms /   129 tokens
0.00.793.484 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.679 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.455 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.466 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.358 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.359 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.360 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.360 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.360 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.361 I llama_model_loader: - type  f32:  194 tensors
0.00.028.361 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.362 I print_info: file format = GGUF V3 (latest)
0.00.028.363 I print_info: file type   = Q6_K
0.00.028.365 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.048.844 I load: special tokens cache size = 25
0.00.055.234 I load: token to piece cache size = 0.2984 MB
0.00.055.238 I print_info: arch             = gptneox
0.00.055.239 I print_info: vocab_only       = 0
0.00.055.239 I print_info: n_ctx_train      = 2048
0.00.055.239 I print_info: n_embd           = 2048
0.00.055.239 I print_info: n_layer          = 24
0.00.055.243 I print_info: n_head           = 16
0.00.055.244 I print_info: n_head_kv        = 16
0.00.055.244 I print_info: n_rot            = 32
0.00.055.244 I print_info: n_swa            = 0
0.00.055.244 I print_info: n_embd_head_k    = 128
0.00.055.244 I print_info: n_embd_head_v    = 128
0.00.055.245 I print_info: n_gqa            = 1
0.00.055.248 I print_info: n_embd_k_gqa     = 2048
0.00.055.249 I print_info: n_embd_v_gqa     = 2048
0.00.055.250 I print_info: f_norm_eps       = 1.0e-05
0.00.055.250 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.250 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.250 I print_info: f_logit_scale    = 0.0e+00
0.00.055.251 I print_info: n_ff             = 8192
0.00.055.251 I print_info: n_expert         = 0
0.00.055.251 I print_info: n_expert_used    = 0
0.00.055.252 I print_info: causal attn      = 1
0.00.055.253 I print_info: pooling type     = 0
0.00.055.254 I print_info: rope type        = 2
0.00.055.254 I print_info: rope scaling     = linear
0.00.055.255 I print_info: freq_base_train  = 10000.0
0.00.055.255 I print_info: freq_scale_train = 1
0.00.055.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.255 I print_info: rope_finetuned   = unknown
0.00.055.255 I print_info: ssm_d_conv       = 0
0.00.055.255 I print_info: ssm_d_inner      = 0
0.00.055.256 I print_info: ssm_d_state      = 0
0.00.055.256 I print_info: ssm_dt_rank      = 0
0.00.055.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.256 I print_info: model type       = 1.4B
0.00.055.256 I print_info: model params     = 1.41 B
0.00.055.257 I print_info: general.name     = 1.4B
0.00.055.257 I print_info: vocab type       = BPE
0.00.055.257 I print_info: n_vocab          = 50304
0.00.055.257 I print_info: n_merges         = 50009
0.00.055.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.258 I print_info: LF token         = 128 'Ä'
0.00.055.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.259 I print_info: max token length = 1024
0.00.057.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.261 I load_tensors: offloading output layer to GPU
0.00.057.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.273 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.057.274 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.057.679 I llama_init_from_model: n_seq_max     = 1
0.00.057.680 I llama_init_from_model: n_ctx         = 128
0.00.057.680 I llama_init_from_model: n_ctx_per_seq = 128
0.00.057.680 I llama_init_from_model: n_batch       = 128
0.00.057.680 I llama_init_from_model: n_ubatch      = 128
0.00.057.680 I llama_init_from_model: flash_attn    = 0
0.00.057.681 I llama_init_from_model: freq_base     = 10000.0
0.00.057.681 I llama_init_from_model: freq_scale    = 1
0.00.057.681 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.682 I ggml_metal_init: allocating
0.00.057.686 I ggml_metal_init: found device: Apple M4
0.00.057.688 I ggml_metal_init: picking default device: Apple M4
0.00.058.184 I ggml_metal_init: using embedded metal library
0.00.060.697 I ggml_metal_init: GPU name:   Apple M4
0.00.060.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.700 I ggml_metal_init: simdgroup reduction   = true
0.00.060.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.700 I ggml_metal_init: has bfloat            = true
0.00.060.700 I ggml_metal_init: use bfloat            = true
0.00.060.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.429 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.435 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.451 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.453 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.454 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.454 I llama_init_from_model: graph nodes  = 967
0.00.073.455 I llama_init_from_model: graph splits = 2
0.00.073.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.256.577 I 
0.00.256.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.256.623 I perplexity: tokenizing the input ..
0.00.264.217 I perplexity: tokenization took 7.592 ms
0.00.264.234 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.403.439 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.405.213 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.405.234 I llama_perf_context_print:        load time =     245.89 ms
0.00.405.235 I llama_perf_context_print: prompt eval time =     138.98 ms /   128 tokens (    1.09 ms per token,   921.00 tokens per second)
0.00.405.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.405.238 I llama_perf_context_print:       total time =     148.66 ms /   129 tokens
0.00.405.548 I ggml_metal_free: deallocating

real	0m0.422s
user	0m0.082s
sys	0m0.048s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.186 I build: 4549 (466ea66f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.833 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.849 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.849 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.495 I llama_model_loader: - type  f32:  194 tensors
0.00.037.495 I llama_model_loader: - type  f16:   98 tensors
0.00.037.496 I print_info: file format = GGUF V3 (latest)
0.00.037.497 I print_info: file type   = all F32 (guessed)
0.00.037.498 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.877 I load: special tokens cache size = 25
0.00.062.835 I load: token to piece cache size = 0.2984 MB
0.00.062.840 I print_info: arch             = gptneox
0.00.062.840 I print_info: vocab_only       = 0
0.00.062.840 I print_info: n_ctx_train      = 2048
0.00.062.840 I print_info: n_embd           = 2048
0.00.062.841 I print_info: n_layer          = 24
0.00.062.845 I print_info: n_head           = 16
0.00.062.845 I print_info: n_head_kv        = 16
0.00.062.846 I print_info: n_rot            = 32
0.00.062.846 I print_info: n_swa            = 0
0.00.062.846 I print_info: n_embd_head_k    = 128
0.00.062.846 I print_info: n_embd_head_v    = 128
0.00.062.847 I print_info: n_gqa            = 1
0.00.062.848 I print_info: n_embd_k_gqa     = 2048
0.00.062.848 I print_info: n_embd_v_gqa     = 2048
0.00.062.849 I print_info: f_norm_eps       = 1.0e-05
0.00.062.849 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.850 I print_info: f_logit_scale    = 0.0e+00
0.00.062.850 I print_info: n_ff             = 8192
0.00.062.850 I print_info: n_expert         = 0
0.00.062.851 I print_info: n_expert_used    = 0
0.00.062.851 I print_info: causal attn      = 1
0.00.062.851 I print_info: pooling type     = 0
0.00.062.851 I print_info: rope type        = 2
0.00.062.851 I print_info: rope scaling     = linear
0.00.062.852 I print_info: freq_base_train  = 10000.0
0.00.062.853 I print_info: freq_scale_train = 1
0.00.062.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.855 I print_info: rope_finetuned   = unknown
0.00.062.855 I print_info: ssm_d_conv       = 0
0.00.062.855 I print_info: ssm_d_inner      = 0
0.00.062.855 I print_info: ssm_d_state      = 0
0.00.062.855 I print_info: ssm_dt_rank      = 0
0.00.062.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.856 I print_info: model type       = 1.4B
0.00.062.856 I print_info: model params     = 1.41 B
0.00.062.856 I print_info: general.name     = 1.4B
0.00.062.857 I print_info: vocab type       = BPE
0.00.062.857 I print_info: n_vocab          = 50304
0.00.062.857 I print_info: n_merges         = 50009
0.00.062.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.858 I print_info: LF token         = 128 'Ä'
0.00.062.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.858 I print_info: max token length = 1024
0.00.064.482 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.482 I load_tensors: offloading output layer to GPU
0.00.064.482 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.493 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.064.494 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.064.784 I llama_init_from_model: n_seq_max     = 1
0.00.064.785 I llama_init_from_model: n_ctx         = 128
0.00.064.785 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.785 I llama_init_from_model: n_batch       = 128
0.00.064.786 I llama_init_from_model: n_ubatch      = 128
0.00.064.786 I llama_init_from_model: flash_attn    = 0
0.00.064.786 I llama_init_from_model: freq_base     = 10000.0
0.00.064.786 I llama_init_from_model: freq_scale    = 1
0.00.064.787 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.787 I ggml_metal_init: allocating
0.00.064.790 I ggml_metal_init: found device: Apple M4
0.00.064.792 I ggml_metal_init: picking default device: Apple M4
0.00.065.319 I ggml_metal_init: using embedded metal library
0.00.067.787 I ggml_metal_init: GPU name:   Apple M4
0.00.067.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.790 I ggml_metal_init: simdgroup reduction   = true
0.00.067.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.790 I ggml_metal_init: has bfloat            = true
0.00.067.790 I ggml_metal_init: use bfloat            = true
0.00.067.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.060 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.346 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.259 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.261 I llama_init_from_model: graph nodes  = 967
0.00.080.261 I llama_init_from_model: graph splits = 2
0.00.080.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.263 I 
0.00.080.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.080.302 I compute_imatrix: tokenizing the input ..
0.00.087.546 I compute_imatrix: tokenization took 7.243 ms
0.00.087.549 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.608.749 I compute_imatrix: 1.52 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.611.312 I llama_perf_context_print:        load time =    1591.81 ms
0.01.611.313 I llama_perf_context_print: prompt eval time =    1520.53 ms /   128 tokens (   11.88 ms per token,    84.18 tokens per second)
0.01.611.314 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.611.314 I llama_perf_context_print:       total time =    1594.37 ms /   129 tokens
0.01.611.907 I ggml_metal_free: deallocating

real	0m1.793s
user	0m0.145s
sys	0m0.226s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4549 (466ea66f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb0d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb0dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb0e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb0f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb11410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb13f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb15200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb17120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb18470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb19250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb1a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb1b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb1bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb1d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb20470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb21860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bb24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bb25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bb25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bb25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bb26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bb26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bb26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bb27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bb27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bb27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bb28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bb28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bb28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bb29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bb29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bb29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bb2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bb2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bb2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bb2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bb2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bb2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bb2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bb2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bb1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bb2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bb2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bb2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bb2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bb2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bb2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bb2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bb2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bb2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bb2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bb30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bb30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bb30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bb31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bb31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bb329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bb32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bb33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bb337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bb33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bb340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bb34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bb34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bb34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bb35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bb35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bb35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bb36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bb365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bb36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bb36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bb373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bb37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bb37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bb381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bb38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bb38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bb38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bb39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bb398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bb39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bb3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bb3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bb3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bb3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bb3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bb3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bb3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bb3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bb3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bb3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bb3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bb3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bb3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bb3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bb3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bb3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bb3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bb3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bb3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bb3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bb3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bb40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bb407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bb40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bb41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bb415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bb41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bb41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bb42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bb42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bb42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bb43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bb43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bb43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bb43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bb443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bb44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bb44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bb451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bb45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bb45b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bb45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bb46450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bb468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bb46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bb47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bb476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bb47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bb48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bb484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bb48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bb48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bb493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bb49940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bb49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bb4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bb4a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bb4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bb4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bb4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bb4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bb4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bb4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bb4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bb4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bb4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bb4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bb4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bb4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bb4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bb4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bb4fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bb501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bb50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bb50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bb511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bb516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bb51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bb52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bb526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bb52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bb53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bb536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bb53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bb54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bb546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bb54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bb55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bb556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bb55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bb56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bb566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bb56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bb57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bb57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bb57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bb58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bb58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bb58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bb59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bb59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bb59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bb5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bb5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bb5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bb5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bb5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bb5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bb5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bb5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bb5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bb5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bb5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bb5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bb5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bb5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bb5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bb5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bb5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bb5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bb600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bb60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bb60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bb610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bb615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bb61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bb61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bb623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bb62870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bb62d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bb631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bb63650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bb63af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bb63f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bb64430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bb648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bb64d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bb65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bb656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bb65b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bb660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bb667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bb66ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bb67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bb67d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bb67fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bb687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bb68a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bb690a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.141.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ba06690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ba06b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ba06f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ba073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ba07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ba07cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ba08130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ba085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ba08a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ba08e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ba092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ba09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ba0a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ba0acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ba0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ba0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ba0c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ba0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ba0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ba0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ba0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ba0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ba0edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ba0f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ba0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ba0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ba101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ba10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ba10a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ba10f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ba11370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ba118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ba11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ba11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ba12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ba128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ba12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ba13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ba13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ba13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ba13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ba14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ba147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ba14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ba150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ba15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ba15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ba15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ba16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ba166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ba16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ba16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ba17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ba17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ba17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ba18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ba186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ba18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ba19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ba194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ba19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ba19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ba1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ba1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ba1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ba1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ba1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ba1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ba1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ba1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ba1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ba1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ba1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ba1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ba1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ba1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ba1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ba1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ba1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ba1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ba1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ba1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ba1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ba1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ba203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ba20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ba20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ba21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ba21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ba219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ba21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ba222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ba22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ba22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ba23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ba23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ba238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ba23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ba241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ba24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ba24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ba24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ba25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ba25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ba25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ba260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ba26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ba269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ba26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ba272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ba27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ba27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ba27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ba28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ba288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ba28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ba291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ba29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ba29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ba29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ba2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ba2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ba2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ba2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ba2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ba2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ba2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ba2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ba2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ba2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ba2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ba2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ba2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ba2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ba2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ba2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ba2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ba2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ba2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ba2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ba2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ba300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ba30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ba30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ba30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ba31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ba316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ba31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ba31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ba32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ba32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ba32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ba33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ba335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ba33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ba33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ba34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ba347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ba34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ba35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ba354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ba35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ba35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ba36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ba366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ba36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ba37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ba37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ba37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ba38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ba385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ba38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ba38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ba39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ba39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ba39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ba3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ba3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ba3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ba3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ba3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ba3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ba3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ba3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ba3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ba3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ba3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ba3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ba3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ba3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ba3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ba3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ba3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ba3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ba3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ba3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ba3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ba3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ba401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ba40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ba40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ba40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ba414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ba419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ba41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ba42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ba42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ba42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ba43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ba435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ba44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ba443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ba44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ba44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ba45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ba45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ba46090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ba46650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ba46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ba471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ba47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ba47d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ba48310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ba488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ba48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ba49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ba49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ba49fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ba4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ba4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ba4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ba4b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ba4bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ba4c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ba4c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ba4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ba4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ba4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ba4df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ba4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ba4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ba4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ba4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ba4fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ba50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ba50750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ba50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ba512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ba51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ba51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ba52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ba529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ba52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ba53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ba53b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ba540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ba54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ba54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ba55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ba557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ba55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ba56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ba56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ba56ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ba57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ba57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ba58010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ba585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ba58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ba58fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ba594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ba599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ba59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ba5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ba5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ba5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ba5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ba5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ba5bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ba5c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ba5c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ba5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ba5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ba5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ba5e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ba5e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ba5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ba5f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ba5faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ba5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ba603c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ba5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ba4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ba4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ba49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ba47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ba56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ba54390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ba52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ba4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ba48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ba457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ba4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ba4b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ba50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ba4dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ba55a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ba49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ba52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ba4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ba4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ba49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ba57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ba46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ba44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ba46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ba57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ba4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ba54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ba4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ba4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ba51590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ba48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ba51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ba53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ba47a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ba56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ba53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ba4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ba582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ba46910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ba57d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ba45d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ba56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ba50450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ba526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ba554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ba53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ba4bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ba43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ba06230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ba5f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ba0d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ba60aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ba60d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ba61020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ba612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ba615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ba61860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ba61b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ba61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ba620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ba62360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ba62620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ba628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ba62ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ba62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ba63120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ba633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ba636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ba63960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ba63c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ba63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ba641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ba64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ba64720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ba649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ba64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ba64f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ba65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ba654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ba657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ba65a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ba65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ba65fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ba662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ba66560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ba66820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ba66ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ba66da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ba67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ba67320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ba675e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ba678a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ba67b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ba67e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ba680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ba683a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ba68660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ba68920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ba68be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ba68ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ba69160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ba69420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ba696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ba699a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ba69c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ba69f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ba6a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ba6a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ba6a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ba6aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ba6ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ba6afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ba6b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ba6b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ba6b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ba6baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ba6bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ba6c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ba6c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ba6c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ba6c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ba6cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ba6cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ba6d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ba6d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ba6d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ba6d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ba6dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ba6de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ba6e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ba6e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ba6e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ba6e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ba6ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ba6eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ba6f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ba6f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ba6f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ba6f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ba6fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ba6ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ba70220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ba704e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ba707a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ba70a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ba70d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ba70fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ba712a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ba71560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ba71820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ba71ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ba71da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ba72060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ba72320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ba725e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ba728a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ba72b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ba72e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ba730e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ba733a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ba73660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ba73920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ba73be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ba73ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ba74160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ba74420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ba746e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ba749a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ba74c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ba74f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ba751e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ba754a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ba75760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ba75a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ba75ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ba75fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ba76260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ba76520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ba767e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ba76aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ba76d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ba77020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ba772e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ba775a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ba77860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ba77b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ba77de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ba780a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ba78360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ba78620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ba788e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ba78ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ba78e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ba79120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ba793e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ba796a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ba79960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ba79c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ba79ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ba7a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ba7a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ba7a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ba7a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ba7aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ba7af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ba7b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ba7b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ba7b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ba7ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ba7c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ba7c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ba7c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ba7c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ba7cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ba7cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ba7d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ba7d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ba7d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ba7db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ba7e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ba7e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ba7eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ba7f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ba7f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ba7fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ba800b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ba80600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ba80b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ba810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ba815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ba81b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ba82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ba825e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ba82b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ba83080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ba835d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ba83b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ba84070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ba845c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ba84b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ba85060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ba855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ba85b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ba86050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ba865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ba86af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ba87040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ba87590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ba87ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ba88030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ba88580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ba88ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ba89020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ba89570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ba89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ba8a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ba8a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ba8aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ba8b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ba8b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ba8baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ba8bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ba8c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ba8ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ba8cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ba8d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ba8d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ba8dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ba8dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ba8e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ba8e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ba8eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ba8f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ba8f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ba8fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ba902b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ba907b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ba90cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ba911b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ba916b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ba91bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ba920b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ba92ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ba931e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ba93900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ba94020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ba942e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ba94ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ba94d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ba953a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.756s
user	0m0.295s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4549 (466ea66f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e11060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e12cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e21380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e22ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e2ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e2b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e2bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e2d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e32120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e32e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e48360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e4c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e50a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e53b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e54850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e54da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e55840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e56830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e56d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e57820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e57d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e58810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e58d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e59800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e5a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e5a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e5b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e5b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e5bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e5c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e5cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e5d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e5d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e5dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e5e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e5ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e5f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e5f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e5fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e60790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e60ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e61230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e61cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e62cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e63210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e64200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e65c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e67120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e67a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e683a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e68840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146e68ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146e69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146e69620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146e69ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146e69f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146e6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146e6a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146e6ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146e6b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146e6b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146e6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146e6c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146e6cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146e6d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146e6d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146e6de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146e6e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146e6e730 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.089.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e6e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e6d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e1cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e6eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e6ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e6f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e6f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e6f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e6f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e6fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e6fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e70190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e70450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e70710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e709d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e70c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e70f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e71210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e714d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e71790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e71a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e71d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e71fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e72290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e72550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e72810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e72ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e72d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e73050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e73310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e735d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e73890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e73b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e73e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e740d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e74390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e74650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e74910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e74bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e74e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e75150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e75410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e756d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e75990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e75c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e75f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e761d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e76490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e76750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e76a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e76cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e76f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e77250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e77510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e777d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e77a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e77d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e78010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e782d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e78590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e78850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e78b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e78dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e79090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e79350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e79610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e798d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e79b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e79e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e7a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e7a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e7a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e7a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e7ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e7aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e7b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e7b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e7b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e7b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e7bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e7bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e7c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e7c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e7c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e7ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e7cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e7cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e7d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e7d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e7d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e7dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e7dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e7e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e7e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e7e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e7e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e7eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e7ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e7f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e7f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e7f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e7f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e7fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e7fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e80150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e80410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e806d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e80990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e80c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e80f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e81490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e81750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e81a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e81cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e81f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e82250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e82510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e827d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e82a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e82d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e83010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e832d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e83590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e83850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e83b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e83dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e84090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e84350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e84610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e848d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e84b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e84e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e85110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e853d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e85690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e85950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e85c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e85ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e86190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e86450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e86710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e869d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e86c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e86f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e87210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e874d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e87790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e87a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e87d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e87fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e88290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e88550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e88810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e88ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e88d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e89050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e89310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e895d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e89890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e89b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e89e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e8a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e8a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e8a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e8a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e8abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e8ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e8b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e8b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e8b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e8b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e8bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e8bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e8c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e8c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e8c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e8ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e8ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e8cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e8d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e8d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e8d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e8da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e8dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e8e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e8e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e8e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e8ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e8f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e8f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e8f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e8fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e90010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e90480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e90d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e911d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e91640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e91ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e91f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e92390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e92800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e92c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e930e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e93550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e939c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e93e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e942a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e94710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e94b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e94ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e95460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e95d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e961b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e96620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e96a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e96f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e97370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e977e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e97c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e980c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e98530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e989a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e98e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e99280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e99b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e99fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e9a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e9a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e9ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e9b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e9b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e9ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e9bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e9c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e9c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e9cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e9d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e9d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e9d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e9ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e9e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e9e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e9eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e9efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e9f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e9f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e9fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146ea0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146ea05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146ea0a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146ea0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146ea1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146ea17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146ea1c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146ea2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146ea24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146ea2960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146ea33d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146ea3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146ea4210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146ea4930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146ea4bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146ea53e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146ea56a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146ea5cb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1488046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1488058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1488065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148807a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148808530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148808ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1488094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148809c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14880a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14880aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14880b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14880b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14880c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14880c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14880cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14880d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14880dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14880dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14880e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14880e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14880eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14880efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14880f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14880f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14880fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148810080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1488104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148810960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148810dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148811240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1488116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148811b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148811f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148812400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148812870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148812ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1488135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148813a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148813ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148814310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148814780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148815060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1488154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148815db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148816220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148816790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148817100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1488179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148817e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1488182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148818730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148818ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148819480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1488198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148819d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14881a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14881a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14881aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14881af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14881b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14881b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14881bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14881c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14881c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14881c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14881ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14881d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14881d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14881db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14881dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14881e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14881e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14881ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14881f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14881f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14881fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14881ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1488207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148820c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1488210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148821530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1488219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148821e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148822280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1488226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148822b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148822fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148823cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148823f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148824400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148824ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1488255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148825ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148826310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148826780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148826bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148827060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1488274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148827940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148827db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148828220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148828690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148828f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1488293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148829850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148829cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14882a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14882a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14882aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14882ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14882b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14882b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14882bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14882c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14882c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14882c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14882cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14882d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14882d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14882dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14882df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14882e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14882e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14882eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14882f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14882f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14882f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14882fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1488302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148830740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148830bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148831020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148831490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148831900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1488321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148832650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148832ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148832f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1488333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148833810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1488340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148834560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1488349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148834e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1488352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148835b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148836470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1488368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148836d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1488371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148837630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148837aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148837f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148838380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1488387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1488390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148839540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1488399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14883a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14883a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14883ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14883afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14883b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14883b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14883bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14883c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14883c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14883ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14883cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14883d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14883d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14883dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14883e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14883e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14883e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14883ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14883f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14883f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14883fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14883ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1488408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148840d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148841d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148842280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1488426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148842b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148842fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148843440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1488438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148843d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148844190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148844600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148844a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148844ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148845350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1488457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148845c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1488460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148846510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148846980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148846df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148847260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1488476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148847b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148847fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148848420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148848890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148848d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148849170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1488495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148849a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148849ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14884a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14884a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14884ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14884b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14884b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14884b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14884bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14884c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14884c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14884cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14884cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14884d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14884d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14884dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14884e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14884e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14884ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14884eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14884f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14884f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14884fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148850060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1488504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148850940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148850db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148851220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148851690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148851b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148851f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1488523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148852850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148852cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1488535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148853a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148853e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1488542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148854760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148854bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148855040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1488554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148855920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148856390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1488571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1488578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148857bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148858020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148858620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148858c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.244s
sys	0m0.135s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
