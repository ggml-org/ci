### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.39 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.28 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  176.89 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.91 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.52 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 218.87 sec*proc (27 tests)

Total Test time (real) = 218.88 sec

real	3m38.992s
user	7m40.561s
sys	0m5.434s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.88 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.22 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   27.44 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.07 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.23 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.04 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  48.98 sec*proc (27 tests)

Total Test time (real) =  48.99 sec

real	0m48.996s
user	1m11.436s
sys	0m4.577s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.104 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.473 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.479 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.481 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.482 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.483 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.483 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.484 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.485 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.486 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.486 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.486 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.490 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.490 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.491 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.491 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.492 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.492 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.493 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.163 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.165 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.166 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.166 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.167 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.027.167 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.167 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.027.168 I llama_model_loader: - type  f32:  124 tensors
0.00.027.169 I llama_model_loader: - type  f16:   73 tensors
0.00.031.320 I llm_load_vocab: special tokens cache size = 5
0.00.033.511 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.033.514 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.033.515 I llm_load_print_meta: arch             = bert
0.00.033.515 I llm_load_print_meta: vocab type       = WPM
0.00.033.516 I llm_load_print_meta: n_vocab          = 30522
0.00.033.516 I llm_load_print_meta: n_merges         = 0
0.00.033.516 I llm_load_print_meta: vocab_only       = 0
0.00.033.516 I llm_load_print_meta: n_ctx_train      = 512
0.00.033.517 I llm_load_print_meta: n_embd           = 384
0.00.033.517 I llm_load_print_meta: n_layer          = 12
0.00.033.520 I llm_load_print_meta: n_head           = 12
0.00.033.520 I llm_load_print_meta: n_head_kv        = 12
0.00.033.521 I llm_load_print_meta: n_rot            = 32
0.00.033.521 I llm_load_print_meta: n_swa            = 0
0.00.033.521 I llm_load_print_meta: n_embd_head_k    = 32
0.00.033.521 I llm_load_print_meta: n_embd_head_v    = 32
0.00.033.522 I llm_load_print_meta: n_gqa            = 1
0.00.033.524 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.033.525 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.033.526 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.033.526 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.033.526 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.033.526 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.033.527 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.033.527 I llm_load_print_meta: n_ff             = 1536
0.00.033.528 I llm_load_print_meta: n_expert         = 0
0.00.033.528 I llm_load_print_meta: n_expert_used    = 0
0.00.033.528 I llm_load_print_meta: causal attn      = 0
0.00.033.528 I llm_load_print_meta: pooling type     = 2
0.00.033.528 I llm_load_print_meta: rope type        = 2
0.00.033.529 I llm_load_print_meta: rope scaling     = linear
0.00.033.529 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.033.530 I llm_load_print_meta: freq_scale_train = 1
0.00.033.530 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.033.530 I llm_load_print_meta: rope_finetuned   = unknown
0.00.033.530 I llm_load_print_meta: ssm_d_conv       = 0
0.00.033.531 I llm_load_print_meta: ssm_d_inner      = 0
0.00.033.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.033.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.033.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.033.543 I llm_load_print_meta: model type       = 33M
0.00.033.545 I llm_load_print_meta: model ftype      = F16
0.00.033.546 I llm_load_print_meta: model params     = 33.21 M
0.00.033.547 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.033.547 I llm_load_print_meta: general.name     = Bge Small
0.00.033.547 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.033.548 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.033.548 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.033.548 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.033.548 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.033.549 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.033.549 I llm_load_print_meta: max token length = 21
0.00.035.415 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.035.415 I llm_load_tensors: offloading output layer to GPU
0.00.035.416 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.035.439 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.441 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.936 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.938 I llama_new_context_with_model: n_ctx         = 512
0.00.035.938 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.938 I llama_new_context_with_model: n_batch       = 2048
0.00.035.939 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.939 I llama_new_context_with_model: flash_attn    = 0
0.00.035.939 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.940 I llama_new_context_with_model: freq_scale    = 1
0.00.035.940 I ggml_metal_init: allocating
0.00.035.944 I ggml_metal_init: found device: Apple M4
0.00.035.947 I ggml_metal_init: picking default device: Apple M4
0.00.036.682 I ggml_metal_init: using embedded metal library
0.00.040.084 I ggml_metal_init: GPU name:   Apple M4
0.00.040.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.088 I ggml_metal_init: simdgroup reduction   = true
0.00.040.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.088 I ggml_metal_init: has bfloat            = true
0.00.040.089 I ggml_metal_init: use bfloat            = true
0.00.040.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.020 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.022 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.024 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.051.889 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.051.890 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.051.891 I llama_new_context_with_model: graph nodes  = 429
0.00.051.891 I llama_new_context_with_model: graph splits = 2
0.00.051.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.297 I 
0.00.058.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.058.986 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.724 I llama_perf_context_print:        load time =      40.50 ms
0.00.063.727 I llama_perf_context_print: prompt eval time =       4.59 ms /     9 tokens (    0.51 ms per token,  1960.78 tokens per second)
0.00.063.727 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.728 I llama_perf_context_print:       total time =       5.43 ms /    10 tokens
0.00.063.853 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.353 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.617 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.623 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.624 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.624 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.624 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.625 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.626 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.626 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.626 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.627 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.629 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.629 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.630 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.630 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.630 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.630 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.631 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.099 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.100 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.101 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.101 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.101 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.016.101 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.102 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.016.102 I llama_model_loader: - type  f32:  124 tensors
0.00.016.102 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.660 I llm_load_vocab: special tokens cache size = 5
0.00.020.006 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.009 I llm_load_print_meta: arch             = bert
0.00.020.009 I llm_load_print_meta: vocab type       = WPM
0.00.020.009 I llm_load_print_meta: n_vocab          = 30522
0.00.020.009 I llm_load_print_meta: n_merges         = 0
0.00.020.010 I llm_load_print_meta: vocab_only       = 0
0.00.020.010 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.010 I llm_load_print_meta: n_embd           = 384
0.00.020.010 I llm_load_print_meta: n_layer          = 12
0.00.020.012 I llm_load_print_meta: n_head           = 12
0.00.020.013 I llm_load_print_meta: n_head_kv        = 12
0.00.020.013 I llm_load_print_meta: n_rot            = 32
0.00.020.013 I llm_load_print_meta: n_swa            = 0
0.00.020.013 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.014 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.014 I llm_load_print_meta: n_gqa            = 1
0.00.020.015 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.015 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.016 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.016 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.017 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.018 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.019 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.019 I llm_load_print_meta: n_ff             = 1536
0.00.020.019 I llm_load_print_meta: n_expert         = 0
0.00.020.020 I llm_load_print_meta: n_expert_used    = 0
0.00.020.020 I llm_load_print_meta: causal attn      = 0
0.00.020.021 I llm_load_print_meta: pooling type     = 2
0.00.020.021 I llm_load_print_meta: rope type        = 2
0.00.020.022 I llm_load_print_meta: rope scaling     = linear
0.00.020.022 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.022 I llm_load_print_meta: freq_scale_train = 1
0.00.020.022 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.023 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.023 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.023 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.023 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.023 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.023 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.029 I llm_load_print_meta: model type       = 33M
0.00.020.029 I llm_load_print_meta: model ftype      = Q8_0
0.00.020.030 I llm_load_print_meta: model params     = 33.21 M
0.00.020.030 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.020.030 I llm_load_print_meta: general.name     = Bge Small
0.00.020.031 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.031 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.031 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.031 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.032 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.032 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.032 I llm_load_print_meta: max token length = 21
0.00.021.389 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.021.389 I llm_load_tensors: offloading output layer to GPU
0.00.021.389 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.021.395 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.396 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.745 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.745 I llama_new_context_with_model: n_ctx         = 512
0.00.021.746 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.746 I llama_new_context_with_model: n_batch       = 2048
0.00.021.746 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.746 I llama_new_context_with_model: flash_attn    = 0
0.00.021.747 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.747 I llama_new_context_with_model: freq_scale    = 1
0.00.021.747 I ggml_metal_init: allocating
0.00.021.755 I ggml_metal_init: found device: Apple M4
0.00.021.762 I ggml_metal_init: picking default device: Apple M4
0.00.022.296 I ggml_metal_init: using embedded metal library
0.00.024.442 I ggml_metal_init: GPU name:   Apple M4
0.00.024.445 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.445 I ggml_metal_init: simdgroup reduction   = true
0.00.024.446 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.446 I ggml_metal_init: has bfloat            = true
0.00.024.446 I ggml_metal_init: use bfloat            = true
0.00.024.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.447 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.352 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.355 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.358 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.994 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.995 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.995 I llama_new_context_with_model: graph nodes  = 429
0.00.033.995 I llama_new_context_with_model: graph splits = 2
0.00.034.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.316 I 
0.00.038.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.038.857 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.040 I llama_perf_context_print:        load time =      27.96 ms
0.00.043.041 I llama_perf_context_print: prompt eval time =       4.05 ms /     9 tokens (    0.45 ms per token,  2222.77 tokens per second)
0.00.043.042 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.043 I llama_perf_context_print:       total time =       4.73 ms /    10 tokens
0.00.043.219 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.151 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.266 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.593 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.601 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.605 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.605 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.606 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.607 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.608 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.608 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.609 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.610 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.613 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.614 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.614 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.166 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.166 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.166 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.167 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.167 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.167 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.168 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.168 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.169 I llama_model_loader: - type  f32:   41 tensors
0.00.045.169 I llama_model_loader: - type  f16:   29 tensors
0.00.062.839 W llm_load_vocab: empty token at index 5
0.00.067.204 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.068.483 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.068.510 I llm_load_vocab: special tokens cache size = 5
0.00.332.174 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.332.187 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.332.187 I llm_load_print_meta: arch             = jina-bert-v2
0.00.332.188 I llm_load_print_meta: vocab type       = BPE
0.00.332.188 I llm_load_print_meta: n_vocab          = 61056
0.00.332.188 I llm_load_print_meta: n_merges         = 39382
0.00.332.188 I llm_load_print_meta: vocab_only       = 0
0.00.332.188 I llm_load_print_meta: n_ctx_train      = 8192
0.00.332.189 I llm_load_print_meta: n_embd           = 384
0.00.332.189 I llm_load_print_meta: n_layer          = 4
0.00.332.197 I llm_load_print_meta: n_head           = 12
0.00.332.198 I llm_load_print_meta: n_head_kv        = 12
0.00.332.198 I llm_load_print_meta: n_rot            = 32
0.00.332.198 I llm_load_print_meta: n_swa            = 0
0.00.332.198 I llm_load_print_meta: n_embd_head_k    = 32
0.00.332.198 I llm_load_print_meta: n_embd_head_v    = 32
0.00.332.199 I llm_load_print_meta: n_gqa            = 1
0.00.332.199 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.332.200 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.332.201 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.332.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.332.202 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.332.202 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.332.203 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.332.203 I llm_load_print_meta: n_ff             = 1536
0.00.332.204 I llm_load_print_meta: n_expert         = 0
0.00.332.204 I llm_load_print_meta: n_expert_used    = 0
0.00.332.204 I llm_load_print_meta: causal attn      = 0
0.00.332.204 I llm_load_print_meta: pooling type     = -1
0.00.332.204 I llm_load_print_meta: rope type        = -1
0.00.332.204 I llm_load_print_meta: rope scaling     = linear
0.00.332.205 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.332.205 I llm_load_print_meta: freq_scale_train = 1
0.00.332.205 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.332.205 I llm_load_print_meta: rope_finetuned   = unknown
0.00.332.205 I llm_load_print_meta: ssm_d_conv       = 0
0.00.332.205 I llm_load_print_meta: ssm_d_inner      = 0
0.00.332.206 I llm_load_print_meta: ssm_d_state      = 0
0.00.332.206 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.332.206 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.332.229 I llm_load_print_meta: model type       = 33M
0.00.332.230 I llm_load_print_meta: model ftype      = F16
0.00.332.231 I llm_load_print_meta: model params     = 32.90 M
0.00.332.231 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.332.231 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.332.232 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.332.232 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.332.233 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.332.233 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.332.233 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.332.233 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.332.234 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.332.234 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.332.234 I llm_load_print_meta: max token length = 45
0.00.333.334 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.334 I llm_load_tensors: offloading output layer to GPU
0.00.333.334 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.348 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.350 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.197 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.198 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.199 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.199 I llama_new_context_with_model: n_batch       = 2048
0.00.334.199 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.199 I llama_new_context_with_model: flash_attn    = 0
0.00.334.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.200 I llama_new_context_with_model: freq_scale    = 1
0.00.334.200 I ggml_metal_init: allocating
0.00.334.204 I ggml_metal_init: found device: Apple M4
0.00.334.209 I ggml_metal_init: picking default device: Apple M4
0.00.334.987 I ggml_metal_init: using embedded metal library
0.00.337.407 I ggml_metal_init: GPU name:   Apple M4
0.00.337.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.410 I ggml_metal_init: simdgroup reduction   = true
0.00.337.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.411 I ggml_metal_init: has bfloat            = true
0.00.337.411 I ggml_metal_init: use bfloat            = true
0.00.337.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.589 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.591 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.593 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.349.176 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.349.178 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.349.178 I llama_new_context_with_model: graph nodes  = 154
0.00.349.178 I llama_new_context_with_model: graph splits = 2
0.00.349.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.841 I 
0.00.358.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.359.022 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.023 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.026 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.026 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.030 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.030 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.567 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.026 I llama_perf_context_print:        load time =     338.57 ms
0.00.363.027 I llama_perf_context_print: prompt eval time =       3.45 ms /    62 tokens (    0.06 ms per token, 17981.44 tokens per second)
0.00.363.027 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.028 I llama_perf_context_print:       total time =       4.19 ms /    63 tokens
0.00.363.237 I ggml_metal_free: deallocating

real	0m1.050s
user	0m0.342s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.144 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.249 I main: llama backend init
0.00.000.254 I main: load the model and apply lora adapter, if any
0.00.076.613 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.087.435 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.087.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.087.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.087.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.087.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.087.457 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.087.457 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.087.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.087.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.087.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.087.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.087.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.087.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.087.463 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.087.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.087.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.087.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.094.340 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.096.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.103.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.103.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.103.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.103.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.103.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.103.525 I llama_model_loader: - type  f32:  194 tensors
0.00.103.526 I llama_model_loader: - type  f16:   98 tensors
0.00.142.812 I llm_load_vocab: special tokens cache size = 25
0.00.150.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.150.752 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.150.752 I llm_load_print_meta: arch             = gptneox
0.00.150.752 I llm_load_print_meta: vocab type       = BPE
0.00.150.753 I llm_load_print_meta: n_vocab          = 50304
0.00.150.753 I llm_load_print_meta: n_merges         = 50009
0.00.150.753 I llm_load_print_meta: vocab_only       = 0
0.00.150.753 I llm_load_print_meta: n_ctx_train      = 2048
0.00.150.753 I llm_load_print_meta: n_embd           = 2048
0.00.150.753 I llm_load_print_meta: n_layer          = 24
0.00.150.757 I llm_load_print_meta: n_head           = 16
0.00.150.758 I llm_load_print_meta: n_head_kv        = 16
0.00.150.758 I llm_load_print_meta: n_rot            = 32
0.00.150.758 I llm_load_print_meta: n_swa            = 0
0.00.150.758 I llm_load_print_meta: n_embd_head_k    = 128
0.00.150.759 I llm_load_print_meta: n_embd_head_v    = 128
0.00.150.759 I llm_load_print_meta: n_gqa            = 1
0.00.150.760 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.150.761 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.150.761 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.150.762 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.150.762 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.150.762 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.150.762 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.150.763 I llm_load_print_meta: n_ff             = 8192
0.00.150.764 I llm_load_print_meta: n_expert         = 0
0.00.150.764 I llm_load_print_meta: n_expert_used    = 0
0.00.150.764 I llm_load_print_meta: causal attn      = 1
0.00.150.764 I llm_load_print_meta: pooling type     = 0
0.00.150.764 I llm_load_print_meta: rope type        = 2
0.00.150.765 I llm_load_print_meta: rope scaling     = linear
0.00.150.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.150.765 I llm_load_print_meta: freq_scale_train = 1
0.00.150.765 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.150.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.150.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.150.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.150.768 I llm_load_print_meta: ssm_d_state      = 0
0.00.150.768 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.150.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.150.779 I llm_load_print_meta: model type       = 1.4B
0.00.150.779 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.150.780 I llm_load_print_meta: model params     = 1.41 B
0.00.150.780 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.150.780 I llm_load_print_meta: general.name     = 1.4B
0.00.150.781 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.150.781 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.150.782 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.150.783 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.150.783 I llm_load_print_meta: LF token         = 128 ''
0.00.150.783 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.150.783 I llm_load_print_meta: max token length = 1024
0.00.152.797 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.152.798 I llm_load_tensors: offloading output layer to GPU
0.00.152.798 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.152.816 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.152.817 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.153.827 I llama_new_context_with_model: n_seq_max     = 1
0.00.153.828 I llama_new_context_with_model: n_ctx         = 2048
0.00.153.828 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.153.828 I llama_new_context_with_model: n_batch       = 2048
0.00.153.829 I llama_new_context_with_model: n_ubatch      = 512
0.00.153.829 I llama_new_context_with_model: flash_attn    = 0
0.00.153.829 I llama_new_context_with_model: freq_base     = 10000.0
0.00.153.830 I llama_new_context_with_model: freq_scale    = 1
0.00.153.830 I ggml_metal_init: allocating
0.00.153.834 I ggml_metal_init: found device: Apple M4
0.00.153.836 I ggml_metal_init: picking default device: Apple M4
0.00.154.512 I ggml_metal_init: using embedded metal library
0.00.165.203 I ggml_metal_init: GPU name:   Apple M4
0.00.165.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.165.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.165.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.165.206 I ggml_metal_init: simdgroup reduction   = true
0.00.165.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.165.207 I ggml_metal_init: has bfloat            = true
0.00.165.207 I ggml_metal_init: use bfloat            = true
0.00.165.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.165.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.203.475 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.203.481 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.203.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.204.461 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.204.463 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.204.464 I llama_new_context_with_model: graph nodes  = 967
0.00.204.464 I llama_new_context_with_model: graph splits = 2
0.00.204.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.284.522 I main: llama threadpool init, n_threads = 4
0.00.284.562 I 
0.00.284.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.284.594 I 
0.00.284.675 I sampler seed: 1234
0.00.284.679 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.284.714 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.284.716 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.284.716 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.133.883 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.02.133.884 I llama_perf_context_print:        load time =     207.90 ms
0.02.133.884 I llama_perf_context_print: prompt eval time =      38.90 ms /     7 tokens (    5.56 ms per token,   179.93 tokens per second)
0.02.133.885 I llama_perf_context_print:        eval time =    1807.41 ms /    63 runs   (   28.69 ms per token,    34.86 tokens per second)
0.02.133.885 I llama_perf_context_print:       total time =    1849.36 ms /    70 tokens
0.02.134.063 I ggml_metal_free: deallocating

real	0m2.420s
user	0m0.153s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.811 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.260 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.368 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.769 I llama_model_loader: - type  f32:  194 tensors
0.00.058.770 I llama_model_loader: - type  f16:   98 tensors
0.00.088.512 I llm_load_vocab: special tokens cache size = 25
0.00.095.075 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.078 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.078 I llm_load_print_meta: arch             = gptneox
0.00.095.078 I llm_load_print_meta: vocab type       = BPE
0.00.095.078 I llm_load_print_meta: n_vocab          = 50304
0.00.095.079 I llm_load_print_meta: n_merges         = 50009
0.00.095.079 I llm_load_print_meta: vocab_only       = 0
0.00.095.079 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.079 I llm_load_print_meta: n_embd           = 2048
0.00.095.079 I llm_load_print_meta: n_layer          = 24
0.00.095.082 I llm_load_print_meta: n_head           = 16
0.00.095.083 I llm_load_print_meta: n_head_kv        = 16
0.00.095.083 I llm_load_print_meta: n_rot            = 32
0.00.095.083 I llm_load_print_meta: n_swa            = 0
0.00.095.083 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.084 I llm_load_print_meta: n_gqa            = 1
0.00.095.085 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.086 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.087 I llm_load_print_meta: n_ff             = 8192
0.00.095.087 I llm_load_print_meta: n_expert         = 0
0.00.095.087 I llm_load_print_meta: n_expert_used    = 0
0.00.095.087 I llm_load_print_meta: causal attn      = 1
0.00.095.088 I llm_load_print_meta: pooling type     = 0
0.00.095.088 I llm_load_print_meta: rope type        = 2
0.00.095.088 I llm_load_print_meta: rope scaling     = linear
0.00.095.088 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.088 I llm_load_print_meta: freq_scale_train = 1
0.00.095.091 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.091 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.091 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.091 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.091 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.091 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.098 I llm_load_print_meta: model type       = 1.4B
0.00.095.098 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.099 I llm_load_print_meta: model params     = 1.41 B
0.00.095.099 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.099 I llm_load_print_meta: general.name     = 1.4B
0.00.095.100 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.100 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.101 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.101 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.103 I llm_load_print_meta: LF token         = 128 ''
0.00.095.103 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.103 I llm_load_print_meta: max token length = 1024
0.00.097.109 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.109 I llm_load_tensors: offloading output layer to GPU
0.00.097.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.114 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.114 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.035 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.036 I llama_new_context_with_model: n_ctx         = 128
0.00.098.036 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.036 I llama_new_context_with_model: n_batch       = 128
0.00.098.037 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.037 I llama_new_context_with_model: flash_attn    = 0
0.00.098.037 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.038 I llama_new_context_with_model: freq_scale    = 1
0.00.098.038 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.038 I ggml_metal_init: allocating
0.00.098.041 I ggml_metal_init: found device: Apple M4
0.00.098.043 I ggml_metal_init: picking default device: Apple M4
0.00.098.618 I ggml_metal_init: using embedded metal library
0.00.100.743 I ggml_metal_init: GPU name:   Apple M4
0.00.100.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.746 I ggml_metal_init: simdgroup reduction   = true
0.00.100.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.746 I ggml_metal_init: has bfloat            = true
0.00.100.746 I ggml_metal_init: use bfloat            = true
0.00.100.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.985 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.987 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.002 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.932 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.934 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.934 I llama_new_context_with_model: graph nodes  = 967
0.00.110.934 I llama_new_context_with_model: graph splits = 2
0.00.110.946 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.098.664 I 
0.01.098.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.098.723 I perplexity: tokenizing the input ..
0.01.106.516 I perplexity: tokenization took 7.79 ms
0.01.106.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.224.731 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.226.433 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.226.464 I llama_perf_context_print:        load time =    1070.39 ms
0.01.226.465 I llama_perf_context_print: prompt eval time =     117.98 ms /   128 tokens (    0.92 ms per token,  1084.96 tokens per second)
0.01.226.466 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.226.466 I llama_perf_context_print:       total time =     127.80 ms /   129 tokens
0.01.226.839 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.119s
sys	0m0.197s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.657 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.307 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.310 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.399 I llama_model_loader: - type  f32:  194 tensors
0.00.033.399 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.407 I llm_load_vocab: special tokens cache size = 25
0.00.062.440 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.443 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.444 I llm_load_print_meta: arch             = gptneox
0.00.062.444 I llm_load_print_meta: vocab type       = BPE
0.00.062.444 I llm_load_print_meta: n_vocab          = 50304
0.00.062.444 I llm_load_print_meta: n_merges         = 50009
0.00.062.445 I llm_load_print_meta: vocab_only       = 0
0.00.062.447 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.447 I llm_load_print_meta: n_embd           = 2048
0.00.062.447 I llm_load_print_meta: n_layer          = 24
0.00.062.451 I llm_load_print_meta: n_head           = 16
0.00.062.451 I llm_load_print_meta: n_head_kv        = 16
0.00.062.452 I llm_load_print_meta: n_rot            = 32
0.00.062.452 I llm_load_print_meta: n_swa            = 0
0.00.062.452 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.453 I llm_load_print_meta: n_gqa            = 1
0.00.062.454 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.456 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.456 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.457 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.457 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.458 I llm_load_print_meta: n_ff             = 8192
0.00.062.458 I llm_load_print_meta: n_expert         = 0
0.00.062.458 I llm_load_print_meta: n_expert_used    = 0
0.00.062.458 I llm_load_print_meta: causal attn      = 1
0.00.062.458 I llm_load_print_meta: pooling type     = 0
0.00.062.458 I llm_load_print_meta: rope type        = 2
0.00.062.461 I llm_load_print_meta: rope scaling     = linear
0.00.062.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.462 I llm_load_print_meta: freq_scale_train = 1
0.00.062.462 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.462 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.462 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.462 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.462 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.462 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.463 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.476 I llm_load_print_meta: model type       = 1.4B
0.00.062.477 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.478 I llm_load_print_meta: model params     = 1.41 B
0.00.062.478 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.478 I llm_load_print_meta: general.name     = 1.4B
0.00.062.479 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.480 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.480 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.480 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.481 I llm_load_print_meta: LF token         = 128 ''
0.00.062.481 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.481 I llm_load_print_meta: max token length = 1024
0.00.064.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.955 I llm_load_tensors: offloading output layer to GPU
0.00.064.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.966 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.967 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.946 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.947 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.947 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.947 I llama_new_context_with_model: n_batch       = 2048
0.00.065.948 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.948 I llama_new_context_with_model: flash_attn    = 0
0.00.065.948 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.948 I llama_new_context_with_model: freq_scale    = 1
0.00.065.949 I ggml_metal_init: allocating
0.00.065.952 I ggml_metal_init: found device: Apple M4
0.00.065.954 I ggml_metal_init: picking default device: Apple M4
0.00.066.659 I ggml_metal_init: using embedded metal library
0.00.068.963 I ggml_metal_init: GPU name:   Apple M4
0.00.068.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.966 I ggml_metal_init: simdgroup reduction   = true
0.00.068.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.966 I ggml_metal_init: has bfloat            = true
0.00.068.966 I ggml_metal_init: use bfloat            = true
0.00.068.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.537 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.549 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.577 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.678 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.680 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.680 I llama_new_context_with_model: graph nodes  = 967
0.00.104.681 I llama_new_context_with_model: graph splits = 2
0.00.104.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.318.087 I main: llama threadpool init, n_threads = 4
0.01.318.126 I 
0.01.318.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.318.152 I 
0.01.318.387 I sampler seed: 1234
0.01.318.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.318.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.318.436 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.318.442 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.414.247 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.02.414.248 I llama_perf_context_print:        load time =    1308.43 ms
0.02.414.249 I llama_perf_context_print: prompt eval time =      36.94 ms /     7 tokens (    5.28 ms per token,   189.52 tokens per second)
0.02.414.250 I llama_perf_context_print:        eval time =    1056.10 ms /    63 runs   (   16.76 ms per token,    59.65 tokens per second)
0.02.414.250 I llama_perf_context_print:       total time =    1096.16 ms /    70 tokens
0.02.414.445 I ggml_metal_free: deallocating

real	0m2.434s
user	0m0.115s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.479 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.460 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.461 I llama_model_loader: - type  f32:  194 tensors
0.00.025.461 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.967 I llm_load_vocab: special tokens cache size = 25
0.00.056.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.800 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.801 I llm_load_print_meta: arch             = gptneox
0.00.056.801 I llm_load_print_meta: vocab type       = BPE
0.00.056.801 I llm_load_print_meta: n_vocab          = 50304
0.00.056.801 I llm_load_print_meta: n_merges         = 50009
0.00.056.801 I llm_load_print_meta: vocab_only       = 0
0.00.056.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.802 I llm_load_print_meta: n_embd           = 2048
0.00.056.802 I llm_load_print_meta: n_layer          = 24
0.00.056.805 I llm_load_print_meta: n_head           = 16
0.00.056.805 I llm_load_print_meta: n_head_kv        = 16
0.00.056.805 I llm_load_print_meta: n_rot            = 32
0.00.056.806 I llm_load_print_meta: n_swa            = 0
0.00.056.806 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.806 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.806 I llm_load_print_meta: n_gqa            = 1
0.00.056.807 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.808 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.808 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.808 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.809 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.809 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.809 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.809 I llm_load_print_meta: n_ff             = 8192
0.00.056.810 I llm_load_print_meta: n_expert         = 0
0.00.056.810 I llm_load_print_meta: n_expert_used    = 0
0.00.056.810 I llm_load_print_meta: causal attn      = 1
0.00.056.810 I llm_load_print_meta: pooling type     = 0
0.00.056.810 I llm_load_print_meta: rope type        = 2
0.00.056.810 I llm_load_print_meta: rope scaling     = linear
0.00.056.811 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.813 I llm_load_print_meta: freq_scale_train = 1
0.00.056.813 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.813 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.813 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.813 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.813 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.813 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.814 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.825 I llm_load_print_meta: model type       = 1.4B
0.00.056.826 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.826 I llm_load_print_meta: model params     = 1.41 B
0.00.056.827 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.827 I llm_load_print_meta: general.name     = 1.4B
0.00.056.827 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.827 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.827 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.827 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.828 I llm_load_print_meta: LF token         = 128 ''
0.00.056.828 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.828 I llm_load_print_meta: max token length = 1024
0.00.059.048 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.048 I llm_load_tensors: offloading output layer to GPU
0.00.059.049 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.058 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.060 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.020 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.021 I llama_new_context_with_model: n_ctx         = 128
0.00.060.021 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.060.021 I llama_new_context_with_model: n_batch       = 128
0.00.060.022 I llama_new_context_with_model: n_ubatch      = 128
0.00.060.022 I llama_new_context_with_model: flash_attn    = 0
0.00.060.022 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.023 I llama_new_context_with_model: freq_scale    = 1
0.00.060.023 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.023 I ggml_metal_init: allocating
0.00.060.029 I ggml_metal_init: found device: Apple M4
0.00.060.031 I ggml_metal_init: picking default device: Apple M4
0.00.060.610 I ggml_metal_init: using embedded metal library
0.00.062.789 I ggml_metal_init: GPU name:   Apple M4
0.00.062.791 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.791 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.792 I ggml_metal_init: simdgroup reduction   = true
0.00.062.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.792 I ggml_metal_init: has bfloat            = true
0.00.062.792 I ggml_metal_init: use bfloat            = true
0.00.062.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.689 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.692 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.709 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.633 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.634 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.634 I llama_new_context_with_model: graph nodes  = 967
0.00.072.634 I llama_new_context_with_model: graph splits = 2
0.00.072.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.948.760 I 
0.00.948.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.948.795 I perplexity: tokenizing the input ..
0.00.956.490 I perplexity: tokenization took 7.694 ms
0.00.956.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.078.621 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.079.836 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.079.863 I llama_perf_context_print:        load time =     940.27 ms
0.01.079.864 I llama_perf_context_print: prompt eval time =     121.89 ms /   128 tokens (    0.95 ms per token,  1050.12 tokens per second)
0.01.079.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.079.866 I llama_perf_context_print:       total time =     131.11 ms /   129 tokens
0.01.080.273 I ggml_metal_free: deallocating

real	0m1.095s
user	0m0.084s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.014.748 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.580 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.580 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.580 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.581 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.581 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.581 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.584 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.312 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.313 I llama_model_loader: - type  f32:  194 tensors
0.00.044.313 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.313 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.038 I llm_load_vocab: special tokens cache size = 25
0.00.080.700 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.703 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.704 I llm_load_print_meta: arch             = gptneox
0.00.080.704 I llm_load_print_meta: vocab type       = BPE
0.00.080.705 I llm_load_print_meta: n_vocab          = 50304
0.00.080.705 I llm_load_print_meta: n_merges         = 50009
0.00.080.705 I llm_load_print_meta: vocab_only       = 0
0.00.080.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.706 I llm_load_print_meta: n_embd           = 2048
0.00.080.706 I llm_load_print_meta: n_layer          = 24
0.00.080.710 I llm_load_print_meta: n_head           = 16
0.00.080.711 I llm_load_print_meta: n_head_kv        = 16
0.00.080.713 I llm_load_print_meta: n_rot            = 32
0.00.080.714 I llm_load_print_meta: n_swa            = 0
0.00.080.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.714 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.715 I llm_load_print_meta: n_gqa            = 1
0.00.080.716 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.717 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.717 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.718 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.718 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.719 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.719 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.720 I llm_load_print_meta: n_ff             = 8192
0.00.080.720 I llm_load_print_meta: n_expert         = 0
0.00.080.720 I llm_load_print_meta: n_expert_used    = 0
0.00.080.720 I llm_load_print_meta: causal attn      = 1
0.00.080.721 I llm_load_print_meta: pooling type     = 0
0.00.080.721 I llm_load_print_meta: rope type        = 2
0.00.080.721 I llm_load_print_meta: rope scaling     = linear
0.00.080.724 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.725 I llm_load_print_meta: freq_scale_train = 1
0.00.080.725 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.725 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.725 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.725 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.726 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.726 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.726 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.734 I llm_load_print_meta: model type       = 1.4B
0.00.080.734 I llm_load_print_meta: model ftype      = Q4_0
0.00.080.735 I llm_load_print_meta: model params     = 1.41 B
0.00.080.736 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.080.736 I llm_load_print_meta: general.name     = 1.4B
0.00.080.737 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.737 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.737 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.738 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.738 I llm_load_print_meta: LF token         = 128 ''
0.00.080.738 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.739 I llm_load_print_meta: max token length = 1024
0.00.083.388 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.388 I llm_load_tensors: offloading output layer to GPU
0.00.083.388 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.394 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.083.395 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.084.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.766 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.767 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.767 I llama_new_context_with_model: n_batch       = 2048
0.00.084.767 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.767 I llama_new_context_with_model: flash_attn    = 0
0.00.084.768 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.768 I llama_new_context_with_model: freq_scale    = 1
0.00.084.769 I ggml_metal_init: allocating
0.00.084.779 I ggml_metal_init: found device: Apple M4
0.00.084.782 I ggml_metal_init: picking default device: Apple M4
0.00.085.643 I ggml_metal_init: using embedded metal library
0.00.088.558 I ggml_metal_init: GPU name:   Apple M4
0.00.088.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.562 I ggml_metal_init: simdgroup reduction   = true
0.00.088.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.562 I ggml_metal_init: has bfloat            = true
0.00.088.562 I ggml_metal_init: use bfloat            = true
0.00.088.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.125.997 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.009 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.033 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.076 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.078 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.079 I llama_new_context_with_model: graph nodes  = 967
0.00.127.079 I llama_new_context_with_model: graph splits = 2
0.00.127.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.958 I main: llama threadpool init, n_threads = 4
0.00.749.021 I 
0.00.749.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.749.061 I 
0.00.749.399 I sampler seed: 1234
0.00.749.406 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.449 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.450 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.426.719 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.426.720 I llama_perf_context_print:        load time =     734.20 ms
0.01.426.721 I llama_perf_context_print: prompt eval time =      38.85 ms /     7 tokens (    5.55 ms per token,   180.16 tokens per second)
0.01.426.721 I llama_perf_context_print:        eval time =     635.44 ms /    63 runs   (   10.09 ms per token,    99.14 tokens per second)
0.01.426.722 I llama_perf_context_print:       total time =     677.76 ms /    70 tokens
0.01.426.893 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.128s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.549 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.716 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.719 I llama_model_loader: - type  f32:  194 tensors
0.00.024.719 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.453 I llm_load_vocab: special tokens cache size = 25
0.00.051.144 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.147 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.147 I llm_load_print_meta: arch             = gptneox
0.00.051.148 I llm_load_print_meta: vocab type       = BPE
0.00.051.148 I llm_load_print_meta: n_vocab          = 50304
0.00.051.148 I llm_load_print_meta: n_merges         = 50009
0.00.051.148 I llm_load_print_meta: vocab_only       = 0
0.00.051.148 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.148 I llm_load_print_meta: n_embd           = 2048
0.00.051.149 I llm_load_print_meta: n_layer          = 24
0.00.051.152 I llm_load_print_meta: n_head           = 16
0.00.051.153 I llm_load_print_meta: n_head_kv        = 16
0.00.051.155 I llm_load_print_meta: n_rot            = 32
0.00.051.155 I llm_load_print_meta: n_swa            = 0
0.00.051.156 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.156 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.157 I llm_load_print_meta: n_gqa            = 1
0.00.051.159 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.159 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.160 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.160 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.160 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.162 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.162 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.162 I llm_load_print_meta: n_ff             = 8192
0.00.051.163 I llm_load_print_meta: n_expert         = 0
0.00.051.163 I llm_load_print_meta: n_expert_used    = 0
0.00.051.163 I llm_load_print_meta: causal attn      = 1
0.00.051.163 I llm_load_print_meta: pooling type     = 0
0.00.051.163 I llm_load_print_meta: rope type        = 2
0.00.051.163 I llm_load_print_meta: rope scaling     = linear
0.00.051.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.166 I llm_load_print_meta: freq_scale_train = 1
0.00.051.166 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.172 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.184 I llm_load_print_meta: model type       = 1.4B
0.00.051.184 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.184 I llm_load_print_meta: model params     = 1.41 B
0.00.051.185 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.185 I llm_load_print_meta: general.name     = 1.4B
0.00.051.185 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.185 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.185 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: LF token         = 128 ''
0.00.051.186 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: max token length = 1024
0.00.053.119 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.120 I llm_load_tensors: offloading output layer to GPU
0.00.053.120 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.130 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.131 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.076 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.077 I llama_new_context_with_model: n_ctx         = 128
0.00.054.078 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.078 I llama_new_context_with_model: n_batch       = 128
0.00.054.078 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.078 I llama_new_context_with_model: flash_attn    = 0
0.00.054.078 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.079 I llama_new_context_with_model: freq_scale    = 1
0.00.054.079 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.079 I ggml_metal_init: allocating
0.00.054.084 I ggml_metal_init: found device: Apple M4
0.00.054.086 I ggml_metal_init: picking default device: Apple M4
0.00.054.647 I ggml_metal_init: using embedded metal library
0.00.056.591 I ggml_metal_init: GPU name:   Apple M4
0.00.056.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.594 I ggml_metal_init: simdgroup reduction   = true
0.00.056.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.594 I ggml_metal_init: has bfloat            = true
0.00.056.594 I ggml_metal_init: use bfloat            = true
0.00.056.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.796 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.800 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.674 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.676 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.676 I llama_new_context_with_model: graph nodes  = 967
0.00.067.676 I llama_new_context_with_model: graph splits = 2
0.00.067.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.408 I 
0.00.609.432 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.609.436 I perplexity: tokenizing the input ..
0.00.616.817 I perplexity: tokenization took 7.379 ms
0.00.616.822 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.607 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.740.997 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.031 I llama_perf_context_print:        load time =     599.86 ms
0.00.741.032 I llama_perf_context_print: prompt eval time =     122.56 ms /   128 tokens (    0.96 ms per token,  1044.37 tokens per second)
0.00.741.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.033 I llama_perf_context_print:       total time =     131.62 ms /   129 tokens
0.00.741.434 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.076s
sys	0m0.096s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.042 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.672 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.673 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.755 I llama_model_loader: - type  f32:  194 tensors
0.00.024.755 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.755 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.553 I llm_load_vocab: special tokens cache size = 25
0.00.050.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.347 I llm_load_print_meta: arch             = gptneox
0.00.050.347 I llm_load_print_meta: vocab type       = BPE
0.00.050.347 I llm_load_print_meta: n_vocab          = 50304
0.00.050.348 I llm_load_print_meta: n_merges         = 50009
0.00.050.348 I llm_load_print_meta: vocab_only       = 0
0.00.050.348 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.348 I llm_load_print_meta: n_embd           = 2048
0.00.050.348 I llm_load_print_meta: n_layer          = 24
0.00.050.351 I llm_load_print_meta: n_head           = 16
0.00.050.354 I llm_load_print_meta: n_head_kv        = 16
0.00.050.354 I llm_load_print_meta: n_rot            = 32
0.00.050.355 I llm_load_print_meta: n_swa            = 0
0.00.050.355 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.356 I llm_load_print_meta: n_gqa            = 1
0.00.050.356 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.357 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.358 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.363 I llm_load_print_meta: n_ff             = 8192
0.00.050.363 I llm_load_print_meta: n_expert         = 0
0.00.050.363 I llm_load_print_meta: n_expert_used    = 0
0.00.050.363 I llm_load_print_meta: causal attn      = 1
0.00.050.365 I llm_load_print_meta: pooling type     = 0
0.00.050.365 I llm_load_print_meta: rope type        = 2
0.00.050.366 I llm_load_print_meta: rope scaling     = linear
0.00.050.366 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.367 I llm_load_print_meta: freq_scale_train = 1
0.00.050.367 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.367 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.367 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.367 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.367 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.368 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.380 I llm_load_print_meta: model type       = 1.4B
0.00.050.382 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.382 I llm_load_print_meta: model params     = 1.41 B
0.00.050.383 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.383 I llm_load_print_meta: general.name     = 1.4B
0.00.050.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: LF token         = 128 ''
0.00.050.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.386 I llm_load_print_meta: max token length = 1024
0.00.052.314 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.314 I llm_load_tensors: offloading output layer to GPU
0.00.052.314 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.324 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.325 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.226 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.227 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.227 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.227 I llama_new_context_with_model: n_batch       = 2048
0.00.053.227 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.227 I llama_new_context_with_model: flash_attn    = 0
0.00.053.228 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.228 I llama_new_context_with_model: freq_scale    = 1
0.00.053.228 I ggml_metal_init: allocating
0.00.053.235 I ggml_metal_init: found device: Apple M4
0.00.053.237 I ggml_metal_init: picking default device: Apple M4
0.00.053.813 I ggml_metal_init: using embedded metal library
0.00.055.780 I ggml_metal_init: GPU name:   Apple M4
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.783 I ggml_metal_init: simdgroup reduction   = true
0.00.055.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.783 I ggml_metal_init: has bfloat            = true
0.00.055.783 I ggml_metal_init: use bfloat            = true
0.00.055.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.597 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.618 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.564 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.565 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.566 I llama_new_context_with_model: graph nodes  = 967
0.00.083.566 I llama_new_context_with_model: graph splits = 2
0.00.083.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.071 I main: llama threadpool init, n_threads = 4
0.00.726.107 I 
0.00.726.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.726.153 I 
0.00.726.393 I sampler seed: 1234
0.00.726.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.412 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.412 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.412 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.445.479 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.445.480 I llama_perf_context_print:        load time =     717.02 ms
0.01.445.482 I llama_perf_context_print: prompt eval time =      32.80 ms /     7 tokens (    4.69 ms per token,   213.42 tokens per second)
0.01.445.483 I llama_perf_context_print:        eval time =     683.54 ms /    63 runs   (   10.85 ms per token,    92.17 tokens per second)
0.01.445.483 I llama_perf_context_print:       total time =     719.41 ms /    70 tokens
0.01.445.692 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.108s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.644 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.823 I llama_model_loader: - type  f32:  194 tensors
0.00.023.823 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.824 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.617 I llm_load_vocab: special tokens cache size = 25
0.00.050.523 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.526 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.526 I llm_load_print_meta: arch             = gptneox
0.00.050.527 I llm_load_print_meta: vocab type       = BPE
0.00.050.527 I llm_load_print_meta: n_vocab          = 50304
0.00.050.527 I llm_load_print_meta: n_merges         = 50009
0.00.050.527 I llm_load_print_meta: vocab_only       = 0
0.00.050.528 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.528 I llm_load_print_meta: n_embd           = 2048
0.00.050.528 I llm_load_print_meta: n_layer          = 24
0.00.050.531 I llm_load_print_meta: n_head           = 16
0.00.050.531 I llm_load_print_meta: n_head_kv        = 16
0.00.050.532 I llm_load_print_meta: n_rot            = 32
0.00.050.532 I llm_load_print_meta: n_swa            = 0
0.00.050.532 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.532 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.533 I llm_load_print_meta: n_gqa            = 1
0.00.050.534 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.537 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.538 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.538 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.538 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.539 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.539 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.540 I llm_load_print_meta: n_ff             = 8192
0.00.050.540 I llm_load_print_meta: n_expert         = 0
0.00.050.540 I llm_load_print_meta: n_expert_used    = 0
0.00.050.540 I llm_load_print_meta: causal attn      = 1
0.00.050.541 I llm_load_print_meta: pooling type     = 0
0.00.050.541 I llm_load_print_meta: rope type        = 2
0.00.050.541 I llm_load_print_meta: rope scaling     = linear
0.00.050.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.542 I llm_load_print_meta: freq_scale_train = 1
0.00.050.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.550 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.550 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.550 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.551 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.551 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.562 I llm_load_print_meta: model type       = 1.4B
0.00.050.563 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.563 I llm_load_print_meta: model params     = 1.41 B
0.00.050.564 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.564 I llm_load_print_meta: general.name     = 1.4B
0.00.050.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.565 I llm_load_print_meta: LF token         = 128 ''
0.00.050.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.565 I llm_load_print_meta: max token length = 1024
0.00.052.535 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.535 I llm_load_tensors: offloading output layer to GPU
0.00.052.535 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.546 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.547 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.467 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.468 I llama_new_context_with_model: n_ctx         = 128
0.00.053.468 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.468 I llama_new_context_with_model: n_batch       = 128
0.00.053.468 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.468 I llama_new_context_with_model: flash_attn    = 0
0.00.053.469 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.469 I llama_new_context_with_model: freq_scale    = 1
0.00.053.469 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.470 I ggml_metal_init: allocating
0.00.053.476 I ggml_metal_init: found device: Apple M4
0.00.053.478 I ggml_metal_init: picking default device: Apple M4
0.00.054.029 I ggml_metal_init: using embedded metal library
0.00.055.951 I ggml_metal_init: GPU name:   Apple M4
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.953 I ggml_metal_init: simdgroup reduction   = true
0.00.055.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.953 I ggml_metal_init: has bfloat            = true
0.00.055.953 I ggml_metal_init: use bfloat            = true
0.00.055.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.100 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.984 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.985 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.986 I llama_new_context_with_model: graph nodes  = 967
0.00.065.986 I llama_new_context_with_model: graph splits = 2
0.00.065.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.095 I 
0.00.658.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.658.129 I perplexity: tokenizing the input ..
0.00.665.887 I perplexity: tokenization took 7.756 ms
0.00.665.890 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.867 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.112 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.143 I llama_perf_context_print:        load time =     649.36 ms
0.00.790.144 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.85 tokens per second)
0.00.790.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.148 I llama_perf_context_print:       total time =     132.05 ms /   129 tokens
0.00.790.559 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.077s
sys	0m0.101s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.373 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.668 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.669 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.734 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.735 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.736 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.737 I llama_model_loader: - type  f32:  194 tensors
0.00.024.737 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.738 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.798 I llm_load_vocab: special tokens cache size = 25
0.00.051.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.558 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.558 I llm_load_print_meta: arch             = gptneox
0.00.051.558 I llm_load_print_meta: vocab type       = BPE
0.00.051.559 I llm_load_print_meta: n_vocab          = 50304
0.00.051.559 I llm_load_print_meta: n_merges         = 50009
0.00.051.559 I llm_load_print_meta: vocab_only       = 0
0.00.051.559 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.559 I llm_load_print_meta: n_embd           = 2048
0.00.051.560 I llm_load_print_meta: n_layer          = 24
0.00.051.562 I llm_load_print_meta: n_head           = 16
0.00.051.563 I llm_load_print_meta: n_head_kv        = 16
0.00.051.564 I llm_load_print_meta: n_rot            = 32
0.00.051.564 I llm_load_print_meta: n_swa            = 0
0.00.051.564 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.564 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.565 I llm_load_print_meta: n_gqa            = 1
0.00.051.565 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.566 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.567 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.567 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.567 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.567 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.568 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.568 I llm_load_print_meta: n_ff             = 8192
0.00.051.568 I llm_load_print_meta: n_expert         = 0
0.00.051.569 I llm_load_print_meta: n_expert_used    = 0
0.00.051.570 I llm_load_print_meta: causal attn      = 1
0.00.051.572 I llm_load_print_meta: pooling type     = 0
0.00.051.572 I llm_load_print_meta: rope type        = 2
0.00.051.573 I llm_load_print_meta: rope scaling     = linear
0.00.051.573 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.573 I llm_load_print_meta: freq_scale_train = 1
0.00.051.573 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.574 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.574 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.574 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.574 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.574 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.574 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.586 I llm_load_print_meta: model type       = 1.4B
0.00.051.586 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.586 I llm_load_print_meta: model params     = 1.41 B
0.00.051.587 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.588 I llm_load_print_meta: general.name     = 1.4B
0.00.051.589 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: LF token         = 128 ''
0.00.051.590 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.590 I llm_load_print_meta: max token length = 1024
0.00.053.648 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.648 I llm_load_tensors: offloading output layer to GPU
0.00.053.648 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.659 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.660 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.574 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.574 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.574 I llama_new_context_with_model: n_batch       = 2048
0.00.054.575 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.575 I llama_new_context_with_model: flash_attn    = 0
0.00.054.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.575 I llama_new_context_with_model: freq_scale    = 1
0.00.054.576 I ggml_metal_init: allocating
0.00.054.579 I ggml_metal_init: found device: Apple M4
0.00.054.581 I ggml_metal_init: picking default device: Apple M4
0.00.055.178 I ggml_metal_init: using embedded metal library
0.00.057.179 I ggml_metal_init: GPU name:   Apple M4
0.00.057.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.181 I ggml_metal_init: simdgroup reduction   = true
0.00.057.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.182 I ggml_metal_init: has bfloat            = true
0.00.057.182 I ggml_metal_init: use bfloat            = true
0.00.057.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.185 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.307 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.329 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.311 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.313 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.313 I llama_new_context_with_model: graph nodes  = 967
0.00.087.313 I llama_new_context_with_model: graph splits = 2
0.00.087.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.218 I main: llama threadpool init, n_threads = 4
0.00.749.254 I 
0.00.749.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.749.280 I 
0.00.749.415 I sampler seed: 1234
0.00.749.419 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.431 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.432 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.539.440 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.539.441 I llama_perf_context_print:        load time =     739.84 ms
0.01.539.442 I llama_perf_context_print: prompt eval time =      36.57 ms /     7 tokens (    5.22 ms per token,   191.41 tokens per second)
0.01.539.443 I llama_perf_context_print:        eval time =     750.31 ms /    63 runs   (   11.91 ms per token,    83.97 tokens per second)
0.01.539.443 I llama_perf_context_print:       total time =     790.23 ms /    70 tokens
0.01.539.624 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.847 I llama_model_loader: - type  f32:  194 tensors
0.00.024.848 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.848 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.586 I llm_load_vocab: special tokens cache size = 25
0.00.050.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.442 I llm_load_print_meta: arch             = gptneox
0.00.050.443 I llm_load_print_meta: vocab type       = BPE
0.00.050.443 I llm_load_print_meta: n_vocab          = 50304
0.00.050.443 I llm_load_print_meta: n_merges         = 50009
0.00.050.443 I llm_load_print_meta: vocab_only       = 0
0.00.050.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.444 I llm_load_print_meta: n_embd           = 2048
0.00.050.444 I llm_load_print_meta: n_layer          = 24
0.00.050.447 I llm_load_print_meta: n_head           = 16
0.00.050.447 I llm_load_print_meta: n_head_kv        = 16
0.00.050.448 I llm_load_print_meta: n_rot            = 32
0.00.050.448 I llm_load_print_meta: n_swa            = 0
0.00.050.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.449 I llm_load_print_meta: n_gqa            = 1
0.00.050.450 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.450 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.451 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.451 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.451 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.452 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.452 I llm_load_print_meta: n_ff             = 8192
0.00.050.453 I llm_load_print_meta: n_expert         = 0
0.00.050.453 I llm_load_print_meta: n_expert_used    = 0
0.00.050.455 I llm_load_print_meta: causal attn      = 1
0.00.050.455 I llm_load_print_meta: pooling type     = 0
0.00.050.455 I llm_load_print_meta: rope type        = 2
0.00.050.455 I llm_load_print_meta: rope scaling     = linear
0.00.050.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.456 I llm_load_print_meta: freq_scale_train = 1
0.00.050.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.458 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.469 I llm_load_print_meta: model type       = 1.4B
0.00.050.470 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.470 I llm_load_print_meta: model params     = 1.41 B
0.00.050.471 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.471 I llm_load_print_meta: general.name     = 1.4B
0.00.050.471 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.471 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.471 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.471 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.475 I llm_load_print_meta: LF token         = 128 ''
0.00.050.475 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.475 I llm_load_print_meta: max token length = 1024
0.00.052.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.482 I llm_load_tensors: offloading output layer to GPU
0.00.052.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.492 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.494 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.415 I llama_new_context_with_model: n_ctx         = 128
0.00.053.415 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.416 I llama_new_context_with_model: n_batch       = 128
0.00.053.416 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.416 I llama_new_context_with_model: flash_attn    = 0
0.00.053.416 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.417 I llama_new_context_with_model: freq_scale    = 1
0.00.053.417 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.417 I ggml_metal_init: allocating
0.00.053.423 I ggml_metal_init: found device: Apple M4
0.00.053.425 I ggml_metal_init: picking default device: Apple M4
0.00.053.992 I ggml_metal_init: using embedded metal library
0.00.055.953 I ggml_metal_init: GPU name:   Apple M4
0.00.055.955 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.955 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.955 I ggml_metal_init: simdgroup reduction   = true
0.00.055.956 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.956 I ggml_metal_init: has bfloat            = true
0.00.055.956 I ggml_metal_init: use bfloat            = true
0.00.055.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.122 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.137 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.022 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.023 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.024 I llama_new_context_with_model: graph nodes  = 967
0.00.066.024 I llama_new_context_with_model: graph splits = 2
0.00.066.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.477 I 
0.00.742.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.742.509 I perplexity: tokenizing the input ..
0.00.750.855 I perplexity: tokenization took 8.345 ms
0.00.750.863 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.150 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.887.393 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.887.422 I llama_perf_context_print:        load time =     732.56 ms
0.00.887.423 I llama_perf_context_print: prompt eval time =     135.05 ms /   128 tokens (    1.06 ms per token,   947.83 tokens per second)
0.00.887.424 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.424 I llama_perf_context_print:       total time =     144.94 ms /   129 tokens
0.00.887.864 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.076s
sys	0m0.118s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.038 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.039 I llama_model_loader: - type  f32:  194 tensors
0.00.024.039 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.958 I llm_load_vocab: special tokens cache size = 25
0.00.049.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.872 I llm_load_print_meta: arch             = gptneox
0.00.049.872 I llm_load_print_meta: vocab type       = BPE
0.00.049.872 I llm_load_print_meta: n_vocab          = 50304
0.00.049.873 I llm_load_print_meta: n_merges         = 50009
0.00.049.873 I llm_load_print_meta: vocab_only       = 0
0.00.049.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.873 I llm_load_print_meta: n_embd           = 2048
0.00.049.873 I llm_load_print_meta: n_layer          = 24
0.00.049.876 I llm_load_print_meta: n_head           = 16
0.00.049.877 I llm_load_print_meta: n_head_kv        = 16
0.00.049.877 I llm_load_print_meta: n_rot            = 32
0.00.049.877 I llm_load_print_meta: n_swa            = 0
0.00.049.877 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.878 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.879 I llm_load_print_meta: n_gqa            = 1
0.00.049.880 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.881 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.881 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.882 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.882 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.882 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.883 I llm_load_print_meta: n_ff             = 8192
0.00.049.883 I llm_load_print_meta: n_expert         = 0
0.00.049.883 I llm_load_print_meta: n_expert_used    = 0
0.00.049.885 I llm_load_print_meta: causal attn      = 1
0.00.049.887 I llm_load_print_meta: pooling type     = 0
0.00.049.887 I llm_load_print_meta: rope type        = 2
0.00.049.887 I llm_load_print_meta: rope scaling     = linear
0.00.049.887 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.888 I llm_load_print_meta: freq_scale_train = 1
0.00.049.888 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.888 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.888 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.889 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.901 I llm_load_print_meta: model type       = 1.4B
0.00.049.901 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.901 I llm_load_print_meta: model params     = 1.41 B
0.00.049.902 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.902 I llm_load_print_meta: general.name     = 1.4B
0.00.049.902 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.902 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.903 I llm_load_print_meta: LF token         = 128 ''
0.00.049.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.903 I llm_load_print_meta: max token length = 1024
0.00.051.908 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.908 I llm_load_tensors: offloading output layer to GPU
0.00.051.908 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.918 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.919 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.803 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.803 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.803 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.804 I llama_new_context_with_model: n_batch       = 2048
0.00.052.804 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.804 I llama_new_context_with_model: flash_attn    = 0
0.00.052.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.805 I llama_new_context_with_model: freq_scale    = 1
0.00.052.805 I ggml_metal_init: allocating
0.00.052.811 I ggml_metal_init: found device: Apple M4
0.00.052.814 I ggml_metal_init: picking default device: Apple M4
0.00.053.363 I ggml_metal_init: using embedded metal library
0.00.055.289 I ggml_metal_init: GPU name:   Apple M4
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.292 I ggml_metal_init: simdgroup reduction   = true
0.00.055.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.293 I ggml_metal_init: has bfloat            = true
0.00.055.293 I ggml_metal_init: use bfloat            = true
0.00.055.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.863 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.872 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.892 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.780 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.782 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.782 I llama_new_context_with_model: graph nodes  = 967
0.00.082.782 I llama_new_context_with_model: graph splits = 2
0.00.082.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.925 I main: llama threadpool init, n_threads = 4
0.00.718.962 I 
0.00.718.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.719.000 I 
0.00.719.237 I sampler seed: 1234
0.00.719.241 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.256 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.257 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.258 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.556.207 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.556.208 I llama_perf_context_print:        load time =     710.16 ms
0.01.556.209 I llama_perf_context_print: prompt eval time =      36.63 ms /     7 tokens (    5.23 ms per token,   191.11 tokens per second)
0.01.556.210 I llama_perf_context_print:        eval time =     797.44 ms /    63 runs   (   12.66 ms per token,    79.00 tokens per second)
0.01.556.210 I llama_perf_context_print:       total time =     837.28 ms /    70 tokens
0.01.556.392 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.107s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.767 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.769 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.770 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.772 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.772 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.772 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.773 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.814 I llama_model_loader: - type  f32:  194 tensors
0.00.023.814 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.489 I llm_load_vocab: special tokens cache size = 25
0.00.050.496 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.498 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.499 I llm_load_print_meta: arch             = gptneox
0.00.050.499 I llm_load_print_meta: vocab type       = BPE
0.00.050.499 I llm_load_print_meta: n_vocab          = 50304
0.00.050.499 I llm_load_print_meta: n_merges         = 50009
0.00.050.500 I llm_load_print_meta: vocab_only       = 0
0.00.050.500 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.500 I llm_load_print_meta: n_embd           = 2048
0.00.050.500 I llm_load_print_meta: n_layer          = 24
0.00.050.503 I llm_load_print_meta: n_head           = 16
0.00.050.504 I llm_load_print_meta: n_head_kv        = 16
0.00.050.504 I llm_load_print_meta: n_rot            = 32
0.00.050.504 I llm_load_print_meta: n_swa            = 0
0.00.050.504 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.505 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.505 I llm_load_print_meta: n_gqa            = 1
0.00.050.506 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.509 I llm_load_print_meta: n_ff             = 8192
0.00.050.509 I llm_load_print_meta: n_expert         = 0
0.00.050.510 I llm_load_print_meta: n_expert_used    = 0
0.00.050.510 I llm_load_print_meta: causal attn      = 1
0.00.050.510 I llm_load_print_meta: pooling type     = 0
0.00.050.510 I llm_load_print_meta: rope type        = 2
0.00.050.510 I llm_load_print_meta: rope scaling     = linear
0.00.050.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.511 I llm_load_print_meta: freq_scale_train = 1
0.00.050.511 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.511 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.512 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.512 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.524 I llm_load_print_meta: model type       = 1.4B
0.00.050.524 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.525 I llm_load_print_meta: model params     = 1.41 B
0.00.050.525 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.525 I llm_load_print_meta: general.name     = 1.4B
0.00.050.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.526 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.526 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.526 I llm_load_print_meta: LF token         = 128 ''
0.00.050.526 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: max token length = 1024
0.00.052.514 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.514 I llm_load_tensors: offloading output layer to GPU
0.00.052.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.525 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.526 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.422 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.423 I llama_new_context_with_model: n_ctx         = 128
0.00.053.423 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.423 I llama_new_context_with_model: n_batch       = 128
0.00.053.424 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.424 I llama_new_context_with_model: flash_attn    = 0
0.00.053.424 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.424 I llama_new_context_with_model: freq_scale    = 1
0.00.053.425 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.425 I ggml_metal_init: allocating
0.00.053.428 I ggml_metal_init: found device: Apple M4
0.00.053.430 I ggml_metal_init: picking default device: Apple M4
0.00.053.970 I ggml_metal_init: using embedded metal library
0.00.055.847 I ggml_metal_init: GPU name:   Apple M4
0.00.055.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.849 I ggml_metal_init: simdgroup reduction   = true
0.00.055.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.849 I ggml_metal_init: has bfloat            = true
0.00.055.849 I ggml_metal_init: use bfloat            = true
0.00.055.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.199 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.212 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.136 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.137 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.137 I llama_new_context_with_model: graph nodes  = 967
0.00.066.137 I llama_new_context_with_model: graph splits = 2
0.00.066.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.435 I 
0.00.695.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.695.468 I perplexity: tokenizing the input ..
0.00.703.083 I perplexity: tokenization took 7.614 ms
0.00.703.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.679 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.838.094 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.117 I llama_perf_context_print:        load time =     686.54 ms
0.00.838.118 I llama_perf_context_print: prompt eval time =     133.36 ms /   128 tokens (    1.04 ms per token,   959.83 tokens per second)
0.00.838.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.119 I llama_perf_context_print:       total time =     142.68 ms /   129 tokens
0.00.838.493 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.077s
sys	0m0.139s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.878 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.777 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.979 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.980 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.980 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.980 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.981 I llama_model_loader: - type  f32:  194 tensors
0.00.024.981 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.982 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.982 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.736 I llm_load_vocab: special tokens cache size = 25
0.00.051.608 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.611 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.611 I llm_load_print_meta: arch             = gptneox
0.00.051.612 I llm_load_print_meta: vocab type       = BPE
0.00.051.612 I llm_load_print_meta: n_vocab          = 50304
0.00.051.612 I llm_load_print_meta: n_merges         = 50009
0.00.051.612 I llm_load_print_meta: vocab_only       = 0
0.00.051.612 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.613 I llm_load_print_meta: n_embd           = 2048
0.00.051.613 I llm_load_print_meta: n_layer          = 24
0.00.051.615 I llm_load_print_meta: n_head           = 16
0.00.051.616 I llm_load_print_meta: n_head_kv        = 16
0.00.051.616 I llm_load_print_meta: n_rot            = 32
0.00.051.617 I llm_load_print_meta: n_swa            = 0
0.00.051.617 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.617 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.618 I llm_load_print_meta: n_gqa            = 1
0.00.051.619 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.619 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.620 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.620 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.621 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.621 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.621 I llm_load_print_meta: n_ff             = 8192
0.00.051.622 I llm_load_print_meta: n_expert         = 0
0.00.051.622 I llm_load_print_meta: n_expert_used    = 0
0.00.051.622 I llm_load_print_meta: causal attn      = 1
0.00.051.622 I llm_load_print_meta: pooling type     = 0
0.00.051.622 I llm_load_print_meta: rope type        = 2
0.00.051.622 I llm_load_print_meta: rope scaling     = linear
0.00.051.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.623 I llm_load_print_meta: freq_scale_train = 1
0.00.051.623 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.624 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.624 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.624 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.624 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.624 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.624 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.636 I llm_load_print_meta: model type       = 1.4B
0.00.051.636 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.637 I llm_load_print_meta: model params     = 1.41 B
0.00.051.637 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.637 I llm_load_print_meta: general.name     = 1.4B
0.00.051.638 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.638 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.640 I llm_load_print_meta: LF token         = 128 ''
0.00.051.642 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.642 I llm_load_print_meta: max token length = 1024
0.00.053.545 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.545 I llm_load_tensors: offloading output layer to GPU
0.00.053.545 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.555 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.556 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.499 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.500 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.500 I llama_new_context_with_model: n_batch       = 2048
0.00.054.500 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.500 I llama_new_context_with_model: flash_attn    = 0
0.00.054.501 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.501 I llama_new_context_with_model: freq_scale    = 1
0.00.054.501 I ggml_metal_init: allocating
0.00.054.504 I ggml_metal_init: found device: Apple M4
0.00.054.506 I ggml_metal_init: picking default device: Apple M4
0.00.055.086 I ggml_metal_init: using embedded metal library
0.00.057.002 I ggml_metal_init: GPU name:   Apple M4
0.00.057.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.004 I ggml_metal_init: simdgroup reduction   = true
0.00.057.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.005 I ggml_metal_init: has bfloat            = true
0.00.057.005 I ggml_metal_init: use bfloat            = true
0.00.057.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.792 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.800 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.820 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.960 I llama_new_context_with_model: graph nodes  = 967
0.00.085.960 I llama_new_context_with_model: graph splits = 2
0.00.085.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.459 I main: llama threadpool init, n_threads = 4
0.00.434.497 I 
0.00.434.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.434.525 I 
0.00.434.678 I sampler seed: 1234
0.00.434.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.434.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.434.714 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.434.714 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.050 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.116.051 I llama_perf_context_print:        load time =     424.58 ms
0.01.116.051 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.78 tokens per second)
0.01.116.052 I llama_perf_context_print:        eval time =     642.50 ms /    63 runs   (   10.20 ms per token,    98.05 tokens per second)
0.01.116.052 I llama_perf_context_print:       total time =     681.60 ms /    70 tokens
0.01.116.231 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.110s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.336 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.963 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.971 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.973 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.974 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.974 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.975 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.579 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.581 I llama_model_loader: - type  f32:  194 tensors
0.00.024.583 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.585 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.572 I llm_load_vocab: special tokens cache size = 25
0.00.050.459 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.464 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.464 I llm_load_print_meta: arch             = gptneox
0.00.050.464 I llm_load_print_meta: vocab type       = BPE
0.00.050.464 I llm_load_print_meta: n_vocab          = 50304
0.00.050.466 I llm_load_print_meta: n_merges         = 50009
0.00.050.466 I llm_load_print_meta: vocab_only       = 0
0.00.050.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.466 I llm_load_print_meta: n_embd           = 2048
0.00.050.467 I llm_load_print_meta: n_layer          = 24
0.00.050.470 I llm_load_print_meta: n_head           = 16
0.00.050.471 I llm_load_print_meta: n_head_kv        = 16
0.00.050.475 I llm_load_print_meta: n_rot            = 32
0.00.050.476 I llm_load_print_meta: n_swa            = 0
0.00.050.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.478 I llm_load_print_meta: n_gqa            = 1
0.00.050.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.480 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.482 I llm_load_print_meta: n_ff             = 8192
0.00.050.482 I llm_load_print_meta: n_expert         = 0
0.00.050.482 I llm_load_print_meta: n_expert_used    = 0
0.00.050.482 I llm_load_print_meta: causal attn      = 1
0.00.050.485 I llm_load_print_meta: pooling type     = 0
0.00.050.485 I llm_load_print_meta: rope type        = 2
0.00.050.485 I llm_load_print_meta: rope scaling     = linear
0.00.050.486 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.486 I llm_load_print_meta: freq_scale_train = 1
0.00.050.486 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.487 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.487 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.487 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.499 I llm_load_print_meta: model type       = 1.4B
0.00.050.500 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.500 I llm_load_print_meta: model params     = 1.41 B
0.00.050.500 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.501 I llm_load_print_meta: general.name     = 1.4B
0.00.050.501 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.502 I llm_load_print_meta: LF token         = 128 ''
0.00.050.502 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.503 I llm_load_print_meta: max token length = 1024
0.00.052.394 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.395 I llm_load_tensors: offloading output layer to GPU
0.00.052.395 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.405 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.406 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.332 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.333 I llama_new_context_with_model: n_ctx         = 128
0.00.053.333 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.333 I llama_new_context_with_model: n_batch       = 128
0.00.053.333 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.333 I llama_new_context_with_model: flash_attn    = 0
0.00.053.334 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.334 I llama_new_context_with_model: freq_scale    = 1
0.00.053.334 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.334 I ggml_metal_init: allocating
0.00.053.337 I ggml_metal_init: found device: Apple M4
0.00.053.339 I ggml_metal_init: picking default device: Apple M4
0.00.053.926 I ggml_metal_init: using embedded metal library
0.00.055.877 I ggml_metal_init: GPU name:   Apple M4
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.879 I ggml_metal_init: simdgroup reduction   = true
0.00.055.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.879 I ggml_metal_init: has bfloat            = true
0.00.055.879 I ggml_metal_init: use bfloat            = true
0.00.055.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.587 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.590 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.605 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.470 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.471 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.472 I llama_new_context_with_model: graph nodes  = 967
0.00.065.472 I llama_new_context_with_model: graph splits = 2
0.00.065.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.380.042 I 
0.00.380.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.380.073 I perplexity: tokenizing the input ..
0.00.388.557 I perplexity: tokenization took 8.482 ms
0.00.388.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.520.262 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.521.444 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.521.493 I llama_perf_context_print:        load time =     370.70 ms
0.00.521.494 I llama_perf_context_print: prompt eval time =     131.44 ms /   128 tokens (    1.03 ms per token,   973.85 tokens per second)
0.00.521.495 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.521.496 I llama_perf_context_print:       total time =     141.45 ms /   129 tokens
0.00.521.901 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.078s
sys	0m0.063s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.285 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.399 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.412 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.412 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.412 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.554 I llama_model_loader: - type  f32:  194 tensors
0.00.024.554 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.555 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.555 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.489 I llm_load_vocab: special tokens cache size = 25
0.00.050.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.346 I llm_load_print_meta: arch             = gptneox
0.00.050.346 I llm_load_print_meta: vocab type       = BPE
0.00.050.346 I llm_load_print_meta: n_vocab          = 50304
0.00.050.346 I llm_load_print_meta: n_merges         = 50009
0.00.050.347 I llm_load_print_meta: vocab_only       = 0
0.00.050.347 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.347 I llm_load_print_meta: n_embd           = 2048
0.00.050.347 I llm_load_print_meta: n_layer          = 24
0.00.050.350 I llm_load_print_meta: n_head           = 16
0.00.050.351 I llm_load_print_meta: n_head_kv        = 16
0.00.050.354 I llm_load_print_meta: n_rot            = 32
0.00.050.354 I llm_load_print_meta: n_swa            = 0
0.00.050.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.354 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.355 I llm_load_print_meta: n_gqa            = 1
0.00.050.356 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.356 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.357 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.358 I llm_load_print_meta: n_ff             = 8192
0.00.050.360 I llm_load_print_meta: n_expert         = 0
0.00.050.361 I llm_load_print_meta: n_expert_used    = 0
0.00.050.361 I llm_load_print_meta: causal attn      = 1
0.00.050.361 I llm_load_print_meta: pooling type     = 0
0.00.050.361 I llm_load_print_meta: rope type        = 2
0.00.050.362 I llm_load_print_meta: rope scaling     = linear
0.00.050.362 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.362 I llm_load_print_meta: freq_scale_train = 1
0.00.050.362 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.363 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.363 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.363 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.363 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.363 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.363 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.375 I llm_load_print_meta: model type       = 1.4B
0.00.050.376 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.376 I llm_load_print_meta: model params     = 1.41 B
0.00.050.377 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.377 I llm_load_print_meta: general.name     = 1.4B
0.00.050.378 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: LF token         = 128 ''
0.00.050.379 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.380 I llm_load_print_meta: max token length = 1024
0.00.052.306 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.306 I llm_load_tensors: offloading output layer to GPU
0.00.052.307 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.317 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.318 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.260 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.261 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.262 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.262 I llama_new_context_with_model: n_batch       = 2048
0.00.053.262 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.262 I llama_new_context_with_model: flash_attn    = 0
0.00.053.263 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.263 I llama_new_context_with_model: freq_scale    = 1
0.00.053.263 I ggml_metal_init: allocating
0.00.053.269 I ggml_metal_init: found device: Apple M4
0.00.053.271 I ggml_metal_init: picking default device: Apple M4
0.00.053.823 I ggml_metal_init: using embedded metal library
0.00.055.744 I ggml_metal_init: GPU name:   Apple M4
0.00.055.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.748 I ggml_metal_init: simdgroup reduction   = true
0.00.055.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.748 I ggml_metal_init: has bfloat            = true
0.00.055.748 I ggml_metal_init: use bfloat            = true
0.00.055.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.552 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.493 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.495 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.495 I llama_new_context_with_model: graph nodes  = 967
0.00.083.495 I llama_new_context_with_model: graph splits = 2
0.00.083.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.792 I main: llama threadpool init, n_threads = 4
0.00.545.828 I 
0.00.545.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.545.866 I 
0.00.546.101 I sampler seed: 1234
0.00.546.105 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.141 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.143 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.293.033 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.293.034 I llama_perf_context_print:        load time =     536.50 ms
0.01.293.034 I llama_perf_context_print: prompt eval time =      39.04 ms /     7 tokens (    5.58 ms per token,   179.32 tokens per second)
0.01.293.035 I llama_perf_context_print:        eval time =     704.90 ms /    63 runs   (   11.19 ms per token,    89.37 tokens per second)
0.01.293.035 I llama_perf_context_print:       total time =     747.24 ms /    70 tokens
0.01.293.221 I ggml_metal_free: deallocating

real	0m1.311s
user	0m0.107s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.618 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.704 I llama_model_loader: - type  f32:  194 tensors
0.00.023.705 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.705 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.705 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.705 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.415 I llm_load_vocab: special tokens cache size = 25
0.00.050.313 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.316 I llm_load_print_meta: arch             = gptneox
0.00.050.316 I llm_load_print_meta: vocab type       = BPE
0.00.050.316 I llm_load_print_meta: n_vocab          = 50304
0.00.050.317 I llm_load_print_meta: n_merges         = 50009
0.00.050.317 I llm_load_print_meta: vocab_only       = 0
0.00.050.317 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.317 I llm_load_print_meta: n_embd           = 2048
0.00.050.317 I llm_load_print_meta: n_layer          = 24
0.00.050.320 I llm_load_print_meta: n_head           = 16
0.00.050.321 I llm_load_print_meta: n_head_kv        = 16
0.00.050.321 I llm_load_print_meta: n_rot            = 32
0.00.050.321 I llm_load_print_meta: n_swa            = 0
0.00.050.322 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.322 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.325 I llm_load_print_meta: n_gqa            = 1
0.00.050.326 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.329 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.329 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.330 I llm_load_print_meta: n_ff             = 8192
0.00.050.338 I llm_load_print_meta: n_expert         = 0
0.00.050.340 I llm_load_print_meta: n_expert_used    = 0
0.00.050.340 I llm_load_print_meta: causal attn      = 1
0.00.050.340 I llm_load_print_meta: pooling type     = 0
0.00.050.340 I llm_load_print_meta: rope type        = 2
0.00.050.340 I llm_load_print_meta: rope scaling     = linear
0.00.050.341 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.341 I llm_load_print_meta: freq_scale_train = 1
0.00.050.341 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.342 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.342 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.342 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.354 I llm_load_print_meta: model type       = 1.4B
0.00.050.354 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.355 I llm_load_print_meta: model params     = 1.41 B
0.00.050.355 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.355 I llm_load_print_meta: general.name     = 1.4B
0.00.050.356 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: LF token         = 128 ''
0.00.050.357 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.357 I llm_load_print_meta: max token length = 1024
0.00.052.183 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.183 I llm_load_tensors: offloading output layer to GPU
0.00.052.183 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.193 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.194 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.104 I llama_new_context_with_model: n_ctx         = 128
0.00.053.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.104 I llama_new_context_with_model: n_batch       = 128
0.00.053.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.104 I llama_new_context_with_model: flash_attn    = 0
0.00.053.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.105 I llama_new_context_with_model: freq_scale    = 1
0.00.053.106 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.106 I ggml_metal_init: allocating
0.00.053.113 I ggml_metal_init: found device: Apple M4
0.00.053.115 I ggml_metal_init: picking default device: Apple M4
0.00.053.662 I ggml_metal_init: using embedded metal library
0.00.055.633 I ggml_metal_init: GPU name:   Apple M4
0.00.055.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.635 I ggml_metal_init: simdgroup reduction   = true
0.00.055.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.636 I ggml_metal_init: has bfloat            = true
0.00.055.636 I ggml_metal_init: use bfloat            = true
0.00.055.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.637 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.429 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.432 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.351 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.353 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.353 I llama_new_context_with_model: graph nodes  = 967
0.00.066.353 I llama_new_context_with_model: graph splits = 2
0.00.066.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.178 I 
0.00.503.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.503.228 I perplexity: tokenizing the input ..
0.00.510.900 I perplexity: tokenization took 7.669 ms
0.00.510.904 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.141 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.644.329 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.362 I llama_perf_context_print:        load time =     494.56 ms
0.00.644.363 I llama_perf_context_print: prompt eval time =     132.01 ms /   128 tokens (    1.03 ms per token,   969.61 tokens per second)
0.00.644.364 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.364 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.644.873 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.078s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.449 I llama_model_loader: - type  f32:  194 tensors
0.00.025.449 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.449 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.449 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.106 I llm_load_vocab: special tokens cache size = 25
0.00.051.769 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.772 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.772 I llm_load_print_meta: arch             = gptneox
0.00.051.773 I llm_load_print_meta: vocab type       = BPE
0.00.051.773 I llm_load_print_meta: n_vocab          = 50304
0.00.051.773 I llm_load_print_meta: n_merges         = 50009
0.00.051.773 I llm_load_print_meta: vocab_only       = 0
0.00.051.773 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.773 I llm_load_print_meta: n_embd           = 2048
0.00.051.774 I llm_load_print_meta: n_layer          = 24
0.00.051.777 I llm_load_print_meta: n_head           = 16
0.00.051.777 I llm_load_print_meta: n_head_kv        = 16
0.00.051.778 I llm_load_print_meta: n_rot            = 32
0.00.051.780 I llm_load_print_meta: n_swa            = 0
0.00.051.780 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.781 I llm_load_print_meta: n_gqa            = 1
0.00.051.782 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.783 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.783 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.784 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.784 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.784 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.784 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.787 I llm_load_print_meta: n_ff             = 8192
0.00.051.787 I llm_load_print_meta: n_expert         = 0
0.00.051.788 I llm_load_print_meta: n_expert_used    = 0
0.00.051.788 I llm_load_print_meta: causal attn      = 1
0.00.051.788 I llm_load_print_meta: pooling type     = 0
0.00.051.788 I llm_load_print_meta: rope type        = 2
0.00.051.788 I llm_load_print_meta: rope scaling     = linear
0.00.051.789 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.789 I llm_load_print_meta: freq_scale_train = 1
0.00.051.789 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.789 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.790 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.790 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.790 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.790 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.790 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.801 I llm_load_print_meta: model type       = 1.4B
0.00.051.802 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.802 I llm_load_print_meta: model params     = 1.41 B
0.00.051.803 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.803 I llm_load_print_meta: general.name     = 1.4B
0.00.051.803 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.803 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: LF token         = 128 ''
0.00.051.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: max token length = 1024
0.00.053.362 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.362 I llm_load_tensors: offloading output layer to GPU
0.00.053.363 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.372 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.373 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.213 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.214 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.215 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.215 I llama_new_context_with_model: n_batch       = 2048
0.00.054.215 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.215 I llama_new_context_with_model: flash_attn    = 0
0.00.054.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.216 I llama_new_context_with_model: freq_scale    = 1
0.00.054.216 I ggml_metal_init: allocating
0.00.054.221 I ggml_metal_init: found device: Apple M4
0.00.054.224 I ggml_metal_init: picking default device: Apple M4
0.00.054.783 I ggml_metal_init: using embedded metal library
0.00.056.675 I ggml_metal_init: GPU name:   Apple M4
0.00.056.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.678 I ggml_metal_init: simdgroup reduction   = true
0.00.056.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.679 I ggml_metal_init: has bfloat            = true
0.00.056.679 I ggml_metal_init: use bfloat            = true
0.00.056.679 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.989 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.995 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.012 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.047 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.048 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.049 I llama_new_context_with_model: graph nodes  = 967
0.00.085.049 I llama_new_context_with_model: graph splits = 2
0.00.085.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.363 I main: llama threadpool init, n_threads = 4
0.00.683.483 I 
0.00.683.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.683.552 I 
0.00.684.057 I sampler seed: 1234
0.00.684.064 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.128 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.130 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.131 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.456.514 I llama_perf_sampler_print:    sampling time =       1.55 ms /    71 runs   (    0.02 ms per token, 45806.45 tokens per second)
0.01.456.515 I llama_perf_context_print:        load time =     673.66 ms
0.01.456.515 I llama_perf_context_print: prompt eval time =      46.87 ms /     7 tokens (    6.70 ms per token,   149.35 tokens per second)
0.01.456.516 I llama_perf_context_print:        eval time =     722.49 ms /    63 runs   (   11.47 ms per token,    87.20 tokens per second)
0.01.456.517 I llama_perf_context_print:       total time =     773.16 ms /    70 tokens
0.01.456.738 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.121s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.726 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.457 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.459 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.460 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.461 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.432 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.381 I llama_model_loader: - type  f32:  194 tensors
0.00.023.381 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.382 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.382 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.036 I llm_load_vocab: special tokens cache size = 25
0.00.048.866 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.869 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.869 I llm_load_print_meta: arch             = gptneox
0.00.048.870 I llm_load_print_meta: vocab type       = BPE
0.00.048.870 I llm_load_print_meta: n_vocab          = 50304
0.00.048.870 I llm_load_print_meta: n_merges         = 50009
0.00.048.870 I llm_load_print_meta: vocab_only       = 0
0.00.048.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.871 I llm_load_print_meta: n_embd           = 2048
0.00.048.871 I llm_load_print_meta: n_layer          = 24
0.00.048.874 I llm_load_print_meta: n_head           = 16
0.00.048.875 I llm_load_print_meta: n_head_kv        = 16
0.00.048.875 I llm_load_print_meta: n_rot            = 32
0.00.048.875 I llm_load_print_meta: n_swa            = 0
0.00.048.875 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.875 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.876 I llm_load_print_meta: n_gqa            = 1
0.00.048.879 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.880 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.881 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.881 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.881 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.883 I llm_load_print_meta: n_ff             = 8192
0.00.048.883 I llm_load_print_meta: n_expert         = 0
0.00.048.884 I llm_load_print_meta: n_expert_used    = 0
0.00.048.884 I llm_load_print_meta: causal attn      = 1
0.00.048.884 I llm_load_print_meta: pooling type     = 0
0.00.048.884 I llm_load_print_meta: rope type        = 2
0.00.048.884 I llm_load_print_meta: rope scaling     = linear
0.00.048.885 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.885 I llm_load_print_meta: freq_scale_train = 1
0.00.048.885 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.886 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.886 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.886 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.887 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.899 I llm_load_print_meta: model type       = 1.4B
0.00.048.899 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.900 I llm_load_print_meta: model params     = 1.41 B
0.00.048.900 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.900 I llm_load_print_meta: general.name     = 1.4B
0.00.048.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.901 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.901 I llm_load_print_meta: LF token         = 128 ''
0.00.048.902 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.902 I llm_load_print_meta: max token length = 1024
0.00.050.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.763 I llm_load_tensors: offloading output layer to GPU
0.00.050.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.773 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.774 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.628 I llama_new_context_with_model: n_ctx         = 128
0.00.051.628 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.628 I llama_new_context_with_model: n_batch       = 128
0.00.051.629 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.629 I llama_new_context_with_model: flash_attn    = 0
0.00.051.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.629 I llama_new_context_with_model: freq_scale    = 1
0.00.051.630 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.630 I ggml_metal_init: allocating
0.00.051.633 I ggml_metal_init: found device: Apple M4
0.00.051.635 I ggml_metal_init: picking default device: Apple M4
0.00.052.172 I ggml_metal_init: using embedded metal library
0.00.054.076 I ggml_metal_init: GPU name:   Apple M4
0.00.054.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.078 I ggml_metal_init: simdgroup reduction   = true
0.00.054.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.079 I ggml_metal_init: has bfloat            = true
0.00.054.079 I ggml_metal_init: use bfloat            = true
0.00.054.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.926 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.930 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.854 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.855 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.856 I llama_new_context_with_model: graph nodes  = 967
0.00.063.856 I llama_new_context_with_model: graph splits = 2
0.00.063.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.429 I 
0.00.576.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.576.476 I perplexity: tokenizing the input ..
0.00.584.463 I perplexity: tokenization took 7.985 ms
0.00.584.466 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.719.087 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.720.237 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.720.266 I llama_perf_context_print:        load time =     567.70 ms
0.00.720.267 I llama_perf_context_print: prompt eval time =     134.39 ms /   128 tokens (    1.05 ms per token,   952.42 tokens per second)
0.00.720.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.720.269 I llama_perf_context_print:       total time =     143.84 ms /   129 tokens
0.00.720.734 I ggml_metal_free: deallocating

real	0m0.735s
user	0m0.076s
sys	0m0.115s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.956 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.026.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.833 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.036.433 I llama_model_loader: - type  f32:  194 tensors
0.00.036.434 I llama_model_loader: - type q5_K:   61 tensors
0.00.036.434 I llama_model_loader: - type q6_K:   37 tensors
0.00.058.582 I llm_load_vocab: special tokens cache size = 25
0.00.064.756 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.759 I llm_load_print_meta: arch             = gptneox
0.00.064.760 I llm_load_print_meta: vocab type       = BPE
0.00.064.760 I llm_load_print_meta: n_vocab          = 50304
0.00.064.760 I llm_load_print_meta: n_merges         = 50009
0.00.064.760 I llm_load_print_meta: vocab_only       = 0
0.00.064.761 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.761 I llm_load_print_meta: n_embd           = 2048
0.00.064.761 I llm_load_print_meta: n_layer          = 24
0.00.064.767 I llm_load_print_meta: n_head           = 16
0.00.064.768 I llm_load_print_meta: n_head_kv        = 16
0.00.064.768 I llm_load_print_meta: n_rot            = 32
0.00.064.768 I llm_load_print_meta: n_swa            = 0
0.00.064.768 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.769 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.769 I llm_load_print_meta: n_gqa            = 1
0.00.064.771 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.772 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.772 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.772 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.773 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.773 I llm_load_print_meta: n_ff             = 8192
0.00.064.773 I llm_load_print_meta: n_expert         = 0
0.00.064.774 I llm_load_print_meta: n_expert_used    = 0
0.00.064.775 I llm_load_print_meta: causal attn      = 1
0.00.064.777 I llm_load_print_meta: pooling type     = 0
0.00.064.777 I llm_load_print_meta: rope type        = 2
0.00.064.777 I llm_load_print_meta: rope scaling     = linear
0.00.064.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.778 I llm_load_print_meta: freq_scale_train = 1
0.00.064.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.778 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.779 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.779 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.779 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.791 I llm_load_print_meta: model type       = 1.4B
0.00.064.791 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.064.792 I llm_load_print_meta: model params     = 1.41 B
0.00.064.792 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.064.792 I llm_load_print_meta: general.name     = 1.4B
0.00.064.792 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.793 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.793 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.793 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.794 I llm_load_print_meta: LF token         = 128 ''
0.00.064.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.795 I llm_load_print_meta: max token length = 1024
0.00.066.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.690 I llm_load_tensors: offloading output layer to GPU
0.00.066.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.700 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.066.701 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.067.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.681 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.682 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.682 I llama_new_context_with_model: n_batch       = 2048
0.00.067.682 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.682 I llama_new_context_with_model: flash_attn    = 0
0.00.067.683 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.683 I llama_new_context_with_model: freq_scale    = 1
0.00.067.683 I ggml_metal_init: allocating
0.00.067.690 I ggml_metal_init: found device: Apple M4
0.00.067.693 I ggml_metal_init: picking default device: Apple M4
0.00.068.285 I ggml_metal_init: using embedded metal library
0.00.070.330 I ggml_metal_init: GPU name:   Apple M4
0.00.070.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.334 I ggml_metal_init: simdgroup reduction   = true
0.00.070.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.334 I ggml_metal_init: has bfloat            = true
0.00.070.334 I ggml_metal_init: use bfloat            = true
0.00.070.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.101 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.113 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.136 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.128 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.129 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.129 I llama_new_context_with_model: graph nodes  = 967
0.00.100.129 I llama_new_context_with_model: graph splits = 2
0.00.100.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.994.938 I main: llama threadpool init, n_threads = 4
0.00.994.979 I 
0.00.995.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.995.010 I 
0.00.995.225 I sampler seed: 1234
0.00.995.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.995.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.995.286 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.995.286 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.839.481 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.839.481 I llama_perf_context_print:        load time =     983.98 ms
0.01.839.482 I llama_perf_context_print: prompt eval time =      42.44 ms /     7 tokens (    6.06 ms per token,   164.95 tokens per second)
0.01.839.483 I llama_perf_context_print:        eval time =     798.71 ms /    63 runs   (   12.68 ms per token,    78.88 tokens per second)
0.01.839.483 I llama_perf_context_print:       total time =     844.55 ms /    70 tokens
0.01.839.664 I ggml_metal_free: deallocating

real	0m1.859s
user	0m0.113s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.431 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.149 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.150 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.153 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.200 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.203 I llama_model_loader: - type  f32:  194 tensors
0.00.024.203 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.203 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.661 I llm_load_vocab: special tokens cache size = 25
0.00.050.527 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.530 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.530 I llm_load_print_meta: arch             = gptneox
0.00.050.530 I llm_load_print_meta: vocab type       = BPE
0.00.050.531 I llm_load_print_meta: n_vocab          = 50304
0.00.050.531 I llm_load_print_meta: n_merges         = 50009
0.00.050.531 I llm_load_print_meta: vocab_only       = 0
0.00.050.531 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.531 I llm_load_print_meta: n_embd           = 2048
0.00.050.532 I llm_load_print_meta: n_layer          = 24
0.00.050.535 I llm_load_print_meta: n_head           = 16
0.00.050.536 I llm_load_print_meta: n_head_kv        = 16
0.00.050.536 I llm_load_print_meta: n_rot            = 32
0.00.050.536 I llm_load_print_meta: n_swa            = 0
0.00.050.536 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.536 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.537 I llm_load_print_meta: n_gqa            = 1
0.00.050.538 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.539 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.539 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.540 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.540 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.541 I llm_load_print_meta: n_ff             = 8192
0.00.050.541 I llm_load_print_meta: n_expert         = 0
0.00.050.541 I llm_load_print_meta: n_expert_used    = 0
0.00.050.542 I llm_load_print_meta: causal attn      = 1
0.00.050.542 I llm_load_print_meta: pooling type     = 0
0.00.050.542 I llm_load_print_meta: rope type        = 2
0.00.050.542 I llm_load_print_meta: rope scaling     = linear
0.00.050.543 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.543 I llm_load_print_meta: freq_scale_train = 1
0.00.050.543 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.544 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.544 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.546 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.546 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.546 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.546 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.557 I llm_load_print_meta: model type       = 1.4B
0.00.050.558 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.558 I llm_load_print_meta: model params     = 1.41 B
0.00.050.559 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.559 I llm_load_print_meta: general.name     = 1.4B
0.00.050.559 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.559 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.559 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.560 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.560 I llm_load_print_meta: LF token         = 128 ''
0.00.050.560 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.560 I llm_load_print_meta: max token length = 1024
0.00.052.048 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.049 I llm_load_tensors: offloading output layer to GPU
0.00.052.049 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.058 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.059 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.901 I llama_new_context_with_model: n_ctx         = 128
0.00.052.901 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.901 I llama_new_context_with_model: n_batch       = 128
0.00.052.901 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.901 I llama_new_context_with_model: flash_attn    = 0
0.00.052.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.902 I llama_new_context_with_model: freq_scale    = 1
0.00.052.902 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.903 I ggml_metal_init: allocating
0.00.052.905 I ggml_metal_init: found device: Apple M4
0.00.052.907 I ggml_metal_init: picking default device: Apple M4
0.00.053.431 I ggml_metal_init: using embedded metal library
0.00.055.388 I ggml_metal_init: GPU name:   Apple M4
0.00.055.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.390 I ggml_metal_init: simdgroup reduction   = true
0.00.055.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.391 I ggml_metal_init: has bfloat            = true
0.00.055.391 I ggml_metal_init: use bfloat            = true
0.00.055.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.478 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.483 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.371 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.373 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.373 I llama_new_context_with_model: graph nodes  = 967
0.00.065.373 I llama_new_context_with_model: graph splits = 2
0.00.065.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.151 I 
0.00.664.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.664.180 I perplexity: tokenizing the input ..
0.00.671.881 I perplexity: tokenization took 7.7 ms
0.00.671.889 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.891 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.062 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.086 I llama_perf_context_print:        load time =     654.72 ms
0.00.813.087 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.73 tokens per second)
0.00.813.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.089 I llama_perf_context_print:       total time =     148.94 ms /   129 tokens
0.00.813.448 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.077s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.012.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.706 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.711 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.712 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.714 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.030.119 I llama_model_loader: - type  f32:  194 tensors
0.00.030.119 I llama_model_loader: - type q6_K:   98 tensors
0.00.051.396 I llm_load_vocab: special tokens cache size = 25
0.00.057.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.339 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.339 I llm_load_print_meta: arch             = gptneox
0.00.057.340 I llm_load_print_meta: vocab type       = BPE
0.00.057.340 I llm_load_print_meta: n_vocab          = 50304
0.00.057.340 I llm_load_print_meta: n_merges         = 50009
0.00.057.341 I llm_load_print_meta: vocab_only       = 0
0.00.057.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.341 I llm_load_print_meta: n_embd           = 2048
0.00.057.341 I llm_load_print_meta: n_layer          = 24
0.00.057.344 I llm_load_print_meta: n_head           = 16
0.00.057.345 I llm_load_print_meta: n_head_kv        = 16
0.00.057.345 I llm_load_print_meta: n_rot            = 32
0.00.057.345 I llm_load_print_meta: n_swa            = 0
0.00.057.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.346 I llm_load_print_meta: n_gqa            = 1
0.00.057.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.350 I llm_load_print_meta: n_ff             = 8192
0.00.057.350 I llm_load_print_meta: n_expert         = 0
0.00.057.350 I llm_load_print_meta: n_expert_used    = 0
0.00.057.350 I llm_load_print_meta: causal attn      = 1
0.00.057.350 I llm_load_print_meta: pooling type     = 0
0.00.057.350 I llm_load_print_meta: rope type        = 2
0.00.057.351 I llm_load_print_meta: rope scaling     = linear
0.00.057.351 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.351 I llm_load_print_meta: freq_scale_train = 1
0.00.057.351 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.352 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.352 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.352 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.352 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.364 I llm_load_print_meta: model type       = 1.4B
0.00.057.364 I llm_load_print_meta: model ftype      = Q6_K
0.00.057.365 I llm_load_print_meta: model params     = 1.41 B
0.00.057.365 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.057.365 I llm_load_print_meta: general.name     = 1.4B
0.00.057.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.366 I llm_load_print_meta: LF token         = 128 ''
0.00.057.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.368 I llm_load_print_meta: max token length = 1024
0.00.059.433 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.434 I llm_load_tensors: offloading output layer to GPU
0.00.059.434 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.444 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.059.445 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.060.377 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.377 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.378 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.378 I llama_new_context_with_model: n_batch       = 2048
0.00.060.378 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.378 I llama_new_context_with_model: flash_attn    = 0
0.00.060.378 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.379 I llama_new_context_with_model: freq_scale    = 1
0.00.060.379 I ggml_metal_init: allocating
0.00.060.383 I ggml_metal_init: found device: Apple M4
0.00.060.385 I ggml_metal_init: picking default device: Apple M4
0.00.060.961 I ggml_metal_init: using embedded metal library
0.00.062.870 I ggml_metal_init: GPU name:   Apple M4
0.00.062.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.872 I ggml_metal_init: simdgroup reduction   = true
0.00.062.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.872 I ggml_metal_init: has bfloat            = true
0.00.062.873 I ggml_metal_init: use bfloat            = true
0.00.062.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.465 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.473 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.660 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.661 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.662 I llama_new_context_with_model: graph nodes  = 967
0.00.094.662 I llama_new_context_with_model: graph splits = 2
0.00.094.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.365 I main: llama threadpool init, n_threads = 4
0.00.772.398 I 
0.00.772.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.772.426 I 
0.00.772.692 I sampler seed: 1234
0.00.772.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.734 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.636.503 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.636.504 I llama_perf_context_print:        load time =     759.69 ms
0.01.636.505 I llama_perf_context_print: prompt eval time =      38.12 ms /     7 tokens (    5.45 ms per token,   183.64 tokens per second)
0.01.636.505 I llama_perf_context_print:        eval time =     822.63 ms /    63 runs   (   13.06 ms per token,    76.58 tokens per second)
0.01.636.506 I llama_perf_context_print:       total time =     864.14 ms /    70 tokens
0.01.636.698 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4232 (6acce397) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.858 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.356 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.474 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.442 I llama_model_loader: - type  f32:  194 tensors
0.00.023.442 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.189 I llm_load_vocab: special tokens cache size = 25
0.00.048.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.959 I llm_load_print_meta: arch             = gptneox
0.00.048.960 I llm_load_print_meta: vocab type       = BPE
0.00.048.960 I llm_load_print_meta: n_vocab          = 50304
0.00.048.960 I llm_load_print_meta: n_merges         = 50009
0.00.048.960 I llm_load_print_meta: vocab_only       = 0
0.00.048.961 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.961 I llm_load_print_meta: n_embd           = 2048
0.00.048.961 I llm_load_print_meta: n_layer          = 24
0.00.048.963 I llm_load_print_meta: n_head           = 16
0.00.048.964 I llm_load_print_meta: n_head_kv        = 16
0.00.048.964 I llm_load_print_meta: n_rot            = 32
0.00.048.964 I llm_load_print_meta: n_swa            = 0
0.00.048.965 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.965 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.966 I llm_load_print_meta: n_gqa            = 1
0.00.048.966 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.968 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.968 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.968 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.969 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.969 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.969 I llm_load_print_meta: n_ff             = 8192
0.00.048.970 I llm_load_print_meta: n_expert         = 0
0.00.048.970 I llm_load_print_meta: n_expert_used    = 0
0.00.048.970 I llm_load_print_meta: causal attn      = 1
0.00.048.970 I llm_load_print_meta: pooling type     = 0
0.00.048.970 I llm_load_print_meta: rope type        = 2
0.00.048.970 I llm_load_print_meta: rope scaling     = linear
0.00.048.973 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.973 I llm_load_print_meta: freq_scale_train = 1
0.00.048.973 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.974 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.974 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.974 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.981 I llm_load_print_meta: model type       = 1.4B
0.00.048.981 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.982 I llm_load_print_meta: model params     = 1.41 B
0.00.048.982 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.982 I llm_load_print_meta: general.name     = 1.4B
0.00.048.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.983 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.983 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.983 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.983 I llm_load_print_meta: LF token         = 128 ''
0.00.048.984 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.984 I llm_load_print_meta: max token length = 1024
0.00.050.757 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.757 I llm_load_tensors: offloading output layer to GPU
0.00.050.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.763 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.763 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.634 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.635 I llama_new_context_with_model: n_ctx         = 128
0.00.051.635 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.635 I llama_new_context_with_model: n_batch       = 128
0.00.051.635 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.635 I llama_new_context_with_model: flash_attn    = 0
0.00.051.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.636 I llama_new_context_with_model: freq_scale    = 1
0.00.051.636 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.637 I ggml_metal_init: allocating
0.00.051.643 I ggml_metal_init: found device: Apple M4
0.00.051.646 I ggml_metal_init: picking default device: Apple M4
0.00.052.224 I ggml_metal_init: using embedded metal library
0.00.054.210 I ggml_metal_init: GPU name:   Apple M4
0.00.054.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.213 I ggml_metal_init: simdgroup reduction   = true
0.00.054.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.213 I ggml_metal_init: has bfloat            = true
0.00.054.213 I ggml_metal_init: use bfloat            = true
0.00.054.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.389 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.212 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.213 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.214 I llama_new_context_with_model: graph nodes  = 967
0.00.063.214 I llama_new_context_with_model: graph splits = 2
0.00.063.221 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.507 I 
0.00.365.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.365.556 I perplexity: tokenizing the input ..
0.00.373.850 I perplexity: tokenization took 8.292 ms
0.00.373.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.513.791 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.514.952 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.514.979 I llama_perf_context_print:        load time =     356.64 ms
0.00.514.980 I llama_perf_context_print: prompt eval time =     139.71 ms /   128 tokens (    1.09 ms per token,   916.17 tokens per second)
0.00.514.981 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.514.982 I llama_perf_context_print:       total time =     149.48 ms /   129 tokens
0.00.515.457 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.075s
sys	0m0.083s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4232 (6acce397)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb0b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb0bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb0c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb0d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb0fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb19620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb1b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb25850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb25cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bb26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bb26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bb26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bb26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bb27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bb278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bb27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bb281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bb28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bb28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bb28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bb29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bb29910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bb29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bb2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bb2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bb2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bb2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bb2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bb2b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bb2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bb1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bb2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bb2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bb2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bb2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bb2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bb2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bb2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bb2e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bb2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bb2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bb2f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bb2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bb2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bb30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bb30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bb309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bb30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bb31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bb317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bb31c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bb320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bb32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bb32a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bb32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bb33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bb33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bb33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bb34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bb345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bb34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bb34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bb353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bb35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bb35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bb361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bb36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bb36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bb36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bb37420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bb378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bb37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bb38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bb386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bb38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bb38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bb39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bb39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bb39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bb3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bb3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bb3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bb3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bb3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bb3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bb3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bb3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bb3c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bb3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bb3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bb3d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bb3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bb3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bb3e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bb3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bb3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bb3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bb40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bb405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bb40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bb411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bb41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bb41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bb421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bb42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bb42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bb431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bb43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bb43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bb441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bb44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bb44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bb451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bb45700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bb45c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bb461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bb466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bb46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bb47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bb476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bb47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bb48180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bb486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bb48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bb49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bb496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bb49c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bb4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bb4a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bb4ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bb4b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bb4b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bb4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bb4c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bb4c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bb4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bb4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bb4d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bb4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bb4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bb4e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bb4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bb4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bb4f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bb4fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bb50100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bb50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bb50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bb510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bb51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bb51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bb520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bb52630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bb52b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bb530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bb53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bb53b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bb54010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bb544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bb54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bb54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bb55290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bb55730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bb55bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bb56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bb56510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bb569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bb56e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bb572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bb57790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bb57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bb58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bb58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bb59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bb59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bb59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bb5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bb5a840 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb4c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb4c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb4ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb4ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb50e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb53a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb57090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb57500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb57970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb58b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb5a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb5a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb0aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bb14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bb153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bb15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bb15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bb160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bb16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bb169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bb16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bb497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bb49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bb4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bb18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bb19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bb194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bb19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bb19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bb1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bb1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bb1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bb1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bb1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bb1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bb1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bb1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bb1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bb1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bb1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bb1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bb1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bb1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bb1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bb1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bb1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bb1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bb1f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bb1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bb1fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bb1ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bb20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bb20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bb20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bb210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bb21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bb219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bb21e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bb222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bb22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bb22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bb22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bb23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bb238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bb23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bb241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bb24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bb24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bb24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bb25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bb257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bb25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bb260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bb26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bb269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bb26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bb27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bb276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bb27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bb27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bb28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bb288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bb28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bb29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bb29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bb29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bb29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bb2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bb2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bb2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bb2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bb2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bb2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bb2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bb2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bb2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bb2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bb2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bb2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bb2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bb2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bb2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bb2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bb2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bb2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bb2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bb2fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bb2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bb30390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bb30800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bb30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bb310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bb31550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bb319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bb31e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bb322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bb32710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bb32b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bb32ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bb33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bb338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bb33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bb341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bb34620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bb34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bb34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bb35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bb357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bb35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bb360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bb36530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bb369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bb36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bb37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bb376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bb37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bb37fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bb38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bb388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bb38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bb39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bb39600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bb39a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bb39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bb3a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bb3a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bb3ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bb3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bb3b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bb3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bb3bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bb3c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bb3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bb3cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bb3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bb3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bb3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bb3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bb3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bb3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bb3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bb3eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bb3f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bb3f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bb3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bb40080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bb404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bb40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bb40dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bb41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bb416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bb41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bb41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bb42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bb42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bb42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bb43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bb43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bb44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bb448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bb44d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bb45180 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bc04e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bc052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bc05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bc05b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bc05ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bc06460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bc068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bc06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bc071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bc076d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bc07b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bc081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bc08ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bc09490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bc09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bc0a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bc0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bc0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bc0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bc0c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bc0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bc0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bc0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bc0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bc0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bc0e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bc0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bc0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bc0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bc0f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bc0fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bc10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bc10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bc10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bc10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bc11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bc11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bc119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bc11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bc122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bc12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bc12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bc13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bc13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bc13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bc13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bc141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bc14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bc14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bc14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bc153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bc15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bc15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bc160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bc16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bc169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bc16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bc17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bc178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bc17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bc18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bc18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bc18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bc18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bc19350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bc197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bc19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bc1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bc1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bc1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bc1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bc1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bc1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bc1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bc1bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bc1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bc1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bc1cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bc1d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bc1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bc1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bc1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bc1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bc1e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bc1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bc1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bc1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bc1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bc1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bc20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bc206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bc20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bc20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bc21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bc21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bc21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bc22150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bc225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bc22a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bc22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bc23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bc23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bc23bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bc24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bc244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bc24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bc24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bc25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bc25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bc25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bc25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bc263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bc26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bc26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bc27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bc275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bc27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bc27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bc282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bc28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bc28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bc29040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bc294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bc29920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bc29d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bc2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bc2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bc2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bc2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bc2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bc2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bc2bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bc2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bc2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bc2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bc2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bc2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bc2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bc2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bc2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bc2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bc2e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bc2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bc2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bc2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bc2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bc2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bc303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bc30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bc30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bc310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bc31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bc319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bc31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bc322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bc32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bc32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bc33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bc33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bc338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bc33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bc341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bc34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bc34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bc34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bc35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bc357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bc36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bc36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bc36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bc36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bc371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bc37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bc37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bc37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bc383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bc38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bc38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bc390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bc39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bc399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bc39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bc3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bc3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bc3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bc3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bc3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bc3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bc3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bc3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bc3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bc3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bc3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bc3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bc3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bc3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bc3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bc3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bc3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bc3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bc3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bc3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bc3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bc3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bc40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bc408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bc40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bc411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bc41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bc41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bc41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bc42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bc427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bc42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bc430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bc43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bc43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bc43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bc44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bc446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bc44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bc44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bc45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bc458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bc45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bc46180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bc465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bc46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bc46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bc47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bc477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bc47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bc48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bc48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bc48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bc48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bc49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bc496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bc4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bc4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bc4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bc4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bc4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bc4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bc4c150 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.766s
user	0m0.286s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4232 (6acce397)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140f0b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140f0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140f0bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140f0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140f0cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140f0d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140f0d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140f0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140f0e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140f0e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140f0ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140f0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140f0ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140f10f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140f11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140f11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140f12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140f12c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140f13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140f13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140f14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140f15160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140f15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140f166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140f16be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140f16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140f17340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140f17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140f17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140f183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140f18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140f18b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140f18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140f19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140f19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140f19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140f1a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140f1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140f1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140f1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140f1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140f1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140f1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140f1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140f1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140f1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140f1e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140f1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140f1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140f20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140f20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140f20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140f21140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140f215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140f21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140f21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140f223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140f22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140f22d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140f231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140f23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140f23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140f24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140f248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140f24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140f256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140f25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140f25fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140f26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140f26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140f26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140f27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140f27700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140f27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140f28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140f284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140f28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140f28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140f292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140f29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140f29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140f2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140f2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140f2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140f2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140f2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140f2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140f2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140f2c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140f2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140f2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140f2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140f2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140f2d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140f2e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140f2e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140f2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140f2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140f2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140f2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140f2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140f30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140f30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140f30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140f31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140f315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140f31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140f31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140f323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140f32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140f32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140f331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140f33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140f33af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140f33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140f34430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140f348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140f34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140f35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140f356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140f35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140f35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140f36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140f36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140f36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140f37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140f37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140f37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140f384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140f38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140f38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140f392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140f39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140f39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140f3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140f3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140f3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140f3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140f3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140f3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140f3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140f3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140f3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140f3cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140f3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140f3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140f3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140f3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140f3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140f3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140f3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140f3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140f403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140f410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140f41640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140f41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140f420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140f42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140f42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140f430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140f43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140f43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140f440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140f44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140f44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140f450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140f45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140f45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140f460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140f465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140f46b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140f47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140f475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140f47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140f48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140f485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140f48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140f49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140f495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140f49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140f4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140f4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140f4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140f4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140f4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140f4baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140f4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140f4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140f4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140f4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140f4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140f4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140f4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140f4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140f4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140f4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140f4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140f4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140f50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140f50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140f50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140f50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140f51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140f51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140f51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140f52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140f52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140f52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140f53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140f539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140f53e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140f54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140f547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140f54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140f550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140f55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140f55a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140f56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140f56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140f56ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140f57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140f57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140f57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140f584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140f58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140f59310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140f595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140f59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140f5a1f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.093.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142004be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142005050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1420054c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142005930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142005da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142006210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142006680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142006af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142006f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1420073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142007840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142007f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142008a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1420091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1420099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14200a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14200a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14200af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14200b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14200be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14200c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14200cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14200d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14200dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14200e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14200e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14200e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14200ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14200f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14200f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14200f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14200fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1420102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142010570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1420109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142010e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1420112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142011ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142012010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1420128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142012d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1420131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142013640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142013ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142013f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142014390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142014800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1420150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142015550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1420159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142015e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1420162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142016710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142016c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142017180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1420175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142017a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142017ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142018340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1420187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142018c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142019090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142019500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142019970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142019de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14201a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14201a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14201ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14201afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14201b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14201b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14201bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14201c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14201c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14201ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14201ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14201d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14201d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14201dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14201e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14201e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14201e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14201edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14201f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14201f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14201fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14201ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1420203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142020860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142020cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142021140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1420215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142021a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142021e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142022300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142022770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142022be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142023050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1420234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142023930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1420253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1420272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1420284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1420291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14202a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14202a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14202ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14202b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14202b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14202b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14202be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14202c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14202c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14202cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14202d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14202d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14202d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14202dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14202e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14202e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14202eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14202ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14202f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14202f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14202fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1420300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1420309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1420312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1420328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1420331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1420347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142034c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1420350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1420360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142036380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142036640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142036ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142036f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142037390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142037800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142037c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1420380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142038550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1420389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142038e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1420392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142039710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142039b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142039ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14203a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14203a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14203ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14203b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14203b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14203ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14203bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14203c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14203c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14203cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14203d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14203d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14203d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14203de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14203e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14203e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14203eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14203efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14203f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14203f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14203fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142040190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142040600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142040a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142040ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142041350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1420417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142041c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1420420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142042510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142042980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142042df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142043260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1420436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142043b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142043fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142044420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142044890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142044d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142045170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1420455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142045a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142046330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1420467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142046c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1420474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142047960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142047dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142048240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1420486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142048b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142048f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142049400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142049f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14204a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14204ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14204b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14204b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14204ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14204be90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e0a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e0c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e0ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e0d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e0fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e10ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e17140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e17de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e19e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e1a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e21e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e22c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e23100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e23ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e25600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e26d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e27b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e28d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e29220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e2a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e2c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e2c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e2ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e3b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e3d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e3dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e40010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e46a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e48f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e4af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e4bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e4d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e4df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e4f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e4ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e51450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e52990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e52e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e57220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e58060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e58a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e59660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.934s
user	0m0.239s
sys	0m0.141s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.55 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.72 user         0.04 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.24 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.48 sec*proc (2 tests)

Total Test time (real) =   0.49 sec
        0.49 real         0.14 user         0.03 sys
```
