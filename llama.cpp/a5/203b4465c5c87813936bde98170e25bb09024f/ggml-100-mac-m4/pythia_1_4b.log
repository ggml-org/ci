Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.707s
user	0m0.895s
sys	0m1.274s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Built target llama-gguf
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Built target test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-barrier
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-autorelease
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-stats
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-cli
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-passkey
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-perplexity
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-quantize
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.075s
user	0m6.183s
sys	0m9.673s

main: quantize time =  3516.38 ms
main:    total time =  3516.38 ms

main: quantize time =  1582.16 ms
main:    total time =  1582.16 ms

main: quantize time =  1309.06 ms
main:    total time =  1309.06 ms

main: quantize time =  1417.65 ms
main:    total time =  1417.65 ms

main: quantize time =  2115.98 ms
main:    total time =  2115.98 ms

main: quantize time =  5868.49 ms
main:    total time =  5868.49 ms

main: quantize time =  5696.62 ms
main:    total time =  5696.62 ms

main: quantize time =  6845.22 ms
main:    total time =  6845.22 ms

main: quantize time =  6086.74 ms
main:    total time =  6086.74 ms

main: quantize time =  4362.92 ms
main:    total time =  4362.92 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.315 I main: llama backend init
0.00.000.322 I main: load the model and apply lora adapter, if any
0.00.053.568 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.829 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.856 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.866 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.086.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.231 I llama_model_loader: - type  f32:  194 tensors
0.00.086.231 I llama_model_loader: - type  f16:   98 tensors
0.00.086.232 I print_info: file format = GGUF V3 (latest)
0.00.086.242 I print_info: file type   = all F32 (guessed)
0.00.086.246 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.307 I load: special tokens cache size = 25
0.00.108.759 I load: token to piece cache size = 0.2984 MB
0.00.108.762 I print_info: arch             = gptneox
0.00.108.763 I print_info: vocab_only       = 0
0.00.108.763 I print_info: n_ctx_train      = 2048
0.00.108.763 I print_info: n_embd           = 2048
0.00.108.763 I print_info: n_layer          = 24
0.00.108.767 I print_info: n_head           = 16
0.00.108.768 I print_info: n_head_kv        = 16
0.00.108.768 I print_info: n_rot            = 32
0.00.108.768 I print_info: n_swa            = 0
0.00.108.769 I print_info: n_embd_head_k    = 128
0.00.108.769 I print_info: n_embd_head_v    = 128
0.00.108.769 I print_info: n_gqa            = 1
0.00.108.770 I print_info: n_embd_k_gqa     = 2048
0.00.108.771 I print_info: n_embd_v_gqa     = 2048
0.00.108.772 I print_info: f_norm_eps       = 1.0e-05
0.00.108.772 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.772 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.772 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.772 I print_info: f_logit_scale    = 0.0e+00
0.00.108.773 I print_info: n_ff             = 8192
0.00.108.773 I print_info: n_expert         = 0
0.00.108.774 I print_info: n_expert_used    = 0
0.00.108.774 I print_info: causal attn      = 1
0.00.108.774 I print_info: pooling type     = 0
0.00.108.774 I print_info: rope type        = 2
0.00.108.774 I print_info: rope scaling     = linear
0.00.108.777 I print_info: freq_base_train  = 10000.0
0.00.108.777 I print_info: freq_scale_train = 1
0.00.108.777 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.778 I print_info: rope_finetuned   = unknown
0.00.108.778 I print_info: ssm_d_conv       = 0
0.00.108.778 I print_info: ssm_d_inner      = 0
0.00.108.778 I print_info: ssm_d_state      = 0
0.00.108.778 I print_info: ssm_dt_rank      = 0
0.00.108.779 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.779 I print_info: model type       = 1.4B
0.00.108.781 I print_info: model params     = 1.41 B
0.00.108.781 I print_info: general.name     = 1.4B
0.00.108.781 I print_info: vocab type       = BPE
0.00.108.781 I print_info: n_vocab          = 50304
0.00.108.782 I print_info: n_merges         = 50009
0.00.108.782 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.782 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.782 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.783 I print_info: LF token         = 128 'Ä'
0.00.108.783 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.783 I print_info: max token length = 1024
0.00.144.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.144.481 I load_tensors: offloading output layer to GPU
0.00.144.481 I load_tensors: offloaded 25/25 layers to GPU
0.00.144.503 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.144.504 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.144.770 I llama_init_from_model: n_seq_max     = 1
0.00.144.772 I llama_init_from_model: n_ctx         = 2048
0.00.144.772 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.144.772 I llama_init_from_model: n_batch       = 2048
0.00.144.772 I llama_init_from_model: n_ubatch      = 512
0.00.144.772 I llama_init_from_model: flash_attn    = 0
0.00.144.773 I llama_init_from_model: freq_base     = 10000.0
0.00.144.773 I llama_init_from_model: freq_scale    = 1
0.00.144.774 I ggml_metal_init: allocating
0.00.144.791 I ggml_metal_init: found device: Apple M4
0.00.144.796 I ggml_metal_init: picking default device: Apple M4
0.00.145.358 I ggml_metal_init: using embedded metal library
0.00.156.601 I ggml_metal_init: GPU name:   Apple M4
0.00.156.603 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.603 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.604 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.604 I ggml_metal_init: simdgroup reduction   = true
0.00.156.604 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.604 I ggml_metal_init: has residency sets    = true
0.00.156.604 I ggml_metal_init: has bfloat            = true
0.00.156.604 I ggml_metal_init: use bfloat            = true
0.00.156.605 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.179.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.207.081 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.207.087 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.207.109 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.211.173 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.211.175 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.211.175 I llama_init_from_model: graph nodes  = 967
0.00.211.176 I llama_init_from_model: graph splits = 2
0.00.211.179 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.211.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.211.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.277.147 I main: llama threadpool init, n_threads = 4
0.00.277.191 I 
0.00.277.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.277.227 I 
0.00.277.272 I sampler seed: 1234
0.00.277.277 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.277.301 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.277.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.277.303 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.108.744 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.02.108.745 I llama_perf_context_print:        load time =     222.54 ms
0.02.108.745 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.26 tokens per second)
0.02.108.746 I llama_perf_context_print:        eval time =    1784.93 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.108.747 I llama_perf_context_print:       total time =    1832.63 ms /    70 tokens
0.02.109.015 I ggml_metal_free: deallocating

real	0m2.445s
user	0m0.131s
sys	0m0.129s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.447 I llama_model_loader: - type  f32:  194 tensors
0.00.036.447 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.448 I print_info: file format = GGUF V3 (latest)
0.00.036.449 I print_info: file type   = Q8_0
0.00.036.450 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.026 I load: special tokens cache size = 25
0.00.051.940 I load: token to piece cache size = 0.2984 MB
0.00.051.945 I print_info: arch             = gptneox
0.00.051.945 I print_info: vocab_only       = 0
0.00.051.945 I print_info: n_ctx_train      = 2048
0.00.051.945 I print_info: n_embd           = 2048
0.00.051.946 I print_info: n_layer          = 24
0.00.051.950 I print_info: n_head           = 16
0.00.051.951 I print_info: n_head_kv        = 16
0.00.051.951 I print_info: n_rot            = 32
0.00.051.951 I print_info: n_swa            = 0
0.00.051.951 I print_info: n_embd_head_k    = 128
0.00.051.951 I print_info: n_embd_head_v    = 128
0.00.051.952 I print_info: n_gqa            = 1
0.00.051.953 I print_info: n_embd_k_gqa     = 2048
0.00.051.953 I print_info: n_embd_v_gqa     = 2048
0.00.051.954 I print_info: f_norm_eps       = 1.0e-05
0.00.051.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.957 I print_info: f_logit_scale    = 0.0e+00
0.00.051.958 I print_info: n_ff             = 8192
0.00.051.959 I print_info: n_expert         = 0
0.00.051.959 I print_info: n_expert_used    = 0
0.00.051.959 I print_info: causal attn      = 1
0.00.051.959 I print_info: pooling type     = 0
0.00.051.960 I print_info: rope type        = 2
0.00.051.960 I print_info: rope scaling     = linear
0.00.051.961 I print_info: freq_base_train  = 10000.0
0.00.051.961 I print_info: freq_scale_train = 1
0.00.051.961 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.962 I print_info: rope_finetuned   = unknown
0.00.051.962 I print_info: ssm_d_conv       = 0
0.00.051.962 I print_info: ssm_d_inner      = 0
0.00.051.962 I print_info: ssm_d_state      = 0
0.00.051.962 I print_info: ssm_dt_rank      = 0
0.00.051.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.965 I print_info: model type       = 1.4B
0.00.051.965 I print_info: model params     = 1.41 B
0.00.051.965 I print_info: general.name     = 1.4B
0.00.051.966 I print_info: vocab type       = BPE
0.00.051.966 I print_info: n_vocab          = 50304
0.00.051.967 I print_info: n_merges         = 50009
0.00.051.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: LF token         = 128 'Ä'
0.00.051.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.974 I print_info: max token length = 1024
0.01.188.822 I load_tensors: offloading 24 repeating layers to GPU
0.01.188.827 I load_tensors: offloading output layer to GPU
0.01.188.828 I load_tensors: offloaded 25/25 layers to GPU
0.01.188.853 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.188.854 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.189.532 I llama_init_from_model: n_seq_max     = 1
0.01.189.534 I llama_init_from_model: n_ctx         = 2048
0.01.189.534 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.189.534 I llama_init_from_model: n_batch       = 2048
0.01.189.535 I llama_init_from_model: n_ubatch      = 512
0.01.189.536 I llama_init_from_model: flash_attn    = 0
0.01.189.536 I llama_init_from_model: freq_base     = 10000.0
0.01.189.537 I llama_init_from_model: freq_scale    = 1
0.01.189.538 I ggml_metal_init: allocating
0.01.189.554 I ggml_metal_init: found device: Apple M4
0.01.189.563 I ggml_metal_init: picking default device: Apple M4
0.01.190.880 I ggml_metal_init: using embedded metal library
0.01.196.412 I ggml_metal_init: GPU name:   Apple M4
0.01.196.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.196.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.196.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.196.417 I ggml_metal_init: simdgroup reduction   = true
0.01.196.417 I ggml_metal_init: simdgroup matrix mul. = true
0.01.196.418 I ggml_metal_init: has residency sets    = true
0.01.196.418 I ggml_metal_init: has bfloat            = true
0.01.196.418 I ggml_metal_init: use bfloat            = true
0.01.196.419 I ggml_metal_init: hasUnifiedMemory      = true
0.01.196.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.212.225 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.263.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.263.922 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.263.946 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.268.086 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.268.088 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.268.088 I llama_init_from_model: graph nodes  = 967
0.01.268.088 I llama_init_from_model: graph splits = 2
0.01.268.094 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.268.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.268.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.323.962 I main: llama threadpool init, n_threads = 4
0.01.324.000 I 
0.01.324.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.324.023 I 
0.01.324.174 I sampler seed: 1234
0.01.324.178 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.324.218 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.324.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.324.222 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.442.264 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48200.95 tokens per second)
0.02.442.264 I llama_perf_context_print:        load time =    1313.44 ms
0.02.442.265 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.26 tokens per second)
0.02.442.266 I llama_perf_context_print:        eval time =    1066.29 ms /    63 runs   (   16.93 ms per token,    59.08 tokens per second)
0.02.442.267 I llama_perf_context_print:       total time =    1119.16 ms /    70 tokens
0.02.442.531 I ggml_metal_free: deallocating

real	0m2.460s
user	0m0.110s
sys	0m0.262s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.017.256 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.186 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.191 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.196 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.829 I llama_model_loader: - type  f32:  194 tensors
0.00.042.830 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.831 I print_info: file format = GGUF V3 (latest)
0.00.042.831 I print_info: file type   = Q4_0
0.00.042.832 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.053.670 I load: special tokens cache size = 25
0.00.063.642 I load: token to piece cache size = 0.2984 MB
0.00.063.646 I print_info: arch             = gptneox
0.00.063.646 I print_info: vocab_only       = 0
0.00.063.647 I print_info: n_ctx_train      = 2048
0.00.063.647 I print_info: n_embd           = 2048
0.00.063.647 I print_info: n_layer          = 24
0.00.063.651 I print_info: n_head           = 16
0.00.063.652 I print_info: n_head_kv        = 16
0.00.063.652 I print_info: n_rot            = 32
0.00.063.652 I print_info: n_swa            = 0
0.00.063.653 I print_info: n_embd_head_k    = 128
0.00.063.655 I print_info: n_embd_head_v    = 128
0.00.063.656 I print_info: n_gqa            = 1
0.00.063.657 I print_info: n_embd_k_gqa     = 2048
0.00.063.658 I print_info: n_embd_v_gqa     = 2048
0.00.063.659 I print_info: f_norm_eps       = 1.0e-05
0.00.063.672 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.673 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.673 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.674 I print_info: f_logit_scale    = 0.0e+00
0.00.063.677 I print_info: n_ff             = 8192
0.00.063.677 I print_info: n_expert         = 0
0.00.063.677 I print_info: n_expert_used    = 0
0.00.063.678 I print_info: causal attn      = 1
0.00.063.678 I print_info: pooling type     = 0
0.00.063.680 I print_info: rope type        = 2
0.00.063.682 I print_info: rope scaling     = linear
0.00.063.682 I print_info: freq_base_train  = 10000.0
0.00.063.683 I print_info: freq_scale_train = 1
0.00.063.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.684 I print_info: rope_finetuned   = unknown
0.00.063.684 I print_info: ssm_d_conv       = 0
0.00.063.684 I print_info: ssm_d_inner      = 0
0.00.063.684 I print_info: ssm_d_state      = 0
0.00.063.684 I print_info: ssm_dt_rank      = 0
0.00.063.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.685 I print_info: model type       = 1.4B
0.00.063.686 I print_info: model params     = 1.41 B
0.00.063.686 I print_info: general.name     = 1.4B
0.00.063.687 I print_info: vocab type       = BPE
0.00.063.687 I print_info: n_vocab          = 50304
0.00.063.687 I print_info: n_merges         = 50009
0.00.063.692 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.693 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.693 I print_info: LF token         = 128 'Ä'
0.00.063.697 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.698 I print_info: max token length = 1024
0.00.958.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.958.053 I load_tensors: offloading output layer to GPU
0.00.958.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.958.085 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.958.086 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.959.146 I llama_init_from_model: n_seq_max     = 1
0.00.959.153 I llama_init_from_model: n_ctx         = 2048
0.00.959.154 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.959.154 I llama_init_from_model: n_batch       = 2048
0.00.959.154 I llama_init_from_model: n_ubatch      = 512
0.00.959.155 I llama_init_from_model: flash_attn    = 0
0.00.959.156 I llama_init_from_model: freq_base     = 10000.0
0.00.959.157 I llama_init_from_model: freq_scale    = 1
0.00.959.165 I ggml_metal_init: allocating
0.00.959.254 I ggml_metal_init: found device: Apple M4
0.00.959.268 I ggml_metal_init: picking default device: Apple M4
0.00.961.017 I ggml_metal_init: using embedded metal library
0.00.965.863 I ggml_metal_init: GPU name:   Apple M4
0.00.965.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.965.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.965.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.965.872 I ggml_metal_init: simdgroup reduction   = true
0.00.965.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.965.872 I ggml_metal_init: has residency sets    = true
0.00.965.873 I ggml_metal_init: has bfloat            = true
0.00.965.873 I ggml_metal_init: use bfloat            = true
0.00.965.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.965.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.983.877 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.040.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.040.829 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.040.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.045.088 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.045.090 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.045.091 I llama_init_from_model: graph nodes  = 967
0.01.045.091 I llama_init_from_model: graph splits = 2
0.01.045.096 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.045.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.045.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.099.153 I main: llama threadpool init, n_threads = 4
0.01.099.197 I 
0.01.099.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.099.222 I 
0.01.099.375 I sampler seed: 1234
0.01.099.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.099.391 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.099.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.099.391 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.789.269 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.789.269 I llama_perf_context_print:        load time =    1081.03 ms
0.01.789.270 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.00 tokens per second)
0.01.789.271 I llama_perf_context_print:        eval time =     638.00 ms /    63 runs   (   10.13 ms per token,    98.75 tokens per second)
0.01.789.272 I llama_perf_context_print:       total time =     690.98 ms /    70 tokens
0.01.789.466 I ggml_metal_free: deallocating

real	0m1.820s
user	0m0.119s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.065 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.490 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.258 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.022 I llama_model_loader: - type  f32:  194 tensors
0.00.026.023 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.024 I print_info: file format = GGUF V3 (latest)
0.00.026.024 I print_info: file type   = Q4_1
0.00.026.025 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.899 I load: special tokens cache size = 25
0.00.039.810 I load: token to piece cache size = 0.2984 MB
0.00.039.813 I print_info: arch             = gptneox
0.00.039.813 I print_info: vocab_only       = 0
0.00.039.813 I print_info: n_ctx_train      = 2048
0.00.039.813 I print_info: n_embd           = 2048
0.00.039.814 I print_info: n_layer          = 24
0.00.039.816 I print_info: n_head           = 16
0.00.039.817 I print_info: n_head_kv        = 16
0.00.039.817 I print_info: n_rot            = 32
0.00.039.818 I print_info: n_swa            = 0
0.00.039.818 I print_info: n_embd_head_k    = 128
0.00.039.818 I print_info: n_embd_head_v    = 128
0.00.039.819 I print_info: n_gqa            = 1
0.00.039.820 I print_info: n_embd_k_gqa     = 2048
0.00.039.820 I print_info: n_embd_v_gqa     = 2048
0.00.039.821 I print_info: f_norm_eps       = 1.0e-05
0.00.039.821 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.822 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.822 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.822 I print_info: f_logit_scale    = 0.0e+00
0.00.039.823 I print_info: n_ff             = 8192
0.00.039.823 I print_info: n_expert         = 0
0.00.039.823 I print_info: n_expert_used    = 0
0.00.039.823 I print_info: causal attn      = 1
0.00.039.823 I print_info: pooling type     = 0
0.00.039.825 I print_info: rope type        = 2
0.00.039.826 I print_info: rope scaling     = linear
0.00.039.827 I print_info: freq_base_train  = 10000.0
0.00.039.827 I print_info: freq_scale_train = 1
0.00.039.827 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.828 I print_info: rope_finetuned   = unknown
0.00.039.828 I print_info: ssm_d_conv       = 0
0.00.039.828 I print_info: ssm_d_inner      = 0
0.00.039.828 I print_info: ssm_d_state      = 0
0.00.039.828 I print_info: ssm_dt_rank      = 0
0.00.039.828 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.828 I print_info: model type       = 1.4B
0.00.039.829 I print_info: model params     = 1.41 B
0.00.039.829 I print_info: general.name     = 1.4B
0.00.039.830 I print_info: vocab type       = BPE
0.00.039.830 I print_info: n_vocab          = 50304
0.00.039.830 I print_info: n_merges         = 50009
0.00.039.830 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.831 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.831 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.831 I print_info: LF token         = 128 'Ä'
0.00.039.835 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.837 I print_info: max token length = 1024
0.00.627.197 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.210 I load_tensors: offloading output layer to GPU
0.00.627.211 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.243 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.244 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.628.705 I llama_init_from_model: n_seq_max     = 1
0.00.628.711 I llama_init_from_model: n_ctx         = 2048
0.00.628.711 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.711 I llama_init_from_model: n_batch       = 2048
0.00.628.712 I llama_init_from_model: n_ubatch      = 512
0.00.628.712 I llama_init_from_model: flash_attn    = 0
0.00.628.714 I llama_init_from_model: freq_base     = 10000.0
0.00.628.715 I llama_init_from_model: freq_scale    = 1
0.00.628.723 I ggml_metal_init: allocating
0.00.628.805 I ggml_metal_init: found device: Apple M4
0.00.628.820 I ggml_metal_init: picking default device: Apple M4
0.00.630.633 I ggml_metal_init: using embedded metal library
0.00.637.232 I ggml_metal_init: GPU name:   Apple M4
0.00.637.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.240 I ggml_metal_init: simdgroup reduction   = true
0.00.637.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.240 I ggml_metal_init: has residency sets    = true
0.00.637.241 I ggml_metal_init: has bfloat            = true
0.00.637.241 I ggml_metal_init: use bfloat            = true
0.00.637.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.804 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.486 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.493 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.447 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.450 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.451 I llama_init_from_model: graph nodes  = 967
0.00.714.451 I llama_init_from_model: graph splits = 2
0.00.714.458 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.849 I main: llama threadpool init, n_threads = 4
0.00.770.894 I 
0.00.770.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.921 I 
0.00.771.097 I sampler seed: 1234
0.00.771.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.113 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.113 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.508.393 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.508.393 I llama_perf_context_print:        load time =     759.90 ms
0.01.508.394 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.24 tokens per second)
0.01.508.395 I llama_perf_context_print:        eval time =     685.11 ms /    63 runs   (   10.87 ms per token,    91.96 tokens per second)
0.01.508.395 I llama_perf_context_print:       total time =     738.43 ms /    70 tokens
0.01.508.691 I ggml_metal_free: deallocating

real	0m1.527s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.687 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.687 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.563 I llama_model_loader: - type  f32:  194 tensors
0.00.028.563 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.563 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.564 I print_info: file format = GGUF V3 (latest)
0.00.028.565 I print_info: file type   = Q5_0
0.00.028.566 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.782 I load: special tokens cache size = 25
0.00.042.723 I load: token to piece cache size = 0.2984 MB
0.00.042.725 I print_info: arch             = gptneox
0.00.042.726 I print_info: vocab_only       = 0
0.00.042.726 I print_info: n_ctx_train      = 2048
0.00.042.726 I print_info: n_embd           = 2048
0.00.042.726 I print_info: n_layer          = 24
0.00.042.730 I print_info: n_head           = 16
0.00.042.731 I print_info: n_head_kv        = 16
0.00.042.731 I print_info: n_rot            = 32
0.00.042.731 I print_info: n_swa            = 0
0.00.042.731 I print_info: n_embd_head_k    = 128
0.00.042.731 I print_info: n_embd_head_v    = 128
0.00.042.732 I print_info: n_gqa            = 1
0.00.042.733 I print_info: n_embd_k_gqa     = 2048
0.00.042.734 I print_info: n_embd_v_gqa     = 2048
0.00.042.734 I print_info: f_norm_eps       = 1.0e-05
0.00.042.734 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.735 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.735 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.735 I print_info: f_logit_scale    = 0.0e+00
0.00.042.736 I print_info: n_ff             = 8192
0.00.042.736 I print_info: n_expert         = 0
0.00.042.736 I print_info: n_expert_used    = 0
0.00.042.736 I print_info: causal attn      = 1
0.00.042.736 I print_info: pooling type     = 0
0.00.042.738 I print_info: rope type        = 2
0.00.042.740 I print_info: rope scaling     = linear
0.00.042.740 I print_info: freq_base_train  = 10000.0
0.00.042.740 I print_info: freq_scale_train = 1
0.00.042.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.741 I print_info: rope_finetuned   = unknown
0.00.042.741 I print_info: ssm_d_conv       = 0
0.00.042.741 I print_info: ssm_d_inner      = 0
0.00.042.741 I print_info: ssm_d_state      = 0
0.00.042.741 I print_info: ssm_dt_rank      = 0
0.00.042.742 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.742 I print_info: model type       = 1.4B
0.00.042.742 I print_info: model params     = 1.41 B
0.00.042.742 I print_info: general.name     = 1.4B
0.00.042.743 I print_info: vocab type       = BPE
0.00.042.743 I print_info: n_vocab          = 50304
0.00.042.743 I print_info: n_merges         = 50009
0.00.042.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.744 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.744 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.744 I print_info: LF token         = 128 'Ä'
0.00.042.746 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.746 I print_info: max token length = 1024
0.00.634.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.121 I load_tensors: offloading output layer to GPU
0.00.634.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.164 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.634.168 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.635.532 I llama_init_from_model: n_seq_max     = 1
0.00.635.537 I llama_init_from_model: n_ctx         = 2048
0.00.635.538 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.538 I llama_init_from_model: n_batch       = 2048
0.00.635.539 I llama_init_from_model: n_ubatch      = 512
0.00.635.539 I llama_init_from_model: flash_attn    = 0
0.00.635.541 I llama_init_from_model: freq_base     = 10000.0
0.00.635.542 I llama_init_from_model: freq_scale    = 1
0.00.635.549 I ggml_metal_init: allocating
0.00.635.629 I ggml_metal_init: found device: Apple M4
0.00.635.643 I ggml_metal_init: picking default device: Apple M4
0.00.637.421 I ggml_metal_init: using embedded metal library
0.00.644.064 I ggml_metal_init: GPU name:   Apple M4
0.00.644.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.070 I ggml_metal_init: simdgroup reduction   = true
0.00.644.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.071 I ggml_metal_init: has residency sets    = true
0.00.644.071 I ggml_metal_init: has bfloat            = true
0.00.644.071 I ggml_metal_init: use bfloat            = true
0.00.644.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.669 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.990 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.997 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.718.018 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.681 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.683 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.684 I llama_init_from_model: graph nodes  = 967
0.00.722.684 I llama_init_from_model: graph splits = 2
0.00.722.690 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.336 I main: llama threadpool init, n_threads = 4
0.00.781.379 I 
0.00.781.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.406 I 
0.00.781.551 I sampler seed: 1234
0.00.781.556 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.600 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.604 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.604 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.581.587 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.581.588 I llama_perf_context_print:        load time =     768.30 ms
0.01.581.589 I llama_perf_context_print: prompt eval time =      53.21 ms /     7 tokens (    7.60 ms per token,   131.55 tokens per second)
0.01.581.591 I llama_perf_context_print:        eval time =     743.76 ms /    63 runs   (   11.81 ms per token,    84.70 tokens per second)
0.01.581.591 I llama_perf_context_print:       total time =     801.30 ms /    70 tokens
0.01.581.812 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.112s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.696 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.816 I llama_model_loader: - type  f32:  194 tensors
0.00.024.816 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.817 I print_info: file format = GGUF V3 (latest)
0.00.024.818 I print_info: file type   = Q5_1
0.00.024.818 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.647 I load: special tokens cache size = 25
0.00.038.627 I load: token to piece cache size = 0.2984 MB
0.00.038.630 I print_info: arch             = gptneox
0.00.038.630 I print_info: vocab_only       = 0
0.00.038.630 I print_info: n_ctx_train      = 2048
0.00.038.631 I print_info: n_embd           = 2048
0.00.038.631 I print_info: n_layer          = 24
0.00.038.634 I print_info: n_head           = 16
0.00.038.635 I print_info: n_head_kv        = 16
0.00.038.635 I print_info: n_rot            = 32
0.00.038.635 I print_info: n_swa            = 0
0.00.038.635 I print_info: n_embd_head_k    = 128
0.00.038.635 I print_info: n_embd_head_v    = 128
0.00.038.636 I print_info: n_gqa            = 1
0.00.038.637 I print_info: n_embd_k_gqa     = 2048
0.00.038.638 I print_info: n_embd_v_gqa     = 2048
0.00.038.638 I print_info: f_norm_eps       = 1.0e-05
0.00.038.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.639 I print_info: f_logit_scale    = 0.0e+00
0.00.038.640 I print_info: n_ff             = 8192
0.00.038.640 I print_info: n_expert         = 0
0.00.038.640 I print_info: n_expert_used    = 0
0.00.038.640 I print_info: causal attn      = 1
0.00.038.641 I print_info: pooling type     = 0
0.00.038.642 I print_info: rope type        = 2
0.00.038.644 I print_info: rope scaling     = linear
0.00.038.644 I print_info: freq_base_train  = 10000.0
0.00.038.644 I print_info: freq_scale_train = 1
0.00.038.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.645 I print_info: rope_finetuned   = unknown
0.00.038.645 I print_info: ssm_d_conv       = 0
0.00.038.645 I print_info: ssm_d_inner      = 0
0.00.038.645 I print_info: ssm_d_state      = 0
0.00.038.645 I print_info: ssm_dt_rank      = 0
0.00.038.645 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.646 I print_info: model type       = 1.4B
0.00.038.646 I print_info: model params     = 1.41 B
0.00.038.646 I print_info: general.name     = 1.4B
0.00.038.647 I print_info: vocab type       = BPE
0.00.038.647 I print_info: n_vocab          = 50304
0.00.038.647 I print_info: n_merges         = 50009
0.00.038.647 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: LF token         = 128 'Ä'
0.00.038.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: max token length = 1024
0.00.598.396 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.412 I load_tensors: offloading output layer to GPU
0.00.598.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.445 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.598.447 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.600.018 I llama_init_from_model: n_seq_max     = 1
0.00.600.021 I llama_init_from_model: n_ctx         = 2048
0.00.600.022 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.600.022 I llama_init_from_model: n_batch       = 2048
0.00.600.023 I llama_init_from_model: n_ubatch      = 512
0.00.600.023 I llama_init_from_model: flash_attn    = 0
0.00.600.024 I llama_init_from_model: freq_base     = 10000.0
0.00.600.025 I llama_init_from_model: freq_scale    = 1
0.00.600.030 I ggml_metal_init: allocating
0.00.600.057 I ggml_metal_init: found device: Apple M4
0.00.600.066 I ggml_metal_init: picking default device: Apple M4
0.00.601.553 I ggml_metal_init: using embedded metal library
0.00.607.875 I ggml_metal_init: GPU name:   Apple M4
0.00.607.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.882 I ggml_metal_init: simdgroup reduction   = true
0.00.607.882 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.882 I ggml_metal_init: has residency sets    = true
0.00.607.883 I ggml_metal_init: has bfloat            = true
0.00.607.883 I ggml_metal_init: use bfloat            = true
0.00.607.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.341 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.064 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.093 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.364 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.366 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.366 I llama_init_from_model: graph nodes  = 967
0.00.688.366 I llama_init_from_model: graph splits = 2
0.00.688.372 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.522 I main: llama threadpool init, n_threads = 4
0.00.746.572 I 
0.00.746.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.596 I 
0.00.746.764 I sampler seed: 1234
0.00.746.768 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.790 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.790 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.790 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.596.651 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.596.652 I llama_perf_context_print:        load time =     736.96 ms
0.01.596.653 I llama_perf_context_print: prompt eval time =      51.75 ms /     7 tokens (    7.39 ms per token,   135.26 tokens per second)
0.01.596.653 I llama_perf_context_print:        eval time =     795.06 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.596.654 I llama_perf_context_print:       total time =     850.99 ms /    70 tokens
0.01.596.904 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.078 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.079 I llama_model_loader: - type  f32:  194 tensors
0.00.025.080 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.080 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.080 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.081 I print_info: file format = GGUF V3 (latest)
0.00.025.081 I print_info: file type   = Q2_K - Medium
0.00.025.082 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.929 I load: special tokens cache size = 25
0.00.038.624 I load: token to piece cache size = 0.2984 MB
0.00.038.627 I print_info: arch             = gptneox
0.00.038.627 I print_info: vocab_only       = 0
0.00.038.627 I print_info: n_ctx_train      = 2048
0.00.038.627 I print_info: n_embd           = 2048
0.00.038.627 I print_info: n_layer          = 24
0.00.038.630 I print_info: n_head           = 16
0.00.038.631 I print_info: n_head_kv        = 16
0.00.038.631 I print_info: n_rot            = 32
0.00.038.632 I print_info: n_swa            = 0
0.00.038.632 I print_info: n_embd_head_k    = 128
0.00.038.632 I print_info: n_embd_head_v    = 128
0.00.038.633 I print_info: n_gqa            = 1
0.00.038.633 I print_info: n_embd_k_gqa     = 2048
0.00.038.634 I print_info: n_embd_v_gqa     = 2048
0.00.038.635 I print_info: f_norm_eps       = 1.0e-05
0.00.038.635 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.635 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.635 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.636 I print_info: f_logit_scale    = 0.0e+00
0.00.038.636 I print_info: n_ff             = 8192
0.00.038.637 I print_info: n_expert         = 0
0.00.038.637 I print_info: n_expert_used    = 0
0.00.038.637 I print_info: causal attn      = 1
0.00.038.637 I print_info: pooling type     = 0
0.00.038.637 I print_info: rope type        = 2
0.00.038.640 I print_info: rope scaling     = linear
0.00.038.641 I print_info: freq_base_train  = 10000.0
0.00.038.641 I print_info: freq_scale_train = 1
0.00.038.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.641 I print_info: rope_finetuned   = unknown
0.00.038.641 I print_info: ssm_d_conv       = 0
0.00.038.643 I print_info: ssm_d_inner      = 0
0.00.038.643 I print_info: ssm_d_state      = 0
0.00.038.643 I print_info: ssm_dt_rank      = 0
0.00.038.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.643 I print_info: model type       = 1.4B
0.00.038.644 I print_info: model params     = 1.41 B
0.00.038.644 I print_info: general.name     = 1.4B
0.00.038.644 I print_info: vocab type       = BPE
0.00.038.645 I print_info: n_vocab          = 50304
0.00.038.645 I print_info: n_merges         = 50009
0.00.038.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.647 I print_info: LF token         = 128 'Ä'
0.00.038.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: max token length = 1024
0.00.341.524 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.540 I load_tensors: offloading output layer to GPU
0.00.341.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.575 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.576 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.254 I llama_init_from_model: n_seq_max     = 1
0.00.343.258 I llama_init_from_model: n_ctx         = 2048
0.00.343.259 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.343.259 I llama_init_from_model: n_batch       = 2048
0.00.343.260 I llama_init_from_model: n_ubatch      = 512
0.00.343.260 I llama_init_from_model: flash_attn    = 0
0.00.343.262 I llama_init_from_model: freq_base     = 10000.0
0.00.343.266 I llama_init_from_model: freq_scale    = 1
0.00.343.273 I ggml_metal_init: allocating
0.00.343.382 I ggml_metal_init: found device: Apple M4
0.00.343.398 I ggml_metal_init: picking default device: Apple M4
0.00.345.256 I ggml_metal_init: using embedded metal library
0.00.350.859 I ggml_metal_init: GPU name:   Apple M4
0.00.350.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.880 I ggml_metal_init: simdgroup reduction   = true
0.00.350.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.881 I ggml_metal_init: has residency sets    = true
0.00.350.881 I ggml_metal_init: has bfloat            = true
0.00.350.881 I ggml_metal_init: use bfloat            = true
0.00.350.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.999 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.433.653 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.664 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.698 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.149 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.438.151 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.438.152 I llama_init_from_model: graph nodes  = 967
0.00.438.152 I llama_init_from_model: graph splits = 2
0.00.438.157 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.492 I main: llama threadpool init, n_threads = 4
0.00.497.536 I 
0.00.497.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.561 I 
0.00.497.734 I sampler seed: 1234
0.00.497.738 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.497.750 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.497.750 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.497.750 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.057 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51079.14 tokens per second)
0.01.172.057 I llama_perf_context_print:        load time =     486.76 ms
0.01.172.058 I llama_perf_context_print: prompt eval time =      35.53 ms /     7 tokens (    5.08 ms per token,   196.99 tokens per second)
0.01.172.059 I llama_perf_context_print:        eval time =     635.86 ms /    63 runs   (   10.09 ms per token,    99.08 tokens per second)
0.01.172.059 I llama_perf_context_print:       total time =     675.45 ms /    70 tokens
0.01.172.285 I ggml_metal_free: deallocating

real	0m1.189s
user	0m0.111s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.506 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.019 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.028 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.576 I llama_model_loader: - type  f32:  194 tensors
0.00.026.576 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.577 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.577 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.578 I print_info: file format = GGUF V3 (latest)
0.00.026.578 I print_info: file type   = Q3_K - Medium
0.00.026.579 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.480 I load: special tokens cache size = 25
0.00.040.193 I load: token to piece cache size = 0.2984 MB
0.00.040.195 I print_info: arch             = gptneox
0.00.040.196 I print_info: vocab_only       = 0
0.00.040.196 I print_info: n_ctx_train      = 2048
0.00.040.196 I print_info: n_embd           = 2048
0.00.040.196 I print_info: n_layer          = 24
0.00.040.198 I print_info: n_head           = 16
0.00.040.199 I print_info: n_head_kv        = 16
0.00.040.199 I print_info: n_rot            = 32
0.00.040.200 I print_info: n_swa            = 0
0.00.040.200 I print_info: n_embd_head_k    = 128
0.00.040.200 I print_info: n_embd_head_v    = 128
0.00.040.201 I print_info: n_gqa            = 1
0.00.040.202 I print_info: n_embd_k_gqa     = 2048
0.00.040.202 I print_info: n_embd_v_gqa     = 2048
0.00.040.203 I print_info: f_norm_eps       = 1.0e-05
0.00.040.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.204 I print_info: f_logit_scale    = 0.0e+00
0.00.040.205 I print_info: n_ff             = 8192
0.00.040.205 I print_info: n_expert         = 0
0.00.040.205 I print_info: n_expert_used    = 0
0.00.040.205 I print_info: causal attn      = 1
0.00.040.205 I print_info: pooling type     = 0
0.00.040.205 I print_info: rope type        = 2
0.00.040.206 I print_info: rope scaling     = linear
0.00.040.206 I print_info: freq_base_train  = 10000.0
0.00.040.206 I print_info: freq_scale_train = 1
0.00.040.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.207 I print_info: rope_finetuned   = unknown
0.00.040.207 I print_info: ssm_d_conv       = 0
0.00.040.207 I print_info: ssm_d_inner      = 0
0.00.040.207 I print_info: ssm_d_state      = 0
0.00.040.207 I print_info: ssm_dt_rank      = 0
0.00.040.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.210 I print_info: model type       = 1.4B
0.00.040.210 I print_info: model params     = 1.41 B
0.00.040.210 I print_info: general.name     = 1.4B
0.00.040.210 I print_info: vocab type       = BPE
0.00.040.211 I print_info: n_vocab          = 50304
0.00.040.211 I print_info: n_merges         = 50009
0.00.040.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: LF token         = 128 'Ä'
0.00.040.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: max token length = 1024
0.00.436.823 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.835 I load_tensors: offloading output layer to GPU
0.00.436.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.868 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.870 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.438.203 I llama_init_from_model: n_seq_max     = 1
0.00.438.207 I llama_init_from_model: n_ctx         = 2048
0.00.438.208 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.438.208 I llama_init_from_model: n_batch       = 2048
0.00.438.209 I llama_init_from_model: n_ubatch      = 512
0.00.438.209 I llama_init_from_model: flash_attn    = 0
0.00.438.211 I llama_init_from_model: freq_base     = 10000.0
0.00.438.212 I llama_init_from_model: freq_scale    = 1
0.00.438.214 I ggml_metal_init: allocating
0.00.438.272 I ggml_metal_init: found device: Apple M4
0.00.438.285 I ggml_metal_init: picking default device: Apple M4
0.00.439.999 I ggml_metal_init: using embedded metal library
0.00.445.505 I ggml_metal_init: GPU name:   Apple M4
0.00.445.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.512 I ggml_metal_init: simdgroup reduction   = true
0.00.445.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.513 I ggml_metal_init: has residency sets    = true
0.00.445.513 I ggml_metal_init: has bfloat            = true
0.00.445.514 I ggml_metal_init: use bfloat            = true
0.00.445.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.543 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.641 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.522.648 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.671 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.527.113 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.527.115 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.527.115 I llama_init_from_model: graph nodes  = 967
0.00.527.115 I llama_init_from_model: graph splits = 2
0.00.527.121 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.527.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.527.256 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.664 I main: llama threadpool init, n_threads = 4
0.00.585.706 I 
0.00.585.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.730 I 
0.00.585.878 I sampler seed: 1234
0.00.585.883 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.585.894 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.585.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.585.894 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.335.210 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.335.211 I llama_perf_context_print:        load time =     574.27 ms
0.01.335.211 I llama_perf_context_print: prompt eval time =      50.00 ms /     7 tokens (    7.14 ms per token,   139.99 tokens per second)
0.01.335.213 I llama_perf_context_print:        eval time =     696.34 ms /    63 runs   (   11.05 ms per token,    90.47 tokens per second)
0.01.335.213 I llama_perf_context_print:       total time =     750.43 ms /    70 tokens
0.01.335.442 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.110s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.576 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.576 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.579 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.579 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.335 I llama_model_loader: - type  f32:  194 tensors
0.00.025.335 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.335 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.336 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.336 I print_info: file format = GGUF V3 (latest)
0.00.025.337 I print_info: file type   = Q4_K - Medium
0.00.025.341 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.237 I load: special tokens cache size = 25
0.00.039.169 I load: token to piece cache size = 0.2984 MB
0.00.039.171 I print_info: arch             = gptneox
0.00.039.172 I print_info: vocab_only       = 0
0.00.039.172 I print_info: n_ctx_train      = 2048
0.00.039.172 I print_info: n_embd           = 2048
0.00.039.172 I print_info: n_layer          = 24
0.00.039.175 I print_info: n_head           = 16
0.00.039.176 I print_info: n_head_kv        = 16
0.00.039.176 I print_info: n_rot            = 32
0.00.039.176 I print_info: n_swa            = 0
0.00.039.176 I print_info: n_embd_head_k    = 128
0.00.039.177 I print_info: n_embd_head_v    = 128
0.00.039.178 I print_info: n_gqa            = 1
0.00.039.179 I print_info: n_embd_k_gqa     = 2048
0.00.039.179 I print_info: n_embd_v_gqa     = 2048
0.00.039.180 I print_info: f_norm_eps       = 1.0e-05
0.00.039.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.181 I print_info: f_logit_scale    = 0.0e+00
0.00.039.182 I print_info: n_ff             = 8192
0.00.039.182 I print_info: n_expert         = 0
0.00.039.182 I print_info: n_expert_used    = 0
0.00.039.182 I print_info: causal attn      = 1
0.00.039.184 I print_info: pooling type     = 0
0.00.039.185 I print_info: rope type        = 2
0.00.039.185 I print_info: rope scaling     = linear
0.00.039.186 I print_info: freq_base_train  = 10000.0
0.00.039.186 I print_info: freq_scale_train = 1
0.00.039.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.187 I print_info: rope_finetuned   = unknown
0.00.039.187 I print_info: ssm_d_conv       = 0
0.00.039.187 I print_info: ssm_d_inner      = 0
0.00.039.187 I print_info: ssm_d_state      = 0
0.00.039.187 I print_info: ssm_dt_rank      = 0
0.00.039.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.188 I print_info: model type       = 1.4B
0.00.039.188 I print_info: model params     = 1.41 B
0.00.039.188 I print_info: general.name     = 1.4B
0.00.039.189 I print_info: vocab type       = BPE
0.00.039.189 I print_info: n_vocab          = 50304
0.00.039.189 I print_info: n_merges         = 50009
0.00.039.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.190 I print_info: LF token         = 128 'Ä'
0.00.039.192 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.192 I print_info: max token length = 1024
0.00.516.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.582 I load_tensors: offloading output layer to GPU
0.00.516.583 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.617 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.619 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.517.968 I llama_init_from_model: n_seq_max     = 1
0.00.517.973 I llama_init_from_model: n_ctx         = 2048
0.00.517.974 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.517.974 I llama_init_from_model: n_batch       = 2048
0.00.517.974 I llama_init_from_model: n_ubatch      = 512
0.00.517.975 I llama_init_from_model: flash_attn    = 0
0.00.517.989 I llama_init_from_model: freq_base     = 10000.0
0.00.517.994 I llama_init_from_model: freq_scale    = 1
0.00.517.998 I ggml_metal_init: allocating
0.00.518.074 I ggml_metal_init: found device: Apple M4
0.00.518.088 I ggml_metal_init: picking default device: Apple M4
0.00.519.949 I ggml_metal_init: using embedded metal library
0.00.526.439 I ggml_metal_init: GPU name:   Apple M4
0.00.526.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.446 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.447 I ggml_metal_init: simdgroup reduction   = true
0.00.526.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.447 I ggml_metal_init: has residency sets    = true
0.00.526.448 I ggml_metal_init: has bfloat            = true
0.00.526.448 I ggml_metal_init: use bfloat            = true
0.00.526.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.297 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.600.891 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.600.915 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.841 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.604.842 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.604.843 I llama_init_from_model: graph nodes  = 967
0.00.604.843 I llama_init_from_model: graph splits = 2
0.00.604.848 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.604.964 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.604.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.749 I main: llama threadpool init, n_threads = 4
0.00.665.792 I 
0.00.665.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.817 I 
0.00.665.963 I sampler seed: 1234
0.00.665.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.978 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.978 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.979 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.430.774 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.430.775 I llama_perf_context_print:        load time =     656.18 ms
0.01.430.775 I llama_perf_context_print: prompt eval time =      57.57 ms /     7 tokens (    8.22 ms per token,   121.59 tokens per second)
0.01.430.776 I llama_perf_context_print:        eval time =     704.44 ms /    63 runs   (   11.18 ms per token,    89.43 tokens per second)
0.01.430.777 I llama_perf_context_print:       total time =     765.88 ms /    70 tokens
0.01.431.032 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.396 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.145 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.146 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.998 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.773 I llama_model_loader: - type  f32:  194 tensors
0.00.026.774 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.774 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.774 I print_info: file format = GGUF V3 (latest)
0.00.026.775 I print_info: file type   = Q5_K - Medium
0.00.026.775 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.152 I load: special tokens cache size = 25
0.00.040.898 I load: token to piece cache size = 0.2984 MB
0.00.040.901 I print_info: arch             = gptneox
0.00.040.901 I print_info: vocab_only       = 0
0.00.040.901 I print_info: n_ctx_train      = 2048
0.00.040.901 I print_info: n_embd           = 2048
0.00.040.901 I print_info: n_layer          = 24
0.00.040.904 I print_info: n_head           = 16
0.00.040.905 I print_info: n_head_kv        = 16
0.00.040.905 I print_info: n_rot            = 32
0.00.040.906 I print_info: n_swa            = 0
0.00.040.906 I print_info: n_embd_head_k    = 128
0.00.040.906 I print_info: n_embd_head_v    = 128
0.00.040.907 I print_info: n_gqa            = 1
0.00.040.907 I print_info: n_embd_k_gqa     = 2048
0.00.040.908 I print_info: n_embd_v_gqa     = 2048
0.00.040.909 I print_info: f_norm_eps       = 1.0e-05
0.00.040.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.909 I print_info: f_logit_scale    = 0.0e+00
0.00.040.910 I print_info: n_ff             = 8192
0.00.040.910 I print_info: n_expert         = 0
0.00.040.910 I print_info: n_expert_used    = 0
0.00.040.911 I print_info: causal attn      = 1
0.00.040.911 I print_info: pooling type     = 0
0.00.040.914 I print_info: rope type        = 2
0.00.040.916 I print_info: rope scaling     = linear
0.00.040.916 I print_info: freq_base_train  = 10000.0
0.00.040.916 I print_info: freq_scale_train = 1
0.00.040.917 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.917 I print_info: rope_finetuned   = unknown
0.00.040.917 I print_info: ssm_d_conv       = 0
0.00.040.917 I print_info: ssm_d_inner      = 0
0.00.040.917 I print_info: ssm_d_state      = 0
0.00.040.917 I print_info: ssm_dt_rank      = 0
0.00.040.917 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.918 I print_info: model type       = 1.4B
0.00.040.918 I print_info: model params     = 1.41 B
0.00.040.918 I print_info: general.name     = 1.4B
0.00.040.919 I print_info: vocab type       = BPE
0.00.040.919 I print_info: n_vocab          = 50304
0.00.040.919 I print_info: n_merges         = 50009
0.00.040.919 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.920 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.920 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.922 I print_info: LF token         = 128 'Ä'
0.00.040.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.923 I print_info: max token length = 1024
0.00.600.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.293 I load_tensors: offloading output layer to GPU
0.00.600.293 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.315 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.317 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.601.101 I llama_init_from_model: n_seq_max     = 1
0.00.601.104 I llama_init_from_model: n_ctx         = 2048
0.00.601.105 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.105 I llama_init_from_model: n_batch       = 2048
0.00.601.106 I llama_init_from_model: n_ubatch      = 512
0.00.601.106 I llama_init_from_model: flash_attn    = 0
0.00.601.107 I llama_init_from_model: freq_base     = 10000.0
0.00.601.108 I llama_init_from_model: freq_scale    = 1
0.00.601.111 I ggml_metal_init: allocating
0.00.601.147 I ggml_metal_init: found device: Apple M4
0.00.601.159 I ggml_metal_init: picking default device: Apple M4
0.00.602.175 I ggml_metal_init: using embedded metal library
0.00.606.451 I ggml_metal_init: GPU name:   Apple M4
0.00.606.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.460 I ggml_metal_init: simdgroup reduction   = true
0.00.606.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.460 I ggml_metal_init: has residency sets    = true
0.00.606.461 I ggml_metal_init: has bfloat            = true
0.00.606.461 I ggml_metal_init: use bfloat            = true
0.00.606.462 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.460 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.864 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.649.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.649.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.936 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.653.937 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.653.937 I llama_init_from_model: graph nodes  = 967
0.00.653.938 I llama_init_from_model: graph splits = 2
0.00.653.943 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.654.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.654.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.884 I main: llama threadpool init, n_threads = 4
0.00.713.918 I 
0.00.713.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.942 I 
0.00.714.085 I sampler seed: 1234
0.00.714.090 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.133 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.136 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.136 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.562.757 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48663.47 tokens per second)
0.01.562.758 I llama_perf_context_print:        load time =     702.60 ms
0.01.562.759 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.64 tokens per second)
0.01.562.761 I llama_perf_context_print:        eval time =     794.56 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.562.761 I llama_perf_context_print:       total time =     849.76 ms /    70 tokens
0.01.562.990 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.102s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.167 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.011 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.777 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.779 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.780 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.780 I llama_model_loader: - type  f32:  194 tensors
0.00.024.781 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.781 I print_info: file format = GGUF V3 (latest)
0.00.024.782 I print_info: file type   = Q6_K
0.00.024.782 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.123 I load: special tokens cache size = 25
0.00.039.198 I load: token to piece cache size = 0.2984 MB
0.00.039.203 I print_info: arch             = gptneox
0.00.039.204 I print_info: vocab_only       = 0
0.00.039.204 I print_info: n_ctx_train      = 2048
0.00.039.204 I print_info: n_embd           = 2048
0.00.039.204 I print_info: n_layer          = 24
0.00.039.208 I print_info: n_head           = 16
0.00.039.209 I print_info: n_head_kv        = 16
0.00.039.209 I print_info: n_rot            = 32
0.00.039.210 I print_info: n_swa            = 0
0.00.039.210 I print_info: n_embd_head_k    = 128
0.00.039.210 I print_info: n_embd_head_v    = 128
0.00.039.211 I print_info: n_gqa            = 1
0.00.039.212 I print_info: n_embd_k_gqa     = 2048
0.00.039.212 I print_info: n_embd_v_gqa     = 2048
0.00.039.213 I print_info: f_norm_eps       = 1.0e-05
0.00.039.213 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.213 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.213 I print_info: f_logit_scale    = 0.0e+00
0.00.039.214 I print_info: n_ff             = 8192
0.00.039.214 I print_info: n_expert         = 0
0.00.039.214 I print_info: n_expert_used    = 0
0.00.039.214 I print_info: causal attn      = 1
0.00.039.214 I print_info: pooling type     = 0
0.00.039.215 I print_info: rope type        = 2
0.00.039.217 I print_info: rope scaling     = linear
0.00.039.219 I print_info: freq_base_train  = 10000.0
0.00.039.219 I print_info: freq_scale_train = 1
0.00.039.219 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.219 I print_info: rope_finetuned   = unknown
0.00.039.219 I print_info: ssm_d_conv       = 0
0.00.039.220 I print_info: ssm_d_inner      = 0
0.00.039.220 I print_info: ssm_d_state      = 0
0.00.039.220 I print_info: ssm_dt_rank      = 0
0.00.039.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.220 I print_info: model type       = 1.4B
0.00.039.220 I print_info: model params     = 1.41 B
0.00.039.221 I print_info: general.name     = 1.4B
0.00.039.221 I print_info: vocab type       = BPE
0.00.039.221 I print_info: n_vocab          = 50304
0.00.039.221 I print_info: n_merges         = 50009
0.00.039.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: LF token         = 128 'Ä'
0.00.039.224 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.224 I print_info: max token length = 1024
0.00.653.540 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.546 I load_tensors: offloading output layer to GPU
0.00.653.547 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.567 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.653.568 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.654.356 I llama_init_from_model: n_seq_max     = 1
0.00.654.361 I llama_init_from_model: n_ctx         = 2048
0.00.654.361 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.362 I llama_init_from_model: n_batch       = 2048
0.00.654.362 I llama_init_from_model: n_ubatch      = 512
0.00.654.362 I llama_init_from_model: flash_attn    = 0
0.00.654.363 I llama_init_from_model: freq_base     = 10000.0
0.00.654.364 I llama_init_from_model: freq_scale    = 1
0.00.654.365 I ggml_metal_init: allocating
0.00.654.426 I ggml_metal_init: found device: Apple M4
0.00.654.442 I ggml_metal_init: picking default device: Apple M4
0.00.655.499 I ggml_metal_init: using embedded metal library
0.00.659.733 I ggml_metal_init: GPU name:   Apple M4
0.00.659.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.740 I ggml_metal_init: simdgroup reduction   = true
0.00.659.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.741 I ggml_metal_init: has residency sets    = true
0.00.659.741 I ggml_metal_init: has bfloat            = true
0.00.659.742 I ggml_metal_init: use bfloat            = true
0.00.659.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.268 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.779 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.705.788 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.812 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.710.220 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.710.221 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.710.222 I llama_init_from_model: graph nodes  = 967
0.00.710.222 I llama_init_from_model: graph splits = 2
0.00.710.228 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.710.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.710.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.723 I main: llama threadpool init, n_threads = 4
0.00.780.761 I 
0.00.780.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.784 I 
0.00.780.955 I sampler seed: 1234
0.00.780.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.971 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.971 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.971 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.666.227 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.666.228 I llama_perf_context_print:        load time =     770.64 ms
0.01.666.228 I llama_perf_context_print: prompt eval time =      54.02 ms /     7 tokens (    7.72 ms per token,   129.58 tokens per second)
0.01.666.229 I llama_perf_context_print:        eval time =     828.34 ms /    63 runs   (   13.15 ms per token,    76.06 tokens per second)
0.01.666.229 I llama_perf_context_print:       total time =     886.41 ms /    70 tokens
0.01.666.516 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.103s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.708 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.030.492 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.318 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.185 I llama_model_loader: - type  f32:  194 tensors
0.00.064.186 I llama_model_loader: - type  f16:   98 tensors
0.00.064.195 I print_info: file format = GGUF V3 (latest)
0.00.064.197 I print_info: file type   = all F32 (guessed)
0.00.064.200 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.383 I load: special tokens cache size = 25
0.00.090.863 I load: token to piece cache size = 0.2984 MB
0.00.090.867 I print_info: arch             = gptneox
0.00.090.867 I print_info: vocab_only       = 0
0.00.090.867 I print_info: n_ctx_train      = 2048
0.00.090.868 I print_info: n_embd           = 2048
0.00.090.868 I print_info: n_layer          = 24
0.00.090.871 I print_info: n_head           = 16
0.00.090.872 I print_info: n_head_kv        = 16
0.00.090.872 I print_info: n_rot            = 32
0.00.090.873 I print_info: n_swa            = 0
0.00.090.873 I print_info: n_embd_head_k    = 128
0.00.090.873 I print_info: n_embd_head_v    = 128
0.00.090.874 I print_info: n_gqa            = 1
0.00.090.875 I print_info: n_embd_k_gqa     = 2048
0.00.090.876 I print_info: n_embd_v_gqa     = 2048
0.00.090.878 I print_info: f_norm_eps       = 1.0e-05
0.00.090.878 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.878 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.878 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.879 I print_info: f_logit_scale    = 0.0e+00
0.00.090.881 I print_info: n_ff             = 8192
0.00.090.882 I print_info: n_expert         = 0
0.00.090.882 I print_info: n_expert_used    = 0
0.00.090.882 I print_info: causal attn      = 1
0.00.090.882 I print_info: pooling type     = 0
0.00.090.882 I print_info: rope type        = 2
0.00.090.883 I print_info: rope scaling     = linear
0.00.090.883 I print_info: freq_base_train  = 10000.0
0.00.090.883 I print_info: freq_scale_train = 1
0.00.090.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.884 I print_info: rope_finetuned   = unknown
0.00.090.884 I print_info: ssm_d_conv       = 0
0.00.090.884 I print_info: ssm_d_inner      = 0
0.00.090.884 I print_info: ssm_d_state      = 0
0.00.090.884 I print_info: ssm_dt_rank      = 0
0.00.090.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.885 I print_info: model type       = 1.4B
0.00.090.885 I print_info: model params     = 1.41 B
0.00.090.886 I print_info: general.name     = 1.4B
0.00.090.886 I print_info: vocab type       = BPE
0.00.090.886 I print_info: n_vocab          = 50304
0.00.090.886 I print_info: n_merges         = 50009
0.00.090.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.888 I print_info: LF token         = 128 'Ä'
0.00.090.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.889 I print_info: max token length = 1024
0.01.266.323 I load_tensors: offloading 24 repeating layers to GPU
0.01.266.326 I load_tensors: offloading output layer to GPU
0.01.266.327 I load_tensors: offloaded 25/25 layers to GPU
0.01.266.360 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.266.362 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.267.054 I llama_init_from_model: n_seq_max     = 1
0.01.267.055 I llama_init_from_model: n_ctx         = 128
0.01.267.056 I llama_init_from_model: n_ctx_per_seq = 128
0.01.267.056 I llama_init_from_model: n_batch       = 128
0.01.267.056 I llama_init_from_model: n_ubatch      = 128
0.01.267.056 I llama_init_from_model: flash_attn    = 0
0.01.267.057 I llama_init_from_model: freq_base     = 10000.0
0.01.267.057 I llama_init_from_model: freq_scale    = 1
0.01.267.058 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.267.058 I ggml_metal_init: allocating
0.01.267.135 I ggml_metal_init: found device: Apple M4
0.01.267.141 I ggml_metal_init: picking default device: Apple M4
0.01.268.277 I ggml_metal_init: using embedded metal library
0.01.272.223 I ggml_metal_init: GPU name:   Apple M4
0.01.272.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.272.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.272.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.272.227 I ggml_metal_init: simdgroup reduction   = true
0.01.272.227 I ggml_metal_init: simdgroup matrix mul. = true
0.01.272.227 I ggml_metal_init: has residency sets    = true
0.01.272.227 I ggml_metal_init: has bfloat            = true
0.01.272.227 I ggml_metal_init: use bfloat            = true
0.01.272.228 I ggml_metal_init: hasUnifiedMemory      = true
0.01.272.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.282.918 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.284.623 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.284.628 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.284.653 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.286.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.286.352 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.286.352 I llama_init_from_model: graph nodes  = 967
0.01.286.352 I llama_init_from_model: graph splits = 2
0.01.286.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.286.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.321.757 I 
0.01.321.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.321.816 I perplexity: tokenizing the input ..
0.01.326.965 I perplexity: tokenization took 5.147 ms
0.01.326.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.445.307 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.446.660 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.446.675 I llama_perf_context_print:        load time =    1291.25 ms
0.01.446.676 I llama_perf_context_print: prompt eval time =     118.01 ms /   128 tokens (    0.92 ms per token,  1084.63 tokens per second)
0.01.446.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.446.677 I llama_perf_context_print:       total time =     124.92 ms /   129 tokens
0.01.447.028 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.103s
sys	0m0.255s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.433 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.434 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.434 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.435 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.436 I llama_model_loader: - type  f32:  194 tensors
0.00.026.437 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.437 I print_info: file format = GGUF V3 (latest)
0.00.026.438 I print_info: file type   = Q8_0
0.00.026.441 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.495 I load: special tokens cache size = 25
0.00.045.642 I load: token to piece cache size = 0.2984 MB
0.00.045.645 I print_info: arch             = gptneox
0.00.045.646 I print_info: vocab_only       = 0
0.00.045.646 I print_info: n_ctx_train      = 2048
0.00.045.646 I print_info: n_embd           = 2048
0.00.045.646 I print_info: n_layer          = 24
0.00.045.650 I print_info: n_head           = 16
0.00.045.650 I print_info: n_head_kv        = 16
0.00.045.651 I print_info: n_rot            = 32
0.00.045.651 I print_info: n_swa            = 0
0.00.045.651 I print_info: n_embd_head_k    = 128
0.00.045.651 I print_info: n_embd_head_v    = 128
0.00.045.652 I print_info: n_gqa            = 1
0.00.045.653 I print_info: n_embd_k_gqa     = 2048
0.00.045.653 I print_info: n_embd_v_gqa     = 2048
0.00.045.654 I print_info: f_norm_eps       = 1.0e-05
0.00.045.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.654 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.655 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.655 I print_info: f_logit_scale    = 0.0e+00
0.00.045.655 I print_info: n_ff             = 8192
0.00.045.656 I print_info: n_expert         = 0
0.00.045.656 I print_info: n_expert_used    = 0
0.00.045.656 I print_info: causal attn      = 1
0.00.045.656 I print_info: pooling type     = 0
0.00.045.656 I print_info: rope type        = 2
0.00.045.656 I print_info: rope scaling     = linear
0.00.045.657 I print_info: freq_base_train  = 10000.0
0.00.045.657 I print_info: freq_scale_train = 1
0.00.045.657 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.657 I print_info: rope_finetuned   = unknown
0.00.045.658 I print_info: ssm_d_conv       = 0
0.00.045.658 I print_info: ssm_d_inner      = 0
0.00.045.658 I print_info: ssm_d_state      = 0
0.00.045.658 I print_info: ssm_dt_rank      = 0
0.00.045.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.659 I print_info: model type       = 1.4B
0.00.045.659 I print_info: model params     = 1.41 B
0.00.045.659 I print_info: general.name     = 1.4B
0.00.045.660 I print_info: vocab type       = BPE
0.00.045.660 I print_info: n_vocab          = 50304
0.00.045.660 I print_info: n_merges         = 50009
0.00.045.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.661 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.661 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.661 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.661 I print_info: LF token         = 128 'Ä'
0.00.045.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.662 I print_info: max token length = 1024
0.01.068.609 I load_tensors: offloading 24 repeating layers to GPU
0.01.068.612 I load_tensors: offloading output layer to GPU
0.01.068.613 I load_tensors: offloaded 25/25 layers to GPU
0.01.068.637 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.068.639 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.069.944 I llama_init_from_model: n_seq_max     = 1
0.01.069.947 I llama_init_from_model: n_ctx         = 128
0.01.069.948 I llama_init_from_model: n_ctx_per_seq = 128
0.01.069.948 I llama_init_from_model: n_batch       = 128
0.01.069.952 I llama_init_from_model: n_ubatch      = 128
0.01.069.952 I llama_init_from_model: flash_attn    = 0
0.01.069.953 I llama_init_from_model: freq_base     = 10000.0
0.01.069.954 I llama_init_from_model: freq_scale    = 1
0.01.069.954 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.069.955 I ggml_metal_init: allocating
0.01.070.011 I ggml_metal_init: found device: Apple M4
0.01.070.022 I ggml_metal_init: picking default device: Apple M4
0.01.071.427 I ggml_metal_init: using embedded metal library
0.01.076.879 I ggml_metal_init: GPU name:   Apple M4
0.01.076.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.076.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.076.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.076.884 I ggml_metal_init: simdgroup reduction   = true
0.01.076.885 I ggml_metal_init: simdgroup matrix mul. = true
0.01.076.885 I ggml_metal_init: has residency sets    = true
0.01.076.885 I ggml_metal_init: has bfloat            = true
0.01.076.885 I ggml_metal_init: use bfloat            = true
0.01.076.886 I ggml_metal_init: hasUnifiedMemory      = true
0.01.076.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.092.503 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.095.846 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.095.853 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.095.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.098.837 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.098.839 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.098.839 I llama_init_from_model: graph nodes  = 967
0.01.098.840 I llama_init_from_model: graph splits = 2
0.01.098.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.098.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.126.097 I 
0.01.126.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.126.191 I perplexity: tokenizing the input ..
0.01.133.126 I perplexity: tokenization took 6.933 ms
0.01.133.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.271.473 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.272.843 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.272.861 I llama_perf_context_print:        load time =    1117.12 ms
0.01.272.862 I llama_perf_context_print: prompt eval time =     137.48 ms /   128 tokens (    1.07 ms per token,   931.04 tokens per second)
0.01.272.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.272.863 I llama_perf_context_print:       total time =     146.77 ms /   129 tokens
0.01.273.270 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.082s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.639 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.647 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.653 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.474 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.231 I llama_model_loader: - type  f32:  194 tensors
0.00.025.231 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.232 I print_info: file format = GGUF V3 (latest)
0.00.025.233 I print_info: file type   = Q4_0
0.00.025.234 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.109 I load: special tokens cache size = 25
0.00.039.006 I load: token to piece cache size = 0.2984 MB
0.00.039.009 I print_info: arch             = gptneox
0.00.039.009 I print_info: vocab_only       = 0
0.00.039.009 I print_info: n_ctx_train      = 2048
0.00.039.009 I print_info: n_embd           = 2048
0.00.039.009 I print_info: n_layer          = 24
0.00.039.013 I print_info: n_head           = 16
0.00.039.014 I print_info: n_head_kv        = 16
0.00.039.014 I print_info: n_rot            = 32
0.00.039.014 I print_info: n_swa            = 0
0.00.039.014 I print_info: n_embd_head_k    = 128
0.00.039.014 I print_info: n_embd_head_v    = 128
0.00.039.015 I print_info: n_gqa            = 1
0.00.039.016 I print_info: n_embd_k_gqa     = 2048
0.00.039.017 I print_info: n_embd_v_gqa     = 2048
0.00.039.017 I print_info: f_norm_eps       = 1.0e-05
0.00.039.017 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.018 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.018 I print_info: f_logit_scale    = 0.0e+00
0.00.039.019 I print_info: n_ff             = 8192
0.00.039.019 I print_info: n_expert         = 0
0.00.039.019 I print_info: n_expert_used    = 0
0.00.039.019 I print_info: causal attn      = 1
0.00.039.019 I print_info: pooling type     = 0
0.00.039.019 I print_info: rope type        = 2
0.00.039.021 I print_info: rope scaling     = linear
0.00.039.021 I print_info: freq_base_train  = 10000.0
0.00.039.022 I print_info: freq_scale_train = 1
0.00.039.022 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.022 I print_info: rope_finetuned   = unknown
0.00.039.022 I print_info: ssm_d_conv       = 0
0.00.039.022 I print_info: ssm_d_inner      = 0
0.00.039.023 I print_info: ssm_d_state      = 0
0.00.039.025 I print_info: ssm_dt_rank      = 0
0.00.039.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.025 I print_info: model type       = 1.4B
0.00.039.025 I print_info: model params     = 1.41 B
0.00.039.026 I print_info: general.name     = 1.4B
0.00.039.026 I print_info: vocab type       = BPE
0.00.039.026 I print_info: n_vocab          = 50304
0.00.039.026 I print_info: n_merges         = 50009
0.00.039.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.027 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: LF token         = 128 'Ä'
0.00.039.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: max token length = 1024
0.00.603.574 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.588 I load_tensors: offloading output layer to GPU
0.00.603.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.621 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.603.622 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.605.153 I llama_init_from_model: n_seq_max     = 1
0.00.605.158 I llama_init_from_model: n_ctx         = 128
0.00.605.158 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.159 I llama_init_from_model: n_batch       = 128
0.00.605.160 I llama_init_from_model: n_ubatch      = 128
0.00.605.160 I llama_init_from_model: flash_attn    = 0
0.00.605.162 I llama_init_from_model: freq_base     = 10000.0
0.00.605.162 I llama_init_from_model: freq_scale    = 1
0.00.605.163 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.165 I ggml_metal_init: allocating
0.00.605.246 I ggml_metal_init: found device: Apple M4
0.00.605.261 I ggml_metal_init: picking default device: Apple M4
0.00.606.990 I ggml_metal_init: using embedded metal library
0.00.612.446 I ggml_metal_init: GPU name:   Apple M4
0.00.612.452 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.453 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.454 I ggml_metal_init: simdgroup reduction   = true
0.00.612.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.455 I ggml_metal_init: has residency sets    = true
0.00.612.455 I ggml_metal_init: has bfloat            = true
0.00.612.456 I ggml_metal_init: use bfloat            = true
0.00.612.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.601 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.635.608 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.635.662 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.110 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.112 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.112 I llama_init_from_model: graph nodes  = 967
0.00.639.113 I llama_init_from_model: graph splits = 2
0.00.639.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.256 I 
0.00.667.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.364 I perplexity: tokenizing the input ..
0.00.674.984 I perplexity: tokenization took 7.617 ms
0.00.675.006 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.213 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.812.560 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.812.576 I llama_perf_context_print:        load time =     657.41 ms
0.00.812.577 I llama_perf_context_print: prompt eval time =     135.32 ms /   128 tokens (    1.06 ms per token,   945.92 tokens per second)
0.00.812.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.578 I llama_perf_context_print:       total time =     145.33 ms /   129 tokens
0.00.812.965 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.080s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.754 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.751 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.760 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.656 I llama_model_loader: - type  f32:  194 tensors
0.00.024.656 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.656 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.657 I print_info: file format = GGUF V3 (latest)
0.00.024.657 I print_info: file type   = Q4_1
0.00.024.659 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.257 I load: special tokens cache size = 25
0.00.039.295 I load: token to piece cache size = 0.2984 MB
0.00.039.299 I print_info: arch             = gptneox
0.00.039.300 I print_info: vocab_only       = 0
0.00.039.300 I print_info: n_ctx_train      = 2048
0.00.039.300 I print_info: n_embd           = 2048
0.00.039.300 I print_info: n_layer          = 24
0.00.039.304 I print_info: n_head           = 16
0.00.039.305 I print_info: n_head_kv        = 16
0.00.039.305 I print_info: n_rot            = 32
0.00.039.305 I print_info: n_swa            = 0
0.00.039.306 I print_info: n_embd_head_k    = 128
0.00.039.306 I print_info: n_embd_head_v    = 128
0.00.039.306 I print_info: n_gqa            = 1
0.00.039.307 I print_info: n_embd_k_gqa     = 2048
0.00.039.308 I print_info: n_embd_v_gqa     = 2048
0.00.039.308 I print_info: f_norm_eps       = 1.0e-05
0.00.039.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.309 I print_info: f_logit_scale    = 0.0e+00
0.00.039.310 I print_info: n_ff             = 8192
0.00.039.310 I print_info: n_expert         = 0
0.00.039.310 I print_info: n_expert_used    = 0
0.00.039.310 I print_info: causal attn      = 1
0.00.039.310 I print_info: pooling type     = 0
0.00.039.311 I print_info: rope type        = 2
0.00.039.311 I print_info: rope scaling     = linear
0.00.039.311 I print_info: freq_base_train  = 10000.0
0.00.039.311 I print_info: freq_scale_train = 1
0.00.039.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.312 I print_info: rope_finetuned   = unknown
0.00.039.312 I print_info: ssm_d_conv       = 0
0.00.039.312 I print_info: ssm_d_inner      = 0
0.00.039.312 I print_info: ssm_d_state      = 0
0.00.039.312 I print_info: ssm_dt_rank      = 0
0.00.039.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.315 I print_info: model type       = 1.4B
0.00.039.315 I print_info: model params     = 1.41 B
0.00.039.316 I print_info: general.name     = 1.4B
0.00.039.316 I print_info: vocab type       = BPE
0.00.039.317 I print_info: n_vocab          = 50304
0.00.039.317 I print_info: n_merges         = 50009
0.00.039.317 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.318 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.318 I print_info: LF token         = 128 'Ä'
0.00.039.318 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: max token length = 1024
0.00.627.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.230 I load_tensors: offloading output layer to GPU
0.00.627.231 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.263 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.264 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.628.393 I llama_init_from_model: n_seq_max     = 1
0.00.628.399 I llama_init_from_model: n_ctx         = 128
0.00.628.399 I llama_init_from_model: n_ctx_per_seq = 128
0.00.628.400 I llama_init_from_model: n_batch       = 128
0.00.628.400 I llama_init_from_model: n_ubatch      = 128
0.00.628.401 I llama_init_from_model: flash_attn    = 0
0.00.628.403 I llama_init_from_model: freq_base     = 10000.0
0.00.628.403 I llama_init_from_model: freq_scale    = 1
0.00.628.404 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.628.411 I ggml_metal_init: allocating
0.00.628.515 I ggml_metal_init: found device: Apple M4
0.00.628.529 I ggml_metal_init: picking default device: Apple M4
0.00.630.401 I ggml_metal_init: using embedded metal library
0.00.636.870 I ggml_metal_init: GPU name:   Apple M4
0.00.636.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.877 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.877 I ggml_metal_init: simdgroup reduction   = true
0.00.636.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.878 I ggml_metal_init: has residency sets    = true
0.00.636.878 I ggml_metal_init: has bfloat            = true
0.00.636.879 I ggml_metal_init: use bfloat            = true
0.00.636.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.969 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.658.566 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.658.591 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.083 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.085 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.085 I llama_init_from_model: graph nodes  = 967
0.00.662.086 I llama_init_from_model: graph splits = 2
0.00.662.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.593 I 
0.00.689.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.697 I perplexity: tokenizing the input ..
0.00.697.200 I perplexity: tokenization took 7.499 ms
0.00.697.221 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.499 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.834.831 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.834.846 I llama_perf_context_print:        load time =     680.83 ms
0.00.834.846 I llama_perf_context_print: prompt eval time =     135.39 ms /   128 tokens (    1.06 ms per token,   945.39 tokens per second)
0.00.834.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.848 I llama_perf_context_print:       total time =     145.26 ms /   129 tokens
0.00.835.192 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.080s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.455 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.370 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.375 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.386 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.125 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.920 I llama_model_loader: - type  f32:  194 tensors
0.00.024.920 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.921 I print_info: file format = GGUF V3 (latest)
0.00.024.921 I print_info: file type   = Q5_0
0.00.024.922 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.804 I load: special tokens cache size = 25
0.00.038.741 I load: token to piece cache size = 0.2984 MB
0.00.038.744 I print_info: arch             = gptneox
0.00.038.745 I print_info: vocab_only       = 0
0.00.038.745 I print_info: n_ctx_train      = 2048
0.00.038.745 I print_info: n_embd           = 2048
0.00.038.745 I print_info: n_layer          = 24
0.00.038.748 I print_info: n_head           = 16
0.00.038.749 I print_info: n_head_kv        = 16
0.00.038.749 I print_info: n_rot            = 32
0.00.038.749 I print_info: n_swa            = 0
0.00.038.749 I print_info: n_embd_head_k    = 128
0.00.038.750 I print_info: n_embd_head_v    = 128
0.00.038.751 I print_info: n_gqa            = 1
0.00.038.752 I print_info: n_embd_k_gqa     = 2048
0.00.038.752 I print_info: n_embd_v_gqa     = 2048
0.00.038.753 I print_info: f_norm_eps       = 1.0e-05
0.00.038.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.753 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.754 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.754 I print_info: f_logit_scale    = 0.0e+00
0.00.038.755 I print_info: n_ff             = 8192
0.00.038.755 I print_info: n_expert         = 0
0.00.038.755 I print_info: n_expert_used    = 0
0.00.038.755 I print_info: causal attn      = 1
0.00.038.755 I print_info: pooling type     = 0
0.00.038.755 I print_info: rope type        = 2
0.00.038.756 I print_info: rope scaling     = linear
0.00.038.756 I print_info: freq_base_train  = 10000.0
0.00.038.756 I print_info: freq_scale_train = 1
0.00.038.757 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.757 I print_info: rope_finetuned   = unknown
0.00.038.757 I print_info: ssm_d_conv       = 0
0.00.038.757 I print_info: ssm_d_inner      = 0
0.00.038.757 I print_info: ssm_d_state      = 0
0.00.038.757 I print_info: ssm_dt_rank      = 0
0.00.038.757 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.758 I print_info: model type       = 1.4B
0.00.038.758 I print_info: model params     = 1.41 B
0.00.038.758 I print_info: general.name     = 1.4B
0.00.038.759 I print_info: vocab type       = BPE
0.00.038.759 I print_info: n_vocab          = 50304
0.00.038.759 I print_info: n_merges         = 50009
0.00.038.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.760 I print_info: LF token         = 128 'Ä'
0.00.038.761 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.762 I print_info: max token length = 1024
0.00.691.871 I load_tensors: offloading 24 repeating layers to GPU
0.00.691.882 I load_tensors: offloading output layer to GPU
0.00.691.883 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.913 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.691.915 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.693.355 I llama_init_from_model: n_seq_max     = 1
0.00.693.365 I llama_init_from_model: n_ctx         = 128
0.00.693.366 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.366 I llama_init_from_model: n_batch       = 128
0.00.693.367 I llama_init_from_model: n_ubatch      = 128
0.00.693.367 I llama_init_from_model: flash_attn    = 0
0.00.693.368 I llama_init_from_model: freq_base     = 10000.0
0.00.693.368 I llama_init_from_model: freq_scale    = 1
0.00.693.369 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.371 I ggml_metal_init: allocating
0.00.693.423 I ggml_metal_init: found device: Apple M4
0.00.693.433 I ggml_metal_init: picking default device: Apple M4
0.00.695.030 I ggml_metal_init: using embedded metal library
0.00.702.012 I ggml_metal_init: GPU name:   Apple M4
0.00.702.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.023 I ggml_metal_init: simdgroup reduction   = true
0.00.702.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.024 I ggml_metal_init: has residency sets    = true
0.00.702.024 I ggml_metal_init: has bfloat            = true
0.00.702.024 I ggml_metal_init: use bfloat            = true
0.00.702.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.723.764 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.723.808 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.330 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.727.331 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.727.332 I llama_init_from_model: graph nodes  = 967
0.00.727.332 I llama_init_from_model: graph splits = 2
0.00.727.335 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.727.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.900 I 
0.00.758.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.005 I perplexity: tokenizing the input ..
0.00.766.252 I perplexity: tokenization took 7.246 ms
0.00.766.278 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.677 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.912.004 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.912.019 I llama_perf_context_print:        load time =     749.44 ms
0.00.912.020 I llama_perf_context_print: prompt eval time =     143.45 ms /   128 tokens (    1.12 ms per token,   892.32 tokens per second)
0.00.912.021 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.021 I llama_perf_context_print:       total time =     153.12 ms /   129 tokens
0.00.912.425 I ggml_metal_free: deallocating

real	0m0.929s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.086 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.645 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.657 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.300 I llama_model_loader: - type  f32:  194 tensors
0.00.024.300 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.300 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.301 I print_info: file format = GGUF V3 (latest)
0.00.024.301 I print_info: file type   = Q5_1
0.00.024.302 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.119 I load: special tokens cache size = 25
0.00.038.089 I load: token to piece cache size = 0.2984 MB
0.00.038.092 I print_info: arch             = gptneox
0.00.038.092 I print_info: vocab_only       = 0
0.00.038.093 I print_info: n_ctx_train      = 2048
0.00.038.093 I print_info: n_embd           = 2048
0.00.038.093 I print_info: n_layer          = 24
0.00.038.096 I print_info: n_head           = 16
0.00.038.097 I print_info: n_head_kv        = 16
0.00.038.097 I print_info: n_rot            = 32
0.00.038.097 I print_info: n_swa            = 0
0.00.038.097 I print_info: n_embd_head_k    = 128
0.00.038.097 I print_info: n_embd_head_v    = 128
0.00.038.098 I print_info: n_gqa            = 1
0.00.038.099 I print_info: n_embd_k_gqa     = 2048
0.00.038.100 I print_info: n_embd_v_gqa     = 2048
0.00.038.100 I print_info: f_norm_eps       = 1.0e-05
0.00.038.101 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.101 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.101 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.101 I print_info: f_logit_scale    = 0.0e+00
0.00.038.102 I print_info: n_ff             = 8192
0.00.038.102 I print_info: n_expert         = 0
0.00.038.102 I print_info: n_expert_used    = 0
0.00.038.102 I print_info: causal attn      = 1
0.00.038.103 I print_info: pooling type     = 0
0.00.038.103 I print_info: rope type        = 2
0.00.038.103 I print_info: rope scaling     = linear
0.00.038.103 I print_info: freq_base_train  = 10000.0
0.00.038.106 I print_info: freq_scale_train = 1
0.00.038.106 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.106 I print_info: rope_finetuned   = unknown
0.00.038.107 I print_info: ssm_d_conv       = 0
0.00.038.107 I print_info: ssm_d_inner      = 0
0.00.038.107 I print_info: ssm_d_state      = 0
0.00.038.107 I print_info: ssm_dt_rank      = 0
0.00.038.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.107 I print_info: model type       = 1.4B
0.00.038.108 I print_info: model params     = 1.41 B
0.00.038.108 I print_info: general.name     = 1.4B
0.00.038.108 I print_info: vocab type       = BPE
0.00.038.109 I print_info: n_vocab          = 50304
0.00.038.109 I print_info: n_merges         = 50009
0.00.038.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.109 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.110 I print_info: LF token         = 128 'Ä'
0.00.038.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.110 I print_info: max token length = 1024
0.00.592.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.590 I load_tensors: offloading output layer to GPU
0.00.592.590 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.624 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.592.628 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.594.088 I llama_init_from_model: n_seq_max     = 1
0.00.594.092 I llama_init_from_model: n_ctx         = 128
0.00.594.092 I llama_init_from_model: n_ctx_per_seq = 128
0.00.594.093 I llama_init_from_model: n_batch       = 128
0.00.594.094 I llama_init_from_model: n_ubatch      = 128
0.00.594.094 I llama_init_from_model: flash_attn    = 0
0.00.594.096 I llama_init_from_model: freq_base     = 10000.0
0.00.594.097 I llama_init_from_model: freq_scale    = 1
0.00.594.097 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.594.102 I ggml_metal_init: allocating
0.00.594.146 I ggml_metal_init: found device: Apple M4
0.00.594.159 I ggml_metal_init: picking default device: Apple M4
0.00.595.849 I ggml_metal_init: using embedded metal library
0.00.602.395 I ggml_metal_init: GPU name:   Apple M4
0.00.602.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.400 I ggml_metal_init: simdgroup reduction   = true
0.00.602.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.401 I ggml_metal_init: has residency sets    = true
0.00.602.401 I ggml_metal_init: has bfloat            = true
0.00.602.401 I ggml_metal_init: use bfloat            = true
0.00.602.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.761 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.128 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.135 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.380 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.382 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.382 I llama_init_from_model: graph nodes  = 967
0.00.626.383 I llama_init_from_model: graph splits = 2
0.00.626.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.821 I 
0.00.652.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.926 I perplexity: tokenizing the input ..
0.00.660.753 I perplexity: tokenization took 7.824 ms
0.00.660.779 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.925 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.798.265 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.798.282 I llama_perf_context_print:        load time =     643.73 ms
0.00.798.283 I llama_perf_context_print: prompt eval time =     135.23 ms /   128 tokens (    1.06 ms per token,   946.56 tokens per second)
0.00.798.283 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.284 I llama_perf_context_print:       total time =     145.47 ms /   129 tokens
0.00.798.647 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.027 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.357 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.359 I llama_model_loader: - type  f32:  194 tensors
0.00.025.360 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.360 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.360 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.361 I print_info: file format = GGUF V3 (latest)
0.00.025.361 I print_info: file type   = Q2_K - Medium
0.00.025.362 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.180 I load: special tokens cache size = 25
0.00.039.091 I load: token to piece cache size = 0.2984 MB
0.00.039.093 I print_info: arch             = gptneox
0.00.039.093 I print_info: vocab_only       = 0
0.00.039.094 I print_info: n_ctx_train      = 2048
0.00.039.094 I print_info: n_embd           = 2048
0.00.039.094 I print_info: n_layer          = 24
0.00.039.097 I print_info: n_head           = 16
0.00.039.098 I print_info: n_head_kv        = 16
0.00.039.098 I print_info: n_rot            = 32
0.00.039.098 I print_info: n_swa            = 0
0.00.039.099 I print_info: n_embd_head_k    = 128
0.00.039.099 I print_info: n_embd_head_v    = 128
0.00.039.102 I print_info: n_gqa            = 1
0.00.039.103 I print_info: n_embd_k_gqa     = 2048
0.00.039.104 I print_info: n_embd_v_gqa     = 2048
0.00.039.104 I print_info: f_norm_eps       = 1.0e-05
0.00.039.105 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.105 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.105 I print_info: f_logit_scale    = 0.0e+00
0.00.039.106 I print_info: n_ff             = 8192
0.00.039.106 I print_info: n_expert         = 0
0.00.039.106 I print_info: n_expert_used    = 0
0.00.039.106 I print_info: causal attn      = 1
0.00.039.106 I print_info: pooling type     = 0
0.00.039.106 I print_info: rope type        = 2
0.00.039.107 I print_info: rope scaling     = linear
0.00.039.107 I print_info: freq_base_train  = 10000.0
0.00.039.107 I print_info: freq_scale_train = 1
0.00.039.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.108 I print_info: rope_finetuned   = unknown
0.00.039.108 I print_info: ssm_d_conv       = 0
0.00.039.108 I print_info: ssm_d_inner      = 0
0.00.039.108 I print_info: ssm_d_state      = 0
0.00.039.108 I print_info: ssm_dt_rank      = 0
0.00.039.108 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.109 I print_info: model type       = 1.4B
0.00.039.109 I print_info: model params     = 1.41 B
0.00.039.109 I print_info: general.name     = 1.4B
0.00.039.110 I print_info: vocab type       = BPE
0.00.039.110 I print_info: n_vocab          = 50304
0.00.039.110 I print_info: n_merges         = 50009
0.00.039.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: LF token         = 128 'Ä'
0.00.039.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: max token length = 1024
0.00.351.088 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.104 I load_tensors: offloading output layer to GPU
0.00.351.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.141 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.142 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.352.584 I llama_init_from_model: n_seq_max     = 1
0.00.352.588 I llama_init_from_model: n_ctx         = 128
0.00.352.588 I llama_init_from_model: n_ctx_per_seq = 128
0.00.352.589 I llama_init_from_model: n_batch       = 128
0.00.352.589 I llama_init_from_model: n_ubatch      = 128
0.00.352.590 I llama_init_from_model: flash_attn    = 0
0.00.352.591 I llama_init_from_model: freq_base     = 10000.0
0.00.352.592 I llama_init_from_model: freq_scale    = 1
0.00.352.592 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.352.595 I ggml_metal_init: allocating
0.00.352.659 I ggml_metal_init: found device: Apple M4
0.00.352.673 I ggml_metal_init: picking default device: Apple M4
0.00.354.384 I ggml_metal_init: using embedded metal library
0.00.359.877 I ggml_metal_init: GPU name:   Apple M4
0.00.359.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.891 I ggml_metal_init: simdgroup reduction   = true
0.00.359.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.891 I ggml_metal_init: has residency sets    = true
0.00.359.891 I ggml_metal_init: has bfloat            = true
0.00.359.892 I ggml_metal_init: use bfloat            = true
0.00.359.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.446 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.385.169 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.385.203 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.388.844 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.388.846 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.388.847 I llama_init_from_model: graph nodes  = 967
0.00.388.847 I llama_init_from_model: graph splits = 2
0.00.388.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.388.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.967 I 
0.00.418.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.074 I perplexity: tokenizing the input ..
0.00.425.243 I perplexity: tokenization took 7.165 ms
0.00.425.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.751 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.086 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.099 I llama_perf_context_print:        load time =     407.93 ms
0.00.560.100 I llama_perf_context_print: prompt eval time =     132.56 ms /   128 tokens (    1.04 ms per token,   965.59 tokens per second)
0.00.560.101 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.101 I llama_perf_context_print:       total time =     142.14 ms /   129 tokens
0.00.560.489 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.081s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.235 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.888 I llama_model_loader: - type  f32:  194 tensors
0.00.024.889 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.889 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.889 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.889 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.890 I print_info: file format = GGUF V3 (latest)
0.00.024.890 I print_info: file type   = Q3_K - Medium
0.00.024.892 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.961 I load: special tokens cache size = 25
0.00.038.907 I load: token to piece cache size = 0.2984 MB
0.00.038.909 I print_info: arch             = gptneox
0.00.038.910 I print_info: vocab_only       = 0
0.00.038.910 I print_info: n_ctx_train      = 2048
0.00.038.910 I print_info: n_embd           = 2048
0.00.038.910 I print_info: n_layer          = 24
0.00.038.913 I print_info: n_head           = 16
0.00.038.914 I print_info: n_head_kv        = 16
0.00.038.914 I print_info: n_rot            = 32
0.00.038.914 I print_info: n_swa            = 0
0.00.038.915 I print_info: n_embd_head_k    = 128
0.00.038.915 I print_info: n_embd_head_v    = 128
0.00.038.915 I print_info: n_gqa            = 1
0.00.038.916 I print_info: n_embd_k_gqa     = 2048
0.00.038.917 I print_info: n_embd_v_gqa     = 2048
0.00.038.917 I print_info: f_norm_eps       = 1.0e-05
0.00.038.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.918 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.918 I print_info: f_logit_scale    = 0.0e+00
0.00.038.919 I print_info: n_ff             = 8192
0.00.038.919 I print_info: n_expert         = 0
0.00.038.919 I print_info: n_expert_used    = 0
0.00.038.919 I print_info: causal attn      = 1
0.00.038.919 I print_info: pooling type     = 0
0.00.038.920 I print_info: rope type        = 2
0.00.038.920 I print_info: rope scaling     = linear
0.00.038.922 I print_info: freq_base_train  = 10000.0
0.00.038.922 I print_info: freq_scale_train = 1
0.00.038.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.923 I print_info: rope_finetuned   = unknown
0.00.038.923 I print_info: ssm_d_conv       = 0
0.00.038.923 I print_info: ssm_d_inner      = 0
0.00.038.923 I print_info: ssm_d_state      = 0
0.00.038.923 I print_info: ssm_dt_rank      = 0
0.00.038.923 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.924 I print_info: model type       = 1.4B
0.00.038.924 I print_info: model params     = 1.41 B
0.00.038.924 I print_info: general.name     = 1.4B
0.00.038.925 I print_info: vocab type       = BPE
0.00.038.925 I print_info: n_vocab          = 50304
0.00.038.925 I print_info: n_merges         = 50009
0.00.038.925 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: LF token         = 128 'Ä'
0.00.038.927 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.927 I print_info: max token length = 1024
0.00.454.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.252 I load_tensors: offloading output layer to GPU
0.00.454.253 I load_tensors: offloaded 25/25 layers to GPU
0.00.454.285 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.454.286 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.455.869 I llama_init_from_model: n_seq_max     = 1
0.00.455.873 I llama_init_from_model: n_ctx         = 128
0.00.455.874 I llama_init_from_model: n_ctx_per_seq = 128
0.00.455.874 I llama_init_from_model: n_batch       = 128
0.00.455.875 I llama_init_from_model: n_ubatch      = 128
0.00.455.875 I llama_init_from_model: flash_attn    = 0
0.00.455.877 I llama_init_from_model: freq_base     = 10000.0
0.00.455.878 I llama_init_from_model: freq_scale    = 1
0.00.455.879 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.455.881 I ggml_metal_init: allocating
0.00.455.931 I ggml_metal_init: found device: Apple M4
0.00.455.948 I ggml_metal_init: picking default device: Apple M4
0.00.457.886 I ggml_metal_init: using embedded metal library
0.00.463.901 I ggml_metal_init: GPU name:   Apple M4
0.00.463.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.463.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.463.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.463.913 I ggml_metal_init: simdgroup reduction   = true
0.00.463.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.463.913 I ggml_metal_init: has residency sets    = true
0.00.463.913 I ggml_metal_init: has bfloat            = true
0.00.463.914 I ggml_metal_init: use bfloat            = true
0.00.463.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.463.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.484.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.488.491 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.488.504 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.488.552 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.491.906 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.491.908 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.491.909 I llama_init_from_model: graph nodes  = 967
0.00.491.909 I llama_init_from_model: graph splits = 2
0.00.491.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.491.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.675 I 
0.00.522.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.789 I perplexity: tokenizing the input ..
0.00.530.122 I perplexity: tokenization took 7.329 ms
0.00.530.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.672.940 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.674.286 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.674.303 I llama_perf_context_print:        load time =     513.64 ms
0.00.674.304 I llama_perf_context_print: prompt eval time =     141.87 ms /   128 tokens (    1.11 ms per token,   902.23 tokens per second)
0.00.674.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.674.305 I llama_perf_context_print:       total time =     151.63 ms /   129 tokens
0.00.674.679 I ggml_metal_free: deallocating

real	0m0.688s
user	0m0.081s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.731 I llama_model_loader: - type  f32:  194 tensors
0.00.025.731 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.731 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.732 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.732 I print_info: file format = GGUF V3 (latest)
0.00.025.733 I print_info: file type   = Q4_K - Medium
0.00.025.733 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.851 I load: special tokens cache size = 25
0.00.039.824 I load: token to piece cache size = 0.2984 MB
0.00.039.826 I print_info: arch             = gptneox
0.00.039.826 I print_info: vocab_only       = 0
0.00.039.827 I print_info: n_ctx_train      = 2048
0.00.039.827 I print_info: n_embd           = 2048
0.00.039.827 I print_info: n_layer          = 24
0.00.039.830 I print_info: n_head           = 16
0.00.039.830 I print_info: n_head_kv        = 16
0.00.039.831 I print_info: n_rot            = 32
0.00.039.831 I print_info: n_swa            = 0
0.00.039.831 I print_info: n_embd_head_k    = 128
0.00.039.833 I print_info: n_embd_head_v    = 128
0.00.039.834 I print_info: n_gqa            = 1
0.00.039.835 I print_info: n_embd_k_gqa     = 2048
0.00.039.836 I print_info: n_embd_v_gqa     = 2048
0.00.039.836 I print_info: f_norm_eps       = 1.0e-05
0.00.039.837 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.837 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.837 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.837 I print_info: f_logit_scale    = 0.0e+00
0.00.039.838 I print_info: n_ff             = 8192
0.00.039.839 I print_info: n_expert         = 0
0.00.039.840 I print_info: n_expert_used    = 0
0.00.039.841 I print_info: causal attn      = 1
0.00.039.841 I print_info: pooling type     = 0
0.00.039.841 I print_info: rope type        = 2
0.00.039.841 I print_info: rope scaling     = linear
0.00.039.841 I print_info: freq_base_train  = 10000.0
0.00.039.842 I print_info: freq_scale_train = 1
0.00.039.842 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.842 I print_info: rope_finetuned   = unknown
0.00.039.842 I print_info: ssm_d_conv       = 0
0.00.039.842 I print_info: ssm_d_inner      = 0
0.00.039.842 I print_info: ssm_d_state      = 0
0.00.039.843 I print_info: ssm_dt_rank      = 0
0.00.039.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.844 I print_info: model type       = 1.4B
0.00.039.844 I print_info: model params     = 1.41 B
0.00.039.844 I print_info: general.name     = 1.4B
0.00.039.845 I print_info: vocab type       = BPE
0.00.039.845 I print_info: n_vocab          = 50304
0.00.039.845 I print_info: n_merges         = 50009
0.00.039.846 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.846 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: LF token         = 128 'Ä'
0.00.039.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: max token length = 1024
0.00.508.696 I load_tensors: offloading 24 repeating layers to GPU
0.00.508.711 I load_tensors: offloading output layer to GPU
0.00.508.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.508.744 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.508.745 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.510.350 I llama_init_from_model: n_seq_max     = 1
0.00.510.355 I llama_init_from_model: n_ctx         = 128
0.00.510.356 I llama_init_from_model: n_ctx_per_seq = 128
0.00.510.356 I llama_init_from_model: n_batch       = 128
0.00.510.356 I llama_init_from_model: n_ubatch      = 128
0.00.510.357 I llama_init_from_model: flash_attn    = 0
0.00.510.359 I llama_init_from_model: freq_base     = 10000.0
0.00.510.359 I llama_init_from_model: freq_scale    = 1
0.00.510.360 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.510.363 I ggml_metal_init: allocating
0.00.510.435 I ggml_metal_init: found device: Apple M4
0.00.510.449 I ggml_metal_init: picking default device: Apple M4
0.00.512.229 I ggml_metal_init: using embedded metal library
0.00.518.780 I ggml_metal_init: GPU name:   Apple M4
0.00.518.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.518.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.518.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.518.786 I ggml_metal_init: simdgroup reduction   = true
0.00.518.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.518.787 I ggml_metal_init: has residency sets    = true
0.00.518.787 I ggml_metal_init: has bfloat            = true
0.00.518.787 I ggml_metal_init: use bfloat            = true
0.00.518.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.518.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.753 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.539.343 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.539.346 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.539.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.708 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.542.710 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.542.710 I llama_init_from_model: graph nodes  = 967
0.00.542.711 I llama_init_from_model: graph splits = 2
0.00.542.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.542.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.071 I 
0.00.570.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.175 I perplexity: tokenizing the input ..
0.00.577.975 I perplexity: tokenization took 7.796 ms
0.00.577.999 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.722.009 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.723.354 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.723.370 I llama_perf_context_print:        load time =     560.38 ms
0.00.723.371 I llama_perf_context_print: prompt eval time =     143.05 ms /   128 tokens (    1.12 ms per token,   894.82 tokens per second)
0.00.723.372 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.373 I llama_perf_context_print:       total time =     153.30 ms /   129 tokens
0.00.723.756 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.080s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.427 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.446 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.943 I llama_model_loader: - type  f32:  194 tensors
0.00.028.943 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.944 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.944 I print_info: file format = GGUF V3 (latest)
0.00.028.945 I print_info: file type   = Q5_K - Medium
0.00.028.946 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.720 I load: special tokens cache size = 25
0.00.042.645 I load: token to piece cache size = 0.2984 MB
0.00.042.647 I print_info: arch             = gptneox
0.00.042.648 I print_info: vocab_only       = 0
0.00.042.648 I print_info: n_ctx_train      = 2048
0.00.042.648 I print_info: n_embd           = 2048
0.00.042.648 I print_info: n_layer          = 24
0.00.042.651 I print_info: n_head           = 16
0.00.042.652 I print_info: n_head_kv        = 16
0.00.042.652 I print_info: n_rot            = 32
0.00.042.652 I print_info: n_swa            = 0
0.00.042.652 I print_info: n_embd_head_k    = 128
0.00.042.652 I print_info: n_embd_head_v    = 128
0.00.042.653 I print_info: n_gqa            = 1
0.00.042.654 I print_info: n_embd_k_gqa     = 2048
0.00.042.655 I print_info: n_embd_v_gqa     = 2048
0.00.042.655 I print_info: f_norm_eps       = 1.0e-05
0.00.042.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.656 I print_info: f_logit_scale    = 0.0e+00
0.00.042.657 I print_info: n_ff             = 8192
0.00.042.657 I print_info: n_expert         = 0
0.00.042.657 I print_info: n_expert_used    = 0
0.00.042.657 I print_info: causal attn      = 1
0.00.042.657 I print_info: pooling type     = 0
0.00.042.657 I print_info: rope type        = 2
0.00.042.658 I print_info: rope scaling     = linear
0.00.042.658 I print_info: freq_base_train  = 10000.0
0.00.042.658 I print_info: freq_scale_train = 1
0.00.042.659 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.659 I print_info: rope_finetuned   = unknown
0.00.042.659 I print_info: ssm_d_conv       = 0
0.00.042.659 I print_info: ssm_d_inner      = 0
0.00.042.661 I print_info: ssm_d_state      = 0
0.00.042.662 I print_info: ssm_dt_rank      = 0
0.00.042.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.662 I print_info: model type       = 1.4B
0.00.042.662 I print_info: model params     = 1.41 B
0.00.042.662 I print_info: general.name     = 1.4B
0.00.042.663 I print_info: vocab type       = BPE
0.00.042.663 I print_info: n_vocab          = 50304
0.00.042.663 I print_info: n_merges         = 50009
0.00.042.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.665 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.668 I print_info: LF token         = 128 'Ä'
0.00.042.669 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.669 I print_info: max token length = 1024
0.00.586.101 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.116 I load_tensors: offloading output layer to GPU
0.00.586.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.151 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.153 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.587.701 I llama_init_from_model: n_seq_max     = 1
0.00.587.706 I llama_init_from_model: n_ctx         = 128
0.00.587.707 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.708 I llama_init_from_model: n_batch       = 128
0.00.587.708 I llama_init_from_model: n_ubatch      = 128
0.00.587.709 I llama_init_from_model: flash_attn    = 0
0.00.587.711 I llama_init_from_model: freq_base     = 10000.0
0.00.587.711 I llama_init_from_model: freq_scale    = 1
0.00.587.712 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.714 I ggml_metal_init: allocating
0.00.587.792 I ggml_metal_init: found device: Apple M4
0.00.587.806 I ggml_metal_init: picking default device: Apple M4
0.00.589.475 I ggml_metal_init: using embedded metal library
0.00.595.904 I ggml_metal_init: GPU name:   Apple M4
0.00.595.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.910 I ggml_metal_init: simdgroup reduction   = true
0.00.595.910 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.910 I ggml_metal_init: has residency sets    = true
0.00.595.911 I ggml_metal_init: has bfloat            = true
0.00.595.911 I ggml_metal_init: use bfloat            = true
0.00.595.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.754 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.238 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.242 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.277 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.454 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.456 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.456 I llama_init_from_model: graph nodes  = 967
0.00.619.457 I llama_init_from_model: graph splits = 2
0.00.619.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.551 I 
0.00.649.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.652 I perplexity: tokenizing the input ..
0.00.655.988 I perplexity: tokenization took 6.335 ms
0.00.656.004 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.172 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.831 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.845 I llama_perf_context_print:        load time =     639.12 ms
0.00.798.846 I llama_perf_context_print: prompt eval time =     140.63 ms /   128 tokens (    1.10 ms per token,   910.20 tokens per second)
0.00.798.846 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.847 I llama_perf_context_print:       total time =     149.30 ms /   129 tokens
0.00.799.219 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.076s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.836 I llama_model_loader: - type  f32:  194 tensors
0.00.024.836 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.837 I print_info: file format = GGUF V3 (latest)
0.00.024.837 I print_info: file type   = Q6_K
0.00.024.838 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.980 I load: special tokens cache size = 25
0.00.038.977 I load: token to piece cache size = 0.2984 MB
0.00.038.980 I print_info: arch             = gptneox
0.00.038.980 I print_info: vocab_only       = 0
0.00.038.980 I print_info: n_ctx_train      = 2048
0.00.038.981 I print_info: n_embd           = 2048
0.00.038.981 I print_info: n_layer          = 24
0.00.038.984 I print_info: n_head           = 16
0.00.038.985 I print_info: n_head_kv        = 16
0.00.038.986 I print_info: n_rot            = 32
0.00.038.986 I print_info: n_swa            = 0
0.00.038.986 I print_info: n_embd_head_k    = 128
0.00.038.986 I print_info: n_embd_head_v    = 128
0.00.038.987 I print_info: n_gqa            = 1
0.00.038.988 I print_info: n_embd_k_gqa     = 2048
0.00.038.988 I print_info: n_embd_v_gqa     = 2048
0.00.038.989 I print_info: f_norm_eps       = 1.0e-05
0.00.038.989 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.989 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.989 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.990 I print_info: f_logit_scale    = 0.0e+00
0.00.038.990 I print_info: n_ff             = 8192
0.00.038.992 I print_info: n_expert         = 0
0.00.038.992 I print_info: n_expert_used    = 0
0.00.038.992 I print_info: causal attn      = 1
0.00.038.992 I print_info: pooling type     = 0
0.00.038.992 I print_info: rope type        = 2
0.00.038.992 I print_info: rope scaling     = linear
0.00.038.993 I print_info: freq_base_train  = 10000.0
0.00.038.993 I print_info: freq_scale_train = 1
0.00.038.993 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.994 I print_info: rope_finetuned   = unknown
0.00.038.994 I print_info: ssm_d_conv       = 0
0.00.038.994 I print_info: ssm_d_inner      = 0
0.00.038.994 I print_info: ssm_d_state      = 0
0.00.038.994 I print_info: ssm_dt_rank      = 0
0.00.038.994 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.995 I print_info: model type       = 1.4B
0.00.038.997 I print_info: model params     = 1.41 B
0.00.038.997 I print_info: general.name     = 1.4B
0.00.038.998 I print_info: vocab type       = BPE
0.00.038.998 I print_info: n_vocab          = 50304
0.00.038.998 I print_info: n_merges         = 50009
0.00.038.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.999 I print_info: LF token         = 128 'Ä'
0.00.039.000 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: max token length = 1024
0.00.384.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.384.339 I load_tensors: offloading output layer to GPU
0.00.384.340 I load_tensors: offloaded 25/25 layers to GPU
0.00.384.363 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.384.365 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.385.533 I llama_init_from_model: n_seq_max     = 1
0.00.385.535 I llama_init_from_model: n_ctx         = 128
0.00.385.536 I llama_init_from_model: n_ctx_per_seq = 128
0.00.385.536 I llama_init_from_model: n_batch       = 128
0.00.385.536 I llama_init_from_model: n_ubatch      = 128
0.00.385.537 I llama_init_from_model: flash_attn    = 0
0.00.385.538 I llama_init_from_model: freq_base     = 10000.0
0.00.385.538 I llama_init_from_model: freq_scale    = 1
0.00.385.539 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.385.541 I ggml_metal_init: allocating
0.00.385.563 I ggml_metal_init: found device: Apple M4
0.00.385.572 I ggml_metal_init: picking default device: Apple M4
0.00.386.889 I ggml_metal_init: using embedded metal library
0.00.392.618 I ggml_metal_init: GPU name:   Apple M4
0.00.392.621 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.392.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.392.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.392.624 I ggml_metal_init: simdgroup reduction   = true
0.00.392.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.392.624 I ggml_metal_init: has residency sets    = true
0.00.392.624 I ggml_metal_init: has bfloat            = true
0.00.392.625 I ggml_metal_init: use bfloat            = true
0.00.392.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.392.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.678 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.412.103 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.412.107 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.412.132 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.415.622 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.415.624 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.415.624 I llama_init_from_model: graph nodes  = 967
0.00.415.624 I llama_init_from_model: graph splits = 2
0.00.415.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.415.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.859 I 
0.00.451.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.960 I perplexity: tokenizing the input ..
0.00.459.150 I perplexity: tokenization took 7.187 ms
0.00.459.167 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.600.558 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.601.898 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.601.912 I llama_perf_context_print:        load time =     442.97 ms
0.00.601.912 I llama_perf_context_print: prompt eval time =     140.36 ms /   128 tokens (    1.10 ms per token,   911.93 tokens per second)
0.00.601.913 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.601.913 I llama_perf_context_print:       total time =     150.06 ms /   129 tokens
0.00.602.278 I ggml_metal_free: deallocating

real	0m0.616s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.187 I build: 4566 (a5203b44) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.575 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.316 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.318 I llama_model_loader: - type  f32:  194 tensors
0.00.031.318 I llama_model_loader: - type  f16:   98 tensors
0.00.031.319 I print_info: file format = GGUF V3 (latest)
0.00.031.320 I print_info: file type   = all F32 (guessed)
0.00.031.326 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.039.330 I load: special tokens cache size = 25
0.00.045.168 I load: token to piece cache size = 0.2984 MB
0.00.045.172 I print_info: arch             = gptneox
0.00.045.172 I print_info: vocab_only       = 0
0.00.045.172 I print_info: n_ctx_train      = 2048
0.00.045.172 I print_info: n_embd           = 2048
0.00.045.172 I print_info: n_layer          = 24
0.00.045.176 I print_info: n_head           = 16
0.00.045.176 I print_info: n_head_kv        = 16
0.00.045.178 I print_info: n_rot            = 32
0.00.045.179 I print_info: n_swa            = 0
0.00.045.179 I print_info: n_embd_head_k    = 128
0.00.045.179 I print_info: n_embd_head_v    = 128
0.00.045.179 I print_info: n_gqa            = 1
0.00.045.180 I print_info: n_embd_k_gqa     = 2048
0.00.045.181 I print_info: n_embd_v_gqa     = 2048
0.00.045.181 I print_info: f_norm_eps       = 1.0e-05
0.00.045.182 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.183 I print_info: f_logit_scale    = 0.0e+00
0.00.045.184 I print_info: n_ff             = 8192
0.00.045.184 I print_info: n_expert         = 0
0.00.045.184 I print_info: n_expert_used    = 0
0.00.045.184 I print_info: causal attn      = 1
0.00.045.184 I print_info: pooling type     = 0
0.00.045.185 I print_info: rope type        = 2
0.00.045.185 I print_info: rope scaling     = linear
0.00.045.185 I print_info: freq_base_train  = 10000.0
0.00.045.185 I print_info: freq_scale_train = 1
0.00.045.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.186 I print_info: rope_finetuned   = unknown
0.00.045.186 I print_info: ssm_d_conv       = 0
0.00.045.186 I print_info: ssm_d_inner      = 0
0.00.045.186 I print_info: ssm_d_state      = 0
0.00.045.186 I print_info: ssm_dt_rank      = 0
0.00.045.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.187 I print_info: model type       = 1.4B
0.00.045.187 I print_info: model params     = 1.41 B
0.00.045.187 I print_info: general.name     = 1.4B
0.00.045.188 I print_info: vocab type       = BPE
0.00.045.188 I print_info: n_vocab          = 50304
0.00.045.190 I print_info: n_merges         = 50009
0.00.045.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.190 I print_info: LF token         = 128 'Ä'
0.00.045.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.191 I print_info: max token length = 1024
0.01.287.508 I load_tensors: offloading 24 repeating layers to GPU
0.01.287.514 I load_tensors: offloading output layer to GPU
0.01.287.514 I load_tensors: offloaded 25/25 layers to GPU
0.01.287.541 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.287.543 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.288.293 I llama_init_from_model: n_seq_max     = 1
0.01.288.296 I llama_init_from_model: n_ctx         = 128
0.01.288.296 I llama_init_from_model: n_ctx_per_seq = 128
0.01.288.296 I llama_init_from_model: n_batch       = 128
0.01.288.296 I llama_init_from_model: n_ubatch      = 128
0.01.288.296 I llama_init_from_model: flash_attn    = 0
0.01.288.297 I llama_init_from_model: freq_base     = 10000.0
0.01.288.297 I llama_init_from_model: freq_scale    = 1
0.01.288.298 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.288.299 I ggml_metal_init: allocating
0.01.288.384 I ggml_metal_init: found device: Apple M4
0.01.288.390 I ggml_metal_init: picking default device: Apple M4
0.01.289.455 I ggml_metal_init: using embedded metal library
0.01.293.468 I ggml_metal_init: GPU name:   Apple M4
0.01.293.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.293.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.293.472 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.293.472 I ggml_metal_init: simdgroup reduction   = true
0.01.293.472 I ggml_metal_init: simdgroup matrix mul. = true
0.01.293.472 I ggml_metal_init: has residency sets    = true
0.01.293.472 I ggml_metal_init: has bfloat            = true
0.01.293.472 I ggml_metal_init: use bfloat            = true
0.01.293.473 I ggml_metal_init: hasUnifiedMemory      = true
0.01.293.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.305.019 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.306.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.306.845 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.306.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.308.620 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.308.621 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.308.622 I llama_init_from_model: graph nodes  = 967
0.01.308.622 I llama_init_from_model: graph splits = 2
0.01.308.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.308.624 I 
0.01.308.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.308.661 I compute_imatrix: tokenizing the input ..
0.01.312.913 I compute_imatrix: tokenization took 4.251 ms
0.01.312.915 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.581.832 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.584.394 I llama_perf_context_print:        load time =    1565.92 ms
0.01.584.395 I llama_perf_context_print: prompt eval time =     267.10 ms /   128 tokens (    2.09 ms per token,   479.22 tokens per second)
0.01.584.396 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.584.396 I llama_perf_context_print:       total time =    1568.47 ms /   129 tokens
0.01.585.003 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.103s
sys	0m0.261s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4566 (a5203b44)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b304f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b3055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b305a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b305fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b306590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b306b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b3070f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b3076a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b307c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b308150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b308650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b308b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b309670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b309e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b30a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b30ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b30b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b30bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b30c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b30ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b30d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b30d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b30dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b30e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b30efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b30f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b30f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b3104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b310a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b310ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b311180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b311cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b312210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b3124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b312970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b312e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b3132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b313750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b313bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b314090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b3149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b314e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b315130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b315740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b315d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b316670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b316c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b317290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b3178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b317eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b3184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b318ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b3192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b319760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b319c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b319ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b31a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b31acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b31af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b31b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b31b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b31bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b31c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b31c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b31cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b31cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b31d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b31d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b31ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b31e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b31e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b31ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b31f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b31f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b31fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b320190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b3206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b320c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b321180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b3216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b321c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b322170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b3226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b322c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b323160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b3236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b323c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b324150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b3246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b324bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b325140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b325690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b325be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b326130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b326680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b316360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b326af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b3272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b3277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b327d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b328290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b3287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b328d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b329280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b3297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b329d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b32a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b32a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b32ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b32b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b32b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b32bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b32c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b32c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b32ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b32ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b32d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b32d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b32dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b32e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b32e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b32ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b32ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b32f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b32f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b32fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b3301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b330650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b330af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b330f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b331430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b3318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b331d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b332210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b3326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b332b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b332ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b333490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b333930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b334270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b334710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b334bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b335050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b3354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b335990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b335e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b3362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b336770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b336c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b3370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b337550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b3379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b337e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b338330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b3387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b338c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b339110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b3395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b339a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b339ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b33a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b33a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b33acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b33b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b33b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b33bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b33bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b33c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b33c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b33cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b33d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b33d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b33db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b33dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b33e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b33e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b33ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b33f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b33f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b33fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b340010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b3404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b340950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b340df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b341730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b341bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b342070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b342510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b3429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b342f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b343450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b3439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b343ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b3441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b3447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b344dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b3453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b345bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b346070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b346330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b346940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b346f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b347740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b347be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b348080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b348520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b348cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b349220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b349770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b349cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b34a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b34a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b34acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b34b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b34b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b34bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b34c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b34c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b34cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b34d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b34d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b34dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b34e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b34e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b34ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b34f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b34f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b34fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b3501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b350700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b350c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b3511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b3516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b351c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b352190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b3526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b352c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b353180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b3536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b353c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b354170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b3546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b354c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b355160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b3556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b355c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b356150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b3566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b356bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b357140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b357690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b357be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b358130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b358680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b358bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b359120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b359670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b359bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b35a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b35a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b35abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b35b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b35b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b35baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b35bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b35c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b35c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b35cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b35d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b35d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b35db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b35dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b35e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b35e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b35edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b35f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b35f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b35fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b360100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b360820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b360f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b361660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b361d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b362040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b362830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b362af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b363100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.711.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b208d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b2091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b209650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b209ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b209f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b20a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b20a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b20ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b20b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b20b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b20b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b20c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b20cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b20d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b20dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b20e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b20e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b20f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b20f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b20ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b210670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b210d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b2114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b211bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b2122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b2125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b212870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b212ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b213150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b2135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b213a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b213f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b2143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b214690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b214b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b214f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b2153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b215850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b215cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b216130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b2165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b216a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b216e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b2172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b217760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b217bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b218040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b2184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b218920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b218d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b219200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b219670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b219ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b219f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b21a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b21a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b21ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b21b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b21b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b21bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b21bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b21c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b21c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b21cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b21d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b21d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b21da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b21df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b21e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b21e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b21ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b21f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b21f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b21f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b21fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b220280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b2206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b220b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b220fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b221440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b2218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b221d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b222190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b222600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b222a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b222ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b223350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b2237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b223c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b2240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b224510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b224980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b224df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b225260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b2256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b225b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b225fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b226420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b226890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b226d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b227170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b2275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b227a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b227ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b228330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b2287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b228c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b229080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b2294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b229960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b229dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b22a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b22a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b22ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b22af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b22b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b22b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b22bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b22c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b22c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b22ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b22cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b22d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b22d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b22dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b22e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b22e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b22e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b22edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b22f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b22f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b22fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b22ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b2303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b230850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b230cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b231130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b2315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b231a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b231e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b2322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b232760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b232bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b233040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b2334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b233920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b233d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b234200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b234670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b234ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b234f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b2353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b235830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b235ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b236110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b236580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b2369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b236e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b2372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b237740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b237bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b238020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b238490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b238900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b238d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b2391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b239e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b23a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b23a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b23a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b23ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b23b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b23b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b23b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b23be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b23c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b23c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b23cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b23cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b23d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b23d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b23dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b23e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b23e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b23ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b23ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b23f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b23f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b23fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b2400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b240530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b2409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b240e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b241280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b2416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b241b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b241fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b242440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b2428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b242d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b243190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b243600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b243b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b244070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b2444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b244950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b244dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b245230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b245750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b245c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b2467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b246a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b247050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b247610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b247bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b248190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b248750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b248d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b2492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b249890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b249e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b24a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b24a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b24af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b24b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b24bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b24c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b24c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b24cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b24d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b24d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b24dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b24e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b24e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b24eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b24f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b24fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b250010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b2505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b250b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b251150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b251710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b251cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b252290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b252850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b252e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b2533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b253990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b253f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b254510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b254ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b255090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b255c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b2561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b256790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b256d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b257310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b2578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b257e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b258450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b258a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b258fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b259590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b259b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b25a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b25a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b25ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b25b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b25b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b25bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b25c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b25c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b25ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b25cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b25d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b25d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b25de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b25e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b25e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b25ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b25f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b25f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b2601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b2608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b260fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b261700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b2619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b2621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b262470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b262a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b25fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b250890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b24f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b24c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b249b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b259290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b256a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b2547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b252550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b24a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b247e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b24cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b24e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b253690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b2502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b258150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b24bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b255350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b24ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b250e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b24b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b259850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b248a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b247310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b249590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b259e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b24f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b2575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b24d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b24fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b253c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b24b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b254210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b255910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b24a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b258710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b255ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b2519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b25a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b248fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b25a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b248450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b258cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b252b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b254d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b257b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b256490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b24e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b245f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b208910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b261c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b20fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b263160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b263420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b2636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b2639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b263c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b263f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b2641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b2644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b264760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b264a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b264ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b264fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b265260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b265520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b2657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b265aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b265d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b266020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b2662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b2665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b266860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b266b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b266de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b2670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b267360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b267620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b2678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b267ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b267e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b268120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b2683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b2686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b268960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b268c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b268ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b2691a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b269460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b269720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b2699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b269ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b269f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b26a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b26a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b26a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b26aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b26ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b26afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b26b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b26b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b26b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b26bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b26bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b26c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b26c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b26c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b26c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b26cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b26ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b26d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b26d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b26d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b26d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b26dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b26dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b26e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b26e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b26e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b26e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b26ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b26ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b26f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b26f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b26f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b26fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b26fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b26ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b270260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b270520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b2707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b270aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b270d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b271020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b2712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b2715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b271860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b271b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b271de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b2720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b272360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b272620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b2728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b272ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b272e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b273120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b2733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b2736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b273960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b273c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b273ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b2741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b274460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b274720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b2749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b274ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b274f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b275220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b2754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b2757a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b275a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b275d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b275fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b2762a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b276560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b276820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b276ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b276da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b277060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b277320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b2775e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b2778a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b277b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b277e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b2780e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b2783a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b278660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b278920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b278be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b278ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b279160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b279420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b2796e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b2799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b279c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b279f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b27a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b27a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b27a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b27aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b27ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b27afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b27b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b27b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b27b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b27baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b27bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b27c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b27c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b27c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b27c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b27cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b27cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b27d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b27d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b27d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b27d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b27dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b27de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b27e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b27e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b27e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b27ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b27ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b27f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b27f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b27fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b2801e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b280730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b280c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b2811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b281720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b281c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b2821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b282710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b282c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b2831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b283700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b283c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b2841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b2846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b284c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b285190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b2856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b285c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b286180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b2866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b286c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b287170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b2876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b287c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b288160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b2886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b288c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b289150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b2896a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b289bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b28a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b28a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b28abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b28b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b28b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b28bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b28c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b28c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b28cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b28d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b28d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b28dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b28e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b28e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b28eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b28f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b28f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b28fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b2900e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b290630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b2908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b290bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b2910b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b2915b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b291ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b291fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b2924b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b2929b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b292eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b2933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b2938b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b293db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b2942b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b2947b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b294cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b2951b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b295bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b2962e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b296a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b297120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b2973e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b297bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b297e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b2984a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.772s
user	0m0.280s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4566 (a5203b44)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a60d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a60dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a60e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a60eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a60f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a60f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a60fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a6101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a610780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a611180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a611680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a6121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a613160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a613fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a6146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a6155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a6163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a616b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a6173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a617ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a6183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a61a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a61ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a61b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a61b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a61b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a61bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a61c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a61c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a61cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a61d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a61d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a61d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a61dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a61e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a61f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a61f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c104230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c1046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c104b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c104f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c1053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c105860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c105cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c106240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c106740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c106bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c107020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c107490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c107900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c107d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c1081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c108650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c108ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c108f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c1093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c109810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c109c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c10a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c10a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c10a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c10ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c10b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c10b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c10bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c10c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c10c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c10c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c10cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c10d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c10da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c10df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c10e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c10eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c10f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c10f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c10fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c110160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c110710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c110cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c111270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c111820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c111dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c112380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c112930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c112ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c113490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c113a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c113ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c1145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c114b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c115100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c1156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c115c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c116210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c1167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c116d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c117320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c1178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c117e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c118430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c1189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c118ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c1193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c1198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c119de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c11a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c11a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c11ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c11b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c11b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c11bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c11c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c11c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c11cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c11cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c11d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c11d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c11dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c11e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c11e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c11ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c11f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c11f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c11fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c1201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c1206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c120be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c1210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c1215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c121ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c121fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c1224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c1229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c122ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c1233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c1238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c123de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c1242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c1247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c124ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c1251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c1256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c125be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c1260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c1265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c126ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c126fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c1274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c1279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c127ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c1283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c1288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c128de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c1292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c1297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c129ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c12a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c12a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c12abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c12b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c12b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c12bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c12bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c12c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c12c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c12cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c12d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c12d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c12dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c12e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c12e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c12ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c12f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c12f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c12fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c1300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c1305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c130ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c130fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c1314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c1319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c131f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c132540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c132af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c1330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c1336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c133cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c1342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c134ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c134f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c135220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c135830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c135e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c136630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c136ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c136f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c137410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c137bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c138110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c138660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c138bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c139100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c139650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c139ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c13a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c13a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c13ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c13b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c13b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c13bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c13c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c13c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c13cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c13d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c13d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c13db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c13e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c13e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c13eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c13f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c13f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c13fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c140090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c1405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c140b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c141080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c1415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c141b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c142070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c1425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c142b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c143060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c1435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c143b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c144050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c1445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c144af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c145040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c145590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c145ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c146030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c146580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c146ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c147020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c147570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c147ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c148010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c148560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c148ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c149000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c149550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c149aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c149ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c14a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c14a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c14ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c14b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c14b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c14bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c14c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c14c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c14ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c14cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c14d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c14d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c14dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c14e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c14e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c14eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c14eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c14f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c14fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c150550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c150c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c150f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c151720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c1519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c151ff0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a60f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a60edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a60e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a60e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a61f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a61e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a61df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a617050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a611940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a60ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a61a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a618660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a61eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a61fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a61fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a620190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a6206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a620ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a6213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a621920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a621e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a622580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a622f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a623570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a623b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a624190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a624980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a624e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a625970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a625eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a626610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a626ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a626f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a6273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a627890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a6281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a628670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a628b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a6293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a6299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a62a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a62a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a62ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a62b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a62b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a62be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a62c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a62cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a62d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a62d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a62d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a62de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a62e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a62eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a62ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a62f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a62f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a62fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a630210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a6306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a630b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a631490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a631dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a632270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a6327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a6337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a633d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a634250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a6347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a634cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a635790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a635ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a636230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a636780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a637220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a637cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a638760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a639200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a639750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a639ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a63a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a63a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a63ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a63b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a63b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a63bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a63cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a63d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a63d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a63dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a63e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a63e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a63ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a63f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a63f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a63fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a640030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a6404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a640970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a640e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a6412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a641750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a641bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a642530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a6429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a642e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a6437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a643c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a6440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a645370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a645810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a646150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a6465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a646a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a646f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a6473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a647d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a6481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a648af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a648f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a649430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a6498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a64a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a64a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a64ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a64aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a64b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a64b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a64bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a64c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a64c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a64cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a64d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a64d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a64d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a64de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a64e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a64e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a64ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a64f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a64f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a64f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a64fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a650330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a6507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a650c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a6515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a651a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a651ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a652830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a652cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a653170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a653610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a653ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a653f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a6543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a654890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a654d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a6551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a655670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a655b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a655fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a656450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a6568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a6578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a6580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a658700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a659b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a659fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a65a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a65a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a65ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a65b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a65bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a65bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a65c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a65cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a65d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a65d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a65dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a65e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a65e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a65ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a65f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a65f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a65fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a660130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a660680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a660bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a661120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a661670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a661bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a662110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a662660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a662bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a663100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a663650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a663ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a6640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a664640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a664b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a6650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a665630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a665b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a6660d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a666620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a666b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a6670c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a667610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a667b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a6680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a668600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a668b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a6690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a6695f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a669b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a66a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a66a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a66ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a66b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a66b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a66bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a66c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a66c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a66cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a66d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a66d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a66db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a66e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a66e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a66eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a66f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a66f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a66fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a66fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a670370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a670810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a670cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a671150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a6715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a671a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a671f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a6723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a672870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a672d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a6731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a673650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a673af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a674040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a674760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a674e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a6755a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a675cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a675f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a676770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a676a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a677040 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c0046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c0058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c0065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c006fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c007440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c007ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c0085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c008d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c0095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c009cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c00a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c00ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c00b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c00b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c00c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c00c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c00cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c00d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c00dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c00e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c00e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c00e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c00ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c00f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c00f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c00fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c00fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c010130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c0105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c010e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c0112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c011760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c011bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c012040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c0124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c012920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c012d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c013200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c013670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c013ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c013f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c0143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c014830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c0159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c0162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c016d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c0171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c017620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c017a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c017f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c018370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c0187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c018c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c0190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c019530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c0199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c019e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c01a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c01a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c01ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c01afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c01b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c01b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c01bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c01c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c01c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c01ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c01cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c01d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c01d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c01dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c01e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c01e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c01e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c01edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c01f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c01f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c01fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c01ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c020420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c020890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c020d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c021170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c0215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c021a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c021ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c022330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c0227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c022c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c023080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c0234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c023d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c024040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c0244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c024920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c024d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c025200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c025670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c025ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c025f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c0263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c026830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c026ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c027110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c027580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c0279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c027e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c0282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c028740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c028bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c029020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c029490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c029900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c029d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c02a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c02a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c02aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c02af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c02b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c02b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c02bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c02c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c02c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c02c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c02ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c02d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c02d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c02db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c02e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c02e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c02e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c02ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c02f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c02f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c02faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c02ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c030380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c0307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c030c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c0310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c031540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c0319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c031e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c032290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c032700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c032b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c032fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c033450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c0338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c033d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c0341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c034610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c034a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c034ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c035360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c0357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c035c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c0360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c036520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c036990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c036e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c037270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c0376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c037b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c037fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c038430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c0388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c038d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c0395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c039a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c039ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c03a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c03a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c03ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c03b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c03b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c03b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c03bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c03c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c03c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c03cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c03cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c03d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c03d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c03dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c03e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c03e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c03ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c03eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c03f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c03f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c03fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c040070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c0404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c040950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c041db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c042070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c042330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c0427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c042c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c043080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c0434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c043960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c043dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c0446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c044b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c044f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c045400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c045870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c045ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c046150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c0465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c046a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c046ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c047310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c047780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c047bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c048060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c0484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c048940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c048db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c049220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c049690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c049b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c049f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c04a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c04a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c04acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c04b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c04b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c04ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c04be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c04c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c04c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c04cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c04d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c04d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c04d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c04dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c04e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c04e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c04eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c04ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c04f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c04f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c04fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c050110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c050580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c0509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c050e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c0512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c051740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c051bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c052020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c052490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c052900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c052d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c0531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c053650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c053ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c053f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c0543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c054c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c0550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c055560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c0559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c056440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c056b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c057280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c0579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c057c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c0580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c0586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c058ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.235s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
