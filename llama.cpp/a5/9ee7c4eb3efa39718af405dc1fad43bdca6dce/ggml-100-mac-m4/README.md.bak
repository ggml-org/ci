### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.66 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.64 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.27 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.91 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.96 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.73 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.23 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.83 sec*proc (28 tests)

Total Test time (real) = 221.84 sec

real	3m41.937s
user	7m35.382s
sys	0m6.312s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.32 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.39 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.18 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.02 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.18 sec*proc (28 tests)

Total Test time (real) =  51.19 sec

real	0m51.207s
user	1m11.325s
sys	0m5.611s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.081 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.297 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.254 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.263 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.263 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.264 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.265 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.265 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.267 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.268 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.268 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.272 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.272 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.275 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.276 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.276 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.277 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.277 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.278 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.279 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.830 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.832 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.833 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.833 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.834 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.834 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.835 I llama_model_loader: - type  f32:  124 tensors
0.00.026.835 I llama_model_loader: - type  f16:   73 tensors
0.00.026.836 I print_info: file format = GGUF V3 (latest)
0.00.026.850 I print_info: file type   = F16
0.00.026.852 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.006 I load: special tokens cache size = 5
0.00.032.959 I load: token to piece cache size = 0.2032 MB
0.00.032.963 I print_info: arch             = bert
0.00.032.963 I print_info: vocab_only       = 0
0.00.032.963 I print_info: n_ctx_train      = 512
0.00.032.963 I print_info: n_embd           = 384
0.00.032.964 I print_info: n_layer          = 12
0.00.032.967 I print_info: n_head           = 12
0.00.032.968 I print_info: n_head_kv        = 12
0.00.032.968 I print_info: n_rot            = 32
0.00.032.970 I print_info: n_swa            = 0
0.00.032.971 I print_info: n_embd_head_k    = 32
0.00.032.971 I print_info: n_embd_head_v    = 32
0.00.032.972 I print_info: n_gqa            = 1
0.00.032.972 I print_info: n_embd_k_gqa     = 384
0.00.032.973 I print_info: n_embd_v_gqa     = 384
0.00.032.974 I print_info: f_norm_eps       = 1.0e-12
0.00.032.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.975 I print_info: f_logit_scale    = 0.0e+00
0.00.032.976 I print_info: n_ff             = 1536
0.00.032.976 I print_info: n_expert         = 0
0.00.032.976 I print_info: n_expert_used    = 0
0.00.032.976 I print_info: causal attn      = 0
0.00.032.978 I print_info: pooling type     = 2
0.00.032.979 I print_info: rope type        = 2
0.00.032.979 I print_info: rope scaling     = linear
0.00.032.979 I print_info: freq_base_train  = 10000.0
0.00.032.980 I print_info: freq_scale_train = 1
0.00.032.980 I print_info: n_ctx_orig_yarn  = 512
0.00.032.980 I print_info: rope_finetuned   = unknown
0.00.032.980 I print_info: ssm_d_conv       = 0
0.00.032.981 I print_info: ssm_d_inner      = 0
0.00.032.981 I print_info: ssm_d_state      = 0
0.00.032.981 I print_info: ssm_dt_rank      = 0
0.00.032.981 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.981 I print_info: model type       = 33M
0.00.032.982 I print_info: model params     = 33.21 M
0.00.032.982 I print_info: general.name     = Bge Small
0.00.032.987 I print_info: vocab type       = WPM
0.00.032.988 I print_info: n_vocab          = 30522
0.00.032.988 I print_info: n_merges         = 0
0.00.032.988 I print_info: BOS token        = 101 '[CLS]'
0.00.032.988 I print_info: UNK token        = 100 '[UNK]'
0.00.032.989 I print_info: SEP token        = 102 '[SEP]'
0.00.032.989 I print_info: PAD token        = 0 '[PAD]'
0.00.032.990 I print_info: MASK token       = 103 '[MASK]'
0.00.032.990 I print_info: LF token         = 0 '[PAD]'
0.00.032.992 I print_info: max token length = 21
0.00.034.970 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.972 I load_tensors: offloading output layer to GPU
0.00.034.972 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.998 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.999 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.035.216 I llama_init_from_model: n_seq_max     = 1
0.00.035.218 I llama_init_from_model: n_ctx         = 512
0.00.035.218 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.218 I llama_init_from_model: n_batch       = 2048
0.00.035.219 I llama_init_from_model: n_ubatch      = 2048
0.00.035.219 I llama_init_from_model: flash_attn    = 0
0.00.035.219 I llama_init_from_model: freq_base     = 10000.0
0.00.035.220 I llama_init_from_model: freq_scale    = 1
0.00.035.220 I ggml_metal_init: allocating
0.00.035.224 I ggml_metal_init: found device: Apple M4
0.00.035.227 I ggml_metal_init: picking default device: Apple M4
0.00.036.014 I ggml_metal_init: using embedded metal library
0.00.040.023 I ggml_metal_init: GPU name:   Apple M4
0.00.040.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.027 I ggml_metal_init: simdgroup reduction   = true
0.00.040.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.027 I ggml_metal_init: has bfloat            = true
0.00.040.027 I ggml_metal_init: use bfloat            = true
0.00.040.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.684 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.248 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.250 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.251 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.016 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.018 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.018 I llama_init_from_model: graph nodes  = 429
0.00.053.018 I llama_init_from_model: graph splits = 2
0.00.053.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.281 I 
0.00.059.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.932 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.797 I llama_perf_context_print:        load time =      42.98 ms
0.00.064.798 I llama_perf_context_print: prompt eval time =       4.71 ms /     9 tokens (    0.52 ms per token,  1909.61 tokens per second)
0.00.064.798 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.799 I llama_perf_context_print:       total time =       5.52 ms /    10 tokens
0.00.064.936 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.297 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.110 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.113 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.115 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.116 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.116 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.116 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.117 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.117 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.118 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.118 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.118 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.120 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.121 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.121 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.121 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.122 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.122 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.531 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.150 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.151 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.151 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.152 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.152 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.152 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.153 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.153 I llama_model_loader: - type  f32:  124 tensors
0.00.015.154 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.154 I print_info: file format = GGUF V3 (latest)
0.00.015.162 I print_info: file type   = Q8_0
0.00.015.163 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.747 I load: special tokens cache size = 5
0.00.019.039 I load: token to piece cache size = 0.2032 MB
0.00.019.042 I print_info: arch             = bert
0.00.019.042 I print_info: vocab_only       = 0
0.00.019.042 I print_info: n_ctx_train      = 512
0.00.019.042 I print_info: n_embd           = 384
0.00.019.043 I print_info: n_layer          = 12
0.00.019.045 I print_info: n_head           = 12
0.00.019.046 I print_info: n_head_kv        = 12
0.00.019.046 I print_info: n_rot            = 32
0.00.019.049 I print_info: n_swa            = 0
0.00.019.049 I print_info: n_embd_head_k    = 32
0.00.019.049 I print_info: n_embd_head_v    = 32
0.00.019.050 I print_info: n_gqa            = 1
0.00.019.050 I print_info: n_embd_k_gqa     = 384
0.00.019.051 I print_info: n_embd_v_gqa     = 384
0.00.019.056 I print_info: f_norm_eps       = 1.0e-12
0.00.019.057 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.057 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.057 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.063 I print_info: f_logit_scale    = 0.0e+00
0.00.019.064 I print_info: n_ff             = 1536
0.00.019.064 I print_info: n_expert         = 0
0.00.019.064 I print_info: n_expert_used    = 0
0.00.019.064 I print_info: causal attn      = 0
0.00.019.064 I print_info: pooling type     = 2
0.00.019.064 I print_info: rope type        = 2
0.00.019.065 I print_info: rope scaling     = linear
0.00.019.065 I print_info: freq_base_train  = 10000.0
0.00.019.065 I print_info: freq_scale_train = 1
0.00.019.065 I print_info: n_ctx_orig_yarn  = 512
0.00.019.065 I print_info: rope_finetuned   = unknown
0.00.019.066 I print_info: ssm_d_conv       = 0
0.00.019.066 I print_info: ssm_d_inner      = 0
0.00.019.066 I print_info: ssm_d_state      = 0
0.00.019.066 I print_info: ssm_dt_rank      = 0
0.00.019.066 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.066 I print_info: model type       = 33M
0.00.019.066 I print_info: model params     = 33.21 M
0.00.019.067 I print_info: general.name     = Bge Small
0.00.019.067 I print_info: vocab type       = WPM
0.00.019.067 I print_info: n_vocab          = 30522
0.00.019.067 I print_info: n_merges         = 0
0.00.019.068 I print_info: BOS token        = 101 '[CLS]'
0.00.019.068 I print_info: UNK token        = 100 '[UNK]'
0.00.019.068 I print_info: SEP token        = 102 '[SEP]'
0.00.019.068 I print_info: PAD token        = 0 '[PAD]'
0.00.019.068 I print_info: MASK token       = 103 '[MASK]'
0.00.019.069 I print_info: LF token         = 0 '[PAD]'
0.00.019.069 I print_info: max token length = 21
0.00.020.360 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.360 I load_tensors: offloading output layer to GPU
0.00.020.360 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.368 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.369 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.511 I llama_init_from_model: n_seq_max     = 1
0.00.020.511 I llama_init_from_model: n_ctx         = 512
0.00.020.511 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.512 I llama_init_from_model: n_batch       = 2048
0.00.020.512 I llama_init_from_model: n_ubatch      = 2048
0.00.020.512 I llama_init_from_model: flash_attn    = 0
0.00.020.512 I llama_init_from_model: freq_base     = 10000.0
0.00.020.513 I llama_init_from_model: freq_scale    = 1
0.00.020.513 I ggml_metal_init: allocating
0.00.020.516 I ggml_metal_init: found device: Apple M4
0.00.020.518 I ggml_metal_init: picking default device: Apple M4
0.00.021.133 I ggml_metal_init: using embedded metal library
0.00.023.652 I ggml_metal_init: GPU name:   Apple M4
0.00.023.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.655 I ggml_metal_init: simdgroup reduction   = true
0.00.023.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.656 I ggml_metal_init: has bfloat            = true
0.00.023.656 I ggml_metal_init: use bfloat            = true
0.00.023.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.058 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.539 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.541 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.544 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.133 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.134 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.134 I llama_init_from_model: graph nodes  = 429
0.00.035.134 I llama_init_from_model: graph splits = 2
0.00.035.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.577 I 
0.00.039.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.119 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.337 I llama_perf_context_print:        load time =      30.28 ms
0.00.044.338 I llama_perf_context_print: prompt eval time =       4.08 ms /     9 tokens (    0.45 ms per token,  2205.88 tokens per second)
0.00.044.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.339 I llama_perf_context_print:       total time =       4.76 ms /    10 tokens
0.00.044.515 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.287 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.459 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.589 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.596 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.598 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.599 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.600 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.601 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.602 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.602 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.605 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.605 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.608 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.609 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.609 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.610 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.700 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.700 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.701 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.701 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.702 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.702 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.702 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.703 I llama_model_loader: - type  f32:   40 tensors
0.00.048.703 I llama_model_loader: - type  f16:   30 tensors
0.00.048.704 I print_info: file format = GGUF V3 (latest)
0.00.048.727 I print_info: file type   = F16
0.00.048.728 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.064.566 W load: empty token at index 5
0.00.068.754 W load: model vocab missing newline token, using special_pad_id instead
0.00.070.017 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.048 I load: special tokens cache size = 5
0.00.333.663 I load: token to piece cache size = 1.5060 MB
0.00.333.671 I print_info: arch             = jina-bert-v2
0.00.333.671 I print_info: vocab_only       = 0
0.00.333.671 I print_info: n_ctx_train      = 8192
0.00.333.671 I print_info: n_embd           = 384
0.00.333.672 I print_info: n_layer          = 4
0.00.333.679 I print_info: n_head           = 12
0.00.333.680 I print_info: n_head_kv        = 12
0.00.333.680 I print_info: n_rot            = 32
0.00.333.681 I print_info: n_swa            = 0
0.00.333.681 I print_info: n_embd_head_k    = 32
0.00.333.681 I print_info: n_embd_head_v    = 32
0.00.333.682 I print_info: n_gqa            = 1
0.00.333.682 I print_info: n_embd_k_gqa     = 384
0.00.333.683 I print_info: n_embd_v_gqa     = 384
0.00.333.684 I print_info: f_norm_eps       = 1.0e-12
0.00.333.685 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.333.685 I print_info: f_clamp_kqv      = 0.0e+00
0.00.333.686 I print_info: f_max_alibi_bias = 8.0e+00
0.00.333.686 I print_info: f_logit_scale    = 0.0e+00
0.00.333.687 I print_info: n_ff             = 1536
0.00.333.688 I print_info: n_expert         = 0
0.00.333.688 I print_info: n_expert_used    = 0
0.00.333.688 I print_info: causal attn      = 0
0.00.333.688 I print_info: pooling type     = -1
0.00.333.688 I print_info: rope type        = -1
0.00.333.689 I print_info: rope scaling     = linear
0.00.333.689 I print_info: freq_base_train  = 10000.0
0.00.333.689 I print_info: freq_scale_train = 1
0.00.333.689 I print_info: n_ctx_orig_yarn  = 8192
0.00.333.693 I print_info: rope_finetuned   = unknown
0.00.333.693 I print_info: ssm_d_conv       = 0
0.00.333.693 I print_info: ssm_d_inner      = 0
0.00.333.693 I print_info: ssm_d_state      = 0
0.00.333.693 I print_info: ssm_dt_rank      = 0
0.00.333.693 I print_info: ssm_dt_b_c_rms   = 0
0.00.333.693 I print_info: model type       = 33M
0.00.333.694 I print_info: model params     = 32.90 M
0.00.333.694 I print_info: general.name     = Jina Bert Implementation
0.00.333.696 I print_info: vocab type       = BPE
0.00.333.696 I print_info: n_vocab          = 61056
0.00.333.696 I print_info: n_merges         = 39382
0.00.333.696 I print_info: BOS token        = 0 '<s>'
0.00.333.696 I print_info: EOS token        = 2 '</s>'
0.00.333.697 I print_info: UNK token        = 3 '<unk>'
0.00.333.697 I print_info: SEP token        = 2 '</s>'
0.00.333.697 I print_info: PAD token        = 1 '<pad>'
0.00.333.697 I print_info: MASK token       = 4 '<mask>'
0.00.333.698 I print_info: EOG token        = 2 '</s>'
0.00.333.698 I print_info: max token length = 45
0.00.335.211 I load_tensors: offloading 4 repeating layers to GPU
0.00.335.211 I load_tensors: offloading output layer to GPU
0.00.335.212 I load_tensors: offloaded 5/5 layers to GPU
0.00.335.241 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.242 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.335.671 I llama_init_from_model: n_seq_max     = 1
0.00.335.672 I llama_init_from_model: n_ctx         = 8192
0.00.335.672 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.335.672 I llama_init_from_model: n_batch       = 2048
0.00.335.672 I llama_init_from_model: n_ubatch      = 2048
0.00.335.672 I llama_init_from_model: flash_attn    = 0
0.00.335.673 I llama_init_from_model: freq_base     = 10000.0
0.00.335.673 I llama_init_from_model: freq_scale    = 1
0.00.335.674 I ggml_metal_init: allocating
0.00.335.679 I ggml_metal_init: found device: Apple M4
0.00.335.681 I ggml_metal_init: picking default device: Apple M4
0.00.336.770 I ggml_metal_init: using embedded metal library
0.00.339.716 I ggml_metal_init: GPU name:   Apple M4
0.00.339.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.718 I ggml_metal_init: simdgroup reduction   = true
0.00.339.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.719 I ggml_metal_init: has bfloat            = true
0.00.339.719 I ggml_metal_init: use bfloat            = true
0.00.339.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.213 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.351.746 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.351.748 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.752 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.486 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.487 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.487 I llama_init_from_model: graph nodes  = 154
0.00.352.487 I llama_init_from_model: graph splits = 2
0.00.352.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.215 I 
0.00.367.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.387 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.388 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.401 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.401 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.407 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.407 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.964 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.681 I llama_perf_context_print:        load time =     344.75 ms
0.00.371.682 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16720.60 tokens per second)
0.00.371.684 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.684 I llama_perf_context_print:       total time =       4.47 ms /    63 tokens
0.00.371.944 I ggml_metal_free: deallocating

real	0m1.103s
user	0m0.342s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.281 I main: llama backend init
0.00.000.291 I main: load the model and apply lora adapter, if any
0.00.028.965 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.325 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.241 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.408 I llama_model_loader: - type  f32:  194 tensors
0.00.060.408 I llama_model_loader: - type  f16:   98 tensors
0.00.060.409 I print_info: file format = GGUF V3 (latest)
0.00.060.429 I print_info: file type   = all F32 (guessed)
0.00.060.431 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.566 I load: special tokens cache size = 25
0.00.096.768 I load: token to piece cache size = 0.2984 MB
0.00.096.771 I print_info: arch             = gptneox
0.00.096.771 I print_info: vocab_only       = 0
0.00.096.771 I print_info: n_ctx_train      = 2048
0.00.096.772 I print_info: n_embd           = 2048
0.00.096.772 I print_info: n_layer          = 24
0.00.096.775 I print_info: n_head           = 16
0.00.096.776 I print_info: n_head_kv        = 16
0.00.096.776 I print_info: n_rot            = 32
0.00.096.776 I print_info: n_swa            = 0
0.00.096.776 I print_info: n_embd_head_k    = 128
0.00.096.776 I print_info: n_embd_head_v    = 128
0.00.096.777 I print_info: n_gqa            = 1
0.00.096.778 I print_info: n_embd_k_gqa     = 2048
0.00.096.778 I print_info: n_embd_v_gqa     = 2048
0.00.096.779 I print_info: f_norm_eps       = 1.0e-05
0.00.096.779 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.779 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.780 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.781 I print_info: f_logit_scale    = 0.0e+00
0.00.096.781 I print_info: n_ff             = 8192
0.00.096.781 I print_info: n_expert         = 0
0.00.096.782 I print_info: n_expert_used    = 0
0.00.096.782 I print_info: causal attn      = 1
0.00.096.782 I print_info: pooling type     = 0
0.00.096.783 I print_info: rope type        = 2
0.00.096.784 I print_info: rope scaling     = linear
0.00.096.784 I print_info: freq_base_train  = 10000.0
0.00.096.784 I print_info: freq_scale_train = 1
0.00.096.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.785 I print_info: rope_finetuned   = unknown
0.00.096.785 I print_info: ssm_d_conv       = 0
0.00.096.786 I print_info: ssm_d_inner      = 0
0.00.096.786 I print_info: ssm_d_state      = 0
0.00.096.786 I print_info: ssm_dt_rank      = 0
0.00.096.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.787 I print_info: model type       = 1.4B
0.00.096.787 I print_info: model params     = 1.41 B
0.00.096.787 I print_info: general.name     = 1.4B
0.00.096.787 I print_info: vocab type       = BPE
0.00.096.788 I print_info: n_vocab          = 50304
0.00.096.788 I print_info: n_merges         = 50009
0.00.096.788 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.788 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.788 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.789 I print_info: LF token         = 128 'Ä'
0.00.096.789 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.789 I print_info: max token length = 1024
0.00.099.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.332 I load_tensors: offloading output layer to GPU
0.00.099.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.350 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.351 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.099.641 I llama_init_from_model: n_seq_max     = 1
0.00.099.642 I llama_init_from_model: n_ctx         = 2048
0.00.099.642 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.099.642 I llama_init_from_model: n_batch       = 2048
0.00.099.642 I llama_init_from_model: n_ubatch      = 512
0.00.099.642 I llama_init_from_model: flash_attn    = 0
0.00.099.643 I llama_init_from_model: freq_base     = 10000.0
0.00.099.643 I llama_init_from_model: freq_scale    = 1
0.00.099.644 I ggml_metal_init: allocating
0.00.099.647 I ggml_metal_init: found device: Apple M4
0.00.099.649 I ggml_metal_init: picking default device: Apple M4
0.00.100.324 I ggml_metal_init: using embedded metal library
0.00.115.454 I ggml_metal_init: GPU name:   Apple M4
0.00.115.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.457 I ggml_metal_init: simdgroup reduction   = true
0.00.115.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.457 I ggml_metal_init: has bfloat            = true
0.00.115.457 I ggml_metal_init: use bfloat            = true
0.00.115.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.418 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.160.414 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.421 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.450 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.161.435 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.161.437 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.161.438 I llama_init_from_model: graph nodes  = 967
0.00.161.438 I llama_init_from_model: graph splits = 2
0.00.161.442 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.554 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.555 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.322 I main: llama threadpool init, n_threads = 4
0.00.243.365 I 
0.00.243.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.392 I 
0.00.243.463 I sampler seed: 1234
0.00.243.468 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.494 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.496 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.076.303 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.02.076.303 I llama_perf_context_print:        load time =     214.34 ms
0.02.076.304 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.79 tokens per second)
0.02.076.305 I llama_perf_context_print:        eval time =    1786.44 ms /    63 runs   (   28.36 ms per token,    35.27 tokens per second)
0.02.076.305 I llama_perf_context_print:       total time =    1832.98 ms /    70 tokens
0.02.076.530 I ggml_metal_free: deallocating

real	0m2.370s
user	0m0.144s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.571 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.626 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.652 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.657 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.657 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.083 I llama_model_loader: - type  f32:  194 tensors
0.00.056.083 I llama_model_loader: - type  f16:   98 tensors
0.00.056.084 I print_info: file format = GGUF V3 (latest)
0.00.056.100 I print_info: file type   = all F32 (guessed)
0.00.056.102 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.169 I load: special tokens cache size = 25
0.00.091.935 I load: token to piece cache size = 0.2984 MB
0.00.091.938 I print_info: arch             = gptneox
0.00.091.938 I print_info: vocab_only       = 0
0.00.091.939 I print_info: n_ctx_train      = 2048
0.00.091.939 I print_info: n_embd           = 2048
0.00.091.939 I print_info: n_layer          = 24
0.00.091.942 I print_info: n_head           = 16
0.00.091.943 I print_info: n_head_kv        = 16
0.00.091.943 I print_info: n_rot            = 32
0.00.091.943 I print_info: n_swa            = 0
0.00.091.943 I print_info: n_embd_head_k    = 128
0.00.091.946 I print_info: n_embd_head_v    = 128
0.00.091.947 I print_info: n_gqa            = 1
0.00.091.948 I print_info: n_embd_k_gqa     = 2048
0.00.091.948 I print_info: n_embd_v_gqa     = 2048
0.00.091.949 I print_info: f_norm_eps       = 1.0e-05
0.00.091.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.949 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.949 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.950 I print_info: f_logit_scale    = 0.0e+00
0.00.091.950 I print_info: n_ff             = 8192
0.00.091.950 I print_info: n_expert         = 0
0.00.091.950 I print_info: n_expert_used    = 0
0.00.091.951 I print_info: causal attn      = 1
0.00.091.951 I print_info: pooling type     = 0
0.00.091.951 I print_info: rope type        = 2
0.00.091.952 I print_info: rope scaling     = linear
0.00.091.952 I print_info: freq_base_train  = 10000.0
0.00.091.953 I print_info: freq_scale_train = 1
0.00.091.953 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.953 I print_info: rope_finetuned   = unknown
0.00.091.953 I print_info: ssm_d_conv       = 0
0.00.091.953 I print_info: ssm_d_inner      = 0
0.00.091.953 I print_info: ssm_d_state      = 0
0.00.091.954 I print_info: ssm_dt_rank      = 0
0.00.091.954 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.954 I print_info: model type       = 1.4B
0.00.091.954 I print_info: model params     = 1.41 B
0.00.091.954 I print_info: general.name     = 1.4B
0.00.091.955 I print_info: vocab type       = BPE
0.00.091.955 I print_info: n_vocab          = 50304
0.00.091.955 I print_info: n_merges         = 50009
0.00.091.955 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.956 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.956 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.956 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.956 I print_info: LF token         = 128 'Ä'
0.00.091.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.957 I print_info: max token length = 1024
0.00.094.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.554 I load_tensors: offloading output layer to GPU
0.00.094.554 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.565 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.566 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.853 I llama_init_from_model: n_seq_max     = 1
0.00.094.854 I llama_init_from_model: n_ctx         = 128
0.00.094.854 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.855 I llama_init_from_model: n_batch       = 128
0.00.094.855 I llama_init_from_model: n_ubatch      = 128
0.00.094.855 I llama_init_from_model: flash_attn    = 0
0.00.094.855 I llama_init_from_model: freq_base     = 10000.0
0.00.094.856 I llama_init_from_model: freq_scale    = 1
0.00.094.856 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.856 I ggml_metal_init: allocating
0.00.094.859 I ggml_metal_init: found device: Apple M4
0.00.094.861 I ggml_metal_init: picking default device: Apple M4
0.00.095.479 I ggml_metal_init: using embedded metal library
0.00.098.116 I ggml_metal_init: GPU name:   Apple M4
0.00.098.117 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.118 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.118 I ggml_metal_init: simdgroup reduction   = true
0.00.098.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.119 I ggml_metal_init: has bfloat            = true
0.00.098.119 I ggml_metal_init: use bfloat            = true
0.00.098.119 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.120 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.007 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.287 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.289 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.303 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.155 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.156 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.156 I llama_init_from_model: graph nodes  = 967
0.00.110.156 I llama_init_from_model: graph splits = 2
0.00.110.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.986 I 
0.00.691.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.084 I perplexity: tokenizing the input ..
0.00.704.324 I perplexity: tokenization took 13.237 ms
0.00.704.331 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.005 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.826.612 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.826.655 I llama_perf_context_print:        load time =     667.14 ms
0.00.826.657 I llama_perf_context_print: prompt eval time =     119.73 ms /   128 tokens (    0.94 ms per token,  1069.10 tokens per second)
0.00.826.658 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.659 I llama_perf_context_print:       total time =     135.67 ms /   129 tokens
0.00.827.469 I ggml_metal_free: deallocating

real	0m1.041s
user	0m0.125s
sys	0m0.166s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.640 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.007 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.009 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.011 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.586 I llama_model_loader: - type  f32:  194 tensors
0.00.033.586 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.587 I print_info: file format = GGUF V3 (latest)
0.00.033.598 I print_info: file type   = Q8_0
0.00.033.599 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.634 I load: special tokens cache size = 25
0.00.059.632 I load: token to piece cache size = 0.2984 MB
0.00.059.636 I print_info: arch             = gptneox
0.00.059.636 I print_info: vocab_only       = 0
0.00.059.636 I print_info: n_ctx_train      = 2048
0.00.059.637 I print_info: n_embd           = 2048
0.00.059.637 I print_info: n_layer          = 24
0.00.059.643 I print_info: n_head           = 16
0.00.059.644 I print_info: n_head_kv        = 16
0.00.059.644 I print_info: n_rot            = 32
0.00.059.644 I print_info: n_swa            = 0
0.00.059.644 I print_info: n_embd_head_k    = 128
0.00.059.646 I print_info: n_embd_head_v    = 128
0.00.059.647 I print_info: n_gqa            = 1
0.00.059.648 I print_info: n_embd_k_gqa     = 2048
0.00.059.650 I print_info: n_embd_v_gqa     = 2048
0.00.059.651 I print_info: f_norm_eps       = 1.0e-05
0.00.059.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.652 I print_info: f_logit_scale    = 0.0e+00
0.00.059.653 I print_info: n_ff             = 8192
0.00.059.653 I print_info: n_expert         = 0
0.00.059.654 I print_info: n_expert_used    = 0
0.00.059.654 I print_info: causal attn      = 1
0.00.059.654 I print_info: pooling type     = 0
0.00.059.654 I print_info: rope type        = 2
0.00.059.654 I print_info: rope scaling     = linear
0.00.059.655 I print_info: freq_base_train  = 10000.0
0.00.059.656 I print_info: freq_scale_train = 1
0.00.059.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.657 I print_info: rope_finetuned   = unknown
0.00.059.657 I print_info: ssm_d_conv       = 0
0.00.059.657 I print_info: ssm_d_inner      = 0
0.00.059.657 I print_info: ssm_d_state      = 0
0.00.059.657 I print_info: ssm_dt_rank      = 0
0.00.059.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.657 I print_info: model type       = 1.4B
0.00.059.658 I print_info: model params     = 1.41 B
0.00.059.658 I print_info: general.name     = 1.4B
0.00.059.658 I print_info: vocab type       = BPE
0.00.059.659 I print_info: n_vocab          = 50304
0.00.059.659 I print_info: n_merges         = 50009
0.00.059.659 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.662 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.662 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.662 I print_info: LF token         = 128 'Ä'
0.00.059.662 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.662 I print_info: max token length = 1024
0.00.061.785 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.785 I load_tensors: offloading output layer to GPU
0.00.061.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.792 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.793 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.123 I llama_init_from_model: n_seq_max     = 1
0.00.062.124 I llama_init_from_model: n_ctx         = 2048
0.00.062.124 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.124 I llama_init_from_model: n_batch       = 2048
0.00.062.125 I llama_init_from_model: n_ubatch      = 512
0.00.062.125 I llama_init_from_model: flash_attn    = 0
0.00.062.125 I llama_init_from_model: freq_base     = 10000.0
0.00.062.125 I llama_init_from_model: freq_scale    = 1
0.00.062.126 I ggml_metal_init: allocating
0.00.062.129 I ggml_metal_init: found device: Apple M4
0.00.062.133 I ggml_metal_init: picking default device: Apple M4
0.00.062.851 I ggml_metal_init: using embedded metal library
0.00.065.455 I ggml_metal_init: GPU name:   Apple M4
0.00.065.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.458 I ggml_metal_init: simdgroup reduction   = true
0.00.065.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.458 I ggml_metal_init: has bfloat            = true
0.00.065.458 I ggml_metal_init: use bfloat            = true
0.00.065.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.079 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.906 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.914 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.022 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.023 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.024 I llama_init_from_model: graph nodes  = 967
0.00.102.024 I llama_init_from_model: graph splits = 2
0.00.102.028 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.254.443 I main: llama threadpool init, n_threads = 4
0.01.254.485 I 
0.01.254.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.254.513 I 
0.01.254.735 I sampler seed: 1234
0.01.254.739 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.254.750 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.254.750 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.254.750 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.337.334 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62335.38 tokens per second)
0.02.337.335 I llama_perf_context_print:        load time =    1244.80 ms
0.02.337.336 I llama_perf_context_print: prompt eval time =      39.87 ms /     7 tokens (    5.70 ms per token,   175.59 tokens per second)
0.02.337.337 I llama_perf_context_print:        eval time =    1039.85 ms /    63 runs   (   16.51 ms per token,    60.59 tokens per second)
0.02.337.337 I llama_perf_context_print:       total time =    1082.89 ms /    70 tokens
0.02.337.608 I ggml_metal_free: deallocating

real	0m2.356s
user	0m0.111s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.256 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.471 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.472 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.658 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.659 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.660 I llama_model_loader: - type  f32:  194 tensors
0.00.031.660 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.661 I print_info: file format = GGUF V3 (latest)
0.00.031.674 I print_info: file type   = Q8_0
0.00.031.677 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.114 I load: special tokens cache size = 25
0.00.061.398 I load: token to piece cache size = 0.2984 MB
0.00.061.400 I print_info: arch             = gptneox
0.00.061.401 I print_info: vocab_only       = 0
0.00.061.401 I print_info: n_ctx_train      = 2048
0.00.061.401 I print_info: n_embd           = 2048
0.00.061.401 I print_info: n_layer          = 24
0.00.061.405 I print_info: n_head           = 16
0.00.061.406 I print_info: n_head_kv        = 16
0.00.061.406 I print_info: n_rot            = 32
0.00.061.406 I print_info: n_swa            = 0
0.00.061.406 I print_info: n_embd_head_k    = 128
0.00.061.406 I print_info: n_embd_head_v    = 128
0.00.061.407 I print_info: n_gqa            = 1
0.00.061.408 I print_info: n_embd_k_gqa     = 2048
0.00.061.408 I print_info: n_embd_v_gqa     = 2048
0.00.061.409 I print_info: f_norm_eps       = 1.0e-05
0.00.061.409 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.409 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.410 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.410 I print_info: f_logit_scale    = 0.0e+00
0.00.061.410 I print_info: n_ff             = 8192
0.00.061.411 I print_info: n_expert         = 0
0.00.061.411 I print_info: n_expert_used    = 0
0.00.061.412 I print_info: causal attn      = 1
0.00.061.412 I print_info: pooling type     = 0
0.00.061.413 I print_info: rope type        = 2
0.00.061.413 I print_info: rope scaling     = linear
0.00.061.413 I print_info: freq_base_train  = 10000.0
0.00.061.413 I print_info: freq_scale_train = 1
0.00.061.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.414 I print_info: rope_finetuned   = unknown
0.00.061.414 I print_info: ssm_d_conv       = 0
0.00.061.414 I print_info: ssm_d_inner      = 0
0.00.061.414 I print_info: ssm_d_state      = 0
0.00.061.414 I print_info: ssm_dt_rank      = 0
0.00.061.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.414 I print_info: model type       = 1.4B
0.00.061.415 I print_info: model params     = 1.41 B
0.00.061.416 I print_info: general.name     = 1.4B
0.00.061.416 I print_info: vocab type       = BPE
0.00.061.417 I print_info: n_vocab          = 50304
0.00.061.417 I print_info: n_merges         = 50009
0.00.061.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.418 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.418 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.419 I print_info: LF token         = 128 'Ä'
0.00.061.419 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.423 I print_info: max token length = 1024
0.00.063.644 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.644 I load_tensors: offloading output layer to GPU
0.00.063.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.655 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.656 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.960 I llama_init_from_model: n_seq_max     = 1
0.00.063.961 I llama_init_from_model: n_ctx         = 128
0.00.063.961 I llama_init_from_model: n_ctx_per_seq = 128
0.00.063.961 I llama_init_from_model: n_batch       = 128
0.00.063.962 I llama_init_from_model: n_ubatch      = 128
0.00.063.962 I llama_init_from_model: flash_attn    = 0
0.00.063.962 I llama_init_from_model: freq_base     = 10000.0
0.00.063.962 I llama_init_from_model: freq_scale    = 1
0.00.063.963 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.963 I ggml_metal_init: allocating
0.00.063.966 I ggml_metal_init: found device: Apple M4
0.00.063.968 I ggml_metal_init: picking default device: Apple M4
0.00.064.573 I ggml_metal_init: using embedded metal library
0.00.067.051 I ggml_metal_init: GPU name:   Apple M4
0.00.067.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.054 I ggml_metal_init: simdgroup reduction   = true
0.00.067.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.054 I ggml_metal_init: has bfloat            = true
0.00.067.054 I ggml_metal_init: use bfloat            = true
0.00.067.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.816 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.077.087 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.090 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.112 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.078.074 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.078.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.078.075 I llama_init_from_model: graph nodes  = 967
0.00.078.075 I llama_init_from_model: graph splits = 2
0.00.078.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.078.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.895.536 I 
0.00.895.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.895.633 I perplexity: tokenizing the input ..
0.00.913.437 I perplexity: tokenization took 17.8 ms
0.00.913.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.054.711 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.056.183 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.056.225 I llama_perf_context_print:        load time =     884.27 ms
0.01.056.226 I llama_perf_context_print: prompt eval time =     140.40 ms /   128 tokens (    1.10 ms per token,   911.65 tokens per second)
0.01.056.227 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.056.228 I llama_perf_context_print:       total time =     160.69 ms /   129 tokens
0.01.056.994 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.103s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.011.639 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.528 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.391 I llama_model_loader: - type  f32:  194 tensors
0.00.028.391 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.392 I print_info: file format = GGUF V3 (latest)
0.00.028.406 I print_info: file type   = Q4_0
0.00.028.407 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.442 I load: special tokens cache size = 25
0.00.053.614 I load: token to piece cache size = 0.2984 MB
0.00.053.617 I print_info: arch             = gptneox
0.00.053.618 I print_info: vocab_only       = 0
0.00.053.618 I print_info: n_ctx_train      = 2048
0.00.053.618 I print_info: n_embd           = 2048
0.00.053.618 I print_info: n_layer          = 24
0.00.053.624 I print_info: n_head           = 16
0.00.053.624 I print_info: n_head_kv        = 16
0.00.053.625 I print_info: n_rot            = 32
0.00.053.625 I print_info: n_swa            = 0
0.00.053.625 I print_info: n_embd_head_k    = 128
0.00.053.628 I print_info: n_embd_head_v    = 128
0.00.053.628 I print_info: n_gqa            = 1
0.00.053.629 I print_info: n_embd_k_gqa     = 2048
0.00.053.631 I print_info: n_embd_v_gqa     = 2048
0.00.053.631 I print_info: f_norm_eps       = 1.0e-05
0.00.053.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.632 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.632 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.632 I print_info: f_logit_scale    = 0.0e+00
0.00.053.634 I print_info: n_ff             = 8192
0.00.053.634 I print_info: n_expert         = 0
0.00.053.634 I print_info: n_expert_used    = 0
0.00.053.634 I print_info: causal attn      = 1
0.00.053.635 I print_info: pooling type     = 0
0.00.053.635 I print_info: rope type        = 2
0.00.053.636 I print_info: rope scaling     = linear
0.00.053.636 I print_info: freq_base_train  = 10000.0
0.00.053.636 I print_info: freq_scale_train = 1
0.00.053.637 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.637 I print_info: rope_finetuned   = unknown
0.00.053.637 I print_info: ssm_d_conv       = 0
0.00.053.637 I print_info: ssm_d_inner      = 0
0.00.053.637 I print_info: ssm_d_state      = 0
0.00.053.637 I print_info: ssm_dt_rank      = 0
0.00.053.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.639 I print_info: model type       = 1.4B
0.00.053.639 I print_info: model params     = 1.41 B
0.00.053.639 I print_info: general.name     = 1.4B
0.00.053.640 I print_info: vocab type       = BPE
0.00.053.641 I print_info: n_vocab          = 50304
0.00.053.641 I print_info: n_merges         = 50009
0.00.053.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.641 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.643 I print_info: LF token         = 128 'Ä'
0.00.053.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.643 I print_info: max token length = 1024
0.00.055.962 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.962 I load_tensors: offloading output layer to GPU
0.00.055.962 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.974 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.975 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.298 I llama_init_from_model: n_seq_max     = 1
0.00.056.299 I llama_init_from_model: n_ctx         = 2048
0.00.056.299 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.299 I llama_init_from_model: n_batch       = 2048
0.00.056.300 I llama_init_from_model: n_ubatch      = 512
0.00.056.300 I llama_init_from_model: flash_attn    = 0
0.00.056.300 I llama_init_from_model: freq_base     = 10000.0
0.00.056.300 I llama_init_from_model: freq_scale    = 1
0.00.056.301 I ggml_metal_init: allocating
0.00.056.303 I ggml_metal_init: found device: Apple M4
0.00.056.305 I ggml_metal_init: picking default device: Apple M4
0.00.057.052 I ggml_metal_init: using embedded metal library
0.00.059.584 I ggml_metal_init: GPU name:   Apple M4
0.00.059.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.587 I ggml_metal_init: simdgroup reduction   = true
0.00.059.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.587 I ggml_metal_init: has bfloat            = true
0.00.059.587 I ggml_metal_init: use bfloat            = true
0.00.059.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.750 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.946 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.961 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.327 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.329 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.329 I llama_init_from_model: graph nodes  = 967
0.00.095.330 I llama_init_from_model: graph splits = 2
0.00.095.334 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.378 I main: llama threadpool init, n_threads = 4
0.00.666.424 I 
0.00.666.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.448 I 
0.00.666.681 I sampler seed: 1234
0.00.666.685 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.707 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.352.116 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.352.116 I llama_perf_context_print:        load time =     654.73 ms
0.01.352.118 I llama_perf_context_print: prompt eval time =      45.95 ms /     7 tokens (    6.56 ms per token,   152.36 tokens per second)
0.01.352.118 I llama_perf_context_print:        eval time =     636.48 ms /    63 runs   (   10.10 ms per token,    98.98 tokens per second)
0.01.352.119 I llama_perf_context_print:       total time =     685.74 ms /    70 tokens
0.01.352.348 I ggml_metal_free: deallocating

real	0m1.372s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.118 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.483 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.338 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.339 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.339 I llama_model_loader: - type  f32:  194 tensors
0.00.032.339 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.340 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.340 I print_info: file format = GGUF V3 (latest)
0.00.032.352 I print_info: file type   = Q4_0
0.00.032.353 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.056.521 I load: special tokens cache size = 25
0.00.062.962 I load: token to piece cache size = 0.2984 MB
0.00.062.966 I print_info: arch             = gptneox
0.00.062.966 I print_info: vocab_only       = 0
0.00.062.966 I print_info: n_ctx_train      = 2048
0.00.062.966 I print_info: n_embd           = 2048
0.00.062.966 I print_info: n_layer          = 24
0.00.062.969 I print_info: n_head           = 16
0.00.062.970 I print_info: n_head_kv        = 16
0.00.062.970 I print_info: n_rot            = 32
0.00.062.970 I print_info: n_swa            = 0
0.00.062.972 I print_info: n_embd_head_k    = 128
0.00.062.972 I print_info: n_embd_head_v    = 128
0.00.062.972 I print_info: n_gqa            = 1
0.00.062.973 I print_info: n_embd_k_gqa     = 2048
0.00.062.974 I print_info: n_embd_v_gqa     = 2048
0.00.062.974 I print_info: f_norm_eps       = 1.0e-05
0.00.062.975 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.975 I print_info: f_logit_scale    = 0.0e+00
0.00.062.976 I print_info: n_ff             = 8192
0.00.062.976 I print_info: n_expert         = 0
0.00.062.976 I print_info: n_expert_used    = 0
0.00.062.976 I print_info: causal attn      = 1
0.00.062.977 I print_info: pooling type     = 0
0.00.062.977 I print_info: rope type        = 2
0.00.062.977 I print_info: rope scaling     = linear
0.00.062.979 I print_info: freq_base_train  = 10000.0
0.00.062.979 I print_info: freq_scale_train = 1
0.00.062.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.979 I print_info: rope_finetuned   = unknown
0.00.062.979 I print_info: ssm_d_conv       = 0
0.00.062.980 I print_info: ssm_d_inner      = 0
0.00.062.980 I print_info: ssm_d_state      = 0
0.00.062.980 I print_info: ssm_dt_rank      = 0
0.00.062.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.980 I print_info: model type       = 1.4B
0.00.062.980 I print_info: model params     = 1.41 B
0.00.062.980 I print_info: general.name     = 1.4B
0.00.062.981 I print_info: vocab type       = BPE
0.00.062.981 I print_info: n_vocab          = 50304
0.00.062.985 I print_info: n_merges         = 50009
0.00.062.985 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.985 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.985 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.985 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: LF token         = 128 'Ä'
0.00.062.986 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: max token length = 1024
0.00.065.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.027 I load_tensors: offloading output layer to GPU
0.00.065.027 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.038 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.065.039 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.065.315 I llama_init_from_model: n_seq_max     = 1
0.00.065.316 I llama_init_from_model: n_ctx         = 128
0.00.065.316 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.317 I llama_init_from_model: n_batch       = 128
0.00.065.317 I llama_init_from_model: n_ubatch      = 128
0.00.065.317 I llama_init_from_model: flash_attn    = 0
0.00.065.317 I llama_init_from_model: freq_base     = 10000.0
0.00.065.318 I llama_init_from_model: freq_scale    = 1
0.00.065.318 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.318 I ggml_metal_init: allocating
0.00.065.321 I ggml_metal_init: found device: Apple M4
0.00.065.323 I ggml_metal_init: picking default device: Apple M4
0.00.065.918 I ggml_metal_init: using embedded metal library
0.00.068.511 I ggml_metal_init: GPU name:   Apple M4
0.00.068.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.514 I ggml_metal_init: simdgroup reduction   = true
0.00.068.514 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.514 I ggml_metal_init: has bfloat            = true
0.00.068.514 I ggml_metal_init: use bfloat            = true
0.00.068.515 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.524 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.258 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.260 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.275 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.359 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.361 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.361 I llama_init_from_model: graph nodes  = 967
0.00.081.361 I llama_init_from_model: graph splits = 2
0.00.081.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.792 I 
0.00.602.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.839 I perplexity: tokenizing the input ..
0.00.611.063 I perplexity: tokenization took 8.22 ms
0.00.611.066 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.733.393 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.734.592 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.734.612 I llama_perf_context_print:        load time =     591.30 ms
0.00.734.613 I llama_perf_context_print: prompt eval time =     122.10 ms /   128 tokens (    0.95 ms per token,  1048.30 tokens per second)
0.00.734.614 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.734.614 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.735.057 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.090s
sys	0m0.087s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.886 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.041 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.042 I llama_model_loader: - type  f32:  194 tensors
0.00.026.043 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.044 I print_info: file format = GGUF V3 (latest)
0.00.026.056 I print_info: file type   = Q4_1
0.00.026.058 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.573 I load: special tokens cache size = 25
0.00.051.802 I load: token to piece cache size = 0.2984 MB
0.00.051.806 I print_info: arch             = gptneox
0.00.051.806 I print_info: vocab_only       = 0
0.00.051.806 I print_info: n_ctx_train      = 2048
0.00.051.806 I print_info: n_embd           = 2048
0.00.051.806 I print_info: n_layer          = 24
0.00.051.809 I print_info: n_head           = 16
0.00.051.810 I print_info: n_head_kv        = 16
0.00.051.810 I print_info: n_rot            = 32
0.00.051.810 I print_info: n_swa            = 0
0.00.051.811 I print_info: n_embd_head_k    = 128
0.00.051.811 I print_info: n_embd_head_v    = 128
0.00.051.812 I print_info: n_gqa            = 1
0.00.051.812 I print_info: n_embd_k_gqa     = 2048
0.00.051.813 I print_info: n_embd_v_gqa     = 2048
0.00.051.814 I print_info: f_norm_eps       = 1.0e-05
0.00.051.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.814 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.814 I print_info: f_logit_scale    = 0.0e+00
0.00.051.815 I print_info: n_ff             = 8192
0.00.051.815 I print_info: n_expert         = 0
0.00.051.815 I print_info: n_expert_used    = 0
0.00.051.817 I print_info: causal attn      = 1
0.00.051.819 I print_info: pooling type     = 0
0.00.051.819 I print_info: rope type        = 2
0.00.051.819 I print_info: rope scaling     = linear
0.00.051.820 I print_info: freq_base_train  = 10000.0
0.00.051.820 I print_info: freq_scale_train = 1
0.00.051.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.820 I print_info: rope_finetuned   = unknown
0.00.051.820 I print_info: ssm_d_conv       = 0
0.00.051.821 I print_info: ssm_d_inner      = 0
0.00.051.821 I print_info: ssm_d_state      = 0
0.00.051.821 I print_info: ssm_dt_rank      = 0
0.00.051.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.821 I print_info: model type       = 1.4B
0.00.051.821 I print_info: model params     = 1.41 B
0.00.051.822 I print_info: general.name     = 1.4B
0.00.051.822 I print_info: vocab type       = BPE
0.00.051.822 I print_info: n_vocab          = 50304
0.00.051.822 I print_info: n_merges         = 50009
0.00.051.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.823 I print_info: LF token         = 128 'Ä'
0.00.051.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.829 I print_info: max token length = 1024
0.00.053.807 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.807 I load_tensors: offloading output layer to GPU
0.00.053.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.818 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.819 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.098 I llama_init_from_model: n_seq_max     = 1
0.00.054.098 I llama_init_from_model: n_ctx         = 2048
0.00.054.098 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.099 I llama_init_from_model: n_batch       = 2048
0.00.054.099 I llama_init_from_model: n_ubatch      = 512
0.00.054.099 I llama_init_from_model: flash_attn    = 0
0.00.054.099 I llama_init_from_model: freq_base     = 10000.0
0.00.054.100 I llama_init_from_model: freq_scale    = 1
0.00.054.100 I ggml_metal_init: allocating
0.00.054.103 I ggml_metal_init: found device: Apple M4
0.00.054.105 I ggml_metal_init: picking default device: Apple M4
0.00.054.689 I ggml_metal_init: using embedded metal library
0.00.057.035 I ggml_metal_init: GPU name:   Apple M4
0.00.057.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.038 I ggml_metal_init: simdgroup reduction   = true
0.00.057.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.038 I ggml_metal_init: has bfloat            = true
0.00.057.038 I ggml_metal_init: use bfloat            = true
0.00.057.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.977 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.827 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.839 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.860 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.904 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.906 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.906 I llama_init_from_model: graph nodes  = 967
0.00.086.906 I llama_init_from_model: graph splits = 2
0.00.086.909 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.176 I main: llama threadpool init, n_threads = 4
0.00.703.219 I 
0.00.703.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.240 I 
0.00.703.485 I sampler seed: 1234
0.00.703.490 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.502 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.423.646 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.423.646 I llama_perf_context_print:        load time =     693.28 ms
0.01.423.647 I llama_perf_context_print: prompt eval time =      43.41 ms /     7 tokens (    6.20 ms per token,   161.24 tokens per second)
0.01.423.648 I llama_perf_context_print:        eval time =     673.78 ms /    63 runs   (   10.69 ms per token,    93.50 tokens per second)
0.01.423.648 I llama_perf_context_print:       total time =     720.47 ms /    70 tokens
0.01.423.858 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.723 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.840 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.845 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.848 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.850 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.850 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.851 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.852 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.855 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.481 I llama_model_loader: - type  f32:  194 tensors
0.00.025.481 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.482 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.482 I print_info: file format = GGUF V3 (latest)
0.00.025.489 I print_info: file type   = Q4_1
0.00.025.490 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.074 I load: special tokens cache size = 25
0.00.050.102 I load: token to piece cache size = 0.2984 MB
0.00.050.105 I print_info: arch             = gptneox
0.00.050.105 I print_info: vocab_only       = 0
0.00.050.105 I print_info: n_ctx_train      = 2048
0.00.050.105 I print_info: n_embd           = 2048
0.00.050.106 I print_info: n_layer          = 24
0.00.050.108 I print_info: n_head           = 16
0.00.050.109 I print_info: n_head_kv        = 16
0.00.050.109 I print_info: n_rot            = 32
0.00.050.110 I print_info: n_swa            = 0
0.00.050.110 I print_info: n_embd_head_k    = 128
0.00.050.110 I print_info: n_embd_head_v    = 128
0.00.050.111 I print_info: n_gqa            = 1
0.00.050.111 I print_info: n_embd_k_gqa     = 2048
0.00.050.112 I print_info: n_embd_v_gqa     = 2048
0.00.050.113 I print_info: f_norm_eps       = 1.0e-05
0.00.050.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.114 I print_info: f_logit_scale    = 0.0e+00
0.00.050.114 I print_info: n_ff             = 8192
0.00.050.115 I print_info: n_expert         = 0
0.00.050.115 I print_info: n_expert_used    = 0
0.00.050.115 I print_info: causal attn      = 1
0.00.050.118 I print_info: pooling type     = 0
0.00.050.118 I print_info: rope type        = 2
0.00.050.118 I print_info: rope scaling     = linear
0.00.050.118 I print_info: freq_base_train  = 10000.0
0.00.050.119 I print_info: freq_scale_train = 1
0.00.050.119 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.119 I print_info: rope_finetuned   = unknown
0.00.050.119 I print_info: ssm_d_conv       = 0
0.00.050.119 I print_info: ssm_d_inner      = 0
0.00.050.119 I print_info: ssm_d_state      = 0
0.00.050.120 I print_info: ssm_dt_rank      = 0
0.00.050.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.120 I print_info: model type       = 1.4B
0.00.050.120 I print_info: model params     = 1.41 B
0.00.050.121 I print_info: general.name     = 1.4B
0.00.050.121 I print_info: vocab type       = BPE
0.00.050.121 I print_info: n_vocab          = 50304
0.00.050.121 I print_info: n_merges         = 50009
0.00.050.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: LF token         = 128 'Ä'
0.00.050.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.123 I print_info: max token length = 1024
0.00.052.098 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.098 I load_tensors: offloading output layer to GPU
0.00.052.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.109 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.110 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.379 I llama_init_from_model: n_seq_max     = 1
0.00.052.380 I llama_init_from_model: n_ctx         = 128
0.00.052.381 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.381 I llama_init_from_model: n_batch       = 128
0.00.052.381 I llama_init_from_model: n_ubatch      = 128
0.00.052.381 I llama_init_from_model: flash_attn    = 0
0.00.052.381 I llama_init_from_model: freq_base     = 10000.0
0.00.052.382 I llama_init_from_model: freq_scale    = 1
0.00.052.382 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.382 I ggml_metal_init: allocating
0.00.052.385 I ggml_metal_init: found device: Apple M4
0.00.052.387 I ggml_metal_init: picking default device: Apple M4
0.00.052.933 I ggml_metal_init: using embedded metal library
0.00.055.239 I ggml_metal_init: GPU name:   Apple M4
0.00.055.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.241 I ggml_metal_init: simdgroup reduction   = true
0.00.055.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.241 I ggml_metal_init: has bfloat            = true
0.00.055.242 I ggml_metal_init: use bfloat            = true
0.00.055.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.427 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.437 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.304 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.305 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.305 I llama_init_from_model: graph nodes  = 967
0.00.067.305 I llama_init_from_model: graph splits = 2
0.00.067.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.934 I 
0.00.647.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.982 I perplexity: tokenizing the input ..
0.00.655.761 I perplexity: tokenization took 7.777 ms
0.00.655.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.641 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.779.804 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.779.832 I llama_perf_context_print:        load time =     638.21 ms
0.00.779.833 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.63 tokens per second)
0.00.779.834 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.834 I llama_perf_context_print:       total time =     131.90 ms /   129 tokens
0.00.780.216 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.077s
sys	0m0.110s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.282 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.420 I llama_model_loader: - type  f32:  194 tensors
0.00.027.420 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.421 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.421 I print_info: file format = GGUF V3 (latest)
0.00.027.433 I print_info: file type   = Q5_0
0.00.027.434 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.119 I load: special tokens cache size = 25
0.00.052.128 I load: token to piece cache size = 0.2984 MB
0.00.052.131 I print_info: arch             = gptneox
0.00.052.131 I print_info: vocab_only       = 0
0.00.052.131 I print_info: n_ctx_train      = 2048
0.00.052.132 I print_info: n_embd           = 2048
0.00.052.132 I print_info: n_layer          = 24
0.00.052.135 I print_info: n_head           = 16
0.00.052.136 I print_info: n_head_kv        = 16
0.00.052.136 I print_info: n_rot            = 32
0.00.052.136 I print_info: n_swa            = 0
0.00.052.136 I print_info: n_embd_head_k    = 128
0.00.052.136 I print_info: n_embd_head_v    = 128
0.00.052.137 I print_info: n_gqa            = 1
0.00.052.138 I print_info: n_embd_k_gqa     = 2048
0.00.052.138 I print_info: n_embd_v_gqa     = 2048
0.00.052.139 I print_info: f_norm_eps       = 1.0e-05
0.00.052.141 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.142 I print_info: f_logit_scale    = 0.0e+00
0.00.052.143 I print_info: n_ff             = 8192
0.00.052.143 I print_info: n_expert         = 0
0.00.052.143 I print_info: n_expert_used    = 0
0.00.052.144 I print_info: causal attn      = 1
0.00.052.145 I print_info: pooling type     = 0
0.00.052.146 I print_info: rope type        = 2
0.00.052.146 I print_info: rope scaling     = linear
0.00.052.146 I print_info: freq_base_train  = 10000.0
0.00.052.146 I print_info: freq_scale_train = 1
0.00.052.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.147 I print_info: rope_finetuned   = unknown
0.00.052.147 I print_info: ssm_d_conv       = 0
0.00.052.147 I print_info: ssm_d_inner      = 0
0.00.052.147 I print_info: ssm_d_state      = 0
0.00.052.147 I print_info: ssm_dt_rank      = 0
0.00.052.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.148 I print_info: model type       = 1.4B
0.00.052.148 I print_info: model params     = 1.41 B
0.00.052.148 I print_info: general.name     = 1.4B
0.00.052.149 I print_info: vocab type       = BPE
0.00.052.149 I print_info: n_vocab          = 50304
0.00.052.149 I print_info: n_merges         = 50009
0.00.052.149 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.150 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.153 I print_info: LF token         = 128 'Ä'
0.00.052.154 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.154 I print_info: max token length = 1024
0.00.054.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.133 I load_tensors: offloading output layer to GPU
0.00.054.134 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.144 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.145 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.427 I llama_init_from_model: n_seq_max     = 1
0.00.054.428 I llama_init_from_model: n_ctx         = 2048
0.00.054.428 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.428 I llama_init_from_model: n_batch       = 2048
0.00.054.429 I llama_init_from_model: n_ubatch      = 512
0.00.054.429 I llama_init_from_model: flash_attn    = 0
0.00.054.429 I llama_init_from_model: freq_base     = 10000.0
0.00.054.429 I llama_init_from_model: freq_scale    = 1
0.00.054.430 I ggml_metal_init: allocating
0.00.054.433 I ggml_metal_init: found device: Apple M4
0.00.054.435 I ggml_metal_init: picking default device: Apple M4
0.00.055.018 I ggml_metal_init: using embedded metal library
0.00.057.361 I ggml_metal_init: GPU name:   Apple M4
0.00.057.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.363 I ggml_metal_init: simdgroup reduction   = true
0.00.057.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.363 I ggml_metal_init: has bfloat            = true
0.00.057.364 I ggml_metal_init: use bfloat            = true
0.00.057.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.076 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.090 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.107 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.298 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.299 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.300 I llama_init_from_model: graph nodes  = 967
0.00.088.300 I llama_init_from_model: graph splits = 2
0.00.088.303 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.402 I main: llama threadpool init, n_threads = 4
0.00.794.442 I 
0.00.794.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.471 I 
0.00.794.707 I sampler seed: 1234
0.00.794.712 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.743 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.744 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.744 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.578.655 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.578.655 I llama_perf_context_print:        load time =     783.11 ms
0.01.578.656 I llama_perf_context_print: prompt eval time =      47.00 ms /     7 tokens (    6.71 ms per token,   148.94 tokens per second)
0.01.578.661 I llama_perf_context_print:        eval time =     734.00 ms /    63 runs   (   11.65 ms per token,    85.83 tokens per second)
0.01.578.661 I llama_perf_context_print:       total time =     784.26 ms /    70 tokens
0.01.578.867 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.110s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.010 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.231 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.799 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.799 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.800 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.800 I llama_model_loader: - type  f32:  194 tensors
0.00.024.801 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.801 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.802 I print_info: file format = GGUF V3 (latest)
0.00.024.809 I print_info: file type   = Q5_0
0.00.024.810 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.437 I load: special tokens cache size = 25
0.00.049.327 I load: token to piece cache size = 0.2984 MB
0.00.049.330 I print_info: arch             = gptneox
0.00.049.330 I print_info: vocab_only       = 0
0.00.049.330 I print_info: n_ctx_train      = 2048
0.00.049.331 I print_info: n_embd           = 2048
0.00.049.331 I print_info: n_layer          = 24
0.00.049.334 I print_info: n_head           = 16
0.00.049.334 I print_info: n_head_kv        = 16
0.00.049.342 I print_info: n_rot            = 32
0.00.049.343 I print_info: n_swa            = 0
0.00.049.343 I print_info: n_embd_head_k    = 128
0.00.049.343 I print_info: n_embd_head_v    = 128
0.00.049.347 I print_info: n_gqa            = 1
0.00.049.348 I print_info: n_embd_k_gqa     = 2048
0.00.049.348 I print_info: n_embd_v_gqa     = 2048
0.00.049.349 I print_info: f_norm_eps       = 1.0e-05
0.00.049.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.351 I print_info: f_logit_scale    = 0.0e+00
0.00.049.352 I print_info: n_ff             = 8192
0.00.049.352 I print_info: n_expert         = 0
0.00.049.352 I print_info: n_expert_used    = 0
0.00.049.352 I print_info: causal attn      = 1
0.00.049.353 I print_info: pooling type     = 0
0.00.049.354 I print_info: rope type        = 2
0.00.049.354 I print_info: rope scaling     = linear
0.00.049.354 I print_info: freq_base_train  = 10000.0
0.00.049.354 I print_info: freq_scale_train = 1
0.00.049.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.355 I print_info: rope_finetuned   = unknown
0.00.049.356 I print_info: ssm_d_conv       = 0
0.00.049.356 I print_info: ssm_d_inner      = 0
0.00.049.356 I print_info: ssm_d_state      = 0
0.00.049.356 I print_info: ssm_dt_rank      = 0
0.00.049.356 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.357 I print_info: model type       = 1.4B
0.00.049.357 I print_info: model params     = 1.41 B
0.00.049.357 I print_info: general.name     = 1.4B
0.00.049.358 I print_info: vocab type       = BPE
0.00.049.358 I print_info: n_vocab          = 50304
0.00.049.358 I print_info: n_merges         = 50009
0.00.049.358 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.359 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.359 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.360 I print_info: LF token         = 128 'Ä'
0.00.049.360 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.360 I print_info: max token length = 1024
0.00.051.364 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.365 I load_tensors: offloading output layer to GPU
0.00.051.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.376 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.377 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.652 I llama_init_from_model: n_seq_max     = 1
0.00.051.653 I llama_init_from_model: n_ctx         = 128
0.00.051.653 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.653 I llama_init_from_model: n_batch       = 128
0.00.051.653 I llama_init_from_model: n_ubatch      = 128
0.00.051.653 I llama_init_from_model: flash_attn    = 0
0.00.051.654 I llama_init_from_model: freq_base     = 10000.0
0.00.051.654 I llama_init_from_model: freq_scale    = 1
0.00.051.654 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.655 I ggml_metal_init: allocating
0.00.051.657 I ggml_metal_init: found device: Apple M4
0.00.051.659 I ggml_metal_init: picking default device: Apple M4
0.00.052.204 I ggml_metal_init: using embedded metal library
0.00.054.580 I ggml_metal_init: GPU name:   Apple M4
0.00.054.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.583 I ggml_metal_init: simdgroup reduction   = true
0.00.054.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.583 I ggml_metal_init: has bfloat            = true
0.00.054.583 I ggml_metal_init: use bfloat            = true
0.00.054.584 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.037 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.459 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.463 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.479 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.320 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.321 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.321 I llama_init_from_model: graph nodes  = 967
0.00.066.321 I llama_init_from_model: graph splits = 2
0.00.066.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.234 I 
0.00.715.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.367 I perplexity: tokenizing the input ..
0.00.723.049 I perplexity: tokenization took 7.681 ms
0.00.723.052 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.787 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.858.941 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.858.966 I llama_perf_context_print:        load time =     706.21 ms
0.00.858.967 I llama_perf_context_print: prompt eval time =     134.51 ms /   128 tokens (    1.05 ms per token,   951.62 tokens per second)
0.00.858.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.968 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.859.514 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.077s
sys	0m0.115s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.310 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.807 I llama_model_loader: - type  f32:  194 tensors
0.00.025.807 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.807 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.808 I print_info: file format = GGUF V3 (latest)
0.00.025.815 I print_info: file type   = Q5_1
0.00.025.816 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.161 I load: special tokens cache size = 25
0.00.051.211 I load: token to piece cache size = 0.2984 MB
0.00.051.213 I print_info: arch             = gptneox
0.00.051.214 I print_info: vocab_only       = 0
0.00.051.214 I print_info: n_ctx_train      = 2048
0.00.051.214 I print_info: n_embd           = 2048
0.00.051.214 I print_info: n_layer          = 24
0.00.051.217 I print_info: n_head           = 16
0.00.051.218 I print_info: n_head_kv        = 16
0.00.051.218 I print_info: n_rot            = 32
0.00.051.218 I print_info: n_swa            = 0
0.00.051.219 I print_info: n_embd_head_k    = 128
0.00.051.219 I print_info: n_embd_head_v    = 128
0.00.051.220 I print_info: n_gqa            = 1
0.00.051.221 I print_info: n_embd_k_gqa     = 2048
0.00.051.222 I print_info: n_embd_v_gqa     = 2048
0.00.051.222 I print_info: f_norm_eps       = 1.0e-05
0.00.051.223 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.223 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.223 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.223 I print_info: f_logit_scale    = 0.0e+00
0.00.051.225 I print_info: n_ff             = 8192
0.00.051.225 I print_info: n_expert         = 0
0.00.051.226 I print_info: n_expert_used    = 0
0.00.051.226 I print_info: causal attn      = 1
0.00.051.226 I print_info: pooling type     = 0
0.00.051.226 I print_info: rope type        = 2
0.00.051.226 I print_info: rope scaling     = linear
0.00.051.227 I print_info: freq_base_train  = 10000.0
0.00.051.227 I print_info: freq_scale_train = 1
0.00.051.227 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.227 I print_info: rope_finetuned   = unknown
0.00.051.228 I print_info: ssm_d_conv       = 0
0.00.051.228 I print_info: ssm_d_inner      = 0
0.00.051.229 I print_info: ssm_d_state      = 0
0.00.051.229 I print_info: ssm_dt_rank      = 0
0.00.051.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.230 I print_info: model type       = 1.4B
0.00.051.230 I print_info: model params     = 1.41 B
0.00.051.230 I print_info: general.name     = 1.4B
0.00.051.231 I print_info: vocab type       = BPE
0.00.051.231 I print_info: n_vocab          = 50304
0.00.051.231 I print_info: n_merges         = 50009
0.00.051.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.232 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.232 I print_info: LF token         = 128 'Ä'
0.00.051.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.232 I print_info: max token length = 1024
0.00.052.994 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.995 I load_tensors: offloading output layer to GPU
0.00.052.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.001 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.001 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.374 I llama_init_from_model: n_seq_max     = 1
0.00.053.374 I llama_init_from_model: n_ctx         = 2048
0.00.053.374 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.374 I llama_init_from_model: n_batch       = 2048
0.00.053.375 I llama_init_from_model: n_ubatch      = 512
0.00.053.375 I llama_init_from_model: flash_attn    = 0
0.00.053.375 I llama_init_from_model: freq_base     = 10000.0
0.00.053.375 I llama_init_from_model: freq_scale    = 1
0.00.053.376 I ggml_metal_init: allocating
0.00.053.382 I ggml_metal_init: found device: Apple M4
0.00.053.384 I ggml_metal_init: picking default device: Apple M4
0.00.053.932 I ggml_metal_init: using embedded metal library
0.00.056.271 I ggml_metal_init: GPU name:   Apple M4
0.00.056.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.274 I ggml_metal_init: simdgroup reduction   = true
0.00.056.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.274 I ggml_metal_init: has bfloat            = true
0.00.056.274 I ggml_metal_init: use bfloat            = true
0.00.056.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.825 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.459 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.477 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.373 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.375 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.375 I llama_init_from_model: graph nodes  = 967
0.00.086.375 I llama_init_from_model: graph splits = 2
0.00.086.378 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.071 I main: llama threadpool init, n_threads = 4
0.00.772.109 I 
0.00.772.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.148 I 
0.00.772.368 I sampler seed: 1234
0.00.772.373 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.428 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.442 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.442 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.609.870 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.609.871 I llama_perf_context_print:        load time =     762.75 ms
0.01.609.872 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.70 tokens per second)
0.01.609.875 I llama_perf_context_print:        eval time =     792.19 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.609.877 I llama_perf_context_print:       total time =     837.80 ms /    70 tokens
0.01.610.126 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.858 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.881 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.884 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.887 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.404 I llama_model_loader: - type  f32:  194 tensors
0.00.026.405 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.405 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.406 I print_info: file format = GGUF V3 (latest)
0.00.026.417 I print_info: file type   = Q5_1
0.00.026.418 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.978 I load: special tokens cache size = 25
0.00.051.102 I load: token to piece cache size = 0.2984 MB
0.00.051.105 I print_info: arch             = gptneox
0.00.051.105 I print_info: vocab_only       = 0
0.00.051.105 I print_info: n_ctx_train      = 2048
0.00.051.106 I print_info: n_embd           = 2048
0.00.051.106 I print_info: n_layer          = 24
0.00.051.109 I print_info: n_head           = 16
0.00.051.110 I print_info: n_head_kv        = 16
0.00.051.110 I print_info: n_rot            = 32
0.00.051.110 I print_info: n_swa            = 0
0.00.051.110 I print_info: n_embd_head_k    = 128
0.00.051.111 I print_info: n_embd_head_v    = 128
0.00.051.114 I print_info: n_gqa            = 1
0.00.051.114 I print_info: n_embd_k_gqa     = 2048
0.00.051.115 I print_info: n_embd_v_gqa     = 2048
0.00.051.116 I print_info: f_norm_eps       = 1.0e-05
0.00.051.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.118 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.118 I print_info: f_logit_scale    = 0.0e+00
0.00.051.119 I print_info: n_ff             = 8192
0.00.051.119 I print_info: n_expert         = 0
0.00.051.119 I print_info: n_expert_used    = 0
0.00.051.119 I print_info: causal attn      = 1
0.00.051.119 I print_info: pooling type     = 0
0.00.051.120 I print_info: rope type        = 2
0.00.051.120 I print_info: rope scaling     = linear
0.00.051.120 I print_info: freq_base_train  = 10000.0
0.00.051.120 I print_info: freq_scale_train = 1
0.00.051.121 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.121 I print_info: rope_finetuned   = unknown
0.00.051.121 I print_info: ssm_d_conv       = 0
0.00.051.121 I print_info: ssm_d_inner      = 0
0.00.051.121 I print_info: ssm_d_state      = 0
0.00.051.121 I print_info: ssm_dt_rank      = 0
0.00.051.121 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.126 I print_info: model type       = 1.4B
0.00.051.126 I print_info: model params     = 1.41 B
0.00.051.127 I print_info: general.name     = 1.4B
0.00.051.128 I print_info: vocab type       = BPE
0.00.051.128 I print_info: n_vocab          = 50304
0.00.051.128 I print_info: n_merges         = 50009
0.00.051.128 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.129 I print_info: LF token         = 128 'Ä'
0.00.051.129 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.129 I print_info: max token length = 1024
0.00.052.762 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.762 I load_tensors: offloading output layer to GPU
0.00.052.762 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.772 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.773 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.059 I llama_init_from_model: n_seq_max     = 1
0.00.053.060 I llama_init_from_model: n_ctx         = 128
0.00.053.060 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.060 I llama_init_from_model: n_batch       = 128
0.00.053.061 I llama_init_from_model: n_ubatch      = 128
0.00.053.061 I llama_init_from_model: flash_attn    = 0
0.00.053.061 I llama_init_from_model: freq_base     = 10000.0
0.00.053.061 I llama_init_from_model: freq_scale    = 1
0.00.053.062 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.062 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.067 I ggml_metal_init: picking default device: Apple M4
0.00.053.614 I ggml_metal_init: using embedded metal library
0.00.055.958 I ggml_metal_init: GPU name:   Apple M4
0.00.055.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.961 I ggml_metal_init: simdgroup reduction   = true
0.00.055.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.961 I ggml_metal_init: has bfloat            = true
0.00.055.961 I ggml_metal_init: use bfloat            = true
0.00.055.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.524 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.764 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.670 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.671 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.672 I llama_init_from_model: graph nodes  = 967
0.00.067.672 I llama_init_from_model: graph splits = 2
0.00.067.673 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.673 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.551 I 
0.00.722.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.602 I perplexity: tokenizing the input ..
0.00.730.371 I perplexity: tokenization took 7.767 ms
0.00.730.375 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.044 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.866.195 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.866.223 I llama_perf_context_print:        load time =     711.69 ms
0.00.866.224 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.29 tokens per second)
0.00.866.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.225 I llama_perf_context_print:       total time =     143.67 ms /   129 tokens
0.00.866.781 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.078s
sys	0m0.117s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.316 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.021 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.021 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.024 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.025 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.688 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.691 I llama_model_loader: - type  f32:  194 tensors
0.00.025.692 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.692 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.693 I print_info: file format = GGUF V3 (latest)
0.00.025.700 I print_info: file type   = Q2_K - Medium
0.00.025.701 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.571 I load: special tokens cache size = 25
0.00.050.581 I load: token to piece cache size = 0.2984 MB
0.00.050.584 I print_info: arch             = gptneox
0.00.050.585 I print_info: vocab_only       = 0
0.00.050.585 I print_info: n_ctx_train      = 2048
0.00.050.585 I print_info: n_embd           = 2048
0.00.050.585 I print_info: n_layer          = 24
0.00.050.589 I print_info: n_head           = 16
0.00.050.589 I print_info: n_head_kv        = 16
0.00.050.591 I print_info: n_rot            = 32
0.00.050.591 I print_info: n_swa            = 0
0.00.050.591 I print_info: n_embd_head_k    = 128
0.00.050.591 I print_info: n_embd_head_v    = 128
0.00.050.592 I print_info: n_gqa            = 1
0.00.050.593 I print_info: n_embd_k_gqa     = 2048
0.00.050.594 I print_info: n_embd_v_gqa     = 2048
0.00.050.594 I print_info: f_norm_eps       = 1.0e-05
0.00.050.595 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.595 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.595 I print_info: f_logit_scale    = 0.0e+00
0.00.050.596 I print_info: n_ff             = 8192
0.00.050.596 I print_info: n_expert         = 0
0.00.050.598 I print_info: n_expert_used    = 0
0.00.050.599 I print_info: causal attn      = 1
0.00.050.599 I print_info: pooling type     = 0
0.00.050.599 I print_info: rope type        = 2
0.00.050.600 I print_info: rope scaling     = linear
0.00.050.600 I print_info: freq_base_train  = 10000.0
0.00.050.600 I print_info: freq_scale_train = 1
0.00.050.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.600 I print_info: rope_finetuned   = unknown
0.00.050.601 I print_info: ssm_d_conv       = 0
0.00.050.601 I print_info: ssm_d_inner      = 0
0.00.050.601 I print_info: ssm_d_state      = 0
0.00.050.601 I print_info: ssm_dt_rank      = 0
0.00.050.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.601 I print_info: model type       = 1.4B
0.00.050.602 I print_info: model params     = 1.41 B
0.00.050.602 I print_info: general.name     = 1.4B
0.00.050.602 I print_info: vocab type       = BPE
0.00.050.603 I print_info: n_vocab          = 50304
0.00.050.603 I print_info: n_merges         = 50009
0.00.050.603 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.603 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.605 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.605 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.606 I print_info: LF token         = 128 'Ä'
0.00.050.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.606 I print_info: max token length = 1024
0.00.052.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I load_tensors: offloading output layer to GPU
0.00.052.494 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.505 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.506 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.904 I llama_init_from_model: n_seq_max     = 1
0.00.052.906 I llama_init_from_model: n_ctx         = 2048
0.00.052.906 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.906 I llama_init_from_model: n_batch       = 2048
0.00.052.906 I llama_init_from_model: n_ubatch      = 512
0.00.052.907 I llama_init_from_model: flash_attn    = 0
0.00.052.907 I llama_init_from_model: freq_base     = 10000.0
0.00.052.907 I llama_init_from_model: freq_scale    = 1
0.00.052.908 I ggml_metal_init: allocating
0.00.052.911 I ggml_metal_init: found device: Apple M4
0.00.052.913 I ggml_metal_init: picking default device: Apple M4
0.00.053.521 I ggml_metal_init: using embedded metal library
0.00.055.961 I ggml_metal_init: GPU name:   Apple M4
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.964 I ggml_metal_init: simdgroup reduction   = true
0.00.055.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.964 I ggml_metal_init: has bfloat            = true
0.00.055.964 I ggml_metal_init: use bfloat            = true
0.00.055.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.486 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.137 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.160 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.179 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.180 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.181 I llama_init_from_model: graph nodes  = 967
0.00.088.181 I llama_init_from_model: graph splits = 2
0.00.088.184 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.748 I main: llama threadpool init, n_threads = 4
0.00.486.795 I 
0.00.486.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.826 I 
0.00.487.057 I sampler seed: 1234
0.00.487.062 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.081 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.155.532 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.01.155.533 I llama_perf_context_print:        load time =     476.43 ms
0.01.155.538 I llama_perf_context_print: prompt eval time =      39.37 ms /     7 tokens (    5.62 ms per token,   177.81 tokens per second)
0.01.155.539 I llama_perf_context_print:        eval time =     626.74 ms /    63 runs   (    9.95 ms per token,   100.52 tokens per second)
0.01.155.539 I llama_perf_context_print:       total time =     668.79 ms /    70 tokens
0.01.155.790 I ggml_metal_free: deallocating

real	0m1.174s
user	0m0.108s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.550 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.025 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.816 I llama_model_loader: - type  f32:  194 tensors
0.00.027.816 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.816 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.817 I print_info: file format = GGUF V3 (latest)
0.00.027.828 I print_info: file type   = Q2_K - Medium
0.00.027.831 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.449 I load: special tokens cache size = 25
0.00.052.456 I load: token to piece cache size = 0.2984 MB
0.00.052.459 I print_info: arch             = gptneox
0.00.052.460 I print_info: vocab_only       = 0
0.00.052.460 I print_info: n_ctx_train      = 2048
0.00.052.460 I print_info: n_embd           = 2048
0.00.052.460 I print_info: n_layer          = 24
0.00.052.463 I print_info: n_head           = 16
0.00.052.464 I print_info: n_head_kv        = 16
0.00.052.464 I print_info: n_rot            = 32
0.00.052.464 I print_info: n_swa            = 0
0.00.052.464 I print_info: n_embd_head_k    = 128
0.00.052.465 I print_info: n_embd_head_v    = 128
0.00.052.465 I print_info: n_gqa            = 1
0.00.052.466 I print_info: n_embd_k_gqa     = 2048
0.00.052.467 I print_info: n_embd_v_gqa     = 2048
0.00.052.473 I print_info: f_norm_eps       = 1.0e-05
0.00.052.474 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.476 I print_info: f_logit_scale    = 0.0e+00
0.00.052.477 I print_info: n_ff             = 8192
0.00.052.477 I print_info: n_expert         = 0
0.00.052.477 I print_info: n_expert_used    = 0
0.00.052.477 I print_info: causal attn      = 1
0.00.052.477 I print_info: pooling type     = 0
0.00.052.478 I print_info: rope type        = 2
0.00.052.478 I print_info: rope scaling     = linear
0.00.052.481 I print_info: freq_base_train  = 10000.0
0.00.052.481 I print_info: freq_scale_train = 1
0.00.052.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.482 I print_info: rope_finetuned   = unknown
0.00.052.482 I print_info: ssm_d_conv       = 0
0.00.052.482 I print_info: ssm_d_inner      = 0
0.00.052.482 I print_info: ssm_d_state      = 0
0.00.052.482 I print_info: ssm_dt_rank      = 0
0.00.052.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.483 I print_info: model type       = 1.4B
0.00.052.484 I print_info: model params     = 1.41 B
0.00.052.484 I print_info: general.name     = 1.4B
0.00.052.484 I print_info: vocab type       = BPE
0.00.052.485 I print_info: n_vocab          = 50304
0.00.052.485 I print_info: n_merges         = 50009
0.00.052.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.486 I print_info: LF token         = 128 'Ä'
0.00.052.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.487 I print_info: max token length = 1024
0.00.054.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.105 I load_tensors: offloading output layer to GPU
0.00.054.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.116 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.117 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.425 I llama_init_from_model: n_seq_max     = 1
0.00.054.426 I llama_init_from_model: n_ctx         = 128
0.00.054.426 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.426 I llama_init_from_model: n_batch       = 128
0.00.054.426 I llama_init_from_model: n_ubatch      = 128
0.00.054.427 I llama_init_from_model: flash_attn    = 0
0.00.054.427 I llama_init_from_model: freq_base     = 10000.0
0.00.054.427 I llama_init_from_model: freq_scale    = 1
0.00.054.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.428 I ggml_metal_init: allocating
0.00.054.432 I ggml_metal_init: found device: Apple M4
0.00.054.434 I ggml_metal_init: picking default device: Apple M4
0.00.055.017 I ggml_metal_init: using embedded metal library
0.00.057.333 I ggml_metal_init: GPU name:   Apple M4
0.00.057.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.335 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.336 I ggml_metal_init: simdgroup reduction   = true
0.00.057.336 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.336 I ggml_metal_init: has bfloat            = true
0.00.057.336 I ggml_metal_init: use bfloat            = true
0.00.057.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.029 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.319 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.200 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.201 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.201 I llama_init_from_model: graph nodes  = 967
0.00.069.202 I llama_init_from_model: graph splits = 2
0.00.069.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.688 I 
0.00.437.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.738 I perplexity: tokenizing the input ..
0.00.445.535 I perplexity: tokenization took 7.795 ms
0.00.445.539 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.999 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.178 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.204 I llama_perf_context_print:        load time =     427.13 ms
0.00.579.206 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.19 tokens per second)
0.00.579.207 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.207 I llama_perf_context_print:       total time =     141.52 ms /   129 tokens
0.00.579.669 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.080s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.533 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.108 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.049 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.050 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.051 I llama_model_loader: - type  f32:  194 tensors
0.00.026.051 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.051 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.052 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.052 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.052 I print_info: file format = GGUF V3 (latest)
0.00.026.060 I print_info: file type   = Q3_K - Medium
0.00.026.061 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.601 I load: special tokens cache size = 25
0.00.051.524 I load: token to piece cache size = 0.2984 MB
0.00.051.527 I print_info: arch             = gptneox
0.00.051.527 I print_info: vocab_only       = 0
0.00.051.527 I print_info: n_ctx_train      = 2048
0.00.051.527 I print_info: n_embd           = 2048
0.00.051.527 I print_info: n_layer          = 24
0.00.051.530 I print_info: n_head           = 16
0.00.051.531 I print_info: n_head_kv        = 16
0.00.051.531 I print_info: n_rot            = 32
0.00.051.532 I print_info: n_swa            = 0
0.00.051.532 I print_info: n_embd_head_k    = 128
0.00.051.534 I print_info: n_embd_head_v    = 128
0.00.051.535 I print_info: n_gqa            = 1
0.00.051.536 I print_info: n_embd_k_gqa     = 2048
0.00.051.537 I print_info: n_embd_v_gqa     = 2048
0.00.051.537 I print_info: f_norm_eps       = 1.0e-05
0.00.051.538 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.538 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.538 I print_info: f_logit_scale    = 0.0e+00
0.00.051.540 I print_info: n_ff             = 8192
0.00.051.540 I print_info: n_expert         = 0
0.00.051.541 I print_info: n_expert_used    = 0
0.00.051.541 I print_info: causal attn      = 1
0.00.051.541 I print_info: pooling type     = 0
0.00.051.541 I print_info: rope type        = 2
0.00.051.541 I print_info: rope scaling     = linear
0.00.051.542 I print_info: freq_base_train  = 10000.0
0.00.051.542 I print_info: freq_scale_train = 1
0.00.051.542 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.542 I print_info: rope_finetuned   = unknown
0.00.051.542 I print_info: ssm_d_conv       = 0
0.00.051.542 I print_info: ssm_d_inner      = 0
0.00.051.543 I print_info: ssm_d_state      = 0
0.00.051.543 I print_info: ssm_dt_rank      = 0
0.00.051.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.543 I print_info: model type       = 1.4B
0.00.051.544 I print_info: model params     = 1.41 B
0.00.051.544 I print_info: general.name     = 1.4B
0.00.051.544 I print_info: vocab type       = BPE
0.00.051.544 I print_info: n_vocab          = 50304
0.00.051.545 I print_info: n_merges         = 50009
0.00.051.545 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.547 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.547 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.547 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.547 I print_info: LF token         = 128 'Ä'
0.00.051.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.548 I print_info: max token length = 1024
0.00.053.297 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.298 I load_tensors: offloading output layer to GPU
0.00.053.298 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.304 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.305 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.565 I llama_init_from_model: n_seq_max     = 1
0.00.053.566 I llama_init_from_model: n_ctx         = 2048
0.00.053.566 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.566 I llama_init_from_model: n_batch       = 2048
0.00.053.566 I llama_init_from_model: n_ubatch      = 512
0.00.053.567 I llama_init_from_model: flash_attn    = 0
0.00.053.567 I llama_init_from_model: freq_base     = 10000.0
0.00.053.567 I llama_init_from_model: freq_scale    = 1
0.00.053.568 I ggml_metal_init: allocating
0.00.053.570 I ggml_metal_init: found device: Apple M4
0.00.053.572 I ggml_metal_init: picking default device: Apple M4
0.00.054.136 I ggml_metal_init: using embedded metal library
0.00.056.491 I ggml_metal_init: GPU name:   Apple M4
0.00.056.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.494 I ggml_metal_init: simdgroup reduction   = true
0.00.056.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.494 I ggml_metal_init: has bfloat            = true
0.00.056.494 I ggml_metal_init: use bfloat            = true
0.00.056.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.617 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.474 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.482 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.481 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.483 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.483 I llama_init_from_model: graph nodes  = 967
0.00.087.484 I llama_init_from_model: graph splits = 2
0.00.087.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.606 I main: llama threadpool init, n_threads = 4
0.00.565.644 I 
0.00.565.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.694 I 
0.00.565.916 I sampler seed: 1234
0.00.565.920 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.565.970 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.565.972 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.565.972 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.316.647 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.316.648 I llama_perf_context_print:        load time =     556.07 ms
0.01.316.648 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.65 tokens per second)
0.01.316.649 I llama_perf_context_print:        eval time =     703.18 ms /    63 runs   (   11.16 ms per token,    89.59 tokens per second)
0.01.316.650 I llama_perf_context_print:       total time =     751.04 ms /    70 tokens
0.01.316.873 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.110s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.087 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.272 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.279 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.280 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.282 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.286 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.944 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.944 I llama_model_loader: - type  f32:  194 tensors
0.00.024.944 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.945 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.945 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.945 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.946 I print_info: file format = GGUF V3 (latest)
0.00.024.952 I print_info: file type   = Q3_K - Medium
0.00.024.953 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.387 I load: special tokens cache size = 25
0.00.050.068 I load: token to piece cache size = 0.2984 MB
0.00.050.071 I print_info: arch             = gptneox
0.00.050.071 I print_info: vocab_only       = 0
0.00.050.071 I print_info: n_ctx_train      = 2048
0.00.050.071 I print_info: n_embd           = 2048
0.00.050.071 I print_info: n_layer          = 24
0.00.050.074 I print_info: n_head           = 16
0.00.050.075 I print_info: n_head_kv        = 16
0.00.050.075 I print_info: n_rot            = 32
0.00.050.075 I print_info: n_swa            = 0
0.00.050.076 I print_info: n_embd_head_k    = 128
0.00.050.076 I print_info: n_embd_head_v    = 128
0.00.050.077 I print_info: n_gqa            = 1
0.00.050.077 I print_info: n_embd_k_gqa     = 2048
0.00.050.081 I print_info: n_embd_v_gqa     = 2048
0.00.050.081 I print_info: f_norm_eps       = 1.0e-05
0.00.050.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.082 I print_info: f_logit_scale    = 0.0e+00
0.00.050.089 I print_info: n_ff             = 8192
0.00.050.091 I print_info: n_expert         = 0
0.00.050.091 I print_info: n_expert_used    = 0
0.00.050.092 I print_info: causal attn      = 1
0.00.050.092 I print_info: pooling type     = 0
0.00.050.092 I print_info: rope type        = 2
0.00.050.093 I print_info: rope scaling     = linear
0.00.050.093 I print_info: freq_base_train  = 10000.0
0.00.050.093 I print_info: freq_scale_train = 1
0.00.050.093 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.094 I print_info: rope_finetuned   = unknown
0.00.050.094 I print_info: ssm_d_conv       = 0
0.00.050.094 I print_info: ssm_d_inner      = 0
0.00.050.094 I print_info: ssm_d_state      = 0
0.00.050.094 I print_info: ssm_dt_rank      = 0
0.00.050.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.094 I print_info: model type       = 1.4B
0.00.050.095 I print_info: model params     = 1.41 B
0.00.050.095 I print_info: general.name     = 1.4B
0.00.050.096 I print_info: vocab type       = BPE
0.00.050.096 I print_info: n_vocab          = 50304
0.00.050.096 I print_info: n_merges         = 50009
0.00.050.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: LF token         = 128 'Ä'
0.00.050.098 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: max token length = 1024
0.00.051.730 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.730 I load_tensors: offloading output layer to GPU
0.00.051.730 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.740 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.741 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.014 I llama_init_from_model: n_seq_max     = 1
0.00.052.015 I llama_init_from_model: n_ctx         = 128
0.00.052.015 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.015 I llama_init_from_model: n_batch       = 128
0.00.052.015 I llama_init_from_model: n_ubatch      = 128
0.00.052.015 I llama_init_from_model: flash_attn    = 0
0.00.052.016 I llama_init_from_model: freq_base     = 10000.0
0.00.052.016 I llama_init_from_model: freq_scale    = 1
0.00.052.016 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.017 I ggml_metal_init: allocating
0.00.052.020 I ggml_metal_init: found device: Apple M4
0.00.052.022 I ggml_metal_init: picking default device: Apple M4
0.00.052.574 I ggml_metal_init: using embedded metal library
0.00.054.892 I ggml_metal_init: GPU name:   Apple M4
0.00.054.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.894 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.894 I ggml_metal_init: simdgroup reduction   = true
0.00.054.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.895 I ggml_metal_init: has bfloat            = true
0.00.054.895 I ggml_metal_init: use bfloat            = true
0.00.054.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.498 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.804 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.693 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.694 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.695 I llama_init_from_model: graph nodes  = 967
0.00.066.695 I llama_init_from_model: graph splits = 2
0.00.066.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.513 I 
0.00.513.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.557 I perplexity: tokenizing the input ..
0.00.520.868 I perplexity: tokenization took 7.308 ms
0.00.520.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.652.776 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.653.911 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.653.933 I llama_perf_context_print:        load time =     504.42 ms
0.00.653.938 I llama_perf_context_print: prompt eval time =     131.64 ms /   128 tokens (    1.03 ms per token,   972.32 tokens per second)
0.00.653.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.653.939 I llama_perf_context_print:       total time =     140.42 ms /   129 tokens
0.00.654.480 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.078s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.272 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.855 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.859 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.860 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.860 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.861 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.864 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.554 I llama_model_loader: - type  f32:  194 tensors
0.00.025.555 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.555 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.555 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.556 I print_info: file format = GGUF V3 (latest)
0.00.025.568 I print_info: file type   = Q4_K - Medium
0.00.025.569 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.989 I load: special tokens cache size = 25
0.00.050.973 I load: token to piece cache size = 0.2984 MB
0.00.050.976 I print_info: arch             = gptneox
0.00.050.976 I print_info: vocab_only       = 0
0.00.050.976 I print_info: n_ctx_train      = 2048
0.00.050.976 I print_info: n_embd           = 2048
0.00.050.977 I print_info: n_layer          = 24
0.00.050.979 I print_info: n_head           = 16
0.00.050.980 I print_info: n_head_kv        = 16
0.00.050.980 I print_info: n_rot            = 32
0.00.050.980 I print_info: n_swa            = 0
0.00.050.980 I print_info: n_embd_head_k    = 128
0.00.050.981 I print_info: n_embd_head_v    = 128
0.00.050.981 I print_info: n_gqa            = 1
0.00.050.982 I print_info: n_embd_k_gqa     = 2048
0.00.050.983 I print_info: n_embd_v_gqa     = 2048
0.00.050.985 I print_info: f_norm_eps       = 1.0e-05
0.00.050.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.986 I print_info: f_logit_scale    = 0.0e+00
0.00.050.986 I print_info: n_ff             = 8192
0.00.050.987 I print_info: n_expert         = 0
0.00.050.988 I print_info: n_expert_used    = 0
0.00.050.990 I print_info: causal attn      = 1
0.00.050.990 I print_info: pooling type     = 0
0.00.050.990 I print_info: rope type        = 2
0.00.050.990 I print_info: rope scaling     = linear
0.00.050.991 I print_info: freq_base_train  = 10000.0
0.00.050.991 I print_info: freq_scale_train = 1
0.00.050.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.991 I print_info: rope_finetuned   = unknown
0.00.050.991 I print_info: ssm_d_conv       = 0
0.00.050.991 I print_info: ssm_d_inner      = 0
0.00.050.992 I print_info: ssm_d_state      = 0
0.00.050.992 I print_info: ssm_dt_rank      = 0
0.00.050.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.992 I print_info: model type       = 1.4B
0.00.050.992 I print_info: model params     = 1.41 B
0.00.050.993 I print_info: general.name     = 1.4B
0.00.050.993 I print_info: vocab type       = BPE
0.00.050.993 I print_info: n_vocab          = 50304
0.00.050.994 I print_info: n_merges         = 50009
0.00.050.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.994 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.994 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.998 I print_info: LF token         = 128 'Ä'
0.00.050.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.999 I print_info: max token length = 1024
0.00.052.971 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.971 I load_tensors: offloading output layer to GPU
0.00.052.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.982 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.983 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.259 I llama_init_from_model: n_seq_max     = 1
0.00.053.260 I llama_init_from_model: n_ctx         = 2048
0.00.053.260 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.260 I llama_init_from_model: n_batch       = 2048
0.00.053.260 I llama_init_from_model: n_ubatch      = 512
0.00.053.260 I llama_init_from_model: flash_attn    = 0
0.00.053.261 I llama_init_from_model: freq_base     = 10000.0
0.00.053.261 I llama_init_from_model: freq_scale    = 1
0.00.053.262 I ggml_metal_init: allocating
0.00.053.265 I ggml_metal_init: found device: Apple M4
0.00.053.267 I ggml_metal_init: picking default device: Apple M4
0.00.053.880 I ggml_metal_init: using embedded metal library
0.00.056.257 I ggml_metal_init: GPU name:   Apple M4
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.260 I ggml_metal_init: simdgroup reduction   = true
0.00.056.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.260 I ggml_metal_init: has bfloat            = true
0.00.056.260 I ggml_metal_init: use bfloat            = true
0.00.056.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.170 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.452 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.513 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.514 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.514 I llama_init_from_model: graph nodes  = 967
0.00.086.515 I llama_init_from_model: graph splits = 2
0.00.086.517 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.336 I main: llama threadpool init, n_threads = 4
0.00.612.377 I 
0.00.612.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.401 I 
0.00.612.632 I sampler seed: 1234
0.00.612.636 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.649 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.798 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.378.798 I llama_perf_context_print:        load time =     603.06 ms
0.01.378.799 I llama_perf_context_print: prompt eval time =      50.96 ms /     7 tokens (    7.28 ms per token,   137.35 tokens per second)
0.01.378.800 I llama_perf_context_print:        eval time =     712.18 ms /    63 runs   (   11.30 ms per token,    88.46 tokens per second)
0.01.378.800 I llama_perf_context_print:       total time =     766.46 ms /    70 tokens
0.01.379.007 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.334 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.368 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.372 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.140 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.871 I llama_model_loader: - type  f32:  194 tensors
0.00.025.871 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.871 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.871 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.872 I print_info: file format = GGUF V3 (latest)
0.00.025.878 I print_info: file type   = Q4_K - Medium
0.00.025.879 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.503 I load: special tokens cache size = 25
0.00.050.392 I load: token to piece cache size = 0.2984 MB
0.00.050.395 I print_info: arch             = gptneox
0.00.050.395 I print_info: vocab_only       = 0
0.00.050.396 I print_info: n_ctx_train      = 2048
0.00.050.396 I print_info: n_embd           = 2048
0.00.050.396 I print_info: n_layer          = 24
0.00.050.399 I print_info: n_head           = 16
0.00.050.400 I print_info: n_head_kv        = 16
0.00.050.400 I print_info: n_rot            = 32
0.00.050.401 I print_info: n_swa            = 0
0.00.050.401 I print_info: n_embd_head_k    = 128
0.00.050.401 I print_info: n_embd_head_v    = 128
0.00.050.402 I print_info: n_gqa            = 1
0.00.050.403 I print_info: n_embd_k_gqa     = 2048
0.00.050.403 I print_info: n_embd_v_gqa     = 2048
0.00.050.404 I print_info: f_norm_eps       = 1.0e-05
0.00.050.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.405 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.405 I print_info: f_logit_scale    = 0.0e+00
0.00.050.406 I print_info: n_ff             = 8192
0.00.050.406 I print_info: n_expert         = 0
0.00.050.406 I print_info: n_expert_used    = 0
0.00.050.406 I print_info: causal attn      = 1
0.00.050.406 I print_info: pooling type     = 0
0.00.050.406 I print_info: rope type        = 2
0.00.050.407 I print_info: rope scaling     = linear
0.00.050.407 I print_info: freq_base_train  = 10000.0
0.00.050.408 I print_info: freq_scale_train = 1
0.00.050.408 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.408 I print_info: rope_finetuned   = unknown
0.00.050.409 I print_info: ssm_d_conv       = 0
0.00.050.409 I print_info: ssm_d_inner      = 0
0.00.050.409 I print_info: ssm_d_state      = 0
0.00.050.410 I print_info: ssm_dt_rank      = 0
0.00.050.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.410 I print_info: model type       = 1.4B
0.00.050.410 I print_info: model params     = 1.41 B
0.00.050.410 I print_info: general.name     = 1.4B
0.00.050.411 I print_info: vocab type       = BPE
0.00.050.411 I print_info: n_vocab          = 50304
0.00.050.411 I print_info: n_merges         = 50009
0.00.050.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.414 I print_info: LF token         = 128 'Ä'
0.00.050.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.414 I print_info: max token length = 1024
0.00.052.055 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.055 I load_tensors: offloading output layer to GPU
0.00.052.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.065 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.067 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.347 I llama_init_from_model: n_seq_max     = 1
0.00.052.348 I llama_init_from_model: n_ctx         = 128
0.00.052.348 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.348 I llama_init_from_model: n_batch       = 128
0.00.052.348 I llama_init_from_model: n_ubatch      = 128
0.00.052.349 I llama_init_from_model: flash_attn    = 0
0.00.052.349 I llama_init_from_model: freq_base     = 10000.0
0.00.052.349 I llama_init_from_model: freq_scale    = 1
0.00.052.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.350 I ggml_metal_init: allocating
0.00.052.353 I ggml_metal_init: found device: Apple M4
0.00.052.355 I ggml_metal_init: picking default device: Apple M4
0.00.052.925 I ggml_metal_init: using embedded metal library
0.00.055.232 I ggml_metal_init: GPU name:   Apple M4
0.00.055.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.234 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.234 I ggml_metal_init: simdgroup reduction   = true
0.00.055.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.234 I ggml_metal_init: has bfloat            = true
0.00.055.234 I ggml_metal_init: use bfloat            = true
0.00.055.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.812 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.109 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.111 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.125 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.079 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.080 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.081 I llama_init_from_model: graph nodes  = 967
0.00.067.081 I llama_init_from_model: graph splits = 2
0.00.067.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.924 I 
0.00.555.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.964 I perplexity: tokenizing the input ..
0.00.563.496 I perplexity: tokenization took 7.531 ms
0.00.563.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.738 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.698.912 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.698.936 I llama_perf_context_print:        load time =     545.59 ms
0.00.698.939 I llama_perf_context_print: prompt eval time =     133.98 ms /   128 tokens (    1.05 ms per token,   955.36 tokens per second)
0.00.698.940 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.940 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.699.341 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.077s
sys	0m0.099s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.537 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.985 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.687 I llama_model_loader: - type  f32:  194 tensors
0.00.027.687 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.687 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.688 I print_info: file format = GGUF V3 (latest)
0.00.027.700 I print_info: file type   = Q5_K - Medium
0.00.027.701 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.163 I load: special tokens cache size = 25
0.00.053.206 I load: token to piece cache size = 0.2984 MB
0.00.053.209 I print_info: arch             = gptneox
0.00.053.209 I print_info: vocab_only       = 0
0.00.053.209 I print_info: n_ctx_train      = 2048
0.00.053.209 I print_info: n_embd           = 2048
0.00.053.210 I print_info: n_layer          = 24
0.00.053.212 I print_info: n_head           = 16
0.00.053.213 I print_info: n_head_kv        = 16
0.00.053.213 I print_info: n_rot            = 32
0.00.053.215 I print_info: n_swa            = 0
0.00.053.215 I print_info: n_embd_head_k    = 128
0.00.053.216 I print_info: n_embd_head_v    = 128
0.00.053.216 I print_info: n_gqa            = 1
0.00.053.217 I print_info: n_embd_k_gqa     = 2048
0.00.053.218 I print_info: n_embd_v_gqa     = 2048
0.00.053.218 I print_info: f_norm_eps       = 1.0e-05
0.00.053.219 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.219 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.219 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.219 I print_info: f_logit_scale    = 0.0e+00
0.00.053.220 I print_info: n_ff             = 8192
0.00.053.220 I print_info: n_expert         = 0
0.00.053.220 I print_info: n_expert_used    = 0
0.00.053.221 I print_info: causal attn      = 1
0.00.053.221 I print_info: pooling type     = 0
0.00.053.221 I print_info: rope type        = 2
0.00.053.222 I print_info: rope scaling     = linear
0.00.053.222 I print_info: freq_base_train  = 10000.0
0.00.053.222 I print_info: freq_scale_train = 1
0.00.053.223 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.223 I print_info: rope_finetuned   = unknown
0.00.053.223 I print_info: ssm_d_conv       = 0
0.00.053.223 I print_info: ssm_d_inner      = 0
0.00.053.223 I print_info: ssm_d_state      = 0
0.00.053.223 I print_info: ssm_dt_rank      = 0
0.00.053.224 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.224 I print_info: model type       = 1.4B
0.00.053.224 I print_info: model params     = 1.41 B
0.00.053.224 I print_info: general.name     = 1.4B
0.00.053.226 I print_info: vocab type       = BPE
0.00.053.226 I print_info: n_vocab          = 50304
0.00.053.226 I print_info: n_merges         = 50009
0.00.053.227 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.227 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.227 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.227 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.227 I print_info: LF token         = 128 'Ä'
0.00.053.228 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.228 I print_info: max token length = 1024
0.00.055.225 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.225 I load_tensors: offloading output layer to GPU
0.00.055.225 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.236 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.238 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.547 I llama_init_from_model: n_seq_max     = 1
0.00.055.547 I llama_init_from_model: n_ctx         = 2048
0.00.055.548 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.548 I llama_init_from_model: n_batch       = 2048
0.00.055.548 I llama_init_from_model: n_ubatch      = 512
0.00.055.548 I llama_init_from_model: flash_attn    = 0
0.00.055.549 I llama_init_from_model: freq_base     = 10000.0
0.00.055.549 I llama_init_from_model: freq_scale    = 1
0.00.055.549 I ggml_metal_init: allocating
0.00.055.553 I ggml_metal_init: found device: Apple M4
0.00.055.555 I ggml_metal_init: picking default device: Apple M4
0.00.056.150 I ggml_metal_init: using embedded metal library
0.00.058.540 I ggml_metal_init: GPU name:   Apple M4
0.00.058.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.543 I ggml_metal_init: simdgroup reduction   = true
0.00.058.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.543 I ggml_metal_init: has bfloat            = true
0.00.058.543 I ggml_metal_init: use bfloat            = true
0.00.058.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.476 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.597 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.652 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.653 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.654 I llama_init_from_model: graph nodes  = 967
0.00.089.654 I llama_init_from_model: graph splits = 2
0.00.089.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.791 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.991 I main: llama threadpool init, n_threads = 4
0.00.701.030 I 
0.00.701.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.075 I 
0.00.701.300 I sampler seed: 1234
0.00.701.305 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.335 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.338 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.339 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.552.595 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.552.596 I llama_perf_context_print:        load time =     689.45 ms
0.01.552.596 I llama_perf_context_print: prompt eval time =      51.72 ms /     7 tokens (    7.39 ms per token,   135.35 tokens per second)
0.01.552.597 I llama_perf_context_print:        eval time =     796.45 ms /    63 runs   (   12.64 ms per token,    79.10 tokens per second)
0.01.552.597 I llama_perf_context_print:       total time =     851.61 ms /    70 tokens
0.01.552.806 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.177 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.701 I llama_model_loader: - type  f32:  194 tensors
0.00.025.701 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.701 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.702 I print_info: file format = GGUF V3 (latest)
0.00.025.708 I print_info: file type   = Q5_K - Medium
0.00.025.709 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.265 I load: special tokens cache size = 25
0.00.050.310 I load: token to piece cache size = 0.2984 MB
0.00.050.313 I print_info: arch             = gptneox
0.00.050.313 I print_info: vocab_only       = 0
0.00.050.313 I print_info: n_ctx_train      = 2048
0.00.050.314 I print_info: n_embd           = 2048
0.00.050.314 I print_info: n_layer          = 24
0.00.050.316 I print_info: n_head           = 16
0.00.050.317 I print_info: n_head_kv        = 16
0.00.050.317 I print_info: n_rot            = 32
0.00.050.318 I print_info: n_swa            = 0
0.00.050.318 I print_info: n_embd_head_k    = 128
0.00.050.318 I print_info: n_embd_head_v    = 128
0.00.050.319 I print_info: n_gqa            = 1
0.00.050.319 I print_info: n_embd_k_gqa     = 2048
0.00.050.320 I print_info: n_embd_v_gqa     = 2048
0.00.050.321 I print_info: f_norm_eps       = 1.0e-05
0.00.050.321 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.321 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.322 I print_info: f_logit_scale    = 0.0e+00
0.00.050.323 I print_info: n_ff             = 8192
0.00.050.323 I print_info: n_expert         = 0
0.00.050.323 I print_info: n_expert_used    = 0
0.00.050.323 I print_info: causal attn      = 1
0.00.050.323 I print_info: pooling type     = 0
0.00.050.323 I print_info: rope type        = 2
0.00.050.324 I print_info: rope scaling     = linear
0.00.050.324 I print_info: freq_base_train  = 10000.0
0.00.050.324 I print_info: freq_scale_train = 1
0.00.050.324 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.325 I print_info: rope_finetuned   = unknown
0.00.050.325 I print_info: ssm_d_conv       = 0
0.00.050.325 I print_info: ssm_d_inner      = 0
0.00.050.325 I print_info: ssm_d_state      = 0
0.00.050.325 I print_info: ssm_dt_rank      = 0
0.00.050.326 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.326 I print_info: model type       = 1.4B
0.00.050.326 I print_info: model params     = 1.41 B
0.00.050.328 I print_info: general.name     = 1.4B
0.00.050.328 I print_info: vocab type       = BPE
0.00.050.329 I print_info: n_vocab          = 50304
0.00.050.329 I print_info: n_merges         = 50009
0.00.050.329 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.329 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.329 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.330 I print_info: LF token         = 128 'Ä'
0.00.050.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.330 I print_info: max token length = 1024
0.00.052.041 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.041 I load_tensors: offloading output layer to GPU
0.00.052.041 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.052 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.053 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.313 I llama_init_from_model: n_seq_max     = 1
0.00.052.314 I llama_init_from_model: n_ctx         = 128
0.00.052.314 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.314 I llama_init_from_model: n_batch       = 128
0.00.052.315 I llama_init_from_model: n_ubatch      = 128
0.00.052.315 I llama_init_from_model: flash_attn    = 0
0.00.052.315 I llama_init_from_model: freq_base     = 10000.0
0.00.052.315 I llama_init_from_model: freq_scale    = 1
0.00.052.316 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.316 I ggml_metal_init: allocating
0.00.052.319 I ggml_metal_init: found device: Apple M4
0.00.052.321 I ggml_metal_init: picking default device: Apple M4
0.00.052.877 I ggml_metal_init: using embedded metal library
0.00.055.198 I ggml_metal_init: GPU name:   Apple M4
0.00.055.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.200 I ggml_metal_init: simdgroup reduction   = true
0.00.055.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.201 I ggml_metal_init: has bfloat            = true
0.00.055.201 I ggml_metal_init: use bfloat            = true
0.00.055.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.810 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.117 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.120 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.134 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.017 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.018 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.018 I llama_init_from_model: graph nodes  = 967
0.00.067.018 I llama_init_from_model: graph splits = 2
0.00.067.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.176 I 
0.00.644.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.221 I perplexity: tokenizing the input ..
0.00.651.888 I perplexity: tokenization took 7.665 ms
0.00.651.892 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.514 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.784 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.808 I llama_perf_context_print:        load time =     633.99 ms
0.00.793.809 I llama_perf_context_print: prompt eval time =     140.36 ms /   128 tokens (    1.10 ms per token,   911.91 tokens per second)
0.00.793.810 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.810 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.794.379 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.077s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.060 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.080 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.083 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.083 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.083 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.594 I llama_model_loader: - type  f32:  194 tensors
0.00.026.594 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.595 I print_info: file format = GGUF V3 (latest)
0.00.026.607 I print_info: file type   = Q6_K
0.00.026.607 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.439 I load: special tokens cache size = 25
0.00.051.445 I load: token to piece cache size = 0.2984 MB
0.00.051.448 I print_info: arch             = gptneox
0.00.051.448 I print_info: vocab_only       = 0
0.00.051.449 I print_info: n_ctx_train      = 2048
0.00.051.449 I print_info: n_embd           = 2048
0.00.051.449 I print_info: n_layer          = 24
0.00.051.452 I print_info: n_head           = 16
0.00.051.453 I print_info: n_head_kv        = 16
0.00.051.453 I print_info: n_rot            = 32
0.00.051.454 I print_info: n_swa            = 0
0.00.051.455 I print_info: n_embd_head_k    = 128
0.00.051.455 I print_info: n_embd_head_v    = 128
0.00.051.455 I print_info: n_gqa            = 1
0.00.051.456 I print_info: n_embd_k_gqa     = 2048
0.00.051.457 I print_info: n_embd_v_gqa     = 2048
0.00.051.457 I print_info: f_norm_eps       = 1.0e-05
0.00.051.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.458 I print_info: f_logit_scale    = 0.0e+00
0.00.051.459 I print_info: n_ff             = 8192
0.00.051.459 I print_info: n_expert         = 0
0.00.051.459 I print_info: n_expert_used    = 0
0.00.051.460 I print_info: causal attn      = 1
0.00.051.461 I print_info: pooling type     = 0
0.00.051.462 I print_info: rope type        = 2
0.00.051.463 I print_info: rope scaling     = linear
0.00.051.463 I print_info: freq_base_train  = 10000.0
0.00.051.463 I print_info: freq_scale_train = 1
0.00.051.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.464 I print_info: rope_finetuned   = unknown
0.00.051.464 I print_info: ssm_d_conv       = 0
0.00.051.464 I print_info: ssm_d_inner      = 0
0.00.051.464 I print_info: ssm_d_state      = 0
0.00.051.464 I print_info: ssm_dt_rank      = 0
0.00.051.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.465 I print_info: model type       = 1.4B
0.00.051.465 I print_info: model params     = 1.41 B
0.00.051.465 I print_info: general.name     = 1.4B
0.00.051.466 I print_info: vocab type       = BPE
0.00.051.466 I print_info: n_vocab          = 50304
0.00.051.466 I print_info: n_merges         = 50009
0.00.051.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.467 I print_info: LF token         = 128 'Ä'
0.00.051.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.471 I print_info: max token length = 1024
0.00.053.450 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.450 I load_tensors: offloading output layer to GPU
0.00.053.451 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.461 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.463 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.743 I llama_init_from_model: n_seq_max     = 1
0.00.053.744 I llama_init_from_model: n_ctx         = 2048
0.00.053.744 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.744 I llama_init_from_model: n_batch       = 2048
0.00.053.745 I llama_init_from_model: n_ubatch      = 512
0.00.053.745 I llama_init_from_model: flash_attn    = 0
0.00.053.745 I llama_init_from_model: freq_base     = 10000.0
0.00.053.745 I llama_init_from_model: freq_scale    = 1
0.00.053.746 I ggml_metal_init: allocating
0.00.053.749 I ggml_metal_init: found device: Apple M4
0.00.053.750 I ggml_metal_init: picking default device: Apple M4
0.00.054.361 I ggml_metal_init: using embedded metal library
0.00.056.692 I ggml_metal_init: GPU name:   Apple M4
0.00.056.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.694 I ggml_metal_init: simdgroup reduction   = true
0.00.056.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.695 I ggml_metal_init: has bfloat            = true
0.00.056.695 I ggml_metal_init: use bfloat            = true
0.00.056.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.493 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.518 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.539 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.583 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.584 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.585 I llama_init_from_model: graph nodes  = 967
0.00.086.585 I llama_init_from_model: graph splits = 2
0.00.086.588 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.544 I main: llama threadpool init, n_threads = 4
0.00.765.579 I 
0.00.765.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.607 I 
0.00.765.837 I sampler seed: 1234
0.00.765.841 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.854 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.644.984 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.644.985 I llama_perf_context_print:        load time =     755.71 ms
0.01.644.985 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.87 tokens per second)
0.01.644.986 I llama_perf_context_print:        eval time =     821.77 ms /    63 runs   (   13.04 ms per token,    76.66 tokens per second)
0.01.644.986 I llama_perf_context_print:       total time =     879.44 ms /    70 tokens
0.01.645.228 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.109s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4476 (a59ee7c4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.398 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.188 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.774 I llama_model_loader: - type  f32:  194 tensors
0.00.025.774 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.774 I print_info: file format = GGUF V3 (latest)
0.00.025.781 I print_info: file type   = Q6_K
0.00.025.782 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.405 I load: special tokens cache size = 25
0.00.050.599 I load: token to piece cache size = 0.2984 MB
0.00.050.602 I print_info: arch             = gptneox
0.00.050.602 I print_info: vocab_only       = 0
0.00.050.602 I print_info: n_ctx_train      = 2048
0.00.050.602 I print_info: n_embd           = 2048
0.00.050.603 I print_info: n_layer          = 24
0.00.050.605 I print_info: n_head           = 16
0.00.050.606 I print_info: n_head_kv        = 16
0.00.050.606 I print_info: n_rot            = 32
0.00.050.607 I print_info: n_swa            = 0
0.00.050.607 I print_info: n_embd_head_k    = 128
0.00.050.607 I print_info: n_embd_head_v    = 128
0.00.050.608 I print_info: n_gqa            = 1
0.00.050.609 I print_info: n_embd_k_gqa     = 2048
0.00.050.610 I print_info: n_embd_v_gqa     = 2048
0.00.050.611 I print_info: f_norm_eps       = 1.0e-05
0.00.050.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.614 I print_info: f_logit_scale    = 0.0e+00
0.00.050.614 I print_info: n_ff             = 8192
0.00.050.615 I print_info: n_expert         = 0
0.00.050.615 I print_info: n_expert_used    = 0
0.00.050.615 I print_info: causal attn      = 1
0.00.050.615 I print_info: pooling type     = 0
0.00.050.615 I print_info: rope type        = 2
0.00.050.615 I print_info: rope scaling     = linear
0.00.050.616 I print_info: freq_base_train  = 10000.0
0.00.050.616 I print_info: freq_scale_train = 1
0.00.050.616 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.616 I print_info: rope_finetuned   = unknown
0.00.050.617 I print_info: ssm_d_conv       = 0
0.00.050.617 I print_info: ssm_d_inner      = 0
0.00.050.617 I print_info: ssm_d_state      = 0
0.00.050.618 I print_info: ssm_dt_rank      = 0
0.00.050.618 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.618 I print_info: model type       = 1.4B
0.00.050.619 I print_info: model params     = 1.41 B
0.00.050.619 I print_info: general.name     = 1.4B
0.00.050.619 I print_info: vocab type       = BPE
0.00.050.620 I print_info: n_vocab          = 50304
0.00.050.620 I print_info: n_merges         = 50009
0.00.050.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.621 I print_info: LF token         = 128 'Ä'
0.00.050.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.621 I print_info: max token length = 1024
0.00.052.658 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.659 I load_tensors: offloading output layer to GPU
0.00.052.659 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.669 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.670 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.033 I llama_init_from_model: n_seq_max     = 1
0.00.053.034 I llama_init_from_model: n_ctx         = 128
0.00.053.034 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.034 I llama_init_from_model: n_batch       = 128
0.00.053.034 I llama_init_from_model: n_ubatch      = 128
0.00.053.034 I llama_init_from_model: flash_attn    = 0
0.00.053.035 I llama_init_from_model: freq_base     = 10000.0
0.00.053.035 I llama_init_from_model: freq_scale    = 1
0.00.053.035 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.036 I ggml_metal_init: allocating
0.00.053.038 I ggml_metal_init: found device: Apple M4
0.00.053.040 I ggml_metal_init: picking default device: Apple M4
0.00.053.617 I ggml_metal_init: using embedded metal library
0.00.055.947 I ggml_metal_init: GPU name:   Apple M4
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.949 I ggml_metal_init: simdgroup reduction   = true
0.00.055.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.950 I ggml_metal_init: has bfloat            = true
0.00.055.950 I ggml_metal_init: use bfloat            = true
0.00.055.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.806 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.127 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.002 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.003 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.003 I llama_init_from_model: graph nodes  = 967
0.00.068.003 I llama_init_from_model: graph splits = 2
0.00.068.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.126.613 I 
0.00.126.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.126.669 I perplexity: tokenizing the input ..
0.00.134.243 I perplexity: tokenization took 7.572 ms
0.00.134.247 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.272.725 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.273.875 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.273.892 I llama_perf_context_print:        load time =     116.21 ms
0.00.273.896 I llama_perf_context_print: prompt eval time =     138.23 ms /   128 tokens (    1.08 ms per token,   925.97 tokens per second)
0.00.273.897 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.273.897 I llama_perf_context_print:       total time =     147.28 ms /   129 tokens
0.00.274.314 I ggml_metal_free: deallocating

real	0m0.290s
user	0m0.077s
sys	0m0.039s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4476 (a59ee7c4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13460a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13460aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13460aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13460b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13460bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13460c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13460c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13460cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13460d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13460d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13460dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13460e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13460ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13460f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13460fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1346135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1346162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1346177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1346183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1346191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13461a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13461a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13461ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13461b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13461bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13461c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13461c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13461ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13461d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13461da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13461e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13461e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13461ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13461f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13461f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13461fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1346209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1346217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1346225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1346261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1346271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1346281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1346291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13462a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13462a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13462ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13462b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13462b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13462bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13461b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13462c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13462c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13462cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13462d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13462d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13462dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13462e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13462e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13462ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13462f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13462f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13462fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1346302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1346316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1346344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1346352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1346360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1346369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1346377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1346385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13463a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13463a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13463aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13463af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13463b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13463b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13463bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13463c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13463c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13463cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13463cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13463d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13463d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13463dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13463e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13463e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13463eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13463f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13463f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13463f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13463fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1346419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1346422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1346430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1346447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1346455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1346463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1346484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1346494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13464a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13464a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13464b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13464b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13464b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13464bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13464c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13464cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13464d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13464d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13464dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13464e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13464e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13464ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13464f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13464f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13464fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1346507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1346517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1346527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1346581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1346591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13465a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13465a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13465ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13465b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13465b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13465bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13465c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13465c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13465cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13465d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13465d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13465dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13465e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13465e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13465ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13465f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13465f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13465fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1346606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1346610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1346619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1346627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1346635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1346656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1346680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1346686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127b04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127b05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127b056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127b05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127b05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127b06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127b06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127b06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127b07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127b075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127b07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127b08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127b08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127b093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127b09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127b0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127b0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127b0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127b0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127b0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127b0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127b0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127b0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127b0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127b0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127b0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127b0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127b0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127b0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127b0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127b0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127b0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127b10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127b106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127b10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127b10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127b11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127b118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127b11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127b12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127b12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127b12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127b12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127b13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127b137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127b13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127b140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127b14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127b14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127b14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127b15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127b156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127b15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127b15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127b16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127b16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127b16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127b17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127b17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127b17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127b18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127b184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127b18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127b18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127b19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127b19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127b19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127b19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127b1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127b1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127b1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127b1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127b1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127b1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127b1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127b1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127b1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127b1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127b1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127b1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127b1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127b1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127b1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127b1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127b1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127b1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127b1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127b1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127b1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127b20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127b20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127b209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127b20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127b212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127b21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127b21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127b22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127b22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127b228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127b22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127b231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127b23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127b23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127b23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127b24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127b24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127b24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127b250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127b25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127b259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127b25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127b262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127b26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127b26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127b26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127b27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127b278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127b27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127b281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127b28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127b28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127b28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127b29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127b297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127b29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127b2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127b2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127b2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127b2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127b2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127b2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127b2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127b2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127b2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127b2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127b2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127b2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127b2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127b2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127b2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127b2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127b2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127b2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127b2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127b2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127b2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127b2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127b30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127b306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127b30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127b30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127b31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127b31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127b31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127b32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127b325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127b32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127b32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127b33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127b337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127b33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127b34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127b344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127b34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127b34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127b35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127b35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127b36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127b363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127b36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127b36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127b37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127b375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127b37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127b37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127b38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127b38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127b38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127b39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127b394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127b39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127b39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127b3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127b3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127b3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127b3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127b3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127b3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127b3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127b3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127b3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127b3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127b3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127b3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127b3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127b3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127b3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127b3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127b3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127b3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127b3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127b3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127b3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127b400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127b40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127b409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127b40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127b41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127b417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127b41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127b42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127b42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127b430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127b43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127b43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127b441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127b447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127b44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127b45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127b458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127b45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127b46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127b46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127b46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127b475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127b47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127b48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127b486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127b48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127b49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127b49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127b49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127b4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127b4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127b4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127b4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127b4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127b4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127b4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127b4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127b4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127b4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127b4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127b4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127b4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127b4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127b4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127b4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127b4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127b50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127b50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127b510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127b516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127b51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127b52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127b527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127b52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127b53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127b53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127b53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127b544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127b54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127b55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127b555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127b55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127b56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127b56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127b56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127b571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127b576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127b57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127b580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127b585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127b58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127b58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127b594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127b599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127b59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127b5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127b5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127b5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127b5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127b5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127b5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127b5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127b5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127b5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127b5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127b5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127b5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127b5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127c044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127c04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127c04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127c05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127c056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127c05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127c05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127c063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127c06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127c06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127c07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127c078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127c083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127c08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127c09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127c09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127c0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127c0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127c0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127c0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127c0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127c0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127c0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127c0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127c0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127c0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127c0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127c0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127c0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127c0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127c0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127c0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127c0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127c0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127c10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127c107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127c10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127c110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127c11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127c119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127c11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127c12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127c12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127c12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127c12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127c13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127c138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127c13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127c141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127c14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127c14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127c14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127c15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127c157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127c15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127c160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127c16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127c16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127c16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127c17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127c17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127c17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127c18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127c185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127c18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127c18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127c19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127c19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127c19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127c1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127c1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127c1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127c1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127c1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127c1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127c1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127c1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127c1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127c1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127c1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127c1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127c1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127c1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127c1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127c1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127c1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127c1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127c1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127c1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127c1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127c1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127c20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127c20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127c20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127c20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127c213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127c21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127c21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127c22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127c22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127c229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127c22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127c232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127c23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127c23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127c24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127c24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127c24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127c24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127c25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127c258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127c25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127c261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127c26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127c26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127c26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127c27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127c277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127c27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127c280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127c28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127c28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127c28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127c29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127c296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127c29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127c29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127c2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127c2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127c2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127c2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127c2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127c2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127c2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127c2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127c2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127c2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127c2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127c2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127c2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127c2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127c2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127c2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127c2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127c2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127c2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127c2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127c2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127c30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127c305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127c30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127c30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127c31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127c31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127c31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127c32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127c324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127c32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127c32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127c33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127c336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127c33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127c33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127c343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127c34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127c34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127c35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127c355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127c35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127c35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127c36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127c36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127c36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127c37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127c374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127c37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127c37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127c38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127c38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127c38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127c38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127c393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127c39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127c39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127c3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127c3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127c3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127c3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127c3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127c3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127c3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127c3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127c3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127c3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127c3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127c3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127c3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127c3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127c3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127c3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127c3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127c3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127c3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127c3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127c3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127c3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127c402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127c40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127c40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127c41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127c41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127c41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127c42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127c42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127c429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127c42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127c432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127c43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127c43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127c44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127c44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127c44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127c44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127c451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127c45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127c45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127c45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127c463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127c46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127c46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127c470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127c47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127c479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127c47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127c482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127c48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127c48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127c49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127c49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127c498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127c49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127c4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127c4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127c4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127c4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127c4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127c4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127c4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127c4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127c4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127c4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127c4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127c4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127c4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127c4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127c4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127c4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127c4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127c4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127c4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127c4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127c4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127c4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127c50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127c507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127c50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127c510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127c51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127c51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127c51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127c52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127c526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127c52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127c52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127c53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127c538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127c53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127c54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127c545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127c54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127c54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127c55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127c557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127c56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127c56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127c57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127c57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127c57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127c57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127c584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127c58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.778s
user	0m0.295s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4476 (a59ee7c4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f608a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f608ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f6095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f609b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f60a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f60a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f60ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f60b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f60b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f60bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f60c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f60cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f60d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f60dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f60e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f60ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f60f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f60f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f610040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f610760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f6115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f611e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f612560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f612820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f613aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f613fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f6142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f614740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f614a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f615290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f6157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f6163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f616870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f616d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f6171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f617af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f617f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f618430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f6186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f618d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f619c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f61a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f61a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f61ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f61b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f61ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f61c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f61cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f61d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f61d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f61da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f61e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f61e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f61e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f61ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f61f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f61f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f61fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f6205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f620a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f620ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f621820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f621cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f622210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f622760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f622cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f623200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f623750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f623ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f6241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f624740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f624c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f6251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f625730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f6261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f626720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f626c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f6271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f627710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f6281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f628700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f628c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f6291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f6296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f619920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f62a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f62a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f62adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f62b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f62b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f62bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f62c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f62c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f62cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f62d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f62d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f62dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f62e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f62e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f62ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f62f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f62f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f62fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f62fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f630490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f630930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f630dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f631270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f631710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f631bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f632050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f6324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f632e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f6332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f633770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f6340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f634550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f6349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f635330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f6357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f635c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f636110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f6365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f636a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f636ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f637390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f637830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f637cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f638170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f638610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f638ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f638f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f6393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f639890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f639d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f63a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f63a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f63ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f63afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f63b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f63bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f63c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f63c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f63cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f63d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f63d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f63d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f63ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f63e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f63e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f63ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f63f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f63f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f63f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f63fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f6402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f640790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f640c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f6410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f641570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f641a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f641eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f642350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f6427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f642c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f643130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f6435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f643a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f643f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f6443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f644850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f644cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f645190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f645630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f645ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f645f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f6464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f646a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f646f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f6474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f647770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f648390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f6489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f649630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f6498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f649f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f64a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f64ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f64b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f64b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f64bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f64c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f64c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f64cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f64d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f64d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f64dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f64e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f64e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f64ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f64f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f64f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f64fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f6507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f650cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f651240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f651790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f651ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f652230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f652780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f652cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f653220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f653770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f653cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f654210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f654760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f654cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f655200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f655ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f6561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f656740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f656c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f6571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f657c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f6581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f658720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f658c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f6591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f659710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f659c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f65a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f65a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f65ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f65b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f65b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f65bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f65c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f65c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f65cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f65d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f65d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f65dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f65e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f65e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f65ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f65f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f65f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f65f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f65fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f660330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f6607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f660c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f661110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f6615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f661a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f661ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f662390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f662830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f662cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f663170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f6636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f663de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f664500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f664c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f665340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f665df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f6660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f6666c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.662 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107f073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107f07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107f07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107f089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107f091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107f099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107f0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107f0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107f0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107f0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107f0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107f0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107f0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107f0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107f0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107f0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107f0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107f0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107f0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107f0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107f0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107f0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107f0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107f10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107f109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107f10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107f11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107f11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107f11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107f11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107f12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107f128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107f12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107f131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107f13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107f13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107f13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107f14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107f147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107f14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107f150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107f15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107f15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107f166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107f17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107f175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107f17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107f17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107f18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107f18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107f18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107f19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107f19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107f1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107f1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107f1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107f1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107f1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107f1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107f1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107f1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107f1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107f1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107f1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107f1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107f1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107f1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107f1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107f1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107f1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107f203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107f20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107f20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107f21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107f21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107f219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107f21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107f222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107f22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107f22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107f23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107f23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107f23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107f23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107f241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107f24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107f24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107f253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107f25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107f25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107f260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107f26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107f269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107f26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107f272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107f27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107f27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107f28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107f288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107f28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107f29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107f29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107f29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107f2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107f2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107f2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107f2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107f2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107f2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107f2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107f2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107f2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107f2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107f2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107f2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107f2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107f2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107f2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107f2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107f2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107f2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107f2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107f300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107f30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107f30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107f30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107f31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107f316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107f31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107f32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107f328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107f32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107f33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107f335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107f33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107f33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107f34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107f34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107f35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107f35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107f366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107f36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107f36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107f37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107f37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107f38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107f385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107f38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107f38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107f39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107f39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107f39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107f3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107f3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107f3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107f3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107f3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107f3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107f3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107f3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107f3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107f3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107f3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107f3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107f3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107f3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107f3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107f3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107f3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107f3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107f40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107f40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107f40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107f410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107f41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107f41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107f42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107f42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107f42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107f43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107f44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107f44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107f45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107f45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107f45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107f462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107f46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107f46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107f47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107f48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107f490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107f49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107f49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107f4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107f4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107f4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107f4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107f4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107f4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107f4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107f4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107f4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107f4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107f4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107f4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107f503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107f50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107f50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107f51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107f51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107f52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107f52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107f52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107f531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107f53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107f53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107f54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107f548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107f54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107f55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107f55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107f55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107f56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107f56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107f57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107f57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107f57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107f58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107f58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107f58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107f59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107f59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107f5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107f5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107f5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107f5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107f5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107f5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107f5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107f5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107f5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107f5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107f5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107f5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107f5e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1208044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1208056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1208063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1208092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12080a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12080a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12080af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12080b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12080be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12080c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12080cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12080d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12080dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12080dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12080e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12080e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12080e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12080edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12080f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12080f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12080fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12080fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1208102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1208114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1208133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1208149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1208152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1208177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1208180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1208189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1208196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12081a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12081a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12081ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12081b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12081b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12081ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12081bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12081c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12081c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12081cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12081d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12081d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12081d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12081ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12081e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12081e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12081eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12081efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12081f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12081f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12081fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1208205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1208217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1208224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1208253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1208269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1208272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1208291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12082a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12082a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12082ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12082b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12082b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12082b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12082be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12082c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12082c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12082cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12082d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12082d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12082d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12082dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12082e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12082e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12082eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12082ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12082f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12082f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12082fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1208300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1208309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1208328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1208331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1208347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1208350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1208366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1208378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1208385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1208397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12083a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12083a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12083a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12083ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12083b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12083b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12083bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12083bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12083c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12083c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12083ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12083d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12083d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12083da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12083deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12083e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12083e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12083ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12083f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12083f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12083f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12083fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1208406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1208424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1208436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1208455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1208474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1208493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12084a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12084a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12084aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12084ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12084b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12084b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12084bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12084c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12084c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12084c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12084cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12084d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12084d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12084dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12084df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12084e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12084e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12084eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12084f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12084f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12084f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12084fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1208502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1208521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1208533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1208540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1208549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1208552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1208568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1208576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1208579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.918s
user	0m0.243s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.51 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.07 sec*proc (2 tests)

Total Test time (real) =   1.08 sec
        1.10 real         0.68 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
