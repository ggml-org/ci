### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.23 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.15 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  178.19 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.82 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.23 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 220.51 sec*proc (27 tests)

Total Test time (real) = 220.51 sec

real	3m40.542s
user	7m36.136s
sys	0m6.013s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.93 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.18 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.35 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.37 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.06 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.98 sec*proc (27 tests)

Total Test time (real) =  50.99 sec

real	0m51.026s
user	1m12.125s
sys	0m5.378s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.105 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.001 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.976 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.988 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.990 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.991 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.991 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.007 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.008 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.009 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.009 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.010 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.014 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.015 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.016 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.016 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.017 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.018 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.018 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.726 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.729 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.729 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.730 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.730 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.731 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.731 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.732 I llama_model_loader: - type  f32:  124 tensors
0.00.031.733 I llama_model_loader: - type  f16:   73 tensors
0.00.036.408 I llm_load_vocab: special tokens cache size = 5
0.00.038.907 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.911 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.912 I llm_load_print_meta: arch             = bert
0.00.038.912 I llm_load_print_meta: vocab type       = WPM
0.00.038.913 I llm_load_print_meta: n_vocab          = 30522
0.00.038.913 I llm_load_print_meta: n_merges         = 0
0.00.038.913 I llm_load_print_meta: vocab_only       = 0
0.00.038.914 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.914 I llm_load_print_meta: n_embd           = 384
0.00.038.914 I llm_load_print_meta: n_layer          = 12
0.00.038.918 I llm_load_print_meta: n_head           = 12
0.00.038.919 I llm_load_print_meta: n_head_kv        = 12
0.00.038.919 I llm_load_print_meta: n_rot            = 32
0.00.038.919 I llm_load_print_meta: n_swa            = 0
0.00.038.920 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.920 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.921 I llm_load_print_meta: n_gqa            = 1
0.00.038.922 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.922 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.923 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.924 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.924 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.925 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.925 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.926 I llm_load_print_meta: n_ff             = 1536
0.00.038.926 I llm_load_print_meta: n_expert         = 0
0.00.038.926 I llm_load_print_meta: n_expert_used    = 0
0.00.038.927 I llm_load_print_meta: causal attn      = 0
0.00.038.927 I llm_load_print_meta: pooling type     = 2
0.00.038.927 I llm_load_print_meta: rope type        = 2
0.00.038.927 I llm_load_print_meta: rope scaling     = linear
0.00.038.928 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.929 I llm_load_print_meta: freq_scale_train = 1
0.00.038.929 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.929 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.930 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.930 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.930 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.946 I llm_load_print_meta: model type       = 33M
0.00.038.946 I llm_load_print_meta: model ftype      = F16
0.00.038.947 I llm_load_print_meta: model params     = 33.21 M
0.00.038.950 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.951 I llm_load_print_meta: general.name     = Bge Small
0.00.038.951 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.951 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.952 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.953 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.953 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.953 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.954 I llm_load_print_meta: max token length = 21
0.00.041.072 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.073 I llm_load_tensors: offloading output layer to GPU
0.00.041.073 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.102 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.104 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.721 I llama_new_context_with_model: n_ctx         = 512
0.00.041.721 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.721 I llama_new_context_with_model: n_batch       = 2048
0.00.041.722 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.722 I llama_new_context_with_model: flash_attn    = 0
0.00.041.723 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.723 I llama_new_context_with_model: freq_scale    = 1
0.00.041.723 I ggml_metal_init: allocating
0.00.041.728 I ggml_metal_init: found device: Apple M4
0.00.041.731 I ggml_metal_init: picking default device: Apple M4
0.00.042.543 I ggml_metal_init: using embedded metal library
0.00.046.965 I ggml_metal_init: GPU name:   Apple M4
0.00.046.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.970 I ggml_metal_init: simdgroup reduction   = true
0.00.046.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.971 I ggml_metal_init: has bfloat            = true
0.00.046.971 I ggml_metal_init: use bfloat            = true
0.00.046.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.541 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.543 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.545 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.383 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.384 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.385 I llama_new_context_with_model: graph nodes  = 429
0.00.061.385 I llama_new_context_with_model: graph splits = 2
0.00.061.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.903 I 
0.00.071.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.072.606 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.077.313 I llama_perf_context_print:        load time =      51.89 ms
0.00.077.314 I llama_perf_context_print: prompt eval time =       4.56 ms /     9 tokens (    0.51 ms per token,  1974.55 tokens per second)
0.00.077.315 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.077.316 I llama_perf_context_print:       total time =       5.41 ms /    10 tokens
0.00.077.444 I ggml_metal_free: deallocating

real	0m0.253s
user	0m0.062s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.000 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.010 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.013 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.015 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.015 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.016 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.016 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.023 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.023 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.024 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.024 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.024 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.026 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.027 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.029 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.029 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.030 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.030 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.030 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.044 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.045 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.045 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.046 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.046 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.046 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.046 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.047 I llama_model_loader: - type  f32:  124 tensors
0.00.014.047 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.452 I llm_load_vocab: special tokens cache size = 5
0.00.017.741 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.743 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.743 I llm_load_print_meta: arch             = bert
0.00.017.743 I llm_load_print_meta: vocab type       = WPM
0.00.017.744 I llm_load_print_meta: n_vocab          = 30522
0.00.017.744 I llm_load_print_meta: n_merges         = 0
0.00.017.744 I llm_load_print_meta: vocab_only       = 0
0.00.017.744 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.744 I llm_load_print_meta: n_embd           = 384
0.00.017.744 I llm_load_print_meta: n_layer          = 12
0.00.017.747 I llm_load_print_meta: n_head           = 12
0.00.017.747 I llm_load_print_meta: n_head_kv        = 12
0.00.017.747 I llm_load_print_meta: n_rot            = 32
0.00.017.748 I llm_load_print_meta: n_swa            = 0
0.00.017.748 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.748 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.748 I llm_load_print_meta: n_gqa            = 1
0.00.017.749 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.750 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.750 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.750 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.751 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.751 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.751 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.752 I llm_load_print_meta: n_ff             = 1536
0.00.017.752 I llm_load_print_meta: n_expert         = 0
0.00.017.752 I llm_load_print_meta: n_expert_used    = 0
0.00.017.752 I llm_load_print_meta: causal attn      = 0
0.00.017.752 I llm_load_print_meta: pooling type     = 2
0.00.017.754 I llm_load_print_meta: rope type        = 2
0.00.017.754 I llm_load_print_meta: rope scaling     = linear
0.00.017.755 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.755 I llm_load_print_meta: freq_scale_train = 1
0.00.017.755 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.755 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.755 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.756 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.756 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.756 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.756 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.762 I llm_load_print_meta: model type       = 33M
0.00.017.762 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.763 I llm_load_print_meta: model params     = 33.21 M
0.00.017.763 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.768 I llm_load_print_meta: general.name     = Bge Small
0.00.017.770 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.770 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.771 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.771 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.771 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.772 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.772 I llm_load_print_meta: max token length = 21
0.00.019.013 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.013 I llm_load_tensors: offloading output layer to GPU
0.00.019.016 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.024 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.025 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.383 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.384 I llama_new_context_with_model: n_ctx         = 512
0.00.019.384 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.384 I llama_new_context_with_model: n_batch       = 2048
0.00.019.385 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.385 I llama_new_context_with_model: flash_attn    = 0
0.00.019.385 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.386 I llama_new_context_with_model: freq_scale    = 1
0.00.019.386 I ggml_metal_init: allocating
0.00.019.389 I ggml_metal_init: found device: Apple M4
0.00.019.391 I ggml_metal_init: picking default device: Apple M4
0.00.019.907 I ggml_metal_init: using embedded metal library
0.00.022.412 I ggml_metal_init: GPU name:   Apple M4
0.00.022.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.414 I ggml_metal_init: simdgroup reduction   = true
0.00.022.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.415 I ggml_metal_init: has bfloat            = true
0.00.022.415 I ggml_metal_init: use bfloat            = true
0.00.022.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.416 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.072 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.074 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.076 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.700 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.701 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.701 I llama_new_context_with_model: graph nodes  = 429
0.00.033.701 I llama_new_context_with_model: graph splits = 2
0.00.033.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.076 I 
0.00.038.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.038.620 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.827 I llama_perf_context_print:        load time =      29.07 ms
0.00.042.828 I llama_perf_context_print: prompt eval time =       4.08 ms /     9 tokens (    0.45 ms per token,  2206.96 tokens per second)
0.00.042.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.830 I llama_perf_context_print:       total time =       4.75 ms /    10 tokens
0.00.043.033 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.084 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.162 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.015 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.022 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.028.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.023 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.028.023 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.028.028 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.028.049 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.028.050 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.028.051 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.028.051 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.028.051 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.028.054 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.058 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.058 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.028.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.033.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.035.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.040.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.040.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.040.592 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.040.592 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.040.592 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.040.593 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.040.593 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.040.593 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.040.593 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.040.593 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.040.594 I llama_model_loader: - type  f32:   41 tensors
0.00.040.594 I llama_model_loader: - type  f16:   29 tensors
0.00.056.165 W llm_load_vocab: empty token at index 5
0.00.060.324 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.061.544 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.586 I llm_load_vocab: special tokens cache size = 5
0.00.326.035 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.326.043 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.326.044 I llm_load_print_meta: arch             = jina-bert-v2
0.00.326.049 I llm_load_print_meta: vocab type       = BPE
0.00.326.050 I llm_load_print_meta: n_vocab          = 61056
0.00.326.050 I llm_load_print_meta: n_merges         = 39382
0.00.326.050 I llm_load_print_meta: vocab_only       = 0
0.00.326.050 I llm_load_print_meta: n_ctx_train      = 8192
0.00.326.050 I llm_load_print_meta: n_embd           = 384
0.00.326.050 I llm_load_print_meta: n_layer          = 4
0.00.326.055 I llm_load_print_meta: n_head           = 12
0.00.326.055 I llm_load_print_meta: n_head_kv        = 12
0.00.326.055 I llm_load_print_meta: n_rot            = 32
0.00.326.055 I llm_load_print_meta: n_swa            = 0
0.00.326.056 I llm_load_print_meta: n_embd_head_k    = 32
0.00.326.056 I llm_load_print_meta: n_embd_head_v    = 32
0.00.326.056 I llm_load_print_meta: n_gqa            = 1
0.00.326.057 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.326.057 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.326.057 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.326.058 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.326.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.326.058 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.326.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.326.059 I llm_load_print_meta: n_ff             = 1536
0.00.326.059 I llm_load_print_meta: n_expert         = 0
0.00.326.059 I llm_load_print_meta: n_expert_used    = 0
0.00.326.059 I llm_load_print_meta: causal attn      = 0
0.00.326.059 I llm_load_print_meta: pooling type     = -1
0.00.326.059 I llm_load_print_meta: rope type        = -1
0.00.326.060 I llm_load_print_meta: rope scaling     = linear
0.00.326.062 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.326.062 I llm_load_print_meta: freq_scale_train = 1
0.00.326.062 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.326.062 I llm_load_print_meta: rope_finetuned   = unknown
0.00.326.062 I llm_load_print_meta: ssm_d_conv       = 0
0.00.326.062 I llm_load_print_meta: ssm_d_inner      = 0
0.00.326.063 I llm_load_print_meta: ssm_d_state      = 0
0.00.326.063 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.326.063 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.326.086 I llm_load_print_meta: model type       = 33M
0.00.326.087 I llm_load_print_meta: model ftype      = F16
0.00.326.087 I llm_load_print_meta: model params     = 32.90 M
0.00.326.087 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.326.088 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.326.088 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.326.088 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.326.088 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.326.088 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.326.089 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.326.089 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.326.089 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.326.089 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.326.089 I llm_load_print_meta: max token length = 45
0.00.326.768 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.326.769 I llm_load_tensors: offloading output layer to GPU
0.00.326.770 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.326.789 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.790 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.481 I llama_new_context_with_model: n_seq_max     = 1
0.00.327.482 I llama_new_context_with_model: n_ctx         = 8192
0.00.327.482 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.327.482 I llama_new_context_with_model: n_batch       = 2048
0.00.327.482 I llama_new_context_with_model: n_ubatch      = 2048
0.00.327.482 I llama_new_context_with_model: flash_attn    = 0
0.00.327.483 I llama_new_context_with_model: freq_base     = 10000.0
0.00.327.483 I llama_new_context_with_model: freq_scale    = 1
0.00.327.483 I ggml_metal_init: allocating
0.00.327.487 I ggml_metal_init: found device: Apple M4
0.00.327.491 I ggml_metal_init: picking default device: Apple M4
0.00.328.099 I ggml_metal_init: using embedded metal library
0.00.330.644 I ggml_metal_init: GPU name:   Apple M4
0.00.330.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.648 I ggml_metal_init: simdgroup reduction   = true
0.00.330.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.648 I ggml_metal_init: has bfloat            = true
0.00.330.648 I ggml_metal_init: use bfloat            = true
0.00.330.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.065 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.068 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.069 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.343.651 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.343.652 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.343.652 I llama_new_context_with_model: graph nodes  = 154
0.00.343.653 I llama_new_context_with_model: graph splits = 2
0.00.343.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.355.564 I 
0.00.355.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.355.762 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.355.763 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.355.765 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.355.766 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.355.771 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.355.771 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.327 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.911 I llama_perf_context_print:        load time =     336.39 ms
0.00.359.912 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17342.66 tokens per second)
0.00.359.912 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.913 I llama_perf_context_print:       total time =       4.35 ms /    63 tokens
0.00.360.141 I ggml_metal_free: deallocating

real	0m1.051s
user	0m0.334s
sys	0m0.040s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.101 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.225 I main: llama backend init
0.00.000.234 I main: load the model and apply lora adapter, if any
0.00.062.028 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.072.837 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.855 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.855 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.874 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.875 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.875 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.876 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.876 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.877 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.878 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.880 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.089.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.092 I llama_model_loader: - type  f32:  194 tensors
0.00.089.092 I llama_model_loader: - type  f16:   98 tensors
0.00.113.250 I llm_load_vocab: special tokens cache size = 25
0.00.119.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.119.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.119.315 I llm_load_print_meta: arch             = gptneox
0.00.119.316 I llm_load_print_meta: vocab type       = BPE
0.00.119.316 I llm_load_print_meta: n_vocab          = 50304
0.00.119.316 I llm_load_print_meta: n_merges         = 50009
0.00.119.316 I llm_load_print_meta: vocab_only       = 0
0.00.119.317 I llm_load_print_meta: n_ctx_train      = 2048
0.00.119.317 I llm_load_print_meta: n_embd           = 2048
0.00.119.317 I llm_load_print_meta: n_layer          = 24
0.00.119.323 I llm_load_print_meta: n_head           = 16
0.00.119.324 I llm_load_print_meta: n_head_kv        = 16
0.00.119.324 I llm_load_print_meta: n_rot            = 32
0.00.119.324 I llm_load_print_meta: n_swa            = 0
0.00.119.324 I llm_load_print_meta: n_embd_head_k    = 128
0.00.119.324 I llm_load_print_meta: n_embd_head_v    = 128
0.00.119.326 I llm_load_print_meta: n_gqa            = 1
0.00.119.327 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.119.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.119.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.119.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.119.329 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.119.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.119.332 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.119.333 I llm_load_print_meta: n_ff             = 8192
0.00.119.333 I llm_load_print_meta: n_expert         = 0
0.00.119.334 I llm_load_print_meta: n_expert_used    = 0
0.00.119.334 I llm_load_print_meta: causal attn      = 1
0.00.119.334 I llm_load_print_meta: pooling type     = 0
0.00.119.334 I llm_load_print_meta: rope type        = 2
0.00.119.334 I llm_load_print_meta: rope scaling     = linear
0.00.119.335 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.119.335 I llm_load_print_meta: freq_scale_train = 1
0.00.119.335 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.119.335 I llm_load_print_meta: rope_finetuned   = unknown
0.00.119.335 I llm_load_print_meta: ssm_d_conv       = 0
0.00.119.335 I llm_load_print_meta: ssm_d_inner      = 0
0.00.119.335 I llm_load_print_meta: ssm_d_state      = 0
0.00.119.336 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.119.336 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.119.348 I llm_load_print_meta: model type       = 1.4B
0.00.119.348 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.119.349 I llm_load_print_meta: model params     = 1.41 B
0.00.119.351 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.119.351 I llm_load_print_meta: general.name     = 1.4B
0.00.119.351 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.119.351 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.119.351 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.119.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.119.352 I llm_load_print_meta: LF token         = 128 ''
0.00.119.353 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.119.353 I llm_load_print_meta: max token length = 1024
0.00.121.558 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.121.558 I llm_load_tensors: offloading output layer to GPU
0.00.121.558 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.121.578 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.121.580 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.122.488 I llama_new_context_with_model: n_seq_max     = 1
0.00.122.489 I llama_new_context_with_model: n_ctx         = 2048
0.00.122.489 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.122.489 I llama_new_context_with_model: n_batch       = 2048
0.00.122.489 I llama_new_context_with_model: n_ubatch      = 512
0.00.122.489 I llama_new_context_with_model: flash_attn    = 0
0.00.122.490 I llama_new_context_with_model: freq_base     = 10000.0
0.00.122.490 I llama_new_context_with_model: freq_scale    = 1
0.00.122.491 I ggml_metal_init: allocating
0.00.122.494 I ggml_metal_init: found device: Apple M4
0.00.122.496 I ggml_metal_init: picking default device: Apple M4
0.00.123.122 I ggml_metal_init: using embedded metal library
0.00.154.428 I ggml_metal_init: GPU name:   Apple M4
0.00.154.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.154.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.154.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.154.434 I ggml_metal_init: simdgroup reduction   = true
0.00.154.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.154.434 I ggml_metal_init: has bfloat            = true
0.00.154.434 I ggml_metal_init: use bfloat            = true
0.00.154.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.154.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.552 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.196.558 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.196.579 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.197.533 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.197.535 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.197.535 I llama_new_context_with_model: graph nodes  = 967
0.00.197.535 I llama_new_context_with_model: graph splits = 2
0.00.197.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.276.552 I main: llama threadpool init, n_threads = 4
0.00.276.593 I 
0.00.276.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.276.630 I 
0.00.276.711 I sampler seed: 1234
0.00.276.716 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.276.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.276.741 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.276.742 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.127.970 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.02.127.971 I llama_perf_context_print:        load time =     214.52 ms
0.02.127.972 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.97 tokens per second)
0.02.127.972 I llama_perf_context_print:        eval time =    1804.54 ms /    63 runs   (   28.64 ms per token,    34.91 tokens per second)
0.02.127.974 I llama_perf_context_print:       total time =    1851.42 ms /    70 tokens
0.02.128.143 I ggml_metal_free: deallocating

real	0m2.458s
user	0m0.133s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.890 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.247 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.873 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.883 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.884 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.884 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.885 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.906 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.644 I llama_model_loader: - type  f32:  194 tensors
0.00.052.645 I llama_model_loader: - type  f16:   98 tensors
0.00.080.939 I llm_load_vocab: special tokens cache size = 25
0.00.087.509 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.512 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.512 I llm_load_print_meta: arch             = gptneox
0.00.087.512 I llm_load_print_meta: vocab type       = BPE
0.00.087.512 I llm_load_print_meta: n_vocab          = 50304
0.00.087.512 I llm_load_print_meta: n_merges         = 50009
0.00.087.513 I llm_load_print_meta: vocab_only       = 0
0.00.087.513 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.513 I llm_load_print_meta: n_embd           = 2048
0.00.087.513 I llm_load_print_meta: n_layer          = 24
0.00.087.516 I llm_load_print_meta: n_head           = 16
0.00.087.517 I llm_load_print_meta: n_head_kv        = 16
0.00.087.517 I llm_load_print_meta: n_rot            = 32
0.00.087.517 I llm_load_print_meta: n_swa            = 0
0.00.087.517 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.517 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.519 I llm_load_print_meta: n_gqa            = 1
0.00.087.519 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.520 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.521 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.521 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.521 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.521 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.521 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.522 I llm_load_print_meta: n_ff             = 8192
0.00.087.522 I llm_load_print_meta: n_expert         = 0
0.00.087.523 I llm_load_print_meta: n_expert_used    = 0
0.00.087.524 I llm_load_print_meta: causal attn      = 1
0.00.087.524 I llm_load_print_meta: pooling type     = 0
0.00.087.525 I llm_load_print_meta: rope type        = 2
0.00.087.525 I llm_load_print_meta: rope scaling     = linear
0.00.087.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.525 I llm_load_print_meta: freq_scale_train = 1
0.00.087.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.526 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.526 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.526 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.526 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.526 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.538 I llm_load_print_meta: model type       = 1.4B
0.00.087.538 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.539 I llm_load_print_meta: model params     = 1.41 B
0.00.087.539 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.539 I llm_load_print_meta: general.name     = 1.4B
0.00.087.539 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.540 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.540 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.541 I llm_load_print_meta: LF token         = 128 ''
0.00.087.542 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.543 I llm_load_print_meta: max token length = 1024
0.00.090.756 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.756 I llm_load_tensors: offloading output layer to GPU
0.00.090.757 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.767 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.768 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.723 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.724 I llama_new_context_with_model: n_ctx         = 128
0.00.091.724 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.724 I llama_new_context_with_model: n_batch       = 128
0.00.091.725 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.725 I llama_new_context_with_model: flash_attn    = 0
0.00.091.725 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.725 I llama_new_context_with_model: freq_scale    = 1
0.00.091.726 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.726 I ggml_metal_init: allocating
0.00.091.729 I ggml_metal_init: found device: Apple M4
0.00.091.731 I ggml_metal_init: picking default device: Apple M4
0.00.092.315 I ggml_metal_init: using embedded metal library
0.00.094.864 I ggml_metal_init: GPU name:   Apple M4
0.00.094.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.867 I ggml_metal_init: simdgroup reduction   = true
0.00.094.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.867 I ggml_metal_init: has bfloat            = true
0.00.094.867 I ggml_metal_init: use bfloat            = true
0.00.094.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.495 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.509 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.391 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.392 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.392 I llama_new_context_with_model: graph nodes  = 967
0.00.106.392 I llama_new_context_with_model: graph splits = 2
0.00.106.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.201.594 I 
0.01.201.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.201.694 I perplexity: tokenizing the input ..
0.01.214.524 I perplexity: tokenization took 12.825 ms
0.01.214.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.334.613 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.336.575 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.336.602 I llama_perf_context_print:        load time =    1178.33 ms
0.01.336.603 I llama_perf_context_print: prompt eval time =     119.51 ms /   128 tokens (    0.93 ms per token,  1071.05 tokens per second)
0.01.336.605 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.336.606 I llama_perf_context_print:       total time =     135.01 ms /   129 tokens
0.01.337.267 I ggml_metal_free: deallocating

real	0m1.534s
user	0m0.125s
sys	0m0.229s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.862 I llama_model_loader: - type  f32:  194 tensors
0.00.037.863 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.636 I llm_load_vocab: special tokens cache size = 25
0.00.070.315 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.318 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.319 I llm_load_print_meta: arch             = gptneox
0.00.070.319 I llm_load_print_meta: vocab type       = BPE
0.00.070.319 I llm_load_print_meta: n_vocab          = 50304
0.00.070.320 I llm_load_print_meta: n_merges         = 50009
0.00.070.320 I llm_load_print_meta: vocab_only       = 0
0.00.070.320 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.320 I llm_load_print_meta: n_embd           = 2048
0.00.070.320 I llm_load_print_meta: n_layer          = 24
0.00.070.326 I llm_load_print_meta: n_head           = 16
0.00.070.327 I llm_load_print_meta: n_head_kv        = 16
0.00.070.327 I llm_load_print_meta: n_rot            = 32
0.00.070.327 I llm_load_print_meta: n_swa            = 0
0.00.070.327 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.327 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.328 I llm_load_print_meta: n_gqa            = 1
0.00.070.328 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.329 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.330 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.330 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.330 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.330 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.330 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.331 I llm_load_print_meta: n_ff             = 8192
0.00.070.331 I llm_load_print_meta: n_expert         = 0
0.00.070.331 I llm_load_print_meta: n_expert_used    = 0
0.00.070.332 I llm_load_print_meta: causal attn      = 1
0.00.070.332 I llm_load_print_meta: pooling type     = 0
0.00.070.332 I llm_load_print_meta: rope type        = 2
0.00.070.332 I llm_load_print_meta: rope scaling     = linear
0.00.070.333 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.333 I llm_load_print_meta: freq_scale_train = 1
0.00.070.334 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.334 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.334 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.334 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.334 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.335 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.335 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.348 I llm_load_print_meta: model type       = 1.4B
0.00.070.348 I llm_load_print_meta: model ftype      = Q8_0
0.00.070.348 I llm_load_print_meta: model params     = 1.41 B
0.00.070.349 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.070.349 I llm_load_print_meta: general.name     = 1.4B
0.00.070.349 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.350 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.350 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.350 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.350 I llm_load_print_meta: LF token         = 128 ''
0.00.070.350 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.350 I llm_load_print_meta: max token length = 1024
0.00.072.927 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.928 I llm_load_tensors: offloading output layer to GPU
0.00.072.928 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.940 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.941 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.996 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.997 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.997 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.998 I llama_new_context_with_model: n_batch       = 2048
0.00.073.998 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.998 I llama_new_context_with_model: flash_attn    = 0
0.00.073.998 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.999 I llama_new_context_with_model: freq_scale    = 1
0.00.073.999 I ggml_metal_init: allocating
0.00.074.005 I ggml_metal_init: found device: Apple M4
0.00.074.009 I ggml_metal_init: picking default device: Apple M4
0.00.074.735 I ggml_metal_init: using embedded metal library
0.00.077.731 I ggml_metal_init: GPU name:   Apple M4
0.00.077.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.734 I ggml_metal_init: simdgroup reduction   = true
0.00.077.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.734 I ggml_metal_init: has bfloat            = true
0.00.077.734 I ggml_metal_init: use bfloat            = true
0.00.077.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.735 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.166 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.179 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.206 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.286 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.288 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.289 I llama_new_context_with_model: graph nodes  = 967
0.00.118.289 I llama_new_context_with_model: graph splits = 2
0.00.118.304 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.283.929 I main: llama threadpool init, n_threads = 4
0.01.283.967 I 
0.01.283.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.283.994 I 
0.01.284.231 I sampler seed: 1234
0.01.284.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.284.255 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.284.255 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.284.255 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.383.434 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.02.383.435 I llama_perf_context_print:        load time =    1274.29 ms
0.02.383.436 I llama_perf_context_print: prompt eval time =      46.95 ms /     7 tokens (    6.71 ms per token,   149.10 tokens per second)
0.02.383.436 I llama_perf_context_print:        eval time =    1049.35 ms /    63 runs   (   16.66 ms per token,    60.04 tokens per second)
0.02.383.437 I llama_perf_context_print:       total time =    1099.51 ms /    70 tokens
0.02.383.629 I ggml_metal_free: deallocating

real	0m2.401s
user	0m0.118s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.491 I llama_model_loader: - type  f32:  194 tensors
0.00.034.492 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.182 I llm_load_vocab: special tokens cache size = 25
0.00.066.650 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.653 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.653 I llm_load_print_meta: arch             = gptneox
0.00.066.654 I llm_load_print_meta: vocab type       = BPE
0.00.066.654 I llm_load_print_meta: n_vocab          = 50304
0.00.066.654 I llm_load_print_meta: n_merges         = 50009
0.00.066.654 I llm_load_print_meta: vocab_only       = 0
0.00.066.654 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.654 I llm_load_print_meta: n_embd           = 2048
0.00.066.655 I llm_load_print_meta: n_layer          = 24
0.00.066.658 I llm_load_print_meta: n_head           = 16
0.00.066.658 I llm_load_print_meta: n_head_kv        = 16
0.00.066.658 I llm_load_print_meta: n_rot            = 32
0.00.066.659 I llm_load_print_meta: n_swa            = 0
0.00.066.659 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.659 I llm_load_print_meta: n_gqa            = 1
0.00.066.660 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.661 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.664 I llm_load_print_meta: n_ff             = 8192
0.00.066.665 I llm_load_print_meta: n_expert         = 0
0.00.066.665 I llm_load_print_meta: n_expert_used    = 0
0.00.066.666 I llm_load_print_meta: causal attn      = 1
0.00.066.666 I llm_load_print_meta: pooling type     = 0
0.00.066.666 I llm_load_print_meta: rope type        = 2
0.00.066.667 I llm_load_print_meta: rope scaling     = linear
0.00.066.667 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.667 I llm_load_print_meta: freq_scale_train = 1
0.00.066.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.668 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.668 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.668 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.668 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.668 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.668 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.680 I llm_load_print_meta: model type       = 1.4B
0.00.066.680 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.680 I llm_load_print_meta: model params     = 1.41 B
0.00.066.681 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.681 I llm_load_print_meta: general.name     = 1.4B
0.00.066.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.682 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.682 I llm_load_print_meta: LF token         = 128 ''
0.00.066.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.682 I llm_load_print_meta: max token length = 1024
0.00.068.409 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.409 I llm_load_tensors: offloading output layer to GPU
0.00.068.409 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.419 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.420 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.337 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.338 I llama_new_context_with_model: n_ctx         = 128
0.00.069.338 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.339 I llama_new_context_with_model: n_batch       = 128
0.00.069.339 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.339 I llama_new_context_with_model: flash_attn    = 0
0.00.069.339 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.340 I llama_new_context_with_model: freq_scale    = 1
0.00.069.340 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.340 I ggml_metal_init: allocating
0.00.069.343 I ggml_metal_init: found device: Apple M4
0.00.069.345 I ggml_metal_init: picking default device: Apple M4
0.00.069.969 I ggml_metal_init: using embedded metal library
0.00.072.621 I ggml_metal_init: GPU name:   Apple M4
0.00.072.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.624 I ggml_metal_init: simdgroup reduction   = true
0.00.072.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.624 I ggml_metal_init: has bfloat            = true
0.00.072.624 I ggml_metal_init: use bfloat            = true
0.00.072.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.974 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.978 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.992 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.945 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.946 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.946 I llama_new_context_with_model: graph nodes  = 967
0.00.083.946 I llama_new_context_with_model: graph splits = 2
0.00.083.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.872.472 I 
0.00.872.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.872.536 I perplexity: tokenizing the input ..
0.00.880.065 I perplexity: tokenization took 7.527 ms
0.00.880.075 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.003.905 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.005.200 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.005.218 I llama_perf_context_print:        load time =     859.91 ms
0.01.005.220 I llama_perf_context_print: prompt eval time =     123.61 ms /   128 tokens (    0.97 ms per token,  1035.56 tokens per second)
0.01.005.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.005.221 I llama_perf_context_print:       total time =     132.75 ms /   129 tokens
0.01.005.643 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.095s
sys	0m0.153s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.016.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.204 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.783 I llama_model_loader: - type  f32:  194 tensors
0.00.046.784 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.784 I llama_model_loader: - type q6_K:    1 tensors
0.00.081.675 I llm_load_vocab: special tokens cache size = 25
0.00.091.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.521 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.521 I llm_load_print_meta: arch             = gptneox
0.00.091.522 I llm_load_print_meta: vocab type       = BPE
0.00.091.522 I llm_load_print_meta: n_vocab          = 50304
0.00.091.522 I llm_load_print_meta: n_merges         = 50009
0.00.091.523 I llm_load_print_meta: vocab_only       = 0
0.00.091.523 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.523 I llm_load_print_meta: n_embd           = 2048
0.00.091.525 I llm_load_print_meta: n_layer          = 24
0.00.091.529 I llm_load_print_meta: n_head           = 16
0.00.091.530 I llm_load_print_meta: n_head_kv        = 16
0.00.091.530 I llm_load_print_meta: n_rot            = 32
0.00.091.530 I llm_load_print_meta: n_swa            = 0
0.00.091.531 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.531 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.532 I llm_load_print_meta: n_gqa            = 1
0.00.091.533 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.534 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.535 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.535 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.536 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.536 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.536 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.537 I llm_load_print_meta: n_ff             = 8192
0.00.091.537 I llm_load_print_meta: n_expert         = 0
0.00.091.537 I llm_load_print_meta: n_expert_used    = 0
0.00.091.537 I llm_load_print_meta: causal attn      = 1
0.00.091.538 I llm_load_print_meta: pooling type     = 0
0.00.091.538 I llm_load_print_meta: rope type        = 2
0.00.091.538 I llm_load_print_meta: rope scaling     = linear
0.00.091.539 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.539 I llm_load_print_meta: freq_scale_train = 1
0.00.091.539 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.539 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.540 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.542 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.556 I llm_load_print_meta: model type       = 1.4B
0.00.091.556 I llm_load_print_meta: model ftype      = Q4_0
0.00.091.557 I llm_load_print_meta: model params     = 1.41 B
0.00.091.558 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.091.558 I llm_load_print_meta: general.name     = 1.4B
0.00.091.558 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.558 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.560 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.560 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.561 I llm_load_print_meta: LF token         = 128 ''
0.00.091.561 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.561 I llm_load_print_meta: max token length = 1024
0.00.094.434 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.434 I llm_load_tensors: offloading output layer to GPU
0.00.094.435 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.447 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.094.448 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.095.919 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.920 I llama_new_context_with_model: n_ctx         = 2048
0.00.095.921 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.095.921 I llama_new_context_with_model: n_batch       = 2048
0.00.095.921 I llama_new_context_with_model: n_ubatch      = 512
0.00.095.921 I llama_new_context_with_model: flash_attn    = 0
0.00.095.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.922 I llama_new_context_with_model: freq_scale    = 1
0.00.095.923 I ggml_metal_init: allocating
0.00.095.929 I ggml_metal_init: found device: Apple M4
0.00.095.932 I ggml_metal_init: picking default device: Apple M4
0.00.096.737 I ggml_metal_init: using embedded metal library
0.00.100.322 I ggml_metal_init: GPU name:   Apple M4
0.00.100.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.325 I ggml_metal_init: simdgroup reduction   = true
0.00.100.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.326 I ggml_metal_init: has bfloat            = true
0.00.100.326 I ggml_metal_init: use bfloat            = true
0.00.100.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.794 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.137.808 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.137.842 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.138.852 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.138.853 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.138.854 I llama_new_context_with_model: graph nodes  = 967
0.00.138.854 I llama_new_context_with_model: graph splits = 2
0.00.138.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.335 I main: llama threadpool init, n_threads = 4
0.00.740.420 I 
0.00.740.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.740.482 I 
0.00.740.978 I sampler seed: 1234
0.00.740.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.007 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.010 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.010 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.432.988 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.432.988 I llama_perf_context_print:        load time =     723.90 ms
0.01.432.989 I llama_perf_context_print: prompt eval time =      49.99 ms /     7 tokens (    7.14 ms per token,   140.03 tokens per second)
0.01.432.990 I llama_perf_context_print:        eval time =     639.38 ms /    63 runs   (   10.15 ms per token,    98.53 tokens per second)
0.01.432.990 I llama_perf_context_print:       total time =     692.66 ms /    70 tokens
0.01.433.203 I ggml_metal_free: deallocating

real	0m1.464s
user	0m0.143s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.433 I llama_model_loader: - type  f32:  194 tensors
0.00.024.434 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.434 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.496 I llm_load_vocab: special tokens cache size = 25
0.00.051.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.254 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.257 I llm_load_print_meta: arch             = gptneox
0.00.051.257 I llm_load_print_meta: vocab type       = BPE
0.00.051.258 I llm_load_print_meta: n_vocab          = 50304
0.00.051.258 I llm_load_print_meta: n_merges         = 50009
0.00.051.258 I llm_load_print_meta: vocab_only       = 0
0.00.051.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.260 I llm_load_print_meta: n_embd           = 2048
0.00.051.260 I llm_load_print_meta: n_layer          = 24
0.00.051.262 I llm_load_print_meta: n_head           = 16
0.00.051.263 I llm_load_print_meta: n_head_kv        = 16
0.00.051.263 I llm_load_print_meta: n_rot            = 32
0.00.051.263 I llm_load_print_meta: n_swa            = 0
0.00.051.263 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.265 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.265 I llm_load_print_meta: n_gqa            = 1
0.00.051.266 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.267 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.268 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.268 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.268 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.268 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.269 I llm_load_print_meta: n_ff             = 8192
0.00.051.269 I llm_load_print_meta: n_expert         = 0
0.00.051.269 I llm_load_print_meta: n_expert_used    = 0
0.00.051.269 I llm_load_print_meta: causal attn      = 1
0.00.051.269 I llm_load_print_meta: pooling type     = 0
0.00.051.269 I llm_load_print_meta: rope type        = 2
0.00.051.270 I llm_load_print_meta: rope scaling     = linear
0.00.051.270 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.274 I llm_load_print_meta: freq_scale_train = 1
0.00.051.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.276 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.283 I llm_load_print_meta: model type       = 1.4B
0.00.051.283 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.284 I llm_load_print_meta: model params     = 1.41 B
0.00.051.285 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.285 I llm_load_print_meta: general.name     = 1.4B
0.00.051.285 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.285 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: LF token         = 128 ''
0.00.051.286 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: max token length = 1024
0.00.053.078 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.079 I llm_load_tensors: offloading output layer to GPU
0.00.053.079 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.084 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.085 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.044 I llama_new_context_with_model: n_ctx         = 128
0.00.054.044 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.044 I llama_new_context_with_model: n_batch       = 128
0.00.054.044 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.045 I llama_new_context_with_model: flash_attn    = 0
0.00.054.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.045 I llama_new_context_with_model: freq_scale    = 1
0.00.054.045 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.046 I ggml_metal_init: allocating
0.00.054.049 I ggml_metal_init: found device: Apple M4
0.00.054.051 I ggml_metal_init: picking default device: Apple M4
0.00.054.607 I ggml_metal_init: using embedded metal library
0.00.056.917 I ggml_metal_init: GPU name:   Apple M4
0.00.056.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.920 I ggml_metal_init: simdgroup reduction   = true
0.00.056.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.920 I ggml_metal_init: has bfloat            = true
0.00.056.920 I ggml_metal_init: use bfloat            = true
0.00.056.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.926 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.929 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.944 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.831 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.832 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.832 I llama_new_context_with_model: graph nodes  = 967
0.00.068.832 I llama_new_context_with_model: graph splits = 2
0.00.068.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.424 I 
0.00.615.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.615.477 I perplexity: tokenizing the input ..
0.00.623.003 I perplexity: tokenization took 7.524 ms
0.00.623.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.585 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.746.879 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.746.897 I llama_perf_context_print:        load time =     605.65 ms
0.00.746.898 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.23 tokens per second)
0.00.746.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.899 I llama_perf_context_print:       total time =     131.47 ms /   129 tokens
0.00.747.333 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.080s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.743 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.319 I llama_model_loader: - type  f32:  194 tensors
0.00.024.319 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.320 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.333 I llm_load_vocab: special tokens cache size = 25
0.00.052.730 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.732 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.733 I llm_load_print_meta: arch             = gptneox
0.00.052.733 I llm_load_print_meta: vocab type       = BPE
0.00.052.733 I llm_load_print_meta: n_vocab          = 50304
0.00.052.733 I llm_load_print_meta: n_merges         = 50009
0.00.052.734 I llm_load_print_meta: vocab_only       = 0
0.00.052.734 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.734 I llm_load_print_meta: n_embd           = 2048
0.00.052.734 I llm_load_print_meta: n_layer          = 24
0.00.052.737 I llm_load_print_meta: n_head           = 16
0.00.052.738 I llm_load_print_meta: n_head_kv        = 16
0.00.052.738 I llm_load_print_meta: n_rot            = 32
0.00.052.738 I llm_load_print_meta: n_swa            = 0
0.00.052.738 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.738 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.739 I llm_load_print_meta: n_gqa            = 1
0.00.052.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.741 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.741 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.743 I llm_load_print_meta: n_ff             = 8192
0.00.052.743 I llm_load_print_meta: n_expert         = 0
0.00.052.743 I llm_load_print_meta: n_expert_used    = 0
0.00.052.745 I llm_load_print_meta: causal attn      = 1
0.00.052.746 I llm_load_print_meta: pooling type     = 0
0.00.052.747 I llm_load_print_meta: rope type        = 2
0.00.052.747 I llm_load_print_meta: rope scaling     = linear
0.00.052.747 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.747 I llm_load_print_meta: freq_scale_train = 1
0.00.052.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.748 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.748 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.748 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.748 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.748 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.748 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.760 I llm_load_print_meta: model type       = 1.4B
0.00.052.760 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.761 I llm_load_print_meta: model params     = 1.41 B
0.00.052.761 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.761 I llm_load_print_meta: general.name     = 1.4B
0.00.052.762 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.762 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.762 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.762 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.762 I llm_load_print_meta: LF token         = 128 ''
0.00.052.763 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.763 I llm_load_print_meta: max token length = 1024
0.00.054.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.738 I llm_load_tensors: offloading output layer to GPU
0.00.054.739 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.749 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.750 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.707 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.708 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.708 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.709 I llama_new_context_with_model: n_batch       = 2048
0.00.055.709 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.709 I llama_new_context_with_model: flash_attn    = 0
0.00.055.710 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.710 I llama_new_context_with_model: freq_scale    = 1
0.00.055.710 I ggml_metal_init: allocating
0.00.055.716 I ggml_metal_init: found device: Apple M4
0.00.055.719 I ggml_metal_init: picking default device: Apple M4
0.00.056.279 I ggml_metal_init: using embedded metal library
0.00.058.594 I ggml_metal_init: GPU name:   Apple M4
0.00.058.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.596 I ggml_metal_init: simdgroup reduction   = true
0.00.058.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.596 I ggml_metal_init: has bfloat            = true
0.00.058.598 I ggml_metal_init: use bfloat            = true
0.00.058.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.189 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.209 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.205 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.206 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.206 I llama_new_context_with_model: graph nodes  = 967
0.00.088.206 I llama_new_context_with_model: graph splits = 2
0.00.088.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.300 I main: llama threadpool init, n_threads = 4
0.00.668.346 I 
0.00.668.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.668.376 I 
0.00.668.610 I sampler seed: 1234
0.00.668.615 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.668.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.668.658 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.668.658 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.399.496 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61685.49 tokens per second)
0.01.399.496 I llama_perf_context_print:        load time =     659.55 ms
0.01.399.497 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.66 tokens per second)
0.01.399.498 I llama_perf_context_print:        eval time =     684.30 ms /    63 runs   (   10.86 ms per token,    92.06 tokens per second)
0.01.399.498 I llama_perf_context_print:       total time =     731.20 ms /    70 tokens
0.01.399.696 I ggml_metal_free: deallocating

real	0m1.415s
user	0m0.111s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.439 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.321 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.321 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.923 I llama_model_loader: - type  f32:  194 tensors
0.00.022.924 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.773 I llm_load_vocab: special tokens cache size = 25
0.00.048.565 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.568 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.568 I llm_load_print_meta: arch             = gptneox
0.00.048.569 I llm_load_print_meta: vocab type       = BPE
0.00.048.569 I llm_load_print_meta: n_vocab          = 50304
0.00.048.569 I llm_load_print_meta: n_merges         = 50009
0.00.048.569 I llm_load_print_meta: vocab_only       = 0
0.00.048.569 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.569 I llm_load_print_meta: n_embd           = 2048
0.00.048.570 I llm_load_print_meta: n_layer          = 24
0.00.048.572 I llm_load_print_meta: n_head           = 16
0.00.048.573 I llm_load_print_meta: n_head_kv        = 16
0.00.048.574 I llm_load_print_meta: n_rot            = 32
0.00.048.574 I llm_load_print_meta: n_swa            = 0
0.00.048.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.576 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.577 I llm_load_print_meta: n_gqa            = 1
0.00.048.578 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.579 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.579 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.580 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.581 I llm_load_print_meta: n_ff             = 8192
0.00.048.581 I llm_load_print_meta: n_expert         = 0
0.00.048.581 I llm_load_print_meta: n_expert_used    = 0
0.00.048.581 I llm_load_print_meta: causal attn      = 1
0.00.048.582 I llm_load_print_meta: pooling type     = 0
0.00.048.588 I llm_load_print_meta: rope type        = 2
0.00.048.590 I llm_load_print_meta: rope scaling     = linear
0.00.048.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.593 I llm_load_print_meta: freq_scale_train = 1
0.00.048.594 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.594 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.594 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.595 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.595 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.595 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.595 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.608 I llm_load_print_meta: model type       = 1.4B
0.00.048.608 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.608 I llm_load_print_meta: model params     = 1.41 B
0.00.048.609 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.609 I llm_load_print_meta: general.name     = 1.4B
0.00.048.610 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.610 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.610 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.611 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.611 I llm_load_print_meta: LF token         = 128 ''
0.00.048.611 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.611 I llm_load_print_meta: max token length = 1024
0.00.050.537 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.537 I llm_load_tensors: offloading output layer to GPU
0.00.050.537 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.547 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.549 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.488 I llama_new_context_with_model: n_ctx         = 128
0.00.051.488 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.488 I llama_new_context_with_model: n_batch       = 128
0.00.051.488 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.489 I llama_new_context_with_model: flash_attn    = 0
0.00.051.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.489 I llama_new_context_with_model: freq_scale    = 1
0.00.051.490 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.490 I ggml_metal_init: allocating
0.00.051.493 I ggml_metal_init: found device: Apple M4
0.00.051.495 I ggml_metal_init: picking default device: Apple M4
0.00.052.033 I ggml_metal_init: using embedded metal library
0.00.054.366 I ggml_metal_init: GPU name:   Apple M4
0.00.054.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.368 I ggml_metal_init: simdgroup reduction   = true
0.00.054.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.369 I ggml_metal_init: has bfloat            = true
0.00.054.369 I ggml_metal_init: use bfloat            = true
0.00.054.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.040 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.043 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.058 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.927 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.929 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.929 I llama_new_context_with_model: graph nodes  = 967
0.00.065.929 I llama_new_context_with_model: graph splits = 2
0.00.065.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.088 I 
0.00.642.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.642.143 I perplexity: tokenizing the input ..
0.00.650.506 I perplexity: tokenization took 8.361 ms
0.00.650.516 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.953 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.775.275 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.775.292 I llama_perf_context_print:        load time =     633.64 ms
0.00.775.293 I llama_perf_context_print: prompt eval time =     123.21 ms /   128 tokens (    0.96 ms per token,  1038.85 tokens per second)
0.00.775.294 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.295 I llama_perf_context_print:       total time =     133.21 ms /   129 tokens
0.00.775.731 I ggml_metal_free: deallocating

real	0m0.788s
user	0m0.079s
sys	0m0.106s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.012.693 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.301 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.302 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.036 I llama_model_loader: - type  f32:  194 tensors
0.00.028.037 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.231 I llm_load_vocab: special tokens cache size = 25
0.00.054.199 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.201 I llm_load_print_meta: arch             = gptneox
0.00.054.202 I llm_load_print_meta: vocab type       = BPE
0.00.054.202 I llm_load_print_meta: n_vocab          = 50304
0.00.054.202 I llm_load_print_meta: n_merges         = 50009
0.00.054.202 I llm_load_print_meta: vocab_only       = 0
0.00.054.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.203 I llm_load_print_meta: n_embd           = 2048
0.00.054.203 I llm_load_print_meta: n_layer          = 24
0.00.054.206 I llm_load_print_meta: n_head           = 16
0.00.054.207 I llm_load_print_meta: n_head_kv        = 16
0.00.054.207 I llm_load_print_meta: n_rot            = 32
0.00.054.207 I llm_load_print_meta: n_swa            = 0
0.00.054.207 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.209 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.210 I llm_load_print_meta: n_gqa            = 1
0.00.054.210 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.211 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.211 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.213 I llm_load_print_meta: n_ff             = 8192
0.00.054.213 I llm_load_print_meta: n_expert         = 0
0.00.054.213 I llm_load_print_meta: n_expert_used    = 0
0.00.054.215 I llm_load_print_meta: causal attn      = 1
0.00.054.217 I llm_load_print_meta: pooling type     = 0
0.00.054.217 I llm_load_print_meta: rope type        = 2
0.00.054.217 I llm_load_print_meta: rope scaling     = linear
0.00.054.218 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.218 I llm_load_print_meta: freq_scale_train = 1
0.00.054.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.219 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.219 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.231 I llm_load_print_meta: model type       = 1.4B
0.00.054.232 I llm_load_print_meta: model ftype      = Q5_0
0.00.054.232 I llm_load_print_meta: model params     = 1.41 B
0.00.054.233 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.054.233 I llm_load_print_meta: general.name     = 1.4B
0.00.054.233 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.233 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.234 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.234 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.234 I llm_load_print_meta: LF token         = 128 ''
0.00.054.235 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.235 I llm_load_print_meta: max token length = 1024
0.00.056.233 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.234 I llm_load_tensors: offloading output layer to GPU
0.00.056.234 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.245 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.246 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.057.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.171 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.171 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.171 I llama_new_context_with_model: n_batch       = 2048
0.00.057.171 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.172 I llama_new_context_with_model: flash_attn    = 0
0.00.057.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.172 I llama_new_context_with_model: freq_scale    = 1
0.00.057.173 I ggml_metal_init: allocating
0.00.057.178 I ggml_metal_init: found device: Apple M4
0.00.057.182 I ggml_metal_init: picking default device: Apple M4
0.00.057.735 I ggml_metal_init: using embedded metal library
0.00.060.054 I ggml_metal_init: GPU name:   Apple M4
0.00.060.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.056 I ggml_metal_init: simdgroup reduction   = true
0.00.060.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.057 I ggml_metal_init: has bfloat            = true
0.00.060.057 I ggml_metal_init: use bfloat            = true
0.00.060.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.315 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.319 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.372 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.373 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.373 I llama_new_context_with_model: graph nodes  = 967
0.00.090.373 I llama_new_context_with_model: graph splits = 2
0.00.090.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.592 I main: llama threadpool init, n_threads = 4
0.00.738.630 I 
0.00.738.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.738.658 I 
0.00.738.887 I sampler seed: 1234
0.00.738.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.912 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.878 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.01.527.879 I llama_perf_context_print:        load time =     725.89 ms
0.01.527.880 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.527.881 I llama_perf_context_print:        eval time =     742.95 ms /    63 runs   (   11.79 ms per token,    84.80 tokens per second)
0.01.527.881 I llama_perf_context_print:       total time =     789.29 ms /    70 tokens
0.01.528.074 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.583 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.476 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.479 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.490 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.160 I llama_model_loader: - type  f32:  194 tensors
0.00.024.160 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.161 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.011 I llm_load_vocab: special tokens cache size = 25
0.00.050.961 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.963 I llm_load_print_meta: arch             = gptneox
0.00.050.964 I llm_load_print_meta: vocab type       = BPE
0.00.050.964 I llm_load_print_meta: n_vocab          = 50304
0.00.050.964 I llm_load_print_meta: n_merges         = 50009
0.00.050.964 I llm_load_print_meta: vocab_only       = 0
0.00.050.965 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.965 I llm_load_print_meta: n_embd           = 2048
0.00.050.965 I llm_load_print_meta: n_layer          = 24
0.00.050.967 I llm_load_print_meta: n_head           = 16
0.00.050.968 I llm_load_print_meta: n_head_kv        = 16
0.00.050.968 I llm_load_print_meta: n_rot            = 32
0.00.050.968 I llm_load_print_meta: n_swa            = 0
0.00.050.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.970 I llm_load_print_meta: n_gqa            = 1
0.00.050.970 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.973 I llm_load_print_meta: n_ff             = 8192
0.00.050.973 I llm_load_print_meta: n_expert         = 0
0.00.050.973 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.975 I llm_load_print_meta: pooling type     = 0
0.00.050.975 I llm_load_print_meta: rope type        = 2
0.00.050.975 I llm_load_print_meta: rope scaling     = linear
0.00.050.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.976 I llm_load_print_meta: freq_scale_train = 1
0.00.050.978 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.979 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.979 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.979 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.990 I llm_load_print_meta: model type       = 1.4B
0.00.050.991 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.991 I llm_load_print_meta: model params     = 1.41 B
0.00.050.992 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.992 I llm_load_print_meta: general.name     = 1.4B
0.00.050.992 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: LF token         = 128 ''
0.00.050.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: max token length = 1024
0.00.052.873 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.873 I llm_load_tensors: offloading output layer to GPU
0.00.052.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.884 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.885 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.738 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.738 I llama_new_context_with_model: n_ctx         = 128
0.00.053.739 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.739 I llama_new_context_with_model: n_batch       = 128
0.00.053.739 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.739 I llama_new_context_with_model: flash_attn    = 0
0.00.053.740 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.740 I llama_new_context_with_model: freq_scale    = 1
0.00.053.740 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.740 I ggml_metal_init: allocating
0.00.053.744 I ggml_metal_init: found device: Apple M4
0.00.053.745 I ggml_metal_init: picking default device: Apple M4
0.00.054.282 I ggml_metal_init: using embedded metal library
0.00.056.569 I ggml_metal_init: GPU name:   Apple M4
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.572 I ggml_metal_init: simdgroup reduction   = true
0.00.056.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.572 I ggml_metal_init: has bfloat            = true
0.00.056.572 I ggml_metal_init: use bfloat            = true
0.00.056.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.080 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.086 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.101 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.968 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.969 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.969 I llama_new_context_with_model: graph nodes  = 967
0.00.067.970 I llama_new_context_with_model: graph splits = 2
0.00.067.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.005 I 
0.00.690.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.690.091 I perplexity: tokenizing the input ..
0.00.697.504 I perplexity: tokenization took 7.412 ms
0.00.697.515 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.277 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.833.630 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.833.663 I llama_perf_context_print:        load time =     680.41 ms
0.00.833.664 I llama_perf_context_print: prompt eval time =     134.52 ms /   128 tokens (    1.05 ms per token,   951.50 tokens per second)
0.00.833.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.666 I llama_perf_context_print:       total time =     143.66 ms /   129 tokens
0.00.834.199 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.118s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.670 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.672 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.674 I llama_model_loader: - type  f32:  194 tensors
0.00.024.675 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.675 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.685 I llm_load_vocab: special tokens cache size = 25
0.00.051.758 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.761 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.761 I llm_load_print_meta: arch             = gptneox
0.00.051.762 I llm_load_print_meta: vocab type       = BPE
0.00.051.762 I llm_load_print_meta: n_vocab          = 50304
0.00.051.762 I llm_load_print_meta: n_merges         = 50009
0.00.051.762 I llm_load_print_meta: vocab_only       = 0
0.00.051.762 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.763 I llm_load_print_meta: n_embd           = 2048
0.00.051.763 I llm_load_print_meta: n_layer          = 24
0.00.051.766 I llm_load_print_meta: n_head           = 16
0.00.051.766 I llm_load_print_meta: n_head_kv        = 16
0.00.051.766 I llm_load_print_meta: n_rot            = 32
0.00.051.767 I llm_load_print_meta: n_swa            = 0
0.00.051.767 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.767 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.768 I llm_load_print_meta: n_gqa            = 1
0.00.051.768 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.769 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.770 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.770 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.770 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.771 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.771 I llm_load_print_meta: n_ff             = 8192
0.00.051.772 I llm_load_print_meta: n_expert         = 0
0.00.051.772 I llm_load_print_meta: n_expert_used    = 0
0.00.051.772 I llm_load_print_meta: causal attn      = 1
0.00.051.772 I llm_load_print_meta: pooling type     = 0
0.00.051.772 I llm_load_print_meta: rope type        = 2
0.00.051.772 I llm_load_print_meta: rope scaling     = linear
0.00.051.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.773 I llm_load_print_meta: freq_scale_train = 1
0.00.051.773 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.774 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.774 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.774 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.774 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.774 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.774 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.786 I llm_load_print_meta: model type       = 1.4B
0.00.051.788 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.788 I llm_load_print_meta: model params     = 1.41 B
0.00.051.789 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.789 I llm_load_print_meta: general.name     = 1.4B
0.00.051.789 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.789 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.789 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.793 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.793 I llm_load_print_meta: LF token         = 128 ''
0.00.051.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: max token length = 1024
0.00.053.801 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.801 I llm_load_tensors: offloading output layer to GPU
0.00.053.801 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.811 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.812 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.699 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.699 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.700 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.700 I llama_new_context_with_model: n_batch       = 2048
0.00.054.700 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.700 I llama_new_context_with_model: flash_attn    = 0
0.00.054.700 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.701 I llama_new_context_with_model: freq_scale    = 1
0.00.054.701 I ggml_metal_init: allocating
0.00.054.704 I ggml_metal_init: found device: Apple M4
0.00.054.706 I ggml_metal_init: picking default device: Apple M4
0.00.055.287 I ggml_metal_init: using embedded metal library
0.00.057.603 I ggml_metal_init: GPU name:   Apple M4
0.00.057.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.605 I ggml_metal_init: simdgroup reduction   = true
0.00.057.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.606 I ggml_metal_init: has bfloat            = true
0.00.057.606 I ggml_metal_init: use bfloat            = true
0.00.057.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.771 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.778 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.796 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.772 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.773 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.774 I llama_new_context_with_model: graph nodes  = 967
0.00.087.774 I llama_new_context_with_model: graph splits = 2
0.00.087.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.305 I main: llama threadpool init, n_threads = 4
0.00.802.347 I 
0.00.802.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.802.407 I 
0.00.802.631 I sampler seed: 1234
0.00.802.636 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.692 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.694 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.644.195 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.644.196 I llama_perf_context_print:        load time =     793.63 ms
0.01.644.196 I llama_perf_context_print: prompt eval time =      45.66 ms /     7 tokens (    6.52 ms per token,   153.31 tokens per second)
0.01.644.197 I llama_perf_context_print:        eval time =     792.90 ms /    63 runs   (   12.59 ms per token,    79.46 tokens per second)
0.01.644.198 I llama_perf_context_print:       total time =     841.89 ms /    70 tokens
0.01.644.396 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.487 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.137 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.873 I llama_model_loader: - type  f32:  194 tensors
0.00.022.874 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.888 I llm_load_vocab: special tokens cache size = 25
0.00.048.770 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.773 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.773 I llm_load_print_meta: arch             = gptneox
0.00.048.774 I llm_load_print_meta: vocab type       = BPE
0.00.048.774 I llm_load_print_meta: n_vocab          = 50304
0.00.048.774 I llm_load_print_meta: n_merges         = 50009
0.00.048.774 I llm_load_print_meta: vocab_only       = 0
0.00.048.774 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.774 I llm_load_print_meta: n_embd           = 2048
0.00.048.775 I llm_load_print_meta: n_layer          = 24
0.00.048.777 I llm_load_print_meta: n_head           = 16
0.00.048.778 I llm_load_print_meta: n_head_kv        = 16
0.00.048.778 I llm_load_print_meta: n_rot            = 32
0.00.048.778 I llm_load_print_meta: n_swa            = 0
0.00.048.778 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.778 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.779 I llm_load_print_meta: n_gqa            = 1
0.00.048.780 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.781 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.781 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.782 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.783 I llm_load_print_meta: n_ff             = 8192
0.00.048.783 I llm_load_print_meta: n_expert         = 0
0.00.048.783 I llm_load_print_meta: n_expert_used    = 0
0.00.048.783 I llm_load_print_meta: causal attn      = 1
0.00.048.783 I llm_load_print_meta: pooling type     = 0
0.00.048.784 I llm_load_print_meta: rope type        = 2
0.00.048.784 I llm_load_print_meta: rope scaling     = linear
0.00.048.784 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.785 I llm_load_print_meta: freq_scale_train = 1
0.00.048.785 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.785 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.785 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.785 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.785 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.786 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.798 I llm_load_print_meta: model type       = 1.4B
0.00.048.799 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.799 I llm_load_print_meta: model params     = 1.41 B
0.00.048.799 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.800 I llm_load_print_meta: general.name     = 1.4B
0.00.048.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.800 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.801 I llm_load_print_meta: LF token         = 128 ''
0.00.048.802 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.802 I llm_load_print_meta: max token length = 1024
0.00.050.814 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.815 I llm_load_tensors: offloading output layer to GPU
0.00.050.815 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.825 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.826 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.766 I llama_new_context_with_model: n_ctx         = 128
0.00.051.766 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.766 I llama_new_context_with_model: n_batch       = 128
0.00.051.767 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.767 I llama_new_context_with_model: flash_attn    = 0
0.00.051.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.767 I llama_new_context_with_model: freq_scale    = 1
0.00.051.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.768 I ggml_metal_init: allocating
0.00.051.771 I ggml_metal_init: found device: Apple M4
0.00.051.774 I ggml_metal_init: picking default device: Apple M4
0.00.052.333 I ggml_metal_init: using embedded metal library
0.00.054.615 I ggml_metal_init: GPU name:   Apple M4
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.618 I ggml_metal_init: simdgroup reduction   = true
0.00.054.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.618 I ggml_metal_init: has bfloat            = true
0.00.054.618 I ggml_metal_init: use bfloat            = true
0.00.054.619 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.191 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.204 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.086 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.087 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.087 I llama_new_context_with_model: graph nodes  = 967
0.00.066.088 I llama_new_context_with_model: graph splits = 2
0.00.066.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.836 I 
0.00.742.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.742.885 I perplexity: tokenizing the input ..
0.00.750.311 I perplexity: tokenization took 7.425 ms
0.00.750.321 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.755 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.886.123 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.886.132 I llama_perf_context_print:        load time =     734.34 ms
0.00.886.133 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.73 tokens per second)
0.00.886.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.134 I llama_perf_context_print:       total time =     143.30 ms /   129 tokens
0.00.886.513 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.078s
sys	0m0.123s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.530 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.681 I llama_model_loader: - type  f32:  194 tensors
0.00.023.681 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.681 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.042 I llm_load_vocab: special tokens cache size = 25
0.00.049.928 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.931 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.931 I llm_load_print_meta: arch             = gptneox
0.00.049.932 I llm_load_print_meta: vocab type       = BPE
0.00.049.932 I llm_load_print_meta: n_vocab          = 50304
0.00.049.932 I llm_load_print_meta: n_merges         = 50009
0.00.049.932 I llm_load_print_meta: vocab_only       = 0
0.00.049.932 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.933 I llm_load_print_meta: n_embd           = 2048
0.00.049.933 I llm_load_print_meta: n_layer          = 24
0.00.049.935 I llm_load_print_meta: n_head           = 16
0.00.049.936 I llm_load_print_meta: n_head_kv        = 16
0.00.049.936 I llm_load_print_meta: n_rot            = 32
0.00.049.937 I llm_load_print_meta: n_swa            = 0
0.00.049.937 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.937 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.938 I llm_load_print_meta: n_gqa            = 1
0.00.049.938 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.940 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.941 I llm_load_print_meta: n_ff             = 8192
0.00.049.941 I llm_load_print_meta: n_expert         = 0
0.00.049.943 I llm_load_print_meta: n_expert_used    = 0
0.00.049.944 I llm_load_print_meta: causal attn      = 1
0.00.049.944 I llm_load_print_meta: pooling type     = 0
0.00.049.945 I llm_load_print_meta: rope type        = 2
0.00.049.945 I llm_load_print_meta: rope scaling     = linear
0.00.049.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.947 I llm_load_print_meta: freq_scale_train = 1
0.00.049.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.947 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.960 I llm_load_print_meta: model type       = 1.4B
0.00.049.960 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.960 I llm_load_print_meta: model params     = 1.41 B
0.00.049.961 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.961 I llm_load_print_meta: general.name     = 1.4B
0.00.049.962 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.963 I llm_load_print_meta: LF token         = 128 ''
0.00.049.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.963 I llm_load_print_meta: max token length = 1024
0.00.051.838 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.838 I llm_load_tensors: offloading output layer to GPU
0.00.051.838 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.849 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.850 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.796 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.797 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.797 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.797 I llama_new_context_with_model: n_batch       = 2048
0.00.052.797 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.798 I llama_new_context_with_model: flash_attn    = 0
0.00.052.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.798 I llama_new_context_with_model: freq_scale    = 1
0.00.052.799 I ggml_metal_init: allocating
0.00.052.802 I ggml_metal_init: found device: Apple M4
0.00.052.804 I ggml_metal_init: picking default device: Apple M4
0.00.053.373 I ggml_metal_init: using embedded metal library
0.00.055.687 I ggml_metal_init: GPU name:   Apple M4
0.00.055.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.689 I ggml_metal_init: simdgroup reduction   = true
0.00.055.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.691 I ggml_metal_init: has bfloat            = true
0.00.055.691 I ggml_metal_init: use bfloat            = true
0.00.055.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.863 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.868 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.942 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.943 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.943 I llama_new_context_with_model: graph nodes  = 967
0.00.085.943 I llama_new_context_with_model: graph splits = 2
0.00.085.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.833 I main: llama threadpool init, n_threads = 4
0.00.471.872 I 
0.00.471.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.471.903 I 
0.00.472.124 I sampler seed: 1234
0.00.472.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.472.151 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.472.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.472.151 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.152.070 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.01.152.071 I llama_perf_context_print:        load time =     462.30 ms
0.01.152.072 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.57 tokens per second)
0.01.152.072 I llama_perf_context_print:        eval time =     641.19 ms /    63 runs   (   10.18 ms per token,    98.25 tokens per second)
0.01.152.073 I llama_perf_context_print:       total time =     680.24 ms /    70 tokens
0.01.152.257 I ggml_metal_free: deallocating

real	0m1.171s
user	0m0.109s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.110 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.111 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.111 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.113 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.113 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.855 I llama_model_loader: - type  f32:  194 tensors
0.00.023.855 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.856 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.722 I llm_load_vocab: special tokens cache size = 25
0.00.050.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.706 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.706 I llm_load_print_meta: arch             = gptneox
0.00.050.706 I llm_load_print_meta: vocab type       = BPE
0.00.050.707 I llm_load_print_meta: n_vocab          = 50304
0.00.050.707 I llm_load_print_meta: n_merges         = 50009
0.00.050.707 I llm_load_print_meta: vocab_only       = 0
0.00.050.707 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.708 I llm_load_print_meta: n_embd           = 2048
0.00.050.708 I llm_load_print_meta: n_layer          = 24
0.00.050.710 I llm_load_print_meta: n_head           = 16
0.00.050.711 I llm_load_print_meta: n_head_kv        = 16
0.00.050.711 I llm_load_print_meta: n_rot            = 32
0.00.050.711 I llm_load_print_meta: n_swa            = 0
0.00.050.712 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.712 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.712 I llm_load_print_meta: n_gqa            = 1
0.00.050.713 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.714 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.714 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.715 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.715 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.715 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.715 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.716 I llm_load_print_meta: n_ff             = 8192
0.00.050.716 I llm_load_print_meta: n_expert         = 0
0.00.050.716 I llm_load_print_meta: n_expert_used    = 0
0.00.050.717 I llm_load_print_meta: causal attn      = 1
0.00.050.717 I llm_load_print_meta: pooling type     = 0
0.00.050.718 I llm_load_print_meta: rope type        = 2
0.00.050.720 I llm_load_print_meta: rope scaling     = linear
0.00.050.722 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.722 I llm_load_print_meta: freq_scale_train = 1
0.00.050.722 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.723 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.723 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.723 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.723 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.723 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.728 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.735 I llm_load_print_meta: model type       = 1.4B
0.00.050.735 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.735 I llm_load_print_meta: model params     = 1.41 B
0.00.050.736 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.736 I llm_load_print_meta: general.name     = 1.4B
0.00.050.736 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.737 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.737 I llm_load_print_meta: LF token         = 128 ''
0.00.050.738 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.738 I llm_load_print_meta: max token length = 1024
0.00.052.422 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.422 I llm_load_tensors: offloading output layer to GPU
0.00.052.422 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.428 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.429 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.335 I llama_new_context_with_model: n_ctx         = 128
0.00.053.335 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.335 I llama_new_context_with_model: n_batch       = 128
0.00.053.335 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.336 I llama_new_context_with_model: flash_attn    = 0
0.00.053.336 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.336 I llama_new_context_with_model: freq_scale    = 1
0.00.053.337 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.337 I ggml_metal_init: allocating
0.00.053.343 I ggml_metal_init: found device: Apple M4
0.00.053.345 I ggml_metal_init: picking default device: Apple M4
0.00.053.910 I ggml_metal_init: using embedded metal library
0.00.056.226 I ggml_metal_init: GPU name:   Apple M4
0.00.056.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.228 I ggml_metal_init: simdgroup reduction   = true
0.00.056.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.229 I ggml_metal_init: has bfloat            = true
0.00.056.229 I ggml_metal_init: use bfloat            = true
0.00.056.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.028 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.032 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.900 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.901 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.901 I llama_new_context_with_model: graph nodes  = 967
0.00.067.901 I llama_new_context_with_model: graph splits = 2
0.00.067.908 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.498 I 
0.00.421.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.421.543 I perplexity: tokenizing the input ..
0.00.429.197 I perplexity: tokenization took 7.653 ms
0.00.429.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.561.240 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.611 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.623 I llama_perf_context_print:        load time =     412.07 ms
0.00.562.624 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.23 tokens per second)
0.00.562.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.626 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.563.042 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.078s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.867 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.267 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.054 I llama_model_loader: - type  f32:  194 tensors
0.00.025.054 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.054 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.055 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.253 I llm_load_vocab: special tokens cache size = 25
0.00.051.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.192 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.193 I llm_load_print_meta: arch             = gptneox
0.00.051.193 I llm_load_print_meta: vocab type       = BPE
0.00.051.193 I llm_load_print_meta: n_vocab          = 50304
0.00.051.194 I llm_load_print_meta: n_merges         = 50009
0.00.051.194 I llm_load_print_meta: vocab_only       = 0
0.00.051.194 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.194 I llm_load_print_meta: n_embd           = 2048
0.00.051.194 I llm_load_print_meta: n_layer          = 24
0.00.051.198 I llm_load_print_meta: n_head           = 16
0.00.051.200 I llm_load_print_meta: n_head_kv        = 16
0.00.051.200 I llm_load_print_meta: n_rot            = 32
0.00.051.200 I llm_load_print_meta: n_swa            = 0
0.00.051.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.201 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.201 I llm_load_print_meta: n_gqa            = 1
0.00.051.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.203 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.204 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.204 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.204 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.204 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.205 I llm_load_print_meta: n_ff             = 8192
0.00.051.205 I llm_load_print_meta: n_expert         = 0
0.00.051.205 I llm_load_print_meta: n_expert_used    = 0
0.00.051.205 I llm_load_print_meta: causal attn      = 1
0.00.051.205 I llm_load_print_meta: pooling type     = 0
0.00.051.205 I llm_load_print_meta: rope type        = 2
0.00.051.206 I llm_load_print_meta: rope scaling     = linear
0.00.051.206 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.206 I llm_load_print_meta: freq_scale_train = 1
0.00.051.207 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.207 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.207 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.207 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.207 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.207 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.207 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.219 I llm_load_print_meta: model type       = 1.4B
0.00.051.219 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.219 I llm_load_print_meta: model params     = 1.41 B
0.00.051.220 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.220 I llm_load_print_meta: general.name     = 1.4B
0.00.051.220 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.221 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.221 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.221 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.221 I llm_load_print_meta: LF token         = 128 ''
0.00.051.221 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.221 I llm_load_print_meta: max token length = 1024
0.00.053.164 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.164 I llm_load_tensors: offloading output layer to GPU
0.00.053.164 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.175 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.176 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.134 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.135 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.135 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.135 I llama_new_context_with_model: n_batch       = 2048
0.00.054.136 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.136 I llama_new_context_with_model: flash_attn    = 0
0.00.054.136 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.136 I llama_new_context_with_model: freq_scale    = 1
0.00.054.137 I ggml_metal_init: allocating
0.00.054.143 I ggml_metal_init: found device: Apple M4
0.00.054.145 I ggml_metal_init: picking default device: Apple M4
0.00.054.694 I ggml_metal_init: using embedded metal library
0.00.056.999 I ggml_metal_init: GPU name:   Apple M4
0.00.057.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.001 I ggml_metal_init: simdgroup reduction   = true
0.00.057.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.002 I ggml_metal_init: has bfloat            = true
0.00.057.002 I ggml_metal_init: use bfloat            = true
0.00.057.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.041 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.046 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.062 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.097 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.098 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.098 I llama_new_context_with_model: graph nodes  = 967
0.00.086.098 I llama_new_context_with_model: graph splits = 2
0.00.086.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.002 I main: llama threadpool init, n_threads = 4
0.00.535.041 I 
0.00.535.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.535.077 I 
0.00.535.306 I sampler seed: 1234
0.00.535.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.331 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.331 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.331 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.287.248 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.287.248 I llama_perf_context_print:        load time =     525.13 ms
0.01.287.249 I llama_perf_context_print: prompt eval time =      44.42 ms /     7 tokens (    6.35 ms per token,   157.57 tokens per second)
0.01.287.250 I llama_perf_context_print:        eval time =     704.52 ms /    63 runs   (   11.18 ms per token,    89.42 tokens per second)
0.01.287.250 I llama_perf_context_print:       total time =     752.25 ms /    70 tokens
0.01.287.449 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.277 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.015 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.022 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.664 I llama_model_loader: - type  f32:  194 tensors
0.00.022.665 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.665 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.665 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.701 I llm_load_vocab: special tokens cache size = 25
0.00.048.732 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.735 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.735 I llm_load_print_meta: arch             = gptneox
0.00.048.736 I llm_load_print_meta: vocab type       = BPE
0.00.048.736 I llm_load_print_meta: n_vocab          = 50304
0.00.048.736 I llm_load_print_meta: n_merges         = 50009
0.00.048.736 I llm_load_print_meta: vocab_only       = 0
0.00.048.736 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.737 I llm_load_print_meta: n_embd           = 2048
0.00.048.737 I llm_load_print_meta: n_layer          = 24
0.00.048.739 I llm_load_print_meta: n_head           = 16
0.00.048.740 I llm_load_print_meta: n_head_kv        = 16
0.00.048.740 I llm_load_print_meta: n_rot            = 32
0.00.048.741 I llm_load_print_meta: n_swa            = 0
0.00.048.741 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.741 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.742 I llm_load_print_meta: n_gqa            = 1
0.00.048.742 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.745 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.746 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.746 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.746 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.746 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.748 I llm_load_print_meta: n_ff             = 8192
0.00.048.749 I llm_load_print_meta: n_expert         = 0
0.00.048.749 I llm_load_print_meta: n_expert_used    = 0
0.00.048.749 I llm_load_print_meta: causal attn      = 1
0.00.048.749 I llm_load_print_meta: pooling type     = 0
0.00.048.749 I llm_load_print_meta: rope type        = 2
0.00.048.750 I llm_load_print_meta: rope scaling     = linear
0.00.048.750 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.751 I llm_load_print_meta: freq_scale_train = 1
0.00.048.751 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.751 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.753 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.753 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.753 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.753 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.753 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.765 I llm_load_print_meta: model type       = 1.4B
0.00.048.767 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.767 I llm_load_print_meta: model params     = 1.41 B
0.00.048.768 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.768 I llm_load_print_meta: general.name     = 1.4B
0.00.048.768 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.768 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.768 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.770 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.770 I llm_load_print_meta: LF token         = 128 ''
0.00.048.770 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.770 I llm_load_print_meta: max token length = 1024
0.00.050.306 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.306 I llm_load_tensors: offloading output layer to GPU
0.00.050.306 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.316 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.317 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.158 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.159 I llama_new_context_with_model: n_ctx         = 128
0.00.051.159 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.159 I llama_new_context_with_model: n_batch       = 128
0.00.051.160 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.160 I llama_new_context_with_model: flash_attn    = 0
0.00.051.160 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.160 I llama_new_context_with_model: freq_scale    = 1
0.00.051.161 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.161 I ggml_metal_init: allocating
0.00.051.164 I ggml_metal_init: found device: Apple M4
0.00.051.166 I ggml_metal_init: picking default device: Apple M4
0.00.051.688 I ggml_metal_init: using embedded metal library
0.00.053.937 I ggml_metal_init: GPU name:   Apple M4
0.00.053.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.940 I ggml_metal_init: simdgroup reduction   = true
0.00.053.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.940 I ggml_metal_init: has bfloat            = true
0.00.053.940 I ggml_metal_init: use bfloat            = true
0.00.053.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.580 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.582 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.596 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.475 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.477 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.477 I llama_new_context_with_model: graph nodes  = 967
0.00.065.477 I llama_new_context_with_model: graph splits = 2
0.00.065.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.406 I 
0.00.496.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.496.457 I perplexity: tokenizing the input ..
0.00.504.086 I perplexity: tokenization took 7.627 ms
0.00.504.096 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.922 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.272 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.292 I llama_perf_context_print:        load time =     488.12 ms
0.00.637.293 I llama_perf_context_print: prompt eval time =     131.60 ms /   128 tokens (    1.03 ms per token,   972.63 tokens per second)
0.00.637.296 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.297 I llama_perf_context_print:       total time =     140.89 ms /   129 tokens
0.00.637.822 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.079s
sys	0m0.096s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.035 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.226 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.044 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.045 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.046 I llama_model_loader: - type  f32:  194 tensors
0.00.026.046 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.046 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.047 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.129 I llm_load_vocab: special tokens cache size = 25
0.00.052.925 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.928 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.928 I llm_load_print_meta: arch             = gptneox
0.00.052.928 I llm_load_print_meta: vocab type       = BPE
0.00.052.929 I llm_load_print_meta: n_vocab          = 50304
0.00.052.929 I llm_load_print_meta: n_merges         = 50009
0.00.052.929 I llm_load_print_meta: vocab_only       = 0
0.00.052.929 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.929 I llm_load_print_meta: n_embd           = 2048
0.00.052.930 I llm_load_print_meta: n_layer          = 24
0.00.052.932 I llm_load_print_meta: n_head           = 16
0.00.052.933 I llm_load_print_meta: n_head_kv        = 16
0.00.052.933 I llm_load_print_meta: n_rot            = 32
0.00.052.936 I llm_load_print_meta: n_swa            = 0
0.00.052.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.937 I llm_load_print_meta: n_gqa            = 1
0.00.052.938 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.939 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.940 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.940 I llm_load_print_meta: n_ff             = 8192
0.00.052.941 I llm_load_print_meta: n_expert         = 0
0.00.052.942 I llm_load_print_meta: n_expert_used    = 0
0.00.052.944 I llm_load_print_meta: causal attn      = 1
0.00.052.944 I llm_load_print_meta: pooling type     = 0
0.00.052.944 I llm_load_print_meta: rope type        = 2
0.00.052.944 I llm_load_print_meta: rope scaling     = linear
0.00.052.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.945 I llm_load_print_meta: freq_scale_train = 1
0.00.052.945 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.945 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.945 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.945 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.946 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.946 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.958 I llm_load_print_meta: model type       = 1.4B
0.00.052.958 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.958 I llm_load_print_meta: model params     = 1.41 B
0.00.052.959 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.959 I llm_load_print_meta: general.name     = 1.4B
0.00.052.959 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.959 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.960 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.960 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.960 I llm_load_print_meta: LF token         = 128 ''
0.00.052.961 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.961 I llm_load_print_meta: max token length = 1024
0.00.054.996 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.996 I llm_load_tensors: offloading output layer to GPU
0.00.054.996 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.007 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.008 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.963 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.964 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.964 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.964 I llama_new_context_with_model: n_batch       = 2048
0.00.055.964 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.965 I llama_new_context_with_model: flash_attn    = 0
0.00.055.965 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.965 I llama_new_context_with_model: freq_scale    = 1
0.00.055.966 I ggml_metal_init: allocating
0.00.055.969 I ggml_metal_init: found device: Apple M4
0.00.055.971 I ggml_metal_init: picking default device: Apple M4
0.00.056.563 I ggml_metal_init: using embedded metal library
0.00.058.915 I ggml_metal_init: GPU name:   Apple M4
0.00.058.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.918 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.919 I ggml_metal_init: simdgroup reduction   = true
0.00.058.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.920 I ggml_metal_init: has bfloat            = true
0.00.058.920 I ggml_metal_init: use bfloat            = true
0.00.058.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.004 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.013 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.075 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.077 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.077 I llama_new_context_with_model: graph nodes  = 967
0.00.090.077 I llama_new_context_with_model: graph splits = 2
0.00.090.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.245 I main: llama threadpool init, n_threads = 4
0.00.619.280 I 
0.00.619.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.619.308 I 
0.00.619.453 I sampler seed: 1234
0.00.619.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.619.487 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.619.489 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.619.491 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.383.735 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.383.735 I llama_perf_context_print:        load time =     608.21 ms
0.01.383.736 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.76 tokens per second)
0.01.383.737 I llama_perf_context_print:        eval time =     714.06 ms /    63 runs   (   11.33 ms per token,    88.23 tokens per second)
0.01.383.738 I llama_perf_context_print:       total time =     764.49 ms /    70 tokens
0.01.383.918 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.110s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.509 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.239 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.022 I llama_model_loader: - type  f32:  194 tensors
0.00.023.022 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.022 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.022 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.924 I llm_load_vocab: special tokens cache size = 25
0.00.048.814 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.817 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.817 I llm_load_print_meta: arch             = gptneox
0.00.048.818 I llm_load_print_meta: vocab type       = BPE
0.00.048.818 I llm_load_print_meta: n_vocab          = 50304
0.00.048.818 I llm_load_print_meta: n_merges         = 50009
0.00.048.818 I llm_load_print_meta: vocab_only       = 0
0.00.048.818 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.819 I llm_load_print_meta: n_embd           = 2048
0.00.048.819 I llm_load_print_meta: n_layer          = 24
0.00.048.821 I llm_load_print_meta: n_head           = 16
0.00.048.822 I llm_load_print_meta: n_head_kv        = 16
0.00.048.822 I llm_load_print_meta: n_rot            = 32
0.00.048.823 I llm_load_print_meta: n_swa            = 0
0.00.048.823 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.823 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.824 I llm_load_print_meta: n_gqa            = 1
0.00.048.824 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.825 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.826 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.826 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.826 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.827 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.827 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.827 I llm_load_print_meta: n_ff             = 8192
0.00.048.828 I llm_load_print_meta: n_expert         = 0
0.00.048.828 I llm_load_print_meta: n_expert_used    = 0
0.00.048.828 I llm_load_print_meta: causal attn      = 1
0.00.048.828 I llm_load_print_meta: pooling type     = 0
0.00.048.828 I llm_load_print_meta: rope type        = 2
0.00.048.828 I llm_load_print_meta: rope scaling     = linear
0.00.048.829 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.829 I llm_load_print_meta: freq_scale_train = 1
0.00.048.829 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.832 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.832 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.832 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.833 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.845 I llm_load_print_meta: model type       = 1.4B
0.00.048.845 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.846 I llm_load_print_meta: model params     = 1.41 B
0.00.048.846 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.846 I llm_load_print_meta: general.name     = 1.4B
0.00.048.847 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: LF token         = 128 ''
0.00.048.848 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.848 I llm_load_print_meta: max token length = 1024
0.00.050.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.738 I llm_load_tensors: offloading output layer to GPU
0.00.050.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.749 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.750 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.635 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.635 I llama_new_context_with_model: n_ctx         = 128
0.00.051.636 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.636 I llama_new_context_with_model: n_batch       = 128
0.00.051.636 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.636 I llama_new_context_with_model: flash_attn    = 0
0.00.051.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.637 I llama_new_context_with_model: freq_scale    = 1
0.00.051.637 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.637 I ggml_metal_init: allocating
0.00.051.640 I ggml_metal_init: found device: Apple M4
0.00.051.642 I ggml_metal_init: picking default device: Apple M4
0.00.052.169 I ggml_metal_init: using embedded metal library
0.00.054.624 I ggml_metal_init: GPU name:   Apple M4
0.00.054.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.627 I ggml_metal_init: simdgroup reduction   = true
0.00.054.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.627 I ggml_metal_init: has bfloat            = true
0.00.054.627 I ggml_metal_init: use bfloat            = true
0.00.054.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.122 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.135 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.056 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.057 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.058 I llama_new_context_with_model: graph nodes  = 967
0.00.066.058 I llama_new_context_with_model: graph splits = 2
0.00.066.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.608 I 
0.00.589.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.589.658 I perplexity: tokenizing the input ..
0.00.597.339 I perplexity: tokenization took 7.68 ms
0.00.597.352 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.482 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.821 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.838 I llama_perf_context_print:        load time =     581.10 ms
0.00.732.840 I llama_perf_context_print: prompt eval time =     133.90 ms /   128 tokens (    1.05 ms per token,   955.91 tokens per second)
0.00.732.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.844 I llama_perf_context_print:       total time =     143.23 ms /   129 tokens
0.00.733.250 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.078s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.095 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.152 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.153 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.745 I llama_model_loader: - type  f32:  194 tensors
0.00.024.745 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.745 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.921 I llm_load_vocab: special tokens cache size = 25
0.00.050.894 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.897 I llm_load_print_meta: arch             = gptneox
0.00.050.897 I llm_load_print_meta: vocab type       = BPE
0.00.050.898 I llm_load_print_meta: n_vocab          = 50304
0.00.050.898 I llm_load_print_meta: n_merges         = 50009
0.00.050.898 I llm_load_print_meta: vocab_only       = 0
0.00.050.898 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.898 I llm_load_print_meta: n_embd           = 2048
0.00.050.898 I llm_load_print_meta: n_layer          = 24
0.00.050.902 I llm_load_print_meta: n_head           = 16
0.00.050.904 I llm_load_print_meta: n_head_kv        = 16
0.00.050.905 I llm_load_print_meta: n_rot            = 32
0.00.050.905 I llm_load_print_meta: n_swa            = 0
0.00.050.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.906 I llm_load_print_meta: n_gqa            = 1
0.00.050.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.908 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.909 I llm_load_print_meta: n_ff             = 8192
0.00.050.909 I llm_load_print_meta: n_expert         = 0
0.00.050.909 I llm_load_print_meta: n_expert_used    = 0
0.00.050.909 I llm_load_print_meta: causal attn      = 1
0.00.050.915 I llm_load_print_meta: pooling type     = 0
0.00.050.916 I llm_load_print_meta: rope type        = 2
0.00.050.916 I llm_load_print_meta: rope scaling     = linear
0.00.050.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.917 I llm_load_print_meta: freq_scale_train = 1
0.00.050.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.931 I llm_load_print_meta: model type       = 1.4B
0.00.050.931 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.932 I llm_load_print_meta: model params     = 1.41 B
0.00.050.932 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.932 I llm_load_print_meta: general.name     = 1.4B
0.00.050.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.933 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: LF token         = 128 ''
0.00.050.934 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: max token length = 1024
0.00.052.914 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.914 I llm_load_tensors: offloading output layer to GPU
0.00.052.914 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.925 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.926 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.861 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.862 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.862 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.863 I llama_new_context_with_model: n_batch       = 2048
0.00.053.863 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.863 I llama_new_context_with_model: flash_attn    = 0
0.00.053.863 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.864 I llama_new_context_with_model: freq_scale    = 1
0.00.053.864 I ggml_metal_init: allocating
0.00.053.867 I ggml_metal_init: found device: Apple M4
0.00.053.869 I ggml_metal_init: picking default device: Apple M4
0.00.054.442 I ggml_metal_init: using embedded metal library
0.00.056.790 I ggml_metal_init: GPU name:   Apple M4
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.793 I ggml_metal_init: simdgroup reduction   = true
0.00.056.793 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.793 I ggml_metal_init: has bfloat            = true
0.00.056.793 I ggml_metal_init: use bfloat            = true
0.00.056.793 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.912 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.917 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.935 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.907 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.908 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.908 I llama_new_context_with_model: graph nodes  = 967
0.00.085.909 I llama_new_context_with_model: graph splits = 2
0.00.085.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.653 I main: llama threadpool init, n_threads = 4
0.00.706.693 I 
0.00.706.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.706.719 I 
0.00.706.951 I sampler seed: 1234
0.00.706.955 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.003 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.003 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.719 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.557.721 I llama_perf_context_print:        load time =     697.56 ms
0.01.557.721 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.66 tokens per second)
0.01.557.722 I llama_perf_context_print:        eval time =     796.06 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.557.722 I llama_perf_context_print:       total time =     851.07 ms /    70 tokens
0.01.557.913 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.108s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.202 I llama_model_loader: - type  f32:  194 tensors
0.00.024.202 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.202 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.049 I llm_load_vocab: special tokens cache size = 25
0.00.050.824 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.826 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.827 I llm_load_print_meta: arch             = gptneox
0.00.050.827 I llm_load_print_meta: vocab type       = BPE
0.00.050.827 I llm_load_print_meta: n_vocab          = 50304
0.00.050.827 I llm_load_print_meta: n_merges         = 50009
0.00.050.828 I llm_load_print_meta: vocab_only       = 0
0.00.050.828 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.828 I llm_load_print_meta: n_embd           = 2048
0.00.050.828 I llm_load_print_meta: n_layer          = 24
0.00.050.831 I llm_load_print_meta: n_head           = 16
0.00.050.832 I llm_load_print_meta: n_head_kv        = 16
0.00.050.832 I llm_load_print_meta: n_rot            = 32
0.00.050.832 I llm_load_print_meta: n_swa            = 0
0.00.050.832 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.833 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.834 I llm_load_print_meta: n_gqa            = 1
0.00.050.835 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.836 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.836 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.837 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.837 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.838 I llm_load_print_meta: n_ff             = 8192
0.00.050.838 I llm_load_print_meta: n_expert         = 0
0.00.050.838 I llm_load_print_meta: n_expert_used    = 0
0.00.050.838 I llm_load_print_meta: causal attn      = 1
0.00.050.838 I llm_load_print_meta: pooling type     = 0
0.00.050.839 I llm_load_print_meta: rope type        = 2
0.00.050.839 I llm_load_print_meta: rope scaling     = linear
0.00.050.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.840 I llm_load_print_meta: freq_scale_train = 1
0.00.050.840 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.842 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.854 I llm_load_print_meta: model type       = 1.4B
0.00.050.855 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.855 I llm_load_print_meta: model params     = 1.41 B
0.00.050.856 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.856 I llm_load_print_meta: general.name     = 1.4B
0.00.050.856 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: LF token         = 128 ''
0.00.050.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.857 I llm_load_print_meta: max token length = 1024
0.00.052.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.843 I llm_load_tensors: offloading output layer to GPU
0.00.052.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.854 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.855 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.752 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.753 I llama_new_context_with_model: n_ctx         = 128
0.00.053.753 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.754 I llama_new_context_with_model: n_batch       = 128
0.00.053.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.754 I llama_new_context_with_model: flash_attn    = 0
0.00.053.754 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.754 I llama_new_context_with_model: freq_scale    = 1
0.00.053.755 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.755 I ggml_metal_init: allocating
0.00.053.758 I ggml_metal_init: found device: Apple M4
0.00.053.760 I ggml_metal_init: picking default device: Apple M4
0.00.054.316 I ggml_metal_init: using embedded metal library
0.00.056.636 I ggml_metal_init: GPU name:   Apple M4
0.00.056.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.639 I ggml_metal_init: simdgroup reduction   = true
0.00.056.639 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.639 I ggml_metal_init: has bfloat            = true
0.00.056.639 I ggml_metal_init: use bfloat            = true
0.00.056.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.640 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.505 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.508 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.522 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.440 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.441 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.441 I llama_new_context_with_model: graph nodes  = 967
0.00.068.442 I llama_new_context_with_model: graph splits = 2
0.00.068.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.843 I 
0.00.673.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.673.887 I perplexity: tokenizing the input ..
0.00.681.649 I perplexity: tokenization took 7.761 ms
0.00.681.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.296 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.823.648 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.823.677 I llama_perf_context_print:        load time =     664.15 ms
0.00.823.679 I llama_perf_context_print: prompt eval time =     140.41 ms /   128 tokens (    1.10 ms per token,   911.64 tokens per second)
0.00.823.680 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.680 I llama_perf_context_print:       total time =     149.84 ms /   129 tokens
0.00.824.114 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.184 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.005 I llama_model_loader: - type  f32:  194 tensors
0.00.027.005 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.166 I llm_load_vocab: special tokens cache size = 25
0.00.053.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.046 I llm_load_print_meta: arch             = gptneox
0.00.053.046 I llm_load_print_meta: vocab type       = BPE
0.00.053.047 I llm_load_print_meta: n_vocab          = 50304
0.00.053.047 I llm_load_print_meta: n_merges         = 50009
0.00.053.047 I llm_load_print_meta: vocab_only       = 0
0.00.053.047 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.047 I llm_load_print_meta: n_embd           = 2048
0.00.053.048 I llm_load_print_meta: n_layer          = 24
0.00.053.050 I llm_load_print_meta: n_head           = 16
0.00.053.053 I llm_load_print_meta: n_head_kv        = 16
0.00.053.053 I llm_load_print_meta: n_rot            = 32
0.00.053.053 I llm_load_print_meta: n_swa            = 0
0.00.053.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.054 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.054 I llm_load_print_meta: n_gqa            = 1
0.00.053.055 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.056 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.056 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.057 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.057 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.057 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.062 I llm_load_print_meta: n_ff             = 8192
0.00.053.062 I llm_load_print_meta: n_expert         = 0
0.00.053.064 I llm_load_print_meta: n_expert_used    = 0
0.00.053.064 I llm_load_print_meta: causal attn      = 1
0.00.053.065 I llm_load_print_meta: pooling type     = 0
0.00.053.065 I llm_load_print_meta: rope type        = 2
0.00.053.066 I llm_load_print_meta: rope scaling     = linear
0.00.053.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.066 I llm_load_print_meta: freq_scale_train = 1
0.00.053.066 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.067 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.068 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.068 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.068 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.068 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.079 I llm_load_print_meta: model type       = 1.4B
0.00.053.079 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.080 I llm_load_print_meta: model params     = 1.41 B
0.00.053.080 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.080 I llm_load_print_meta: general.name     = 1.4B
0.00.053.081 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.081 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.081 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.081 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.081 I llm_load_print_meta: LF token         = 128 ''
0.00.053.082 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.082 I llm_load_print_meta: max token length = 1024
0.00.054.668 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.668 I llm_load_tensors: offloading output layer to GPU
0.00.054.668 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.678 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.679 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.526 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.527 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.527 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.528 I llama_new_context_with_model: n_batch       = 2048
0.00.055.528 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.528 I llama_new_context_with_model: flash_attn    = 0
0.00.055.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.528 I llama_new_context_with_model: freq_scale    = 1
0.00.055.529 I ggml_metal_init: allocating
0.00.055.532 I ggml_metal_init: found device: Apple M4
0.00.055.534 I ggml_metal_init: picking default device: Apple M4
0.00.056.078 I ggml_metal_init: using embedded metal library
0.00.058.395 I ggml_metal_init: GPU name:   Apple M4
0.00.058.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.397 I ggml_metal_init: simdgroup reduction   = true
0.00.058.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.399 I ggml_metal_init: has bfloat            = true
0.00.058.399 I ggml_metal_init: use bfloat            = true
0.00.058.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.582 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.588 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.626 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.627 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.627 I llama_new_context_with_model: graph nodes  = 967
0.00.089.627 I llama_new_context_with_model: graph splits = 2
0.00.089.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.609 I main: llama threadpool init, n_threads = 4
0.00.760.641 I 
0.00.760.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.760.667 I 
0.00.760.900 I sampler seed: 1234
0.00.760.905 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.946 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.645.186 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.645.187 I llama_perf_context_print:        load time =     748.77 ms
0.01.645.188 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.645.189 I llama_perf_context_print:        eval time =     826.75 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.645.189 I llama_perf_context_print:       total time =     884.58 ms /    70 tokens
0.01.645.374 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.108s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4256 (a5a915b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.500 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.075 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.093 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.601 I llama_model_loader: - type  f32:  194 tensors
0.00.022.601 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.550 I llm_load_vocab: special tokens cache size = 25
0.00.048.455 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.458 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.458 I llm_load_print_meta: arch             = gptneox
0.00.048.459 I llm_load_print_meta: vocab type       = BPE
0.00.048.459 I llm_load_print_meta: n_vocab          = 50304
0.00.048.459 I llm_load_print_meta: n_merges         = 50009
0.00.048.459 I llm_load_print_meta: vocab_only       = 0
0.00.048.459 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.459 I llm_load_print_meta: n_embd           = 2048
0.00.048.460 I llm_load_print_meta: n_layer          = 24
0.00.048.462 I llm_load_print_meta: n_head           = 16
0.00.048.463 I llm_load_print_meta: n_head_kv        = 16
0.00.048.463 I llm_load_print_meta: n_rot            = 32
0.00.048.463 I llm_load_print_meta: n_swa            = 0
0.00.048.464 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.464 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.465 I llm_load_print_meta: n_gqa            = 1
0.00.048.465 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.466 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.467 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.467 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.467 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.467 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.467 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.469 I llm_load_print_meta: n_ff             = 8192
0.00.048.469 I llm_load_print_meta: n_expert         = 0
0.00.048.469 I llm_load_print_meta: n_expert_used    = 0
0.00.048.470 I llm_load_print_meta: causal attn      = 1
0.00.048.470 I llm_load_print_meta: pooling type     = 0
0.00.048.470 I llm_load_print_meta: rope type        = 2
0.00.048.470 I llm_load_print_meta: rope scaling     = linear
0.00.048.470 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.471 I llm_load_print_meta: freq_scale_train = 1
0.00.048.471 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.471 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.471 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.471 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.472 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.472 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.473 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.485 I llm_load_print_meta: model type       = 1.4B
0.00.048.485 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.486 I llm_load_print_meta: model params     = 1.41 B
0.00.048.486 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.486 I llm_load_print_meta: general.name     = 1.4B
0.00.048.486 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.487 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.487 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.487 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.487 I llm_load_print_meta: LF token         = 128 ''
0.00.048.489 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.489 I llm_load_print_meta: max token length = 1024
0.00.050.476 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.477 I llm_load_tensors: offloading output layer to GPU
0.00.050.477 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.487 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.488 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.341 I llama_new_context_with_model: n_ctx         = 128
0.00.051.341 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.341 I llama_new_context_with_model: n_batch       = 128
0.00.051.341 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.341 I llama_new_context_with_model: flash_attn    = 0
0.00.051.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.342 I llama_new_context_with_model: freq_scale    = 1
0.00.051.342 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.343 I ggml_metal_init: allocating
0.00.051.349 I ggml_metal_init: found device: Apple M4
0.00.051.351 I ggml_metal_init: picking default device: Apple M4
0.00.051.883 I ggml_metal_init: using embedded metal library
0.00.054.201 I ggml_metal_init: GPU name:   Apple M4
0.00.054.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.203 I ggml_metal_init: simdgroup reduction   = true
0.00.054.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.203 I ggml_metal_init: has bfloat            = true
0.00.054.203 I ggml_metal_init: use bfloat            = true
0.00.054.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.690 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.694 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.708 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.559 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.560 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.561 I llama_new_context_with_model: graph nodes  = 967
0.00.065.561 I llama_new_context_with_model: graph splits = 2
0.00.065.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.706 I 
0.00.410.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.410.753 I perplexity: tokenizing the input ..
0.00.418.467 I perplexity: tokenization took 7.712 ms
0.00.418.479 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.776 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.560.120 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.560.139 I llama_perf_context_print:        load time =     402.20 ms
0.00.560.140 I llama_perf_context_print: prompt eval time =     140.05 ms /   128 tokens (    1.09 ms per token,   913.94 tokens per second)
0.00.560.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.142 I llama_perf_context_print:       total time =     149.44 ms /   129 tokens
0.00.560.640 I ggml_metal_free: deallocating

real	0m0.573s
user	0m0.078s
sys	0m0.089s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4256 (a5a915b5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121c0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121c0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121c0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121c0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121c0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121c0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121c0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121c0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121c0d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121c0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121c0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121c0e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121c0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121c0f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121c0ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121c10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121c10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121c11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121c11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121c12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121c12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121c13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121c138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121c14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121c14870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121c14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121c15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121c15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121c162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121c165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121c16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121c16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121c175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121c17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121c17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121c18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121c18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121c19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121c194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121c19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121c19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121c1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121c1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121c1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121c1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121c1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121c1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121c1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121c1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121c1d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121c1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121c1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121c1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121c1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121c1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121c1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121c1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121c1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121c20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121c20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121c20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121c21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121c21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121c21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121c21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121c22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121c228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121c22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121c231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121c23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121c23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121c23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121c24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121c24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121c24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121c25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121c25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121c25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121c26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121c26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121c26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121c274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121c27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121c27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121c284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121c28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121c28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121c294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121c29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121c29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121c2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121c2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121c2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121c2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121c2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121c2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121c1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121c2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121c2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121c2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121c2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121c2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121c2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121c2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121c2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121c2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121c2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121c2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121c30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121c305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121c30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121c31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121c31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121c319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121c31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121c32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121c327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121c32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121c330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121c33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121c33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121c33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121c34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121c34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121c34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121c35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121c355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121c35a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121c35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121c363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121c36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121c371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121c37640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121c37ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121c37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121c38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121c388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121c38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121c39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121c396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121c39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121c39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121c3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121c3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121c3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121c3b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121c3b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121c3bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121c3c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121c3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121c3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121c3ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121c3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121c3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121c3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121c3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121c3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121c3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121c3ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121c3f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121c3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121c3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121c40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121c405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121c40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121c40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121c41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121c41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121c41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121c42160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121c42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121c42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121c42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121c433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121c43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121c43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121c441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121c44660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121c44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121c44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121c45440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121c458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121c45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121c46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121c466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121c46b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121c47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121c474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121c47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121c47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121c48280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121c487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121c48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121c49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121c497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121c49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121c4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121c4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121c4acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121c4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121c4b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121c4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121c4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121c4ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121c4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121c4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121c4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121c4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121c4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121c4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121c4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121c4f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121c4fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121c4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121c504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121c50a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121c50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121c514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121c51a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121c51f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121c524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121c529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121c52f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121c53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121c539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121c53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121c54480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121c549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121c54f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121c55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121c559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121c55f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121c56460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121c569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121c56f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121c57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121c579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121c57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121c58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121c58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121c58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121c59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121c59980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121c59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121c5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121c5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121c5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121c5b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121c5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121c5beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121c5c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121c5c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121c5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121c5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121c5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121c5de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121c5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121c5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121c5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121c5f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121c5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121c5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121c603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121c60910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121c60db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121c61250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121c616f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121c61b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121c62030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121c624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121c62970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121c62e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121c632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121c63750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121c63bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121c64090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121c64530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121c64a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121c651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121c658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121c65fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121c66700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121c669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121c671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121c67470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121c67a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.154.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e6056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e6075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e608c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e6093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e609c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e60a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e60aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e60b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e60b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e60bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e60c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e60cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e60d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e60dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e60e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e60e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e60e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e60ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e60f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e60f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e60fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e60ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e6106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e610b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e6118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e612190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e612a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e612ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e613350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e6137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e6140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e614980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e614df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e615260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e6156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e6184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e61a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e61acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e61b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e61b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e61ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e61be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e61c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e61c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e61cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e61d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e61d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e61dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e61e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e61ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e61ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e61f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e61f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e61fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e6209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e6228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e6231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e6259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e6262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e6278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e6281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e6297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e62a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e62a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e62a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e62ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e62b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e62b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e62bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e62bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e62c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e62c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e62cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e62d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e62d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e62da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e62dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e62e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e62e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e62ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e62f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e62f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e62f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e6306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e6325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e6337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e6344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e6356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e635b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e636870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e636ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e6375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e637a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e637ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e638310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e638780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e638bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e639060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e6394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e639940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e63a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e63ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e63af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e63b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e63b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e63bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e63c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e63c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e63ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e63ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e63d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e63d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e63dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e63e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e63e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e63e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e63ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e63f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e63f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e63fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e63ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e6403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e640ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e641c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e641f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e642200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e642ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e642f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e6433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e643ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e644110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e644580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e6449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e644e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e6452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e645740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e645bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e646020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e646900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e646d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e6471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e647650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e647ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e647f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e6483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e648810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e648c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e6490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e649560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e6499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e649e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e64a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e64a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e64ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e64b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e64b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e64b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e64bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e64c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e64c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e64caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e64cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e64d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e64d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e64dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e64e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e64e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e64e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e64ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e64f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e64f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e64fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e64ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e6508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e650d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e6511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e651610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e651a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e651ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e652360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e6527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e6530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e653520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e653e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e654270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e6546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e654b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e654fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e655b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e656940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e657320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e6575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e657a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e657ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e604ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e605150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e6055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e605a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e605ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e606310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e606780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e606bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e607060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e6074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e607940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e607f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e608f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e609770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e609e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e60a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e60ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e60b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e60bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e60c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e60ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e60d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e60d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e60df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e60e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e60e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e60ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e60f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e60f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e60fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e60fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e6102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e6105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e6112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e6124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e6143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e6159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e6162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e616bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e617900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e6181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e618650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e618ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e618f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e6193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e61a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e61a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e61a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e61ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e61b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e61b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e61c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e61c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e61c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e61cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e61d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e61d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e61daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e61df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e61e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e61e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e61ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e61f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e61f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e61f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e61fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e621450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e6218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e6221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e622a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e622ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e6237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e623c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e6240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e624520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e6256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e625fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e626430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e6268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e6275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e627a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e627ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e628340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e6287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e629090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e629500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e629970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e629de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e62a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e62a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e62ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e62afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e62b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e62b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e62bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e62c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e62c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e62ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e62ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e62d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e62d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e62dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e62e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e62e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e62e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e62edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e62f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e62f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e62fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e62ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e6303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e630860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e631140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e6315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e632300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e632770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e632be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e633050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e6334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e633930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e633da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e634680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e634af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e634f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e6353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e635840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e636a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e6372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e638030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e6384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e638910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e638d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e6391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e639ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e639f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e63a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e63a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e63ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e63b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e63b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e63b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e63be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e63c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e63c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e63cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e63d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e63d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e63d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e63dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e63e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e63e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e63eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e63ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e63f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e63f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e63fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e6400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e640550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e6409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e640e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e6415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e6434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e643da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e644210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e6453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e645840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e6472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e647750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e647bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e648030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e6484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e648910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e6491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e649660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e649f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e64a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e64a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e64ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e64b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e64b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e64b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e64be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e64c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e64c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e64cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e64d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e64d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e64d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e64dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e64e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e64e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e64eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e64ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e64f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e64f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e64fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e6500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e6509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e650e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e6512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e651710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e651b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e651ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e6528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e6531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e653620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e653a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e653f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e6547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e6554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e656980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e657260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e6576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e657b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.805s
user	0m0.292s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4256 (a5a915b5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ff0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ff0b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ff0bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ff0c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ff0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ff0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ff0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ff0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ff0e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ff0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ff0ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ff0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ff0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ff101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ff109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ff11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ff11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ff11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ff12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ff12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ff13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ff13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ff14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ff14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ff15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ff15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ff15c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ff16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ff16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ff17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ff17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ff177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ff18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ff185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ff18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ff18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ff191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ff19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ff19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ff19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ff1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ff1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ff1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ff1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ff1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ff1baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ff1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ff1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ff1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ff1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ff1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ff1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ff1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ff1ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ff1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ff1fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ff1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ff20270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ff20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ff21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ff21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ff217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ff21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ff22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ff225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ff22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ff22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ff23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ff23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ff23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ff24170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ff24610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ff24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ff25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ff25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ff25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ff25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ff26540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ff26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ff26fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ff27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ff27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ff27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ff28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ff28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ff28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ff29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ff29a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ff29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ff2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ff2aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ff2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ff2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ff2ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ff2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ff2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ff2ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ff1c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ff2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ff2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ff2dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ff2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ff2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ff2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ff2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ff2f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ff2fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ff300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ff30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ff30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ff310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ff31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ff31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ff32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ff324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ff32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ff32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ff33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ff33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ff33bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ff34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ff34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ff349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ff34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ff352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ff35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ff35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ff360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ff36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ff36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ff36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ff37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ff377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ff37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ff38120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ff385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ff38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ff38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ff393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ff39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ff39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ff3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ff3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ff3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ff3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ff3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ff3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ff3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ff3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ff3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ff3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ff3cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ff3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ff3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ff3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ff3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ff3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ff3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ff3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ff3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ff3f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ff3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ff402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ff40740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ff40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ff41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ff41520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ff419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ff41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ff42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ff427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ff42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ff430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ff43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ff43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ff43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ff44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ff44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ff44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ff45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ff455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ff45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ff45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ff463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ff46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ff46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ff471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ff47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ff47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ff47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ff48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ff488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ff48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ff492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ff49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ff49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ff4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ff4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ff4ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ff4b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ff4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ff4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ff4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ff4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ff4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ff4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ff4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ff4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ff4e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ff4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ff4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ff4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ff4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ff4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ff50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ff50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ff50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ff514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ff51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ff51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ff524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ff52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ff52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ff534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ff53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ff53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ff544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ff54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ff54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ff554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ff55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ff55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ff564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ff569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ff56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ff57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ff579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ff57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ff58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ff589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ff58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ff59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ff599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ff59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ff5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ff5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ff5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ff5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ff5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ff5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ff5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ff5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ff5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ff5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ff5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ff5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ff5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ff5e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ff5eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ff5f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ff5f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ff5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ff60400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ff60950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ff60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ff613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ff61890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ff61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ff621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ff62670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ff62b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ff62fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ff63450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ff638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ff63d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ff64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ff646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ff64b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ff65010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ff65560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ff65c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ff663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ff66ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ff671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ff674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ff67c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ff67f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ff68560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.099.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151804be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151805050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1518054c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151805930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151805da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151806210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151806680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151806af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151806f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1518073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151807840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151807f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151808a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1518091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1518099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15180a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15180a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15180af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15180b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15180be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15180c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15180cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15180d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15180dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15180e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15180e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15180e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15180ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15180f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15180f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15180f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15180fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1518102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151810570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1518109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151810e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1518112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151811730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151811ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151812010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151812480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1518128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151812d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1518131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151813640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151813ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151813f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151814390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151814800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151814c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1518150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151815550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1518159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151815e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1518162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151816710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151816c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151817180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1518175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151817a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151817ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151818340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1518187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151818c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151819090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151819500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151819970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151819de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15181a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15181a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15181ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15181afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15181b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15181b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15181bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15181c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15181c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15181ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15181ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15181d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15181d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15181dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15181e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15181e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15181e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15181edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15181f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15181f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15181fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15181ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1518203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151820860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151820cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151821140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1518215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151821a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151821e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151822300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151822770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151822be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151823050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1518234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151823930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151823da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151824210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151824680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151824af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151824f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1518253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151825840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151825cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151826120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151826590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151826a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151826e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1518272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151827750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151827bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151828030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1518284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151828910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151828d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1518291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151829660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151829ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151829f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15182a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15182a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15182ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15182b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15182b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15182b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15182be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15182c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15182c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15182cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15182d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15182d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15182d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15182dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15182e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15182e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15182eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15182ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15182f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15182f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15182fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1518300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151830550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1518309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151830e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1518312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151831710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151831b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151831ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151832460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1518328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151832d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1518331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151833620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151833a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151833f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151834370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1518347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151834c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1518350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151835530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1518359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151835e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151836280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1518366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151836b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151836fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151837440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1518378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151837d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151838190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151838600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151838a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151838ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151839350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1518397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151839c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15183a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15183a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15183a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15183adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15183b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15183b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15183bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15183bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15183c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15183c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15183cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15183d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15183d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15183da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15183dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15183e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15183e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15183ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15183f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15183f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15183f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15183fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151840240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1518406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151840b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151840f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1518424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1518436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1518455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1518474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1518493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15184a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15184a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15184aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15184ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15184b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15184b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15184bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15184c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15184c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15184c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15184cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15184d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15184d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15184dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15184df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15184e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15184e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15184eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15184f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15184f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15184f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15184fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1518502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1518521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1518533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1518540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1518549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151855980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1518560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1518567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151856ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1518571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151857460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1518578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151857d40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14fe0a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14fe0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14fe0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14fe0b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14fe0bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14fe0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14fe0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14fe0cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14fe0d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14fe0d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14fe0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14fe0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14fe0e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14fe0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14fe0f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14fe0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14fe10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14fe10cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14fe113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14fe11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14fe124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14fe12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14fe132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14fe13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14fe14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14fe143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14fe149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14fe15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14fe15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14fe15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14fe162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14fe16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14fe16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14fe17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14fe175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14fe17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14fe17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14fe183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14fe18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14fe18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14fe191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14fe19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14fe19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14fe19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14fe1a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14fe1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14fe1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14fe1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14fe1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14fe1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14fe1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14fe1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14fe1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14fe1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14fe1e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14fe1e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14fe1ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14fe1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14fe1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14fe1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14fe1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14fe20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14fe208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14fe20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14fe211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14fe21690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14fe21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14fe21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14fe22470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14fe22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14fe22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14fe23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14fe236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14fe23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14fe24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14fe246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14fe24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14fe25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14fe256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14fe25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14fe26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14fe266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14fe26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14fe27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14fe276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14fe27c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14fe28150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14fe286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14fe28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14fe29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14fe29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14fe29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14fe2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14fe2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14fe2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14fe2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14fe2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14fe2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14fe2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14fe2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14fe2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14fe2d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14fe2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14fe2dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14fe2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14fe2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14fe2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14fe2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14fe2f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14fe2fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14fe300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14fe30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14fe30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14fe31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14fe314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14fe31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14fe31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14fe32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14fe32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14fe32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14fe33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14fe33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14fe339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14fe33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14fe342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14fe34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14fe34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14fe350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14fe35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14fe35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14fe35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14fe36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14fe367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14fe36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14fe37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14fe375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14fe37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14fe37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14fe383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14fe38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14fe38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14fe39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14fe39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14fe39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14fe39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14fe3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14fe3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14fe3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14fe3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14fe3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14fe3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14fe3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14fe3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14fe3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14fe3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14fe3d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14fe3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14fe3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14fe3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14fe3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14fe3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14fe3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14fe3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14fe3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14fe3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14fe40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14fe40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14fe409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14fe40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14fe41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14fe417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14fe41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14fe420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14fe42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14fe42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14fe42ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14fe43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14fe43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14fe43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14fe44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14fe445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14fe44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14fe44f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14fe453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14fe45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14fe45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14fe461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14fe46650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14fe46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14fe46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14fe47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14fe478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14fe47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14fe482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14fe48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14fe48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14fe492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14fe49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14fe49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14fe4a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14fe4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14fe4af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14fe4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14fe4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14fe4bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14fe4c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14fe4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14fe4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14fe4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14fe4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14fe4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14fe4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14fe4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14fe4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14fe4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14fe4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14fe4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14fe50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14fe50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14fe50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14fe514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14fe51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14fe51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14fe524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14fe52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14fe52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14fe534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14fe53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14fe53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14fe544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14fe54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14fe54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14fe554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14fe55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14fe55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14fe564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14fe569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14fe56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14fe57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14fe579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14fe57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14fe58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14fe589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14fe58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14fe59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14fe599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14fe59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14fe5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14fe5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14fe5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14fe5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14fe5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14fe5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14fe5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14fe5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14fe5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14fe5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14fe5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14fe5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14fe5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14fe5e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14fe5eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14fe5f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14fe5f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14fe5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14fe60400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14fe608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14fe60d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14fe611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14fe61680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14fe61b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14fe61fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14fe62460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14fe62900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14fe62da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14fe63240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14fe636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14fe63b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14fe64020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14fe64570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14fe64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14fe653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14fe65ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14fe661f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14fe664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14fe66ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14fe66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14fe67570 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.970s
user	0m0.245s
sys	0m0.150s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.55 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
