Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.651s
user	0m0.701s
sys	0m0.979s
++ nproc
+ make -j10
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target xxhash
[  7%] Built target build_info
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 31%] Built target llama-simple
[ 31%] Built target test-c
[ 31%] Built target llama-simple-chat
[ 31%] Built target llama-quantize-stats
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target common
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-sampling
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Built target llava_shared
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Built target test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Built target test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-tokenizer-0
[ 52%] Built target test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 56%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Built target test-arg-parser
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Built target test-chat-template
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-backend-ops
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-barrier
[ 62%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-quantize-perf
[ 67%] Built target llama-batched-bench
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Built target llama-parallel
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-lookup
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-speculative-simple
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.861s
user	0m5.645s
sys	0m8.313s

main: quantize time =  4325.83 ms
main:    total time =  4325.83 ms

main: quantize time =  2621.69 ms
main:    total time =  2621.69 ms

main: quantize time =  2158.26 ms
main:    total time =  2158.26 ms

main: quantize time =  2953.30 ms
main:    total time =  2953.30 ms

main: quantize time =  2149.95 ms
main:    total time =  2149.95 ms

main: quantize time =  5013.95 ms
main:    total time =  5013.95 ms

main: quantize time =  5696.59 ms
main:    total time =  5696.59 ms

main: quantize time =  6809.08 ms
main:    total time =  6809.08 ms

main: quantize time =  5822.08 ms
main:    total time =  5822.08 ms

main: quantize time =  4489.87 ms
main:    total time =  4489.87 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.124 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.236 I main: llama backend init
0.00.000.242 I main: load the model and apply lora adapter, if any
0.00.032.879 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.063 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.088 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.089 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.091 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.092 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.095 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.095 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.096 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.382 I llama_model_loader: - type  f32:  194 tensors
0.00.064.382 I llama_model_loader: - type  f16:   98 tensors
0.00.098.035 I llm_load_vocab: special tokens cache size = 25
0.00.105.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.059 I llm_load_print_meta: arch             = gptneox
0.00.105.059 I llm_load_print_meta: vocab type       = BPE
0.00.105.059 I llm_load_print_meta: n_vocab          = 50304
0.00.105.059 I llm_load_print_meta: n_merges         = 50009
0.00.105.060 I llm_load_print_meta: vocab_only       = 0
0.00.105.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.060 I llm_load_print_meta: n_embd           = 2048
0.00.105.060 I llm_load_print_meta: n_layer          = 24
0.00.105.062 I llm_load_print_meta: n_head           = 16
0.00.105.063 I llm_load_print_meta: n_head_kv        = 16
0.00.105.063 I llm_load_print_meta: n_rot            = 32
0.00.105.063 I llm_load_print_meta: n_swa            = 0
0.00.105.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.064 I llm_load_print_meta: n_gqa            = 1
0.00.105.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.066 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.069 I llm_load_print_meta: n_ff             = 8192
0.00.105.070 I llm_load_print_meta: n_expert         = 0
0.00.105.071 I llm_load_print_meta: n_expert_used    = 0
0.00.105.071 I llm_load_print_meta: causal attn      = 1
0.00.105.071 I llm_load_print_meta: pooling type     = 0
0.00.105.071 I llm_load_print_meta: rope type        = 2
0.00.105.072 I llm_load_print_meta: rope scaling     = linear
0.00.105.072 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.072 I llm_load_print_meta: freq_scale_train = 1
0.00.105.072 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.074 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.074 I llm_load_print_meta: model type       = 1.4B
0.00.105.075 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.105.075 I llm_load_print_meta: model params     = 1.41 B
0.00.105.076 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.105.076 I llm_load_print_meta: general.name     = 1.4B
0.00.105.077 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.081 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.081 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.081 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.081 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.105.082 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.082 I llm_load_print_meta: max token length = 1024
0.00.107.765 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.766 I llm_load_tensors: offloading output layer to GPU
0.00.107.766 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.785 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.786 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.769 I llama_new_context_with_model: n_ctx         = 2048
0.00.108.769 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.108.769 I llama_new_context_with_model: n_batch       = 2048
0.00.108.769 I llama_new_context_with_model: n_ubatch      = 512
0.00.108.770 I llama_new_context_with_model: flash_attn    = 0
0.00.108.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.770 I llama_new_context_with_model: freq_scale    = 1
0.00.108.771 I ggml_metal_init: allocating
0.00.108.774 I ggml_metal_init: found device: Apple M4
0.00.108.776 I ggml_metal_init: picking default device: Apple M4
0.00.109.468 I ggml_metal_init: using embedded metal library
0.00.119.023 I ggml_metal_init: GPU name:   Apple M4
0.00.119.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.026 I ggml_metal_init: simdgroup reduction   = true
0.00.119.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.026 I ggml_metal_init: has bfloat            = true
0.00.119.026 I ggml_metal_init: use bfloat            = true
0.00.119.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.162.617 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.624 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.162.645 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.521 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.523 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.524 I llama_new_context_with_model: graph nodes  = 967
0.00.163.524 I llama_new_context_with_model: graph splits = 2
0.00.163.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.706 I main: llama threadpool init, n_threads = 4
0.00.241.743 I 
0.00.241.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.784 I 
0.00.241.859 I sampler seed: 1234
0.00.241.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.241.898 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.241.899 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.241.899 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.084.532 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.02.084.533 I llama_perf_context_print:        load time =     208.81 ms
0.02.084.534 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   159.99 tokens per second)
0.02.084.535 I llama_perf_context_print:        eval time =    1795.97 ms /    63 runs   (   28.51 ms per token,    35.08 tokens per second)
0.02.084.535 I llama_perf_context_print:       total time =    1842.83 ms /    70 tokens
0.02.084.727 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.148s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.942 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.953 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.955 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.955 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.956 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.956 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.211 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.214 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.215 I llama_model_loader: - type  f32:  194 tensors
0.00.036.216 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.287 I llm_load_vocab: special tokens cache size = 25
0.00.067.545 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.561 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.562 I llm_load_print_meta: arch             = gptneox
0.00.067.562 I llm_load_print_meta: vocab type       = BPE
0.00.067.563 I llm_load_print_meta: n_vocab          = 50304
0.00.067.563 I llm_load_print_meta: n_merges         = 50009
0.00.067.563 I llm_load_print_meta: vocab_only       = 0
0.00.067.563 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.563 I llm_load_print_meta: n_embd           = 2048
0.00.067.564 I llm_load_print_meta: n_layer          = 24
0.00.067.570 I llm_load_print_meta: n_head           = 16
0.00.067.573 I llm_load_print_meta: n_head_kv        = 16
0.00.067.573 I llm_load_print_meta: n_rot            = 32
0.00.067.573 I llm_load_print_meta: n_swa            = 0
0.00.067.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.576 I llm_load_print_meta: n_gqa            = 1
0.00.067.577 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.577 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.578 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.578 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.579 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.579 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.579 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.579 I llm_load_print_meta: n_ff             = 8192
0.00.067.580 I llm_load_print_meta: n_expert         = 0
0.00.067.580 I llm_load_print_meta: n_expert_used    = 0
0.00.067.580 I llm_load_print_meta: causal attn      = 1
0.00.067.580 I llm_load_print_meta: pooling type     = 0
0.00.067.580 I llm_load_print_meta: rope type        = 2
0.00.067.581 I llm_load_print_meta: rope scaling     = linear
0.00.067.581 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.581 I llm_load_print_meta: freq_scale_train = 1
0.00.067.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.582 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.582 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.582 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.583 I llm_load_print_meta: model type       = 1.4B
0.00.067.583 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.583 I llm_load_print_meta: model params     = 1.41 B
0.00.067.584 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.584 I llm_load_print_meta: general.name     = 1.4B
0.00.067.584 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.584 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.585 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.585 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.585 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.585 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.586 I llm_load_print_meta: max token length = 1024
0.00.070.107 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.107 I llm_load_tensors: offloading output layer to GPU
0.00.070.107 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.119 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.120 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.120 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.121 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.122 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.122 I llama_new_context_with_model: n_batch       = 2048
0.00.071.122 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.122 I llama_new_context_with_model: flash_attn    = 0
0.00.071.123 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.123 I llama_new_context_with_model: freq_scale    = 1
0.00.071.124 I ggml_metal_init: allocating
0.00.071.131 I ggml_metal_init: found device: Apple M4
0.00.071.134 I ggml_metal_init: picking default device: Apple M4
0.00.071.918 I ggml_metal_init: using embedded metal library
0.00.074.825 I ggml_metal_init: GPU name:   Apple M4
0.00.074.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.828 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.828 I ggml_metal_init: simdgroup reduction   = true
0.00.074.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.829 I ggml_metal_init: has bfloat            = true
0.00.074.829 I ggml_metal_init: use bfloat            = true
0.00.074.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.401 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.423 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.543 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.543 I llama_new_context_with_model: graph nodes  = 967
0.00.110.544 I llama_new_context_with_model: graph splits = 2
0.00.110.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.436.725 I main: llama threadpool init, n_threads = 4
0.01.436.756 I 
0.01.436.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.436.783 I 
0.01.436.996 I sampler seed: 1234
0.01.437.000 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.437.015 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.437.016 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.437.016 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.528.340 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.02.528.341 I llama_perf_context_print:        load time =    1426.81 ms
0.02.528.342 I llama_perf_context_print: prompt eval time =      44.22 ms /     7 tokens (    6.32 ms per token,   158.30 tokens per second)
0.02.528.346 I llama_perf_context_print:        eval time =    1044.00 ms /    63 runs   (   16.57 ms per token,    60.35 tokens per second)
0.02.528.346 I llama_perf_context_print:       total time =    1091.62 ms /    70 tokens
0.02.528.542 I ggml_metal_free: deallocating

real	0m2.546s
user	0m0.117s
sys	0m0.244s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.017.640 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.049.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.060.593 I llama_model_loader: - type  f32:  194 tensors
0.00.060.594 I llama_model_loader: - type q4_0:   97 tensors
0.00.060.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.097.149 I llm_load_vocab: special tokens cache size = 25
0.00.106.624 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.106.640 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.106.642 I llm_load_print_meta: arch             = gptneox
0.00.106.643 I llm_load_print_meta: vocab type       = BPE
0.00.106.643 I llm_load_print_meta: n_vocab          = 50304
0.00.106.643 I llm_load_print_meta: n_merges         = 50009
0.00.106.644 I llm_load_print_meta: vocab_only       = 0
0.00.106.644 I llm_load_print_meta: n_ctx_train      = 2048
0.00.106.644 I llm_load_print_meta: n_embd           = 2048
0.00.106.644 I llm_load_print_meta: n_layer          = 24
0.00.106.648 I llm_load_print_meta: n_head           = 16
0.00.106.650 I llm_load_print_meta: n_head_kv        = 16
0.00.106.650 I llm_load_print_meta: n_rot            = 32
0.00.106.650 I llm_load_print_meta: n_swa            = 0
0.00.106.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.106.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.106.651 I llm_load_print_meta: n_gqa            = 1
0.00.106.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.106.653 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.106.654 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.106.656 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.106.659 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.106.659 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.106.659 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.106.660 I llm_load_print_meta: n_ff             = 8192
0.00.106.660 I llm_load_print_meta: n_expert         = 0
0.00.106.660 I llm_load_print_meta: n_expert_used    = 0
0.00.106.661 I llm_load_print_meta: causal attn      = 1
0.00.106.661 I llm_load_print_meta: pooling type     = 0
0.00.106.661 I llm_load_print_meta: rope type        = 2
0.00.106.661 I llm_load_print_meta: rope scaling     = linear
0.00.106.662 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.106.662 I llm_load_print_meta: freq_scale_train = 1
0.00.106.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.106.663 I llm_load_print_meta: rope_finetuned   = unknown
0.00.106.663 I llm_load_print_meta: ssm_d_conv       = 0
0.00.106.664 I llm_load_print_meta: ssm_d_inner      = 0
0.00.106.665 I llm_load_print_meta: ssm_d_state      = 0
0.00.106.665 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.106.665 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.106.665 I llm_load_print_meta: model type       = 1.4B
0.00.106.666 I llm_load_print_meta: model ftype      = Q4_0
0.00.106.666 I llm_load_print_meta: model params     = 1.41 B
0.00.106.667 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.106.667 I llm_load_print_meta: general.name     = 1.4B
0.00.106.668 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.106.669 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.106.669 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.106.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.106.670 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.106.671 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.106.671 I llm_load_print_meta: max token length = 1024
0.00.109.101 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.109.102 I llm_load_tensors: offloading output layer to GPU
0.00.109.102 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.109.113 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.109.115 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.110.384 I llama_new_context_with_model: n_seq_max     = 1
0.00.110.385 I llama_new_context_with_model: n_ctx         = 2048
0.00.110.385 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.110.386 I llama_new_context_with_model: n_batch       = 2048
0.00.110.386 I llama_new_context_with_model: n_ubatch      = 512
0.00.110.386 I llama_new_context_with_model: flash_attn    = 0
0.00.110.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.110.387 I llama_new_context_with_model: freq_scale    = 1
0.00.110.388 I ggml_metal_init: allocating
0.00.110.397 I ggml_metal_init: found device: Apple M4
0.00.110.400 I ggml_metal_init: picking default device: Apple M4
0.00.111.338 I ggml_metal_init: using embedded metal library
0.00.114.964 I ggml_metal_init: GPU name:   Apple M4
0.00.114.966 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.114.967 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.114.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.114.967 I ggml_metal_init: simdgroup reduction   = true
0.00.114.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.114.968 I ggml_metal_init: has bfloat            = true
0.00.114.968 I ggml_metal_init: use bfloat            = true
0.00.114.968 I ggml_metal_init: hasUnifiedMemory      = true
0.00.114.969 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.151.797 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.151.810 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.151.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.152.941 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.152.943 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.152.943 I llama_new_context_with_model: graph nodes  = 967
0.00.152.943 I llama_new_context_with_model: graph splits = 2
0.00.152.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.751 I main: llama threadpool init, n_threads = 4
0.00.933.893 I 
0.00.933.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.998 I 
0.00.934.557 I sampler seed: 1234
0.00.934.566 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.934.610 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.934.613 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.934.613 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.654.528 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.654.529 I llama_perf_context_print:        load time =     916.10 ms
0.01.654.529 I llama_perf_context_print: prompt eval time =      50.09 ms /     7 tokens (    7.16 ms per token,   139.74 tokens per second)
0.01.654.530 I llama_perf_context_print:        eval time =     666.84 ms /    63 runs   (   10.58 ms per token,    94.48 tokens per second)
0.01.654.530 I llama_perf_context_print:       total time =     720.79 ms /    70 tokens
0.01.654.735 I ggml_metal_free: deallocating

real	0m1.677s
user	0m0.151s
sys	0m0.190s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.014.529 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.449 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.449 I llama_model_loader: - type  f32:  194 tensors
0.00.030.449 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.450 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.365 I llm_load_vocab: special tokens cache size = 25
0.00.056.046 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.060 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.061 I llm_load_print_meta: arch             = gptneox
0.00.056.062 I llm_load_print_meta: vocab type       = BPE
0.00.056.062 I llm_load_print_meta: n_vocab          = 50304
0.00.056.062 I llm_load_print_meta: n_merges         = 50009
0.00.056.062 I llm_load_print_meta: vocab_only       = 0
0.00.056.062 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.062 I llm_load_print_meta: n_embd           = 2048
0.00.056.063 I llm_load_print_meta: n_layer          = 24
0.00.056.065 I llm_load_print_meta: n_head           = 16
0.00.056.066 I llm_load_print_meta: n_head_kv        = 16
0.00.056.066 I llm_load_print_meta: n_rot            = 32
0.00.056.067 I llm_load_print_meta: n_swa            = 0
0.00.056.067 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.067 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.068 I llm_load_print_meta: n_gqa            = 1
0.00.056.069 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.069 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.070 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.070 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.070 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.073 I llm_load_print_meta: n_ff             = 8192
0.00.056.074 I llm_load_print_meta: n_expert         = 0
0.00.056.074 I llm_load_print_meta: n_expert_used    = 0
0.00.056.074 I llm_load_print_meta: causal attn      = 1
0.00.056.074 I llm_load_print_meta: pooling type     = 0
0.00.056.074 I llm_load_print_meta: rope type        = 2
0.00.056.074 I llm_load_print_meta: rope scaling     = linear
0.00.056.075 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.075 I llm_load_print_meta: freq_scale_train = 1
0.00.056.075 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.075 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.076 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.076 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.076 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.076 I llm_load_print_meta: model type       = 1.4B
0.00.056.076 I llm_load_print_meta: model ftype      = Q4_1
0.00.056.077 I llm_load_print_meta: model params     = 1.41 B
0.00.056.077 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.077 I llm_load_print_meta: general.name     = 1.4B
0.00.056.078 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.078 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.078 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.078 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.078 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.079 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.080 I llm_load_print_meta: max token length = 1024
0.00.057.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.723 I llm_load_tensors: offloading output layer to GPU
0.00.057.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.734 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.057.735 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.058.575 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.576 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.576 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.576 I llama_new_context_with_model: n_batch       = 2048
0.00.058.576 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.577 I llama_new_context_with_model: flash_attn    = 0
0.00.058.577 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.577 I llama_new_context_with_model: freq_scale    = 1
0.00.058.578 I ggml_metal_init: allocating
0.00.058.584 I ggml_metal_init: found device: Apple M4
0.00.058.586 I ggml_metal_init: picking default device: Apple M4
0.00.059.180 I ggml_metal_init: using embedded metal library
0.00.061.698 I ggml_metal_init: GPU name:   Apple M4
0.00.061.700 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.701 I ggml_metal_init: simdgroup reduction   = true
0.00.061.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.701 I ggml_metal_init: has bfloat            = true
0.00.061.701 I ggml_metal_init: use bfloat            = true
0.00.061.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.955 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.962 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.982 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.154 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.156 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.156 I llama_new_context_with_model: graph nodes  = 967
0.00.093.156 I llama_new_context_with_model: graph splits = 2
0.00.093.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.284 I main: llama threadpool init, n_threads = 4
0.00.713.329 I 
0.00.713.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.362 I 
0.00.713.514 I sampler seed: 1234
0.00.713.519 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.576 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.576 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.576 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.468.608 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.01.468.610 I llama_perf_context_print:        load time =     698.75 ms
0.01.468.610 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.42 tokens per second)
0.01.468.611 I llama_perf_context_print:        eval time =     712.45 ms /    63 runs   (   11.31 ms per token,    88.43 tokens per second)
0.01.468.611 I llama_perf_context_print:       total time =     755.33 ms /    70 tokens
0.01.468.820 I ggml_metal_free: deallocating

real	0m1.486s
user	0m0.111s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.603 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.760 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.768 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.484 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.485 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.486 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.486 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.486 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.487 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.487 I llama_model_loader: - type  f32:  194 tensors
0.00.025.487 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.488 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.425 I llm_load_vocab: special tokens cache size = 25
0.00.051.325 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.338 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.340 I llm_load_print_meta: arch             = gptneox
0.00.051.340 I llm_load_print_meta: vocab type       = BPE
0.00.051.341 I llm_load_print_meta: n_vocab          = 50304
0.00.051.341 I llm_load_print_meta: n_merges         = 50009
0.00.051.341 I llm_load_print_meta: vocab_only       = 0
0.00.051.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.341 I llm_load_print_meta: n_embd           = 2048
0.00.051.341 I llm_load_print_meta: n_layer          = 24
0.00.051.344 I llm_load_print_meta: n_head           = 16
0.00.051.345 I llm_load_print_meta: n_head_kv        = 16
0.00.051.345 I llm_load_print_meta: n_rot            = 32
0.00.051.345 I llm_load_print_meta: n_swa            = 0
0.00.051.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.347 I llm_load_print_meta: n_gqa            = 1
0.00.051.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.350 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.352 I llm_load_print_meta: n_ff             = 8192
0.00.051.353 I llm_load_print_meta: n_expert         = 0
0.00.051.353 I llm_load_print_meta: n_expert_used    = 0
0.00.051.354 I llm_load_print_meta: causal attn      = 1
0.00.051.355 I llm_load_print_meta: pooling type     = 0
0.00.051.355 I llm_load_print_meta: rope type        = 2
0.00.051.356 I llm_load_print_meta: rope scaling     = linear
0.00.051.356 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.356 I llm_load_print_meta: freq_scale_train = 1
0.00.051.356 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.360 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.360 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.360 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.360 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.360 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.361 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.362 I llm_load_print_meta: model type       = 1.4B
0.00.051.362 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.363 I llm_load_print_meta: model params     = 1.41 B
0.00.051.363 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.363 I llm_load_print_meta: general.name     = 1.4B
0.00.051.364 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.364 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.365 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.366 I llm_load_print_meta: max token length = 1024
0.00.052.994 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.995 I llm_load_tensors: offloading output layer to GPU
0.00.052.995 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.005 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.006 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.839 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.839 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.840 I llama_new_context_with_model: n_batch       = 2048
0.00.053.840 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.840 I llama_new_context_with_model: flash_attn    = 0
0.00.053.840 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.841 I llama_new_context_with_model: freq_scale    = 1
0.00.053.841 I ggml_metal_init: allocating
0.00.053.847 I ggml_metal_init: found device: Apple M4
0.00.053.849 I ggml_metal_init: picking default device: Apple M4
0.00.054.429 I ggml_metal_init: using embedded metal library
0.00.056.753 I ggml_metal_init: GPU name:   Apple M4
0.00.056.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.756 I ggml_metal_init: simdgroup reduction   = true
0.00.056.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.756 I ggml_metal_init: has bfloat            = true
0.00.056.756 I ggml_metal_init: use bfloat            = true
0.00.056.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.762 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.768 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.787 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.948 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.949 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.950 I llama_new_context_with_model: graph nodes  = 967
0.00.086.950 I llama_new_context_with_model: graph splits = 2
0.00.086.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.790 I main: llama threadpool init, n_threads = 4
0.00.730.822 I 
0.00.730.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.869 I 
0.00.731.032 I sampler seed: 1234
0.00.731.037 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.053 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.554.071 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.554.072 I llama_perf_context_print:        load time =     721.19 ms
0.01.554.073 I llama_perf_context_print: prompt eval time =      43.22 ms /     7 tokens (    6.17 ms per token,   161.96 tokens per second)
0.01.554.074 I llama_perf_context_print:        eval time =     776.73 ms /    63 runs   (   12.33 ms per token,    81.11 tokens per second)
0.01.554.074 I llama_perf_context_print:       total time =     823.28 ms /    70 tokens
0.01.554.261 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.461 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.360 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.361 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.353 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.263 I llama_model_loader: - type  f32:  194 tensors
0.00.025.263 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.264 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.339 I llm_load_vocab: special tokens cache size = 25
0.00.051.211 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.226 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.227 I llm_load_print_meta: arch             = gptneox
0.00.051.227 I llm_load_print_meta: vocab type       = BPE
0.00.051.228 I llm_load_print_meta: n_vocab          = 50304
0.00.051.228 I llm_load_print_meta: n_merges         = 50009
0.00.051.228 I llm_load_print_meta: vocab_only       = 0
0.00.051.228 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.228 I llm_load_print_meta: n_embd           = 2048
0.00.051.229 I llm_load_print_meta: n_layer          = 24
0.00.051.231 I llm_load_print_meta: n_head           = 16
0.00.051.231 I llm_load_print_meta: n_head_kv        = 16
0.00.051.232 I llm_load_print_meta: n_rot            = 32
0.00.051.232 I llm_load_print_meta: n_swa            = 0
0.00.051.232 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.232 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.233 I llm_load_print_meta: n_gqa            = 1
0.00.051.233 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.234 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.235 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.236 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.236 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.238 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.238 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.239 I llm_load_print_meta: n_ff             = 8192
0.00.051.239 I llm_load_print_meta: n_expert         = 0
0.00.051.239 I llm_load_print_meta: n_expert_used    = 0
0.00.051.240 I llm_load_print_meta: causal attn      = 1
0.00.051.241 I llm_load_print_meta: pooling type     = 0
0.00.051.241 I llm_load_print_meta: rope type        = 2
0.00.051.241 I llm_load_print_meta: rope scaling     = linear
0.00.051.242 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.243 I llm_load_print_meta: freq_scale_train = 1
0.00.051.243 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.243 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.243 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.244 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.244 I llm_load_print_meta: model type       = 1.4B
0.00.051.244 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.245 I llm_load_print_meta: model params     = 1.41 B
0.00.051.245 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.245 I llm_load_print_meta: general.name     = 1.4B
0.00.051.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.246 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.246 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.246 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.246 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.247 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: max token length = 1024
0.00.053.061 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.062 I llm_load_tensors: offloading output layer to GPU
0.00.053.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.067 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.068 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.954 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.955 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.955 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.955 I llama_new_context_with_model: n_batch       = 2048
0.00.053.955 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.955 I llama_new_context_with_model: flash_attn    = 0
0.00.053.956 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.956 I llama_new_context_with_model: freq_scale    = 1
0.00.053.957 I ggml_metal_init: allocating
0.00.053.963 I ggml_metal_init: found device: Apple M4
0.00.053.965 I ggml_metal_init: picking default device: Apple M4
0.00.054.564 I ggml_metal_init: using embedded metal library
0.00.056.909 I ggml_metal_init: GPU name:   Apple M4
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.911 I ggml_metal_init: simdgroup reduction   = true
0.00.056.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.913 I ggml_metal_init: has bfloat            = true
0.00.056.913 I ggml_metal_init: use bfloat            = true
0.00.056.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.192 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.201 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.223 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.236 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.237 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.238 I llama_new_context_with_model: graph nodes  = 967
0.00.087.238 I llama_new_context_with_model: graph splits = 2
0.00.087.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.709 I main: llama threadpool init, n_threads = 4
0.00.793.745 I 
0.00.793.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.794 I 
0.00.794.078 I sampler seed: 1234
0.00.794.083 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.099 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.100 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.635.601 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.635.603 I llama_perf_context_print:        load time =     784.25 ms
0.01.635.603 I llama_perf_context_print: prompt eval time =      45.74 ms /     7 tokens (    6.53 ms per token,   153.04 tokens per second)
0.01.635.604 I llama_perf_context_print:        eval time =     792.77 ms /    63 runs   (   12.58 ms per token,    79.47 tokens per second)
0.01.635.605 I llama_perf_context_print:       total time =     841.89 ms /    70 tokens
0.01.635.795 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.565 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.833 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.834 I llama_model_loader: - type  f32:  194 tensors
0.00.023.834 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.834 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.834 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.800 I llm_load_vocab: special tokens cache size = 25
0.00.049.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.763 I llm_load_print_meta: arch             = gptneox
0.00.049.764 I llm_load_print_meta: vocab type       = BPE
0.00.049.764 I llm_load_print_meta: n_vocab          = 50304
0.00.049.764 I llm_load_print_meta: n_merges         = 50009
0.00.049.764 I llm_load_print_meta: vocab_only       = 0
0.00.049.765 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.765 I llm_load_print_meta: n_embd           = 2048
0.00.049.765 I llm_load_print_meta: n_layer          = 24
0.00.049.768 I llm_load_print_meta: n_head           = 16
0.00.049.768 I llm_load_print_meta: n_head_kv        = 16
0.00.049.769 I llm_load_print_meta: n_rot            = 32
0.00.049.769 I llm_load_print_meta: n_swa            = 0
0.00.049.769 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.769 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.770 I llm_load_print_meta: n_gqa            = 1
0.00.049.771 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.771 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.772 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.772 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.773 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.773 I llm_load_print_meta: n_ff             = 8192
0.00.049.774 I llm_load_print_meta: n_expert         = 0
0.00.049.774 I llm_load_print_meta: n_expert_used    = 0
0.00.049.774 I llm_load_print_meta: causal attn      = 1
0.00.049.774 I llm_load_print_meta: pooling type     = 0
0.00.049.774 I llm_load_print_meta: rope type        = 2
0.00.049.774 I llm_load_print_meta: rope scaling     = linear
0.00.049.775 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.775 I llm_load_print_meta: freq_scale_train = 1
0.00.049.775 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.775 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.775 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.775 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.775 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.776 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.776 I llm_load_print_meta: model type       = 1.4B
0.00.049.776 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.776 I llm_load_print_meta: model params     = 1.41 B
0.00.049.777 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.777 I llm_load_print_meta: general.name     = 1.4B
0.00.049.777 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.778 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.778 I llm_load_print_meta: max token length = 1024
0.00.051.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.599 I llm_load_tensors: offloading output layer to GPU
0.00.051.600 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.610 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.611 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.490 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.491 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.491 I llama_new_context_with_model: n_batch       = 2048
0.00.052.491 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.491 I llama_new_context_with_model: flash_attn    = 0
0.00.052.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.492 I llama_new_context_with_model: freq_scale    = 1
0.00.052.493 I ggml_metal_init: allocating
0.00.052.496 I ggml_metal_init: found device: Apple M4
0.00.052.498 I ggml_metal_init: picking default device: Apple M4
0.00.053.084 I ggml_metal_init: using embedded metal library
0.00.055.412 I ggml_metal_init: GPU name:   Apple M4
0.00.055.414 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.415 I ggml_metal_init: simdgroup reduction   = true
0.00.055.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.415 I ggml_metal_init: has bfloat            = true
0.00.055.415 I ggml_metal_init: use bfloat            = true
0.00.055.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.416 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.259 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.278 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.370 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.371 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.371 I llama_new_context_with_model: graph nodes  = 967
0.00.085.372 I llama_new_context_with_model: graph splits = 2
0.00.085.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.470 I main: llama threadpool init, n_threads = 4
0.00.444.517 I 
0.00.444.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.550 I 
0.00.444.794 I sampler seed: 1234
0.00.444.801 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.835 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.126.468 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.126.469 I llama_perf_context_print:        load time =     434.90 ms
0.01.126.471 I llama_perf_context_print: prompt eval time =      40.34 ms /     7 tokens (    5.76 ms per token,   173.51 tokens per second)
0.01.126.473 I llama_perf_context_print:        eval time =     638.36 ms /    63 runs   (   10.13 ms per token,    98.69 tokens per second)
0.01.126.474 I llama_perf_context_print:       total time =     682.00 ms /    70 tokens
0.01.126.670 I ggml_metal_free: deallocating

real	0m1.147s
user	0m0.108s
sys	0m0.113s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.011.517 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.767 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.796 I llama_model_loader: - type  f32:  194 tensors
0.00.026.797 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.797 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.797 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.797 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.003 I llm_load_vocab: special tokens cache size = 25
0.00.053.009 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.022 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.023 I llm_load_print_meta: arch             = gptneox
0.00.053.023 I llm_load_print_meta: vocab type       = BPE
0.00.053.024 I llm_load_print_meta: n_vocab          = 50304
0.00.053.024 I llm_load_print_meta: n_merges         = 50009
0.00.053.024 I llm_load_print_meta: vocab_only       = 0
0.00.053.024 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.024 I llm_load_print_meta: n_embd           = 2048
0.00.053.025 I llm_load_print_meta: n_layer          = 24
0.00.053.033 I llm_load_print_meta: n_head           = 16
0.00.053.034 I llm_load_print_meta: n_head_kv        = 16
0.00.053.034 I llm_load_print_meta: n_rot            = 32
0.00.053.034 I llm_load_print_meta: n_swa            = 0
0.00.053.035 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.035 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.036 I llm_load_print_meta: n_gqa            = 1
0.00.053.036 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.037 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.037 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.040 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.040 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.040 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.040 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.041 I llm_load_print_meta: n_ff             = 8192
0.00.053.041 I llm_load_print_meta: n_expert         = 0
0.00.053.041 I llm_load_print_meta: n_expert_used    = 0
0.00.053.042 I llm_load_print_meta: causal attn      = 1
0.00.053.042 I llm_load_print_meta: pooling type     = 0
0.00.053.042 I llm_load_print_meta: rope type        = 2
0.00.053.044 I llm_load_print_meta: rope scaling     = linear
0.00.053.044 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.044 I llm_load_print_meta: freq_scale_train = 1
0.00.053.044 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.044 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.045 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.045 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.046 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.046 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.046 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.046 I llm_load_print_meta: model type       = 1.4B
0.00.053.047 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.048 I llm_load_print_meta: model params     = 1.41 B
0.00.053.048 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.049 I llm_load_print_meta: general.name     = 1.4B
0.00.053.049 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.050 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.050 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.050 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.050 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.050 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.051 I llm_load_print_meta: max token length = 1024
0.00.054.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.985 I llm_load_tensors: offloading output layer to GPU
0.00.054.985 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.995 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.996 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.897 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.898 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.898 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.898 I llama_new_context_with_model: n_batch       = 2048
0.00.055.898 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.898 I llama_new_context_with_model: flash_attn    = 0
0.00.055.899 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.899 I llama_new_context_with_model: freq_scale    = 1
0.00.055.899 I ggml_metal_init: allocating
0.00.055.906 I ggml_metal_init: found device: Apple M4
0.00.055.908 I ggml_metal_init: picking default device: Apple M4
0.00.056.494 I ggml_metal_init: using embedded metal library
0.00.058.825 I ggml_metal_init: GPU name:   Apple M4
0.00.058.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.828 I ggml_metal_init: simdgroup reduction   = true
0.00.058.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.828 I ggml_metal_init: has bfloat            = true
0.00.058.828 I ggml_metal_init: use bfloat            = true
0.00.058.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.538 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.544 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.603 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.604 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.605 I llama_new_context_with_model: graph nodes  = 967
0.00.088.605 I llama_new_context_with_model: graph splits = 2
0.00.088.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.922 I main: llama threadpool init, n_threads = 4
0.00.541.963 I 
0.00.542.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.008 I 
0.00.542.221 I sampler seed: 1234
0.00.542.225 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.241 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.241 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.287.635 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.287.636 I llama_perf_context_print:        load time =     530.40 ms
0.01.287.636 I llama_perf_context_print: prompt eval time =      40.44 ms /     7 tokens (    5.78 ms per token,   173.10 tokens per second)
0.01.287.637 I llama_perf_context_print:        eval time =     701.94 ms /    63 runs   (   11.14 ms per token,    89.75 tokens per second)
0.01.287.640 I llama_perf_context_print:       total time =     745.71 ms /    70 tokens
0.01.287.832 I ggml_metal_free: deallocating

real	0m1.305s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.045 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.524 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.528 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.433 I llama_model_loader: - type  f32:  194 tensors
0.00.024.433 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.433 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.434 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.466 I llm_load_vocab: special tokens cache size = 25
0.00.050.370 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.385 I llm_load_print_meta: arch             = gptneox
0.00.050.386 I llm_load_print_meta: vocab type       = BPE
0.00.050.386 I llm_load_print_meta: n_vocab          = 50304
0.00.050.386 I llm_load_print_meta: n_merges         = 50009
0.00.050.386 I llm_load_print_meta: vocab_only       = 0
0.00.050.387 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.387 I llm_load_print_meta: n_embd           = 2048
0.00.050.387 I llm_load_print_meta: n_layer          = 24
0.00.050.389 I llm_load_print_meta: n_head           = 16
0.00.050.390 I llm_load_print_meta: n_head_kv        = 16
0.00.050.390 I llm_load_print_meta: n_rot            = 32
0.00.050.390 I llm_load_print_meta: n_swa            = 0
0.00.050.391 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.391 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.392 I llm_load_print_meta: n_gqa            = 1
0.00.050.392 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.393 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.394 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.396 I llm_load_print_meta: n_ff             = 8192
0.00.050.397 I llm_load_print_meta: n_expert         = 0
0.00.050.397 I llm_load_print_meta: n_expert_used    = 0
0.00.050.397 I llm_load_print_meta: causal attn      = 1
0.00.050.397 I llm_load_print_meta: pooling type     = 0
0.00.050.397 I llm_load_print_meta: rope type        = 2
0.00.050.398 I llm_load_print_meta: rope scaling     = linear
0.00.050.398 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.399 I llm_load_print_meta: freq_scale_train = 1
0.00.050.399 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.399 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.399 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.399 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.399 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.400 I llm_load_print_meta: model type       = 1.4B
0.00.050.400 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.402 I llm_load_print_meta: model params     = 1.41 B
0.00.050.402 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.402 I llm_load_print_meta: general.name     = 1.4B
0.00.050.403 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.403 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.404 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.404 I llm_load_print_meta: max token length = 1024
0.00.052.301 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.301 I llm_load_tensors: offloading output layer to GPU
0.00.052.302 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.312 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.313 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.194 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.195 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.195 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.195 I llama_new_context_with_model: n_batch       = 2048
0.00.053.196 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.196 I llama_new_context_with_model: flash_attn    = 0
0.00.053.196 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.197 I llama_new_context_with_model: freq_scale    = 1
0.00.053.197 I ggml_metal_init: allocating
0.00.053.203 I ggml_metal_init: found device: Apple M4
0.00.053.205 I ggml_metal_init: picking default device: Apple M4
0.00.053.766 I ggml_metal_init: using embedded metal library
0.00.056.062 I ggml_metal_init: GPU name:   Apple M4
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.070 I ggml_metal_init: simdgroup reduction   = true
0.00.056.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.071 I ggml_metal_init: has bfloat            = true
0.00.056.071 I ggml_metal_init: use bfloat            = true
0.00.056.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.795 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.800 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.817 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.829 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.830 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.830 I llama_new_context_with_model: graph nodes  = 967
0.00.086.831 I llama_new_context_with_model: graph splits = 2
0.00.086.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.760 I main: llama threadpool init, n_threads = 4
0.00.625.802 I 
0.00.625.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.853 I 
0.00.626.088 I sampler seed: 1234
0.00.626.093 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.626.109 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.626.109 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.626.109 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.382.685 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.382.685 I llama_perf_context_print:        load time =     616.71 ms
0.01.382.686 I llama_perf_context_print: prompt eval time =      47.26 ms /     7 tokens (    6.75 ms per token,   148.13 tokens per second)
0.01.382.687 I llama_perf_context_print:        eval time =     706.33 ms /    63 runs   (   11.21 ms per token,    89.19 tokens per second)
0.01.382.687 I llama_perf_context_print:       total time =     756.93 ms /    70 tokens
0.01.382.861 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.012.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.993 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.552 I llama_model_loader: - type  f32:  194 tensors
0.00.027.552 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.553 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.640 I llm_load_vocab: special tokens cache size = 25
0.00.053.538 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.552 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.553 I llm_load_print_meta: arch             = gptneox
0.00.053.554 I llm_load_print_meta: vocab type       = BPE
0.00.053.554 I llm_load_print_meta: n_vocab          = 50304
0.00.053.554 I llm_load_print_meta: n_merges         = 50009
0.00.053.554 I llm_load_print_meta: vocab_only       = 0
0.00.053.555 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.555 I llm_load_print_meta: n_embd           = 2048
0.00.053.555 I llm_load_print_meta: n_layer          = 24
0.00.053.558 I llm_load_print_meta: n_head           = 16
0.00.053.559 I llm_load_print_meta: n_head_kv        = 16
0.00.053.559 I llm_load_print_meta: n_rot            = 32
0.00.053.559 I llm_load_print_meta: n_swa            = 0
0.00.053.559 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.560 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.560 I llm_load_print_meta: n_gqa            = 1
0.00.053.561 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.562 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.562 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.563 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.564 I llm_load_print_meta: n_ff             = 8192
0.00.053.564 I llm_load_print_meta: n_expert         = 0
0.00.053.564 I llm_load_print_meta: n_expert_used    = 0
0.00.053.566 I llm_load_print_meta: causal attn      = 1
0.00.053.566 I llm_load_print_meta: pooling type     = 0
0.00.053.566 I llm_load_print_meta: rope type        = 2
0.00.053.566 I llm_load_print_meta: rope scaling     = linear
0.00.053.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.567 I llm_load_print_meta: freq_scale_train = 1
0.00.053.567 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.567 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.567 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.567 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.567 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.567 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.568 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.568 I llm_load_print_meta: model type       = 1.4B
0.00.053.568 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.568 I llm_load_print_meta: model params     = 1.41 B
0.00.053.569 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.569 I llm_load_print_meta: general.name     = 1.4B
0.00.053.569 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.569 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.569 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.570 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.571 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.571 I llm_load_print_meta: max token length = 1024
0.00.055.584 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.584 I llm_load_tensors: offloading output layer to GPU
0.00.055.584 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.595 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.596 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.520 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.521 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.521 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.521 I llama_new_context_with_model: n_batch       = 2048
0.00.056.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.522 I llama_new_context_with_model: flash_attn    = 0
0.00.056.522 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.522 I llama_new_context_with_model: freq_scale    = 1
0.00.056.523 I ggml_metal_init: allocating
0.00.056.525 I ggml_metal_init: found device: Apple M4
0.00.056.527 I ggml_metal_init: picking default device: Apple M4
0.00.057.131 I ggml_metal_init: using embedded metal library
0.00.059.433 I ggml_metal_init: GPU name:   Apple M4
0.00.059.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.435 I ggml_metal_init: simdgroup reduction   = true
0.00.059.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.436 I ggml_metal_init: has bfloat            = true
0.00.059.436 I ggml_metal_init: use bfloat            = true
0.00.059.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.967 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.972 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.997 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.049 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.051 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.051 I llama_new_context_with_model: graph nodes  = 967
0.00.091.051 I llama_new_context_with_model: graph splits = 2
0.00.091.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.294 I main: llama threadpool init, n_threads = 4
0.00.700.347 I 
0.00.700.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.381 I 
0.00.700.607 I sampler seed: 1234
0.00.700.612 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.628 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.552.615 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.552.616 I llama_perf_context_print:        load time =     687.62 ms
0.01.552.616 I llama_perf_context_print: prompt eval time =      54.85 ms /     7 tokens (    7.84 ms per token,   127.62 tokens per second)
0.01.552.617 I llama_perf_context_print:        eval time =     794.04 ms /    63 runs   (   12.60 ms per token,    79.34 tokens per second)
0.01.552.617 I llama_perf_context_print:       total time =     852.33 ms /    70 tokens
0.01.552.816 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.268 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.172 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.182 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.187 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.963 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.964 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.965 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.965 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.965 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.966 I llama_model_loader: - type  f32:  194 tensors
0.00.024.966 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.814 I llm_load_vocab: special tokens cache size = 25
0.00.051.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.682 I llm_load_print_meta: arch             = gptneox
0.00.051.682 I llm_load_print_meta: vocab type       = BPE
0.00.051.683 I llm_load_print_meta: n_vocab          = 50304
0.00.051.683 I llm_load_print_meta: n_merges         = 50009
0.00.051.683 I llm_load_print_meta: vocab_only       = 0
0.00.051.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.683 I llm_load_print_meta: n_embd           = 2048
0.00.051.686 I llm_load_print_meta: n_layer          = 24
0.00.051.688 I llm_load_print_meta: n_head           = 16
0.00.051.693 I llm_load_print_meta: n_head_kv        = 16
0.00.051.693 I llm_load_print_meta: n_rot            = 32
0.00.051.694 I llm_load_print_meta: n_swa            = 0
0.00.051.694 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.694 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.695 I llm_load_print_meta: n_gqa            = 1
0.00.051.696 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.696 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.697 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.697 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.697 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.698 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.698 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.698 I llm_load_print_meta: n_ff             = 8192
0.00.051.699 I llm_load_print_meta: n_expert         = 0
0.00.051.699 I llm_load_print_meta: n_expert_used    = 0
0.00.051.699 I llm_load_print_meta: causal attn      = 1
0.00.051.699 I llm_load_print_meta: pooling type     = 0
0.00.051.699 I llm_load_print_meta: rope type        = 2
0.00.051.700 I llm_load_print_meta: rope scaling     = linear
0.00.051.701 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.701 I llm_load_print_meta: freq_scale_train = 1
0.00.051.701 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.702 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.702 I llm_load_print_meta: model type       = 1.4B
0.00.051.703 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.703 I llm_load_print_meta: model params     = 1.41 B
0.00.051.703 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.703 I llm_load_print_meta: general.name     = 1.4B
0.00.051.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.704 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: max token length = 1024
0.00.053.480 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.481 I llm_load_tensors: offloading output layer to GPU
0.00.053.481 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.486 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.487 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.423 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.424 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.424 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.424 I llama_new_context_with_model: n_batch       = 2048
0.00.054.424 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.425 I llama_new_context_with_model: flash_attn    = 0
0.00.054.425 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.425 I llama_new_context_with_model: freq_scale    = 1
0.00.054.426 I ggml_metal_init: allocating
0.00.054.429 I ggml_metal_init: found device: Apple M4
0.00.054.431 I ggml_metal_init: picking default device: Apple M4
0.00.054.992 I ggml_metal_init: using embedded metal library
0.00.057.296 I ggml_metal_init: GPU name:   Apple M4
0.00.057.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.298 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.298 I ggml_metal_init: simdgroup reduction   = true
0.00.057.298 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.299 I ggml_metal_init: has bfloat            = true
0.00.057.299 I ggml_metal_init: use bfloat            = true
0.00.057.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.300 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.518 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.527 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.547 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.671 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.672 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.672 I llama_new_context_with_model: graph nodes  = 967
0.00.087.673 I llama_new_context_with_model: graph splits = 2
0.00.087.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.101 I main: llama threadpool init, n_threads = 4
0.00.756.140 I 
0.00.756.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.172 I 
0.00.756.395 I sampler seed: 1234
0.00.756.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.454 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.455 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.636.715 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.01.636.716 I llama_perf_context_print:        load time =     746.83 ms
0.01.636.716 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.63 tokens per second)
0.01.636.718 I llama_perf_context_print:        eval time =     822.96 ms /    63 runs   (   13.06 ms per token,    76.55 tokens per second)
0.01.636.718 I llama_perf_context_print:       total time =     880.62 ms /    70 tokens
0.01.636.911 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.471 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.308 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.324 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.326 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.578 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.943 I llama_model_loader: - type  f32:  194 tensors
0.00.055.943 I llama_model_loader: - type  f16:   98 tensors
0.00.085.836 I llm_load_vocab: special tokens cache size = 25
0.00.092.654 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.668 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.669 I llm_load_print_meta: arch             = gptneox
0.00.092.670 I llm_load_print_meta: vocab type       = BPE
0.00.092.670 I llm_load_print_meta: n_vocab          = 50304
0.00.092.670 I llm_load_print_meta: n_merges         = 50009
0.00.092.670 I llm_load_print_meta: vocab_only       = 0
0.00.092.670 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.670 I llm_load_print_meta: n_embd           = 2048
0.00.092.671 I llm_load_print_meta: n_layer          = 24
0.00.092.673 I llm_load_print_meta: n_head           = 16
0.00.092.673 I llm_load_print_meta: n_head_kv        = 16
0.00.092.674 I llm_load_print_meta: n_rot            = 32
0.00.092.674 I llm_load_print_meta: n_swa            = 0
0.00.092.674 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.676 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.677 I llm_load_print_meta: n_gqa            = 1
0.00.092.678 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.678 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.679 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.679 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.679 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.679 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.679 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.680 I llm_load_print_meta: n_ff             = 8192
0.00.092.680 I llm_load_print_meta: n_expert         = 0
0.00.092.680 I llm_load_print_meta: n_expert_used    = 0
0.00.092.680 I llm_load_print_meta: causal attn      = 1
0.00.092.680 I llm_load_print_meta: pooling type     = 0
0.00.092.680 I llm_load_print_meta: rope type        = 2
0.00.092.681 I llm_load_print_meta: rope scaling     = linear
0.00.092.681 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.681 I llm_load_print_meta: freq_scale_train = 1
0.00.092.682 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.684 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.684 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.684 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.684 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.684 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.684 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.684 I llm_load_print_meta: model type       = 1.4B
0.00.092.685 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.689 I llm_load_print_meta: model params     = 1.41 B
0.00.092.690 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.690 I llm_load_print_meta: general.name     = 1.4B
0.00.092.690 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.690 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.690 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.691 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.691 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.691 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.691 I llm_load_print_meta: max token length = 1024
0.00.095.187 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.187 I llm_load_tensors: offloading output layer to GPU
0.00.095.188 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.198 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.199 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.109 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.109 I llama_new_context_with_model: n_ctx         = 128
0.00.096.109 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.110 I llama_new_context_with_model: n_batch       = 128
0.00.096.110 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.110 I llama_new_context_with_model: flash_attn    = 0
0.00.096.110 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.110 I llama_new_context_with_model: freq_scale    = 1
0.00.096.111 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.111 I ggml_metal_init: allocating
0.00.096.114 I ggml_metal_init: found device: Apple M4
0.00.096.116 I ggml_metal_init: picking default device: Apple M4
0.00.096.718 I ggml_metal_init: using embedded metal library
0.00.099.265 I ggml_metal_init: GPU name:   Apple M4
0.00.099.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.267 I ggml_metal_init: simdgroup reduction   = true
0.00.099.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.268 I ggml_metal_init: has bfloat            = true
0.00.099.268 I ggml_metal_init: use bfloat            = true
0.00.099.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.146 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.148 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.162 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.986 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.987 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.987 I llama_new_context_with_model: graph nodes  = 967
0.00.110.987 I llama_new_context_with_model: graph splits = 2
0.00.110.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.101 I 
0.00.836.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.207 I perplexity: tokenizing the input ..
0.00.849.265 I perplexity: tokenization took 13.054 ms
0.00.849.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.970.109 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.971.948 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.971.969 I llama_perf_context_print:        load time =     812.12 ms
0.00.971.973 I llama_perf_context_print: prompt eval time =     119.92 ms /   128 tokens (    0.94 ms per token,  1067.42 tokens per second)
0.00.971.974 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.971.978 I llama_perf_context_print:       total time =     135.87 ms /   129 tokens
0.00.972.667 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.128s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.923 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.568 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.796 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.797 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.798 I llama_model_loader: - type  f32:  194 tensors
0.00.033.798 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.948 I llm_load_vocab: special tokens cache size = 25
0.00.066.086 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.101 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.102 I llm_load_print_meta: arch             = gptneox
0.00.066.103 I llm_load_print_meta: vocab type       = BPE
0.00.066.103 I llm_load_print_meta: n_vocab          = 50304
0.00.066.103 I llm_load_print_meta: n_merges         = 50009
0.00.066.103 I llm_load_print_meta: vocab_only       = 0
0.00.066.103 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.103 I llm_load_print_meta: n_embd           = 2048
0.00.066.103 I llm_load_print_meta: n_layer          = 24
0.00.066.107 I llm_load_print_meta: n_head           = 16
0.00.066.108 I llm_load_print_meta: n_head_kv        = 16
0.00.066.108 I llm_load_print_meta: n_rot            = 32
0.00.066.109 I llm_load_print_meta: n_swa            = 0
0.00.066.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.111 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.111 I llm_load_print_meta: n_gqa            = 1
0.00.066.112 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.113 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.114 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.114 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.114 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.115 I llm_load_print_meta: n_ff             = 8192
0.00.066.115 I llm_load_print_meta: n_expert         = 0
0.00.066.115 I llm_load_print_meta: n_expert_used    = 0
0.00.066.116 I llm_load_print_meta: causal attn      = 1
0.00.066.116 I llm_load_print_meta: pooling type     = 0
0.00.066.116 I llm_load_print_meta: rope type        = 2
0.00.066.116 I llm_load_print_meta: rope scaling     = linear
0.00.066.116 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.116 I llm_load_print_meta: freq_scale_train = 1
0.00.066.117 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.117 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.117 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.117 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.117 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.117 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.117 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.118 I llm_load_print_meta: model type       = 1.4B
0.00.066.118 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.118 I llm_load_print_meta: model params     = 1.41 B
0.00.066.119 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.119 I llm_load_print_meta: general.name     = 1.4B
0.00.066.119 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.119 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.119 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.120 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.120 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.125 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.128 I llm_load_print_meta: max token length = 1024
0.00.068.586 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.586 I llm_load_tensors: offloading output layer to GPU
0.00.068.586 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.597 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.598 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.577 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.578 I llama_new_context_with_model: n_ctx         = 128
0.00.069.578 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.579 I llama_new_context_with_model: n_batch       = 128
0.00.069.579 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.579 I llama_new_context_with_model: flash_attn    = 0
0.00.069.580 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.580 I llama_new_context_with_model: freq_scale    = 1
0.00.069.580 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.581 I ggml_metal_init: allocating
0.00.069.584 I ggml_metal_init: found device: Apple M4
0.00.069.586 I ggml_metal_init: picking default device: Apple M4
0.00.070.248 I ggml_metal_init: using embedded metal library
0.00.072.977 I ggml_metal_init: GPU name:   Apple M4
0.00.072.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.980 I ggml_metal_init: simdgroup reduction   = true
0.00.072.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.980 I ggml_metal_init: has bfloat            = true
0.00.072.980 I ggml_metal_init: use bfloat            = true
0.00.072.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.949 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.951 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.967 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.960 I llama_new_context_with_model: graph nodes  = 967
0.00.084.960 I llama_new_context_with_model: graph splits = 2
0.00.084.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.898.057 I 
0.00.898.086 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.898.094 I perplexity: tokenizing the input ..
0.00.906.132 I perplexity: tokenization took 8.037 ms
0.00.906.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.029.780 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.030.927 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.030.941 I llama_perf_context_print:        load time =     886.13 ms
0.01.030.942 I llama_perf_context_print: prompt eval time =     123.41 ms /   128 tokens (    0.96 ms per token,  1037.18 tokens per second)
0.01.030.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.030.945 I llama_perf_context_print:       total time =     132.88 ms /   129 tokens
0.01.031.336 I ggml_metal_free: deallocating

real	0m1.050s
user	0m0.094s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.685 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.692 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.514 I llama_model_loader: - type  f32:  194 tensors
0.00.024.514 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.514 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.237 I llm_load_vocab: special tokens cache size = 25
0.00.051.112 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.126 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.127 I llm_load_print_meta: arch             = gptneox
0.00.051.128 I llm_load_print_meta: vocab type       = BPE
0.00.051.128 I llm_load_print_meta: n_vocab          = 50304
0.00.051.128 I llm_load_print_meta: n_merges         = 50009
0.00.051.128 I llm_load_print_meta: vocab_only       = 0
0.00.051.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.128 I llm_load_print_meta: n_embd           = 2048
0.00.051.129 I llm_load_print_meta: n_layer          = 24
0.00.051.131 I llm_load_print_meta: n_head           = 16
0.00.051.132 I llm_load_print_meta: n_head_kv        = 16
0.00.051.134 I llm_load_print_meta: n_rot            = 32
0.00.051.134 I llm_load_print_meta: n_swa            = 0
0.00.051.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.136 I llm_load_print_meta: n_gqa            = 1
0.00.051.136 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.137 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.137 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.138 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.138 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.138 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.138 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.139 I llm_load_print_meta: n_ff             = 8192
0.00.051.139 I llm_load_print_meta: n_expert         = 0
0.00.051.139 I llm_load_print_meta: n_expert_used    = 0
0.00.051.139 I llm_load_print_meta: causal attn      = 1
0.00.051.139 I llm_load_print_meta: pooling type     = 0
0.00.051.139 I llm_load_print_meta: rope type        = 2
0.00.051.141 I llm_load_print_meta: rope scaling     = linear
0.00.051.141 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.141 I llm_load_print_meta: freq_scale_train = 1
0.00.051.141 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.142 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.142 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.142 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.142 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.142 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.142 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.143 I llm_load_print_meta: model type       = 1.4B
0.00.051.143 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.143 I llm_load_print_meta: model params     = 1.41 B
0.00.051.144 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.144 I llm_load_print_meta: general.name     = 1.4B
0.00.051.144 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.144 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.144 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.144 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.145 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.145 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.145 I llm_load_print_meta: max token length = 1024
0.00.053.059 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.059 I llm_load_tensors: offloading output layer to GPU
0.00.053.059 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.070 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.071 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.951 I llama_new_context_with_model: n_ctx         = 128
0.00.053.952 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.952 I llama_new_context_with_model: n_batch       = 128
0.00.053.952 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.952 I llama_new_context_with_model: flash_attn    = 0
0.00.053.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.953 I llama_new_context_with_model: freq_scale    = 1
0.00.053.953 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.954 I ggml_metal_init: allocating
0.00.053.959 I ggml_metal_init: found device: Apple M4
0.00.053.962 I ggml_metal_init: picking default device: Apple M4
0.00.054.553 I ggml_metal_init: using embedded metal library
0.00.056.838 I ggml_metal_init: GPU name:   Apple M4
0.00.056.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.841 I ggml_metal_init: simdgroup reduction   = true
0.00.056.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.841 I ggml_metal_init: has bfloat            = true
0.00.056.841 I ggml_metal_init: use bfloat            = true
0.00.056.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.786 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.690 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.691 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.691 I llama_new_context_with_model: graph nodes  = 967
0.00.068.692 I llama_new_context_with_model: graph splits = 2
0.00.068.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.489 I 
0.00.600.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.551 I perplexity: tokenizing the input ..
0.00.608.189 I perplexity: tokenization took 7.637 ms
0.00.608.199 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.172 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.732.419 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.732.436 I llama_perf_context_print:        load time =     590.80 ms
0.00.732.437 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.84 tokens per second)
0.00.732.437 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.438 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.732.929 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.141 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.061 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.036 I llama_model_loader: - type  f32:  194 tensors
0.00.024.036 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.147 I llm_load_vocab: special tokens cache size = 25
0.00.050.045 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.056 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.057 I llm_load_print_meta: arch             = gptneox
0.00.050.057 I llm_load_print_meta: vocab type       = BPE
0.00.050.057 I llm_load_print_meta: n_vocab          = 50304
0.00.050.058 I llm_load_print_meta: n_merges         = 50009
0.00.050.058 I llm_load_print_meta: vocab_only       = 0
0.00.050.058 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.058 I llm_load_print_meta: n_embd           = 2048
0.00.050.058 I llm_load_print_meta: n_layer          = 24
0.00.050.061 I llm_load_print_meta: n_head           = 16
0.00.050.062 I llm_load_print_meta: n_head_kv        = 16
0.00.050.062 I llm_load_print_meta: n_rot            = 32
0.00.050.062 I llm_load_print_meta: n_swa            = 0
0.00.050.062 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.062 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.063 I llm_load_print_meta: n_gqa            = 1
0.00.050.064 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.065 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.065 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.066 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.066 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.066 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.067 I llm_load_print_meta: n_ff             = 8192
0.00.050.067 I llm_load_print_meta: n_expert         = 0
0.00.050.068 I llm_load_print_meta: n_expert_used    = 0
0.00.050.068 I llm_load_print_meta: causal attn      = 1
0.00.050.068 I llm_load_print_meta: pooling type     = 0
0.00.050.069 I llm_load_print_meta: rope type        = 2
0.00.050.069 I llm_load_print_meta: rope scaling     = linear
0.00.050.069 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.070 I llm_load_print_meta: freq_scale_train = 1
0.00.050.070 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.070 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.070 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.070 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.070 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.070 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.071 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.071 I llm_load_print_meta: model type       = 1.4B
0.00.050.071 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.071 I llm_load_print_meta: model params     = 1.41 B
0.00.050.072 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.072 I llm_load_print_meta: general.name     = 1.4B
0.00.050.073 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.075 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: max token length = 1024
0.00.051.827 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.827 I llm_load_tensors: offloading output layer to GPU
0.00.051.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.833 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.835 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.728 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.729 I llama_new_context_with_model: n_ctx         = 128
0.00.052.729 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.729 I llama_new_context_with_model: n_batch       = 128
0.00.052.729 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.730 I llama_new_context_with_model: flash_attn    = 0
0.00.052.730 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.730 I llama_new_context_with_model: freq_scale    = 1
0.00.052.731 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.731 I ggml_metal_init: allocating
0.00.052.734 I ggml_metal_init: found device: Apple M4
0.00.052.736 I ggml_metal_init: picking default device: Apple M4
0.00.053.292 I ggml_metal_init: using embedded metal library
0.00.055.566 I ggml_metal_init: GPU name:   Apple M4
0.00.055.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.567 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.568 I ggml_metal_init: simdgroup reduction   = true
0.00.055.568 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.568 I ggml_metal_init: has bfloat            = true
0.00.055.568 I ggml_metal_init: use bfloat            = true
0.00.055.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.440 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.444 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.461 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.369 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.370 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.370 I llama_new_context_with_model: graph nodes  = 967
0.00.067.370 I llama_new_context_with_model: graph splits = 2
0.00.067.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.830 I 
0.00.636.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.874 I perplexity: tokenizing the input ..
0.00.645.024 I perplexity: tokenization took 8.149 ms
0.00.645.036 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.521 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.768.647 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.768.660 I llama_perf_context_print:        load time =     627.69 ms
0.00.768.661 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.94 tokens per second)
0.00.768.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.662 I llama_perf_context_print:       total time =     131.83 ms /   129 tokens
0.00.769.058 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.944 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.693 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.697 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.698 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.674 I llama_model_loader: - type  f32:  194 tensors
0.00.024.675 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.675 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.465 I llm_load_vocab: special tokens cache size = 25
0.00.051.378 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.387 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.388 I llm_load_print_meta: arch             = gptneox
0.00.051.389 I llm_load_print_meta: vocab type       = BPE
0.00.051.389 I llm_load_print_meta: n_vocab          = 50304
0.00.051.389 I llm_load_print_meta: n_merges         = 50009
0.00.051.389 I llm_load_print_meta: vocab_only       = 0
0.00.051.390 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.390 I llm_load_print_meta: n_embd           = 2048
0.00.051.390 I llm_load_print_meta: n_layer          = 24
0.00.051.392 I llm_load_print_meta: n_head           = 16
0.00.051.393 I llm_load_print_meta: n_head_kv        = 16
0.00.051.393 I llm_load_print_meta: n_rot            = 32
0.00.051.394 I llm_load_print_meta: n_swa            = 0
0.00.051.394 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.397 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.397 I llm_load_print_meta: n_gqa            = 1
0.00.051.398 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.399 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.400 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.400 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.400 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.401 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.401 I llm_load_print_meta: n_ff             = 8192
0.00.051.403 I llm_load_print_meta: n_expert         = 0
0.00.051.403 I llm_load_print_meta: n_expert_used    = 0
0.00.051.403 I llm_load_print_meta: causal attn      = 1
0.00.051.403 I llm_load_print_meta: pooling type     = 0
0.00.051.403 I llm_load_print_meta: rope type        = 2
0.00.051.403 I llm_load_print_meta: rope scaling     = linear
0.00.051.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.404 I llm_load_print_meta: freq_scale_train = 1
0.00.051.404 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.406 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.406 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.406 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.406 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.406 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.407 I llm_load_print_meta: model type       = 1.4B
0.00.051.407 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.407 I llm_load_print_meta: model params     = 1.41 B
0.00.051.408 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.408 I llm_load_print_meta: general.name     = 1.4B
0.00.051.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.409 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: max token length = 1024
0.00.053.178 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.178 I llm_load_tensors: offloading output layer to GPU
0.00.053.178 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.184 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.185 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.060 I llama_new_context_with_model: n_ctx         = 128
0.00.054.060 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.060 I llama_new_context_with_model: n_batch       = 128
0.00.054.060 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.060 I llama_new_context_with_model: flash_attn    = 0
0.00.054.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.061 I llama_new_context_with_model: freq_scale    = 1
0.00.054.062 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.062 I ggml_metal_init: allocating
0.00.054.065 I ggml_metal_init: found device: Apple M4
0.00.054.067 I ggml_metal_init: picking default device: Apple M4
0.00.054.613 I ggml_metal_init: using embedded metal library
0.00.056.917 I ggml_metal_init: GPU name:   Apple M4
0.00.056.918 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.919 I ggml_metal_init: simdgroup reduction   = true
0.00.056.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.920 I ggml_metal_init: has bfloat            = true
0.00.056.920 I ggml_metal_init: use bfloat            = true
0.00.056.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.922 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.483 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.488 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.363 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.365 I llama_new_context_with_model: graph nodes  = 967
0.00.068.365 I llama_new_context_with_model: graph splits = 2
0.00.068.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.254 I 
0.00.603.286 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.294 I perplexity: tokenizing the input ..
0.00.610.981 I perplexity: tokenization took 7.685 ms
0.00.610.995 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.862 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.747.037 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.747.054 I llama_perf_context_print:        load time =     593.31 ms
0.00.747.055 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.68 tokens per second)
0.00.747.056 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.056 I llama_perf_context_print:       total time =     143.80 ms /   129 tokens
0.00.747.513 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.589 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.256 I llama_model_loader: - type  f32:  194 tensors
0.00.023.256 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.256 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.259 I llm_load_vocab: special tokens cache size = 25
0.00.049.141 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.155 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.156 I llm_load_print_meta: arch             = gptneox
0.00.049.157 I llm_load_print_meta: vocab type       = BPE
0.00.049.157 I llm_load_print_meta: n_vocab          = 50304
0.00.049.157 I llm_load_print_meta: n_merges         = 50009
0.00.049.157 I llm_load_print_meta: vocab_only       = 0
0.00.049.158 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.158 I llm_load_print_meta: n_embd           = 2048
0.00.049.158 I llm_load_print_meta: n_layer          = 24
0.00.049.161 I llm_load_print_meta: n_head           = 16
0.00.049.162 I llm_load_print_meta: n_head_kv        = 16
0.00.049.162 I llm_load_print_meta: n_rot            = 32
0.00.049.162 I llm_load_print_meta: n_swa            = 0
0.00.049.162 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.163 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.163 I llm_load_print_meta: n_gqa            = 1
0.00.049.164 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.165 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.165 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.166 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.166 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.166 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.167 I llm_load_print_meta: n_ff             = 8192
0.00.049.167 I llm_load_print_meta: n_expert         = 0
0.00.049.168 I llm_load_print_meta: n_expert_used    = 0
0.00.049.168 I llm_load_print_meta: causal attn      = 1
0.00.049.170 I llm_load_print_meta: pooling type     = 0
0.00.049.170 I llm_load_print_meta: rope type        = 2
0.00.049.170 I llm_load_print_meta: rope scaling     = linear
0.00.049.170 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.171 I llm_load_print_meta: freq_scale_train = 1
0.00.049.171 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.173 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.173 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.173 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.173 I llm_load_print_meta: model type       = 1.4B
0.00.049.173 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.174 I llm_load_print_meta: model params     = 1.41 B
0.00.049.174 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.174 I llm_load_print_meta: general.name     = 1.4B
0.00.049.174 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.175 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.176 I llm_load_print_meta: max token length = 1024
0.00.051.179 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.179 I llm_load_tensors: offloading output layer to GPU
0.00.051.180 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.190 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.191 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.085 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.086 I llama_new_context_with_model: n_ctx         = 128
0.00.052.086 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.086 I llama_new_context_with_model: n_batch       = 128
0.00.052.086 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.087 I llama_new_context_with_model: flash_attn    = 0
0.00.052.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.087 I llama_new_context_with_model: freq_scale    = 1
0.00.052.088 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.088 I ggml_metal_init: allocating
0.00.052.091 I ggml_metal_init: found device: Apple M4
0.00.052.093 I ggml_metal_init: picking default device: Apple M4
0.00.052.688 I ggml_metal_init: using embedded metal library
0.00.054.965 I ggml_metal_init: GPU name:   Apple M4
0.00.054.966 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.967 I ggml_metal_init: simdgroup reduction   = true
0.00.054.967 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.967 I ggml_metal_init: has bfloat            = true
0.00.054.967 I ggml_metal_init: use bfloat            = true
0.00.054.968 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.968 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.611 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.614 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.627 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.548 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.549 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.550 I llama_new_context_with_model: graph nodes  = 967
0.00.066.550 I llama_new_context_with_model: graph splits = 2
0.00.066.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.567 I 
0.00.743.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.605 I perplexity: tokenizing the input ..
0.00.751.285 I perplexity: tokenization took 7.679 ms
0.00.751.296 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.352 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.887.510 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.887.525 I llama_perf_context_print:        load time =     734.85 ms
0.00.887.526 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.34 tokens per second)
0.00.887.527 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.527 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.887.953 I ggml_metal_free: deallocating

real	0m0.901s
user	0m0.077s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.789 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.094 I llama_model_loader: - type  f32:  194 tensors
0.00.024.094 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.094 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.095 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.808 I llm_load_vocab: special tokens cache size = 25
0.00.050.666 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.680 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.681 I llm_load_print_meta: arch             = gptneox
0.00.050.682 I llm_load_print_meta: vocab type       = BPE
0.00.050.682 I llm_load_print_meta: n_vocab          = 50304
0.00.050.682 I llm_load_print_meta: n_merges         = 50009
0.00.050.682 I llm_load_print_meta: vocab_only       = 0
0.00.050.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.683 I llm_load_print_meta: n_embd           = 2048
0.00.050.683 I llm_load_print_meta: n_layer          = 24
0.00.050.686 I llm_load_print_meta: n_head           = 16
0.00.050.687 I llm_load_print_meta: n_head_kv        = 16
0.00.050.687 I llm_load_print_meta: n_rot            = 32
0.00.050.687 I llm_load_print_meta: n_swa            = 0
0.00.050.687 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.687 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.688 I llm_load_print_meta: n_gqa            = 1
0.00.050.689 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.690 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.690 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.690 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.691 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.691 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.691 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.692 I llm_load_print_meta: n_ff             = 8192
0.00.050.692 I llm_load_print_meta: n_expert         = 0
0.00.050.692 I llm_load_print_meta: n_expert_used    = 0
0.00.050.692 I llm_load_print_meta: causal attn      = 1
0.00.050.692 I llm_load_print_meta: pooling type     = 0
0.00.050.692 I llm_load_print_meta: rope type        = 2
0.00.050.692 I llm_load_print_meta: rope scaling     = linear
0.00.050.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.693 I llm_load_print_meta: freq_scale_train = 1
0.00.050.693 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.693 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.697 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.697 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.698 I llm_load_print_meta: model type       = 1.4B
0.00.050.698 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.699 I llm_load_print_meta: model params     = 1.41 B
0.00.050.699 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.699 I llm_load_print_meta: general.name     = 1.4B
0.00.050.700 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.700 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.702 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.702 I llm_load_print_meta: max token length = 1024
0.00.052.583 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.583 I llm_load_tensors: offloading output layer to GPU
0.00.052.583 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.593 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.594 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.488 I llama_new_context_with_model: n_ctx         = 128
0.00.053.488 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.488 I llama_new_context_with_model: n_batch       = 128
0.00.053.489 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.489 I llama_new_context_with_model: flash_attn    = 0
0.00.053.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.490 I llama_new_context_with_model: freq_scale    = 1
0.00.053.490 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.491 I ggml_metal_init: allocating
0.00.053.496 I ggml_metal_init: found device: Apple M4
0.00.053.499 I ggml_metal_init: picking default device: Apple M4
0.00.054.054 I ggml_metal_init: using embedded metal library
0.00.056.404 I ggml_metal_init: GPU name:   Apple M4
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.407 I ggml_metal_init: simdgroup reduction   = true
0.00.056.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.407 I ggml_metal_init: has bfloat            = true
0.00.056.407 I ggml_metal_init: use bfloat            = true
0.00.056.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.903 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.911 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.928 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.785 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.786 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.786 I llama_new_context_with_model: graph nodes  = 967
0.00.067.786 I llama_new_context_with_model: graph splits = 2
0.00.067.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.092 I 
0.00.388.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.136 I perplexity: tokenizing the input ..
0.00.395.945 I perplexity: tokenization took 7.807 ms
0.00.395.959 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.578 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.835 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.854 I llama_perf_context_print:        load time =     378.30 ms
0.00.529.855 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.82 tokens per second)
0.00.529.855 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.856 I llama_perf_context_print:       total time =     141.77 ms /   129 tokens
0.00.530.268 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.078s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.323 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.733 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.734 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.735 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.735 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.736 I llama_model_loader: - type  f32:  194 tensors
0.00.023.736 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.736 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.737 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.737 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.765 I llm_load_vocab: special tokens cache size = 25
0.00.049.689 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.703 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.704 I llm_load_print_meta: arch             = gptneox
0.00.049.705 I llm_load_print_meta: vocab type       = BPE
0.00.049.705 I llm_load_print_meta: n_vocab          = 50304
0.00.049.705 I llm_load_print_meta: n_merges         = 50009
0.00.049.705 I llm_load_print_meta: vocab_only       = 0
0.00.049.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.706 I llm_load_print_meta: n_embd           = 2048
0.00.049.706 I llm_load_print_meta: n_layer          = 24
0.00.049.708 I llm_load_print_meta: n_head           = 16
0.00.049.709 I llm_load_print_meta: n_head_kv        = 16
0.00.049.709 I llm_load_print_meta: n_rot            = 32
0.00.049.710 I llm_load_print_meta: n_swa            = 0
0.00.049.710 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.710 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.711 I llm_load_print_meta: n_gqa            = 1
0.00.049.711 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.712 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.714 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.714 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.715 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.715 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.715 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.715 I llm_load_print_meta: n_ff             = 8192
0.00.049.716 I llm_load_print_meta: n_expert         = 0
0.00.049.716 I llm_load_print_meta: n_expert_used    = 0
0.00.049.716 I llm_load_print_meta: causal attn      = 1
0.00.049.716 I llm_load_print_meta: pooling type     = 0
0.00.049.716 I llm_load_print_meta: rope type        = 2
0.00.049.716 I llm_load_print_meta: rope scaling     = linear
0.00.049.717 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.717 I llm_load_print_meta: freq_scale_train = 1
0.00.049.717 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.717 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.718 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.718 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.718 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.718 I llm_load_print_meta: model type       = 1.4B
0.00.049.718 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.719 I llm_load_print_meta: model params     = 1.41 B
0.00.049.719 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.719 I llm_load_print_meta: general.name     = 1.4B
0.00.049.720 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.720 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.720 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.722 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.722 I llm_load_print_meta: max token length = 1024
0.00.051.641 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.641 I llm_load_tensors: offloading output layer to GPU
0.00.051.642 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.652 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.653 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.620 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.621 I llama_new_context_with_model: n_ctx         = 128
0.00.052.621 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.621 I llama_new_context_with_model: n_batch       = 128
0.00.052.622 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.622 I llama_new_context_with_model: flash_attn    = 0
0.00.052.622 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.622 I llama_new_context_with_model: freq_scale    = 1
0.00.052.623 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.623 I ggml_metal_init: allocating
0.00.052.626 I ggml_metal_init: found device: Apple M4
0.00.052.627 I ggml_metal_init: picking default device: Apple M4
0.00.053.174 I ggml_metal_init: using embedded metal library
0.00.055.450 I ggml_metal_init: GPU name:   Apple M4
0.00.055.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.452 I ggml_metal_init: simdgroup reduction   = true
0.00.055.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.453 I ggml_metal_init: has bfloat            = true
0.00.055.453 I ggml_metal_init: use bfloat            = true
0.00.055.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.187 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.189 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.203 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.117 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.119 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.119 I llama_new_context_with_model: graph nodes  = 967
0.00.067.119 I llama_new_context_with_model: graph splits = 2
0.00.067.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.498 I 
0.00.487.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.552 I perplexity: tokenizing the input ..
0.00.495.660 I perplexity: tokenization took 8.105 ms
0.00.495.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.452 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.613 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.630 I llama_perf_context_print:        load time =     478.17 ms
0.00.628.631 I llama_perf_context_print: prompt eval time =     131.55 ms /   128 tokens (    1.03 ms per token,   973.00 tokens per second)
0.00.628.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.632 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.629.137 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.077s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.139 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.954 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.955 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.955 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.633 I llama_model_loader: - type  f32:  194 tensors
0.00.023.633 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.634 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.634 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.721 I llm_load_vocab: special tokens cache size = 25
0.00.050.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.590 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.592 I llm_load_print_meta: arch             = gptneox
0.00.050.592 I llm_load_print_meta: vocab type       = BPE
0.00.050.592 I llm_load_print_meta: n_vocab          = 50304
0.00.050.593 I llm_load_print_meta: n_merges         = 50009
0.00.050.593 I llm_load_print_meta: vocab_only       = 0
0.00.050.593 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.593 I llm_load_print_meta: n_embd           = 2048
0.00.050.593 I llm_load_print_meta: n_layer          = 24
0.00.050.596 I llm_load_print_meta: n_head           = 16
0.00.050.597 I llm_load_print_meta: n_head_kv        = 16
0.00.050.598 I llm_load_print_meta: n_rot            = 32
0.00.050.598 I llm_load_print_meta: n_swa            = 0
0.00.050.598 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.598 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.600 I llm_load_print_meta: n_gqa            = 1
0.00.050.601 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.601 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.602 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.602 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.602 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.602 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.603 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.603 I llm_load_print_meta: n_ff             = 8192
0.00.050.603 I llm_load_print_meta: n_expert         = 0
0.00.050.603 I llm_load_print_meta: n_expert_used    = 0
0.00.050.604 I llm_load_print_meta: causal attn      = 1
0.00.050.604 I llm_load_print_meta: pooling type     = 0
0.00.050.604 I llm_load_print_meta: rope type        = 2
0.00.050.604 I llm_load_print_meta: rope scaling     = linear
0.00.050.604 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.605 I llm_load_print_meta: freq_scale_train = 1
0.00.050.605 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.605 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.605 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.605 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.605 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.605 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.605 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.606 I llm_load_print_meta: model type       = 1.4B
0.00.050.606 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.606 I llm_load_print_meta: model params     = 1.41 B
0.00.050.607 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.607 I llm_load_print_meta: general.name     = 1.4B
0.00.050.607 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.607 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.607 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.608 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.608 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.608 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.609 I llm_load_print_meta: max token length = 1024
0.00.052.579 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.580 I llm_load_tensors: offloading output layer to GPU
0.00.052.580 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.590 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.591 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.473 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.474 I llama_new_context_with_model: n_ctx         = 128
0.00.053.475 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.475 I llama_new_context_with_model: n_batch       = 128
0.00.053.475 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.475 I llama_new_context_with_model: flash_attn    = 0
0.00.053.475 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.476 I llama_new_context_with_model: freq_scale    = 1
0.00.053.476 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.476 I ggml_metal_init: allocating
0.00.053.480 I ggml_metal_init: found device: Apple M4
0.00.053.482 I ggml_metal_init: picking default device: Apple M4
0.00.054.043 I ggml_metal_init: using embedded metal library
0.00.056.397 I ggml_metal_init: GPU name:   Apple M4
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.399 I ggml_metal_init: simdgroup reduction   = true
0.00.056.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.400 I ggml_metal_init: has bfloat            = true
0.00.056.400 I ggml_metal_init: use bfloat            = true
0.00.056.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.399 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.401 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.415 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.346 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.346 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.347 I llama_new_context_with_model: graph nodes  = 967
0.00.068.347 I llama_new_context_with_model: graph splits = 2
0.00.068.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.893 I 
0.00.572.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.937 I perplexity: tokenizing the input ..
0.00.580.964 I perplexity: tokenization took 8.026 ms
0.00.580.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.397 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.565 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.581 I llama_perf_context_print:        load time =     563.75 ms
0.00.716.582 I llama_perf_context_print: prompt eval time =     134.19 ms /   128 tokens (    1.05 ms per token,   953.85 tokens per second)
0.00.716.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.583 I llama_perf_context_print:       total time =     143.69 ms /   129 tokens
0.00.717.093 I ggml_metal_free: deallocating

real	0m0.733s
user	0m0.079s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.392 I llama_model_loader: - type  f32:  194 tensors
0.00.023.392 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.392 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.326 I llm_load_vocab: special tokens cache size = 25
0.00.049.064 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.078 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.080 I llm_load_print_meta: arch             = gptneox
0.00.049.080 I llm_load_print_meta: vocab type       = BPE
0.00.049.080 I llm_load_print_meta: n_vocab          = 50304
0.00.049.080 I llm_load_print_meta: n_merges         = 50009
0.00.049.081 I llm_load_print_meta: vocab_only       = 0
0.00.049.081 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.081 I llm_load_print_meta: n_embd           = 2048
0.00.049.081 I llm_load_print_meta: n_layer          = 24
0.00.049.084 I llm_load_print_meta: n_head           = 16
0.00.049.085 I llm_load_print_meta: n_head_kv        = 16
0.00.049.085 I llm_load_print_meta: n_rot            = 32
0.00.049.085 I llm_load_print_meta: n_swa            = 0
0.00.049.085 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.086 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.086 I llm_load_print_meta: n_gqa            = 1
0.00.049.087 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.088 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.088 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.088 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.088 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.089 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.089 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.090 I llm_load_print_meta: n_ff             = 8192
0.00.049.090 I llm_load_print_meta: n_expert         = 0
0.00.049.090 I llm_load_print_meta: n_expert_used    = 0
0.00.049.091 I llm_load_print_meta: causal attn      = 1
0.00.049.091 I llm_load_print_meta: pooling type     = 0
0.00.049.091 I llm_load_print_meta: rope type        = 2
0.00.049.091 I llm_load_print_meta: rope scaling     = linear
0.00.049.091 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.091 I llm_load_print_meta: freq_scale_train = 1
0.00.049.092 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.092 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.092 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.092 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.092 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.093 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.094 I llm_load_print_meta: model type       = 1.4B
0.00.049.094 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.095 I llm_load_print_meta: model params     = 1.41 B
0.00.049.095 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.095 I llm_load_print_meta: general.name     = 1.4B
0.00.049.095 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.096 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.096 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.096 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.096 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.096 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.097 I llm_load_print_meta: max token length = 1024
0.00.051.050 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.050 I llm_load_tensors: offloading output layer to GPU
0.00.051.050 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.061 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.062 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.926 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.927 I llama_new_context_with_model: n_ctx         = 128
0.00.051.928 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.928 I llama_new_context_with_model: n_batch       = 128
0.00.051.928 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.928 I llama_new_context_with_model: flash_attn    = 0
0.00.051.928 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.929 I llama_new_context_with_model: freq_scale    = 1
0.00.051.929 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.930 I ggml_metal_init: allocating
0.00.051.933 I ggml_metal_init: found device: Apple M4
0.00.051.935 I ggml_metal_init: picking default device: Apple M4
0.00.052.502 I ggml_metal_init: using embedded metal library
0.00.054.766 I ggml_metal_init: GPU name:   Apple M4
0.00.054.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.768 I ggml_metal_init: simdgroup reduction   = true
0.00.054.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.769 I ggml_metal_init: has bfloat            = true
0.00.054.769 I ggml_metal_init: use bfloat            = true
0.00.054.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.577 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.510 I llama_new_context_with_model: graph nodes  = 967
0.00.066.510 I llama_new_context_with_model: graph splits = 2
0.00.066.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.069 I 
0.00.639.125 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.139 I perplexity: tokenizing the input ..
0.00.647.232 I perplexity: tokenization took 8.092 ms
0.00.647.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.314 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.789.560 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.789.578 I llama_perf_context_print:        load time =     630.32 ms
0.00.789.579 I llama_perf_context_print: prompt eval time =     140.84 ms /   128 tokens (    1.10 ms per token,   908.83 tokens per second)
0.00.789.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.580 I llama_perf_context_print:       total time =     150.51 ms /   129 tokens
0.00.790.070 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.455 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.101 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.101 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.806 I llama_model_loader: - type  f32:  194 tensors
0.00.023.806 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.732 I llm_load_vocab: special tokens cache size = 25
0.00.049.571 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.585 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.586 I llm_load_print_meta: arch             = gptneox
0.00.049.587 I llm_load_print_meta: vocab type       = BPE
0.00.049.587 I llm_load_print_meta: n_vocab          = 50304
0.00.049.587 I llm_load_print_meta: n_merges         = 50009
0.00.049.587 I llm_load_print_meta: vocab_only       = 0
0.00.049.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.587 I llm_load_print_meta: n_embd           = 2048
0.00.049.588 I llm_load_print_meta: n_layer          = 24
0.00.049.590 I llm_load_print_meta: n_head           = 16
0.00.049.591 I llm_load_print_meta: n_head_kv        = 16
0.00.049.591 I llm_load_print_meta: n_rot            = 32
0.00.049.591 I llm_load_print_meta: n_swa            = 0
0.00.049.592 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.592 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.592 I llm_load_print_meta: n_gqa            = 1
0.00.049.593 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.594 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.594 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.595 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.596 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.597 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.597 I llm_load_print_meta: n_ff             = 8192
0.00.049.599 I llm_load_print_meta: n_expert         = 0
0.00.049.599 I llm_load_print_meta: n_expert_used    = 0
0.00.049.599 I llm_load_print_meta: causal attn      = 1
0.00.049.599 I llm_load_print_meta: pooling type     = 0
0.00.049.599 I llm_load_print_meta: rope type        = 2
0.00.049.599 I llm_load_print_meta: rope scaling     = linear
0.00.049.599 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.600 I llm_load_print_meta: freq_scale_train = 1
0.00.049.600 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.600 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.600 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.601 I llm_load_print_meta: model type       = 1.4B
0.00.049.602 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.602 I llm_load_print_meta: model params     = 1.41 B
0.00.049.602 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.602 I llm_load_print_meta: general.name     = 1.4B
0.00.049.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.604 I llm_load_print_meta: max token length = 1024
0.00.051.643 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.644 I llm_load_tensors: offloading output layer to GPU
0.00.051.644 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.654 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.655 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.562 I llama_new_context_with_model: n_ctx         = 128
0.00.052.562 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.562 I llama_new_context_with_model: n_batch       = 128
0.00.052.563 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.563 I llama_new_context_with_model: flash_attn    = 0
0.00.052.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.563 I llama_new_context_with_model: freq_scale    = 1
0.00.052.564 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.564 I ggml_metal_init: allocating
0.00.052.567 I ggml_metal_init: found device: Apple M4
0.00.052.569 I ggml_metal_init: picking default device: Apple M4
0.00.053.132 I ggml_metal_init: using embedded metal library
0.00.055.463 I ggml_metal_init: GPU name:   Apple M4
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.466 I ggml_metal_init: simdgroup reduction   = true
0.00.055.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.466 I ggml_metal_init: has bfloat            = true
0.00.055.466 I ggml_metal_init: use bfloat            = true
0.00.055.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.284 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.309 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.248 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.249 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.249 I llama_new_context_with_model: graph nodes  = 967
0.00.067.249 I llama_new_context_with_model: graph splits = 2
0.00.067.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.116.340 I 
0.00.116.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.116.409 I perplexity: tokenizing the input ..
0.00.124.019 I perplexity: tokenization took 7.607 ms
0.00.124.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.262.401 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.263.583 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.263.595 I llama_perf_context_print:        load time =     106.88 ms
0.00.263.595 I llama_perf_context_print: prompt eval time =     138.14 ms /   128 tokens (    1.08 ms per token,   926.62 tokens per second)
0.00.263.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.263.596 I llama_perf_context_print:       total time =     147.26 ms /   129 tokens
0.00.264.129 I ggml_metal_free: deallocating

real	0m0.281s
user	0m0.077s
sys	0m0.038s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.225 I build: 4333 (a0974156) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.511 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.523 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.079 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.612 I llama_model_loader: - type  f32:  194 tensors
0.00.052.612 I llama_model_loader: - type  f16:   98 tensors
0.00.080.355 I llm_load_vocab: special tokens cache size = 25
0.00.086.782 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.795 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.796 I llm_load_print_meta: arch             = gptneox
0.00.086.796 I llm_load_print_meta: vocab type       = BPE
0.00.086.797 I llm_load_print_meta: n_vocab          = 50304
0.00.086.797 I llm_load_print_meta: n_merges         = 50009
0.00.086.797 I llm_load_print_meta: vocab_only       = 0
0.00.086.797 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.797 I llm_load_print_meta: n_embd           = 2048
0.00.086.797 I llm_load_print_meta: n_layer          = 24
0.00.086.800 I llm_load_print_meta: n_head           = 16
0.00.086.801 I llm_load_print_meta: n_head_kv        = 16
0.00.086.802 I llm_load_print_meta: n_rot            = 32
0.00.086.802 I llm_load_print_meta: n_swa            = 0
0.00.086.803 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.803 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.804 I llm_load_print_meta: n_gqa            = 1
0.00.086.805 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.805 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.806 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.806 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.806 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.806 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.806 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.807 I llm_load_print_meta: n_ff             = 8192
0.00.086.807 I llm_load_print_meta: n_expert         = 0
0.00.086.807 I llm_load_print_meta: n_expert_used    = 0
0.00.086.807 I llm_load_print_meta: causal attn      = 1
0.00.086.807 I llm_load_print_meta: pooling type     = 0
0.00.086.808 I llm_load_print_meta: rope type        = 2
0.00.086.808 I llm_load_print_meta: rope scaling     = linear
0.00.086.809 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.810 I llm_load_print_meta: freq_scale_train = 1
0.00.086.810 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.810 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.810 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.810 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.810 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.812 I llm_load_print_meta: model type       = 1.4B
0.00.086.813 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.813 I llm_load_print_meta: model params     = 1.41 B
0.00.086.814 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.814 I llm_load_print_meta: general.name     = 1.4B
0.00.086.814 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.814 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.814 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.815 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.815 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.815 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.815 I llm_load_print_meta: max token length = 1024
0.00.088.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.493 I llm_load_tensors: offloading output layer to GPU
0.00.088.493 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.503 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.504 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.327 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.328 I llama_new_context_with_model: n_ctx         = 128
0.00.089.328 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.328 I llama_new_context_with_model: n_batch       = 128
0.00.089.328 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.328 I llama_new_context_with_model: flash_attn    = 0
0.00.089.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.329 I llama_new_context_with_model: freq_scale    = 1
0.00.089.329 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.330 I ggml_metal_init: allocating
0.00.089.333 I ggml_metal_init: found device: Apple M4
0.00.089.335 I ggml_metal_init: picking default device: Apple M4
0.00.089.916 I ggml_metal_init: using embedded metal library
0.00.092.378 I ggml_metal_init: GPU name:   Apple M4
0.00.092.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.381 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.381 I ggml_metal_init: simdgroup reduction   = true
0.00.092.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.381 I ggml_metal_init: has bfloat            = true
0.00.092.381 I ggml_metal_init: use bfloat            = true
0.00.092.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.861 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.865 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.879 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.751 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.752 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.753 I llama_new_context_with_model: graph nodes  = 967
0.00.103.753 I llama_new_context_with_model: graph splits = 2
0.00.103.765 I 
0.00.103.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.797 I compute_imatrix: tokenizing the input ..
0.00.110.725 I compute_imatrix: tokenization took 6.927 ms
0.00.110.727 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.594.676 I compute_imatrix: 1.48 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.597.468 I llama_perf_context_print:        load time =    1571.25 ms
0.01.597.470 I llama_perf_context_print: prompt eval time =    1483.41 ms /   128 tokens (   11.59 ms per token,    86.29 tokens per second)
0.01.597.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.597.473 I llama_perf_context_print:       total time =    1574.03 ms /   129 tokens
0.01.598.129 I ggml_metal_free: deallocating

real	0m1.779s
user	0m0.164s
sys	0m0.238s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4333 (a0974156)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d70a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d70a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d70aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d70b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d70ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d70bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d70c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d70d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d70d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d70daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d70dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d70eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d70f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d70fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d7101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d7168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d7176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d7182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d7199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d71a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d71a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d71abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d71b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d71bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d71c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d71c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d71cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d71d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d71df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d71ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d71f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d7208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d7216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d7240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d7260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d7270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d7290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d7295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d72a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d72ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d72b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d72bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d72bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d72c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d72cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d72d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d72d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d72dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d72e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d72e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d72ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d72f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d72f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d72fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d7310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d7335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d7343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d7351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d7368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d7376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d73a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d73a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d73a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d73ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d73b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d73b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d73bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d73c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d73c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d73ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d73d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d73d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d73dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d73e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d73ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d73eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d73f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d73f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d73fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d7413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d7421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d7438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d7446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d7454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d7479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d7483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d7488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d74a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d74a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d74b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d74b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d74b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d74bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d74c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d74cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d74d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d74d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d74d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d74e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d74e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d74ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d74fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d7570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d7580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d7590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d75a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d75a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d75ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d75b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d75bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d75c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d75c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d75cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d75d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d75d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d75db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d75e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d75e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d75eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d75f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d75f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d75fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d7605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d7618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d7626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d7655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d7663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d7685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.156.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d604b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d604fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d605450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d6058c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d605d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d6061a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d606610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d606a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d606ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d607360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d6077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d607ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d6089e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d609190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d6099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d60a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d60a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d60af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d60b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d60bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d60c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d60cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d60d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d60d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d60e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d60e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d60e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d60eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d60ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d60f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d60f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d60fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d6101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d610490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d610900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d6111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d611650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d611ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d611f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d6123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d612810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d612c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d6130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d613560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d6139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d6142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d614b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d615000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d615470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d6158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d615d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d6161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d616630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d616ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d6170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d617510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d617df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d618260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d6186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d618b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d618fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d619420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d619890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d619d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d61a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d61a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d61aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d61aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d61b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d61b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d61bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d61c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d61c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d61c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d61cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d61d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d61d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d61db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d61df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d61e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d61e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d61ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d61f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d61f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d61fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d61fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d620310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d620780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d620bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d621060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d6214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d621940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d621db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d622220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d622690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d622b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d622f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d6233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d624130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d6245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d624e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d6252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d625760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d625bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d626040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d6264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d626920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d626d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d627200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d627670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d627ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d627f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d6283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d628830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d628ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d629580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d6299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d62a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d62a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d62abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d62b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d62b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d62b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d62bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d62c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d62c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d62cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d62cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d62d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d62d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d62dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d62e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d62e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d62e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d62ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d62f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d62f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d62fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d630000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d630470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d6308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d6311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d631630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d631aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d631f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d632380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d6327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d632c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d6330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d633540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d6339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d633e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d634290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d634700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d634b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d634fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d635450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d6358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d635d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d6361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d636610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d636ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d6377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d637c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d6380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d638990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d638e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d6396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d639b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d639fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d63a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d63a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d63ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d63b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d63b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d63ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d63bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d63c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d63c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d63cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d63d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d63d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d63dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d63e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d63e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d63eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d63efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d63f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d63f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d63fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d640160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d6405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d640b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d640fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d641440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d641f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d642250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d642510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d642980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d643260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d6436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d643b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d643fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d644420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d644d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d645170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d6455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d645a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d646330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d6467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d646c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d647080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d6474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d647960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d647dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d648240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d6486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d648b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d648f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d649870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d64a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d64a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d64aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d64aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d64b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d64b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d64bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d64c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d64c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d64c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d64cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d64d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d64d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d64db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d64df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d64e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d64e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d64ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d64f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d64f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d64fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d64fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d6502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d650bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d651040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d6514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d651920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d651d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d652200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d652670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d652ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d652f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d6533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d653830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d653ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d654580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d6549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d6552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d655740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d655bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d656d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d657460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d657e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d6582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d6588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d658ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d6053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d6069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d6072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d607740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d607d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d608610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d608d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d609570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d609c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d60a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d60aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d60b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d60bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d60c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d60c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d60cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d60d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d60dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d60e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d60e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d60eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d60ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d60f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d60f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d60fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d6100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d6103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d610810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d6110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d611560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d6119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d611e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d6122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d612720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d612b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d613470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d6138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d613d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d6141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d614aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d614f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d6157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d615c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d6160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d6169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d616e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d617700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d617b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d6188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d6191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d619610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d61a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d61a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d61ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d61b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d61b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d61b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d61be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d61c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d61c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d61cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d61cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d61d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d61d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d61dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d61e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d61e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d61ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d61eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d61f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d61f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d61fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d620090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d620970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d621250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d621b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d621fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d622410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d623160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d6235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d623a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d623eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d624320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d6254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d625dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d626230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d6266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d626b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d6273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d628140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d6285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d628a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d629be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d62a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d62a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d62a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d62ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d62b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d62b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d62baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d62bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d62c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d62c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d62ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d62d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d62d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d62da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d62de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d62e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d62ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d62f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d62f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d62f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d62fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d6301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d630f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d6313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d631820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d631c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d632100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d6332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d633730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d634010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d634d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d6351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d635640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d635ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d635f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d636c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d6370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d637550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d6379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d6382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d638710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d638b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d639460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d6398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d63a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d63a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d63aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d63af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d63b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d63b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d63bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d63c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d63c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d63c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d63ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d63d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d63d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d63db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d63dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d63e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d63e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d63ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d63f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d63f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d63fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d63fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d640350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d6407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d640c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d6410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d641820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d642570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d6429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d642e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d6432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d643ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d644010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d6448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d6451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d645ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d645f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d646390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d6470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d6479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d6482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d648b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d6498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d649d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d64a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d64a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d64aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d64af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d64b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d64b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d64bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d64c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d64c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d64c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d64ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d64d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d64d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d64db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d64dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d64e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d64e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d64ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d64f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d64f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d64fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d64fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d6507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d650c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d6510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d651510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d651980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d651df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d652260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d6526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d652b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d652fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d653420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d653890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d653d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d654170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d6545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d654a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d654ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d655330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d6557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d656000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d6566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d656de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d6574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d657940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d657db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d658220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d658690 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.789s
user	0m0.297s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4333 (a0974156)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d60e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d60ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d60f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d60f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d60fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d610440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d6109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d610fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d611550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d611a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d611f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d612450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d613720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d613f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d614650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d614d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d615490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d615bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d616380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d616aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d6171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d6178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d618180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d6188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d618b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d619170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d619de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d61a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d61a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d61aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d61ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d61b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d61bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d61c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d61c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d61cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d61d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d61d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d61d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d61e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d61e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d61ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d61f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d61f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d61ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d620b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d6211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d6217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d6223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d622bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d623060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d623500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d6237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d623dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d6245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d624880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d6251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d625660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d625b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d625fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d626440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d6268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d626d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d627220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d6276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d628000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d628550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d628aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d628ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d629540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d629a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d629fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d62a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d62aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d62afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d62b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d62ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d62bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d62c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d62ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d62cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d62d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d62da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d62dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d62e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d62ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d62ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d62f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d62fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d62ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d61fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d6303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d630ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d631640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d631b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d6320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d632630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d632b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d6330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d633620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d633b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d6340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d634610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d6350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d635550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d6359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d635e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d636330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d6367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d636c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d637110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d6375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d637a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d637ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d638390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d638830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d638cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d639170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d639610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d639ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d639f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d63a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d63a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d63ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d63b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d63b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d63bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d63bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d63c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d63c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d63cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d63d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d63d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d63db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d63e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d63e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d63e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d63edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d63f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d63f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d63fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d640070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d640510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d6409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d640e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d6412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d641790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d641c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d6420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d642570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d642a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d642eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d643350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d6437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d643c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d644130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d6445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d644a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d644f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d6453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d645850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d645cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d646190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d646630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d646ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d646f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d647410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d6478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d6481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d648690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d648fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d649470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d649910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d649db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d64a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d64a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d64ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d64b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d64b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d64b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d64be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d64c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d64c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d64cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d64d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d64d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d64dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d64e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d64e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d64ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d64f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d64f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d64fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d650240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d650850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d651040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d6514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d651980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d651e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d6525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d652b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d653070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d6535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d653b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d654060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d6545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d654b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d655050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d6555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d655af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d656040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d656ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d658020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d658570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d658ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d659010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d659560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d65a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d65a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d65aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d65aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d65b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d65ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d65bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d65c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d65ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d65cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d65d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d65da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d65dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d65e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d65ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d65efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d65f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d65fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d65ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d6604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d660a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d6614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d661a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d661f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d6624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d662a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d662f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d6634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d663a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d663f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d6644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d664a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d664f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d6653f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d665890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d665d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d6661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d666670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d666b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d666fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d667450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d6678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d667d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d668230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d6686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d668b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d669010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d6694b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d669a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d66a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d66a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d66af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d66b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d66b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d66c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d66c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d66ca00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.286 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d707790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d707c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d708070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d7084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d708950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d708dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d709230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d7096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d709b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d709f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d70a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d70ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d70b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d70bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d70c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d70cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d70d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d70db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d70e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d70e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d70f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d70f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d70fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d710610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d710d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d7112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d711720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d711b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d712000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d712470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d7129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d712e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d7130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d713540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d7139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d713e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d714290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d714b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d714fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d715450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d7158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d715d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d7161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d716610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d716a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d717360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d7177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d717c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d7180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d718520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d718990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d718e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d719270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d7197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d719ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d71a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d71a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d71aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d71aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d71b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d71b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d71bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d71c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d71c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d71c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d71cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d71d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d71db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d71df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d71e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d71e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d71ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d71f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d71f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d71fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d71fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d7202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d720760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d720bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d721040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d7214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d721920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d721d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d722200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d722670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d722ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d722f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d7233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d723830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d723ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d724110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d724580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d7249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d7252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d725740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d725bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d726020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d726490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d726900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d726d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d7271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d727650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d727ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d727f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d7283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d728810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d728c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d7290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d729560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d7299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d729e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d72a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d72a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d72ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d72b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d72b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d72b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d72bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d72c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d72c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d72caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d72cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d72d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d72d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d72dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d72e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d72e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d72e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d72ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d72f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d72f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d72fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d72ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d730450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d7308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d730d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d7311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d731610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d731a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d731ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d732360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d7327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d732c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d7330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d733520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d733990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d733e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d734270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d7346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d734b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d734fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d7358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d735d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d736180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d7365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d736a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d736ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d737340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d7377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d737c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d738090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d738500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d738de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d739250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d7396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d739b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d739fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d73a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d73a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d73acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d73b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d73b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d73ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d73beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d73c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d73c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d73cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d73d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d73d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d73d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d73ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d73e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d73e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d73eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d73ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d73f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d73f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d73fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d740140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d7405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d740a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d740e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d741300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d741770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d741be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d742050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d7424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d742930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d742da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d743210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d7437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d743c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d744080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d744bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d744e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d745150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d7455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d745a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d746780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d746bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d747060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d7474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d748220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d748690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d748f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d7493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d749850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d749cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d74a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d74a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d74aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d74ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d74b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d74b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d74bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d74c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d74c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d74c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d74cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d74d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d74d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d74dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d74df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d74e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d74eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d74f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d74f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d74f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d74fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d7502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d750740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d750bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d751020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d751900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d751d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d7521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d752ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d752f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d7533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d753810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d7540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d754560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d7549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d754e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d7552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d755720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d755b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d756000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d7568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d756d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d7571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d757aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d757f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d758380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d7587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d759260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d759980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d75a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d75a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d75aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d75aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d75b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d75bb00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d707790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d707c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d708070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d7084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d708950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d708dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d709230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d7096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d709b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d709f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d70a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d70a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d70b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d70ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d70c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d70c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d70d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d70d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d70dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d70e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d70ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d70f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d70fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d710320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d710a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d710e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d7112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d711760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d711bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d712040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d7124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d712920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d712d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d713050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d7134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d713930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d713da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d714210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d714680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d714af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d714f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d7153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d715840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d715cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d716120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d716590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d716a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d716e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d7172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d717750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d717bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d7184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d718910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d718d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d7191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d719660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d719ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d719f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d71a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d71a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d71ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d71b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d71b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d71b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d71be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d71c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d71c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d71cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d71d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d71d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d71d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d71dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d71e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d71e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d71eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d71ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d71f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d71f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d71fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d7200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d720550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d7209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d720e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d7212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d721710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d721b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d721ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d722460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d7228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d722d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d7231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d723620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d723a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d723f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d724370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d7247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d7250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d7259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d725e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d726280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d7266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d726b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d726fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d727440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d7278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d728190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d728a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d728ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d729350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d7297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d729c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d72a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d72a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d72a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d72adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d72b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d72b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d72bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d72bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d72c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d72c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d72cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d72d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d72d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d72da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d72dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d72e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d72e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d72ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d72f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d72f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d72f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d72fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d730240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d7306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d730b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d730f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d731400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d731870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d731ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d7325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d732ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d733310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d733780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d733bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d734060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d7344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d734940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d734db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d735220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d735b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d735f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d7363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d736850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d736cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d737130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d7375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d737a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d7382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d738760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d738bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d739040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d7394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d739920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d739d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d73a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d73a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d73aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d73af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d73b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d73b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d73bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d73c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d73c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d73ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d73d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d73d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d73dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d73e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d73e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d73e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d73ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d73f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d73f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d73fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d73ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d7403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d7410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d741560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d7419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d741e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d7422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d742720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d742b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d743000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d743470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d7438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d743d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d7444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d744940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d744db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d745220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d745690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d745b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d745f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d7463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d746850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d746cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d747130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d7475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d747a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d7482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d748760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d748bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d749040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d7494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d749920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d749d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d74a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d74a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d74aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d74af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d74b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d74b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d74bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d74c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d74c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d74c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d74ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d74d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d74d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d74dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d74e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d74e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d74e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d74ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d74f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d74f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d74fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d74ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d7503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d750810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d750c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d7510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d751560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d7519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d751e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d7522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d752720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d752b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d753000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d7538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d7541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d754630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d754aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d754f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d7557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d755c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d7560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d756540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d7569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d756e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d757290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d757700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d757b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d757fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d758cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d7593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d759a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d75a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d75a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d75aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d75aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d75b340 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.930s
user	0m0.243s
sys	0m0.143s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
