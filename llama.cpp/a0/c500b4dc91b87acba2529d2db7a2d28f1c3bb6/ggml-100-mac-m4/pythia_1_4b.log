Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.580s
user	0m0.913s
sys	0m1.245s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-gguf
[ 64%] Built target test-chat-template
[ 64%] Built target test-backend-ops
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-barrier
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Built target test-rope
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup
[ 83%] Built target llama-parallel
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Generating index.html.gz.hpp
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-save-load-state
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tts
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-run
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.112s
user	0m6.191s
sys	0m9.625s

main: quantize time =  4879.82 ms
main:    total time =  4879.82 ms

main: quantize time =  2026.05 ms
main:    total time =  2026.05 ms

main: quantize time =  2232.01 ms
main:    total time =  2232.01 ms

main: quantize time =  2491.51 ms
main:    total time =  2491.51 ms

main: quantize time =  2914.03 ms
main:    total time =  2914.03 ms

main: quantize time =  5505.25 ms
main:    total time =  5505.25 ms

main: quantize time =  6172.37 ms
main:    total time =  6172.37 ms

main: quantize time =  6865.56 ms
main:    total time =  6865.56 ms

main: quantize time =  5823.23 ms
main:    total time =  5823.23 ms

main: quantize time =  4472.73 ms
main:    total time =  4472.73 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.315 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.030.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.326 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.360 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.863 I llama_model_loader: - type  f32:  194 tensors
0.00.061.863 I llama_model_loader: - type  f16:   98 tensors
0.00.061.865 I print_info: file format = GGUF V3 (latest)
0.00.061.866 I print_info: file type   = all F32 (guessed)
0.00.061.868 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.650 I load: special tokens cache size = 25
0.00.101.138 I load: token to piece cache size = 0.2984 MB
0.00.101.141 I print_info: arch             = gptneox
0.00.101.141 I print_info: vocab_only       = 0
0.00.101.141 I print_info: n_ctx_train      = 2048
0.00.101.142 I print_info: n_embd           = 2048
0.00.101.142 I print_info: n_layer          = 24
0.00.101.145 I print_info: n_head           = 16
0.00.101.146 I print_info: n_head_kv        = 16
0.00.101.146 I print_info: n_rot            = 32
0.00.101.146 I print_info: n_swa            = 0
0.00.101.146 I print_info: n_embd_head_k    = 128
0.00.101.146 I print_info: n_embd_head_v    = 128
0.00.101.147 I print_info: n_gqa            = 1
0.00.101.148 I print_info: n_embd_k_gqa     = 2048
0.00.101.148 I print_info: n_embd_v_gqa     = 2048
0.00.101.149 I print_info: f_norm_eps       = 1.0e-05
0.00.101.149 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.150 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.150 I print_info: f_logit_scale    = 0.0e+00
0.00.101.150 I print_info: n_ff             = 8192
0.00.101.151 I print_info: n_expert         = 0
0.00.101.151 I print_info: n_expert_used    = 0
0.00.101.151 I print_info: causal attn      = 1
0.00.101.151 I print_info: pooling type     = 0
0.00.101.151 I print_info: rope type        = 2
0.00.101.151 I print_info: rope scaling     = linear
0.00.101.152 I print_info: freq_base_train  = 10000.0
0.00.101.152 I print_info: freq_scale_train = 1
0.00.101.152 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.152 I print_info: rope_finetuned   = unknown
0.00.101.152 I print_info: ssm_d_conv       = 0
0.00.101.155 I print_info: ssm_d_inner      = 0
0.00.101.155 I print_info: ssm_d_state      = 0
0.00.101.155 I print_info: ssm_dt_rank      = 0
0.00.101.155 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.156 I print_info: model type       = 1.4B
0.00.101.156 I print_info: model params     = 1.41 B
0.00.101.156 I print_info: general.name     = 1.4B
0.00.101.157 I print_info: vocab type       = BPE
0.00.101.157 I print_info: n_vocab          = 50304
0.00.101.157 I print_info: n_merges         = 50009
0.00.101.157 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.163 I print_info: LF token         = 128 'Ä'
0.00.101.164 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.165 I print_info: max token length = 1024
0.00.137.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.137.802 I load_tensors: offloading output layer to GPU
0.00.137.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.137.825 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.137.826 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.138.127 I llama_context: n_seq_max     = 1
0.00.138.128 I llama_context: n_ctx         = 2048
0.00.138.128 I llama_context: n_ctx_per_seq = 2048
0.00.138.128 I llama_context: n_batch       = 2048
0.00.138.128 I llama_context: n_ubatch      = 512
0.00.138.129 I llama_context: flash_attn    = 0
0.00.138.129 I llama_context: freq_base     = 10000.0
0.00.138.129 I llama_context: freq_scale    = 1
0.00.138.130 I ggml_metal_init: allocating
0.00.138.148 I ggml_metal_init: found device: Apple M4
0.00.138.150 I ggml_metal_init: picking default device: Apple M4
0.00.138.711 I ggml_metal_init: using embedded metal library
0.00.147.392 I ggml_metal_init: GPU name:   Apple M4
0.00.147.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.147.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.147.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.147.395 I ggml_metal_init: simdgroup reduction   = true
0.00.147.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.147.396 I ggml_metal_init: has residency sets    = true
0.00.147.396 I ggml_metal_init: has bfloat            = true
0.00.147.396 I ggml_metal_init: use bfloat            = true
0.00.147.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.147.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.171.260 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.199.443 I init:      Metal KV buffer size =   384.00 MiB
0.00.199.451 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.199.496 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.203.273 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.203.276 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.203.277 I llama_context: graph nodes  = 967
0.00.203.277 I llama_context: graph splits = 2
0.00.203.282 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.203.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.203.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.269.628 I main: llama threadpool init, n_threads = 4
0.00.269.670 I 
0.00.269.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.269.708 I 
0.00.269.770 I sampler seed: 1234
0.00.269.774 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.269.798 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.269.800 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.269.800 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.101.210 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.02.101.210 I llama_perf_context_print:        load time =     237.74 ms
0.02.101.211 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.39 tokens per second)
0.02.101.212 I llama_perf_context_print:        eval time =    1784.85 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.101.212 I llama_perf_context_print:       total time =    1832.60 ms /    70 tokens
0.02.104.967 I ggml_metal_free: deallocating

real	0m2.396s
user	0m0.146s
sys	0m0.131s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.920 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.846 I llama_model_loader: - type  f32:  194 tensors
0.00.026.846 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.847 I print_info: file format = GGUF V3 (latest)
0.00.026.850 I print_info: file type   = Q8_0
0.00.026.851 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.467 I load: special tokens cache size = 25
0.00.052.678 I load: token to piece cache size = 0.2984 MB
0.00.052.683 I print_info: arch             = gptneox
0.00.052.683 I print_info: vocab_only       = 0
0.00.052.683 I print_info: n_ctx_train      = 2048
0.00.052.684 I print_info: n_embd           = 2048
0.00.052.686 I print_info: n_layer          = 24
0.00.052.692 I print_info: n_head           = 16
0.00.052.693 I print_info: n_head_kv        = 16
0.00.052.693 I print_info: n_rot            = 32
0.00.052.693 I print_info: n_swa            = 0
0.00.052.693 I print_info: n_embd_head_k    = 128
0.00.052.694 I print_info: n_embd_head_v    = 128
0.00.052.694 I print_info: n_gqa            = 1
0.00.052.695 I print_info: n_embd_k_gqa     = 2048
0.00.052.696 I print_info: n_embd_v_gqa     = 2048
0.00.052.696 I print_info: f_norm_eps       = 1.0e-05
0.00.052.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.697 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.697 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.697 I print_info: f_logit_scale    = 0.0e+00
0.00.052.698 I print_info: n_ff             = 8192
0.00.052.698 I print_info: n_expert         = 0
0.00.052.698 I print_info: n_expert_used    = 0
0.00.052.698 I print_info: causal attn      = 1
0.00.052.699 I print_info: pooling type     = 0
0.00.052.699 I print_info: rope type        = 2
0.00.052.699 I print_info: rope scaling     = linear
0.00.052.699 I print_info: freq_base_train  = 10000.0
0.00.052.700 I print_info: freq_scale_train = 1
0.00.052.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.700 I print_info: rope_finetuned   = unknown
0.00.052.700 I print_info: ssm_d_conv       = 0
0.00.052.700 I print_info: ssm_d_inner      = 0
0.00.052.700 I print_info: ssm_d_state      = 0
0.00.052.700 I print_info: ssm_dt_rank      = 0
0.00.052.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.701 I print_info: model type       = 1.4B
0.00.052.701 I print_info: model params     = 1.41 B
0.00.052.701 I print_info: general.name     = 1.4B
0.00.052.702 I print_info: vocab type       = BPE
0.00.052.702 I print_info: n_vocab          = 50304
0.00.052.703 I print_info: n_merges         = 50009
0.00.052.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.704 I print_info: LF token         = 128 'Ä'
0.00.052.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.704 I print_info: max token length = 1024
0.00.905.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.905.512 I load_tensors: offloading output layer to GPU
0.00.905.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.905.538 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.905.541 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.906.239 I llama_context: n_seq_max     = 1
0.00.906.240 I llama_context: n_ctx         = 2048
0.00.906.241 I llama_context: n_ctx_per_seq = 2048
0.00.906.241 I llama_context: n_batch       = 2048
0.00.906.241 I llama_context: n_ubatch      = 512
0.00.906.242 I llama_context: flash_attn    = 0
0.00.906.242 I llama_context: freq_base     = 10000.0
0.00.906.243 I llama_context: freq_scale    = 1
0.00.906.244 I ggml_metal_init: allocating
0.00.906.264 I ggml_metal_init: found device: Apple M4
0.00.906.269 I ggml_metal_init: picking default device: Apple M4
0.00.907.500 I ggml_metal_init: using embedded metal library
0.00.913.216 I ggml_metal_init: GPU name:   Apple M4
0.00.913.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.913.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.913.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.913.221 I ggml_metal_init: simdgroup reduction   = true
0.00.913.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.913.222 I ggml_metal_init: has residency sets    = true
0.00.913.222 I ggml_metal_init: has bfloat            = true
0.00.913.222 I ggml_metal_init: use bfloat            = true
0.00.913.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.913.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.929.509 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.982.850 I init:      Metal KV buffer size =   384.00 MiB
0.00.982.857 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.982.900 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.986.968 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.986.970 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.986.970 I llama_context: graph nodes  = 967
0.00.986.971 I llama_context: graph splits = 2
0.00.986.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.987.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.987.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.041.913 I main: llama threadpool init, n_threads = 4
0.01.041.956 I 
0.01.041.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.041.983 I 
0.01.042.210 I sampler seed: 1234
0.01.042.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.042.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.042.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.042.240 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.140.648 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.02.140.649 I llama_perf_context_print:        load time =    1031.09 ms
0.02.140.654 I llama_perf_context_print: prompt eval time =      44.12 ms /     7 tokens (    6.30 ms per token,   158.65 tokens per second)
0.02.140.655 I llama_perf_context_print:        eval time =    1051.74 ms /    63 runs   (   16.69 ms per token,    59.90 tokens per second)
0.02.140.656 I llama_perf_context_print:       total time =    1099.65 ms /    70 tokens
0.02.143.457 I ggml_metal_free: deallocating

real	0m2.159s
user	0m0.120s
sys	0m0.249s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.019.379 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.713 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.325 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.049.326 I llama_model_loader: - type  f32:  194 tensors
0.00.049.326 I llama_model_loader: - type q4_0:   97 tensors
0.00.049.326 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.327 I print_info: file format = GGUF V3 (latest)
0.00.049.328 I print_info: file type   = Q4_0
0.00.049.329 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.069.898 I load: special tokens cache size = 25
0.00.076.306 I load: token to piece cache size = 0.2984 MB
0.00.076.313 I print_info: arch             = gptneox
0.00.076.313 I print_info: vocab_only       = 0
0.00.076.314 I print_info: n_ctx_train      = 2048
0.00.076.314 I print_info: n_embd           = 2048
0.00.076.314 I print_info: n_layer          = 24
0.00.076.317 I print_info: n_head           = 16
0.00.076.318 I print_info: n_head_kv        = 16
0.00.076.318 I print_info: n_rot            = 32
0.00.076.319 I print_info: n_swa            = 0
0.00.076.320 I print_info: n_embd_head_k    = 128
0.00.076.320 I print_info: n_embd_head_v    = 128
0.00.076.322 I print_info: n_gqa            = 1
0.00.076.323 I print_info: n_embd_k_gqa     = 2048
0.00.076.323 I print_info: n_embd_v_gqa     = 2048
0.00.076.324 I print_info: f_norm_eps       = 1.0e-05
0.00.076.324 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.326 I print_info: f_logit_scale    = 0.0e+00
0.00.076.327 I print_info: n_ff             = 8192
0.00.076.327 I print_info: n_expert         = 0
0.00.076.327 I print_info: n_expert_used    = 0
0.00.076.327 I print_info: causal attn      = 1
0.00.076.327 I print_info: pooling type     = 0
0.00.076.327 I print_info: rope type        = 2
0.00.076.328 I print_info: rope scaling     = linear
0.00.076.328 I print_info: freq_base_train  = 10000.0
0.00.076.329 I print_info: freq_scale_train = 1
0.00.076.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.329 I print_info: rope_finetuned   = unknown
0.00.076.329 I print_info: ssm_d_conv       = 0
0.00.076.330 I print_info: ssm_d_inner      = 0
0.00.076.330 I print_info: ssm_d_state      = 0
0.00.076.330 I print_info: ssm_dt_rank      = 0
0.00.076.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.330 I print_info: model type       = 1.4B
0.00.076.331 I print_info: model params     = 1.41 B
0.00.076.331 I print_info: general.name     = 1.4B
0.00.076.331 I print_info: vocab type       = BPE
0.00.076.331 I print_info: n_vocab          = 50304
0.00.076.331 I print_info: n_merges         = 50009
0.00.076.332 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.332 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.332 I print_info: LF token         = 128 'Ä'
0.00.076.333 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.333 I print_info: max token length = 1024
0.00.757.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.757.907 I load_tensors: offloading output layer to GPU
0.00.757.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.757.942 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.757.943 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.758.983 I llama_context: n_seq_max     = 1
0.00.758.989 I llama_context: n_ctx         = 2048
0.00.758.989 I llama_context: n_ctx_per_seq = 2048
0.00.758.990 I llama_context: n_batch       = 2048
0.00.758.990 I llama_context: n_ubatch      = 512
0.00.758.990 I llama_context: flash_attn    = 0
0.00.758.992 I llama_context: freq_base     = 10000.0
0.00.758.993 I llama_context: freq_scale    = 1
0.00.759.003 I ggml_metal_init: allocating
0.00.759.093 I ggml_metal_init: found device: Apple M4
0.00.759.102 I ggml_metal_init: picking default device: Apple M4
0.00.760.911 I ggml_metal_init: using embedded metal library
0.00.766.302 I ggml_metal_init: GPU name:   Apple M4
0.00.766.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.766.311 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.766.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.766.313 I ggml_metal_init: simdgroup reduction   = true
0.00.766.313 I ggml_metal_init: simdgroup matrix mul. = true
0.00.766.313 I ggml_metal_init: has residency sets    = true
0.00.766.314 I ggml_metal_init: has bfloat            = true
0.00.766.314 I ggml_metal_init: use bfloat            = true
0.00.766.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.766.319 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.786.391 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.841.392 I init:      Metal KV buffer size =   384.00 MiB
0.00.841.399 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.841.434 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.846.074 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.846.077 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.846.077 I llama_context: graph nodes  = 967
0.00.846.078 I llama_context: graph splits = 2
0.00.846.084 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.846.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.846.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.388 I main: llama threadpool init, n_threads = 4
0.00.904.433 I 
0.00.904.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.460 I 
0.00.904.681 I sampler seed: 1234
0.00.904.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.904.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.904.697 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.904.698 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.593.908 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.593.908 I llama_perf_context_print:        load time =     884.12 ms
0.01.593.910 I llama_perf_context_print: prompt eval time =      49.06 ms /     7 tokens (    7.01 ms per token,   142.69 tokens per second)
0.01.593.911 I llama_perf_context_print:        eval time =     637.24 ms /    63 runs   (   10.11 ms per token,    98.86 tokens per second)
0.01.593.911 I llama_perf_context_print:       total time =     690.41 ms /    70 tokens
0.01.597.966 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.123s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.309 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.900 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.793 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.566 I llama_model_loader: - type  f32:  194 tensors
0.00.028.566 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.567 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.567 I print_info: file format = GGUF V3 (latest)
0.00.028.568 I print_info: file type   = Q4_1
0.00.028.569 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.663 I load: special tokens cache size = 25
0.00.053.821 I load: token to piece cache size = 0.2984 MB
0.00.053.825 I print_info: arch             = gptneox
0.00.053.825 I print_info: vocab_only       = 0
0.00.053.825 I print_info: n_ctx_train      = 2048
0.00.053.825 I print_info: n_embd           = 2048
0.00.053.826 I print_info: n_layer          = 24
0.00.053.828 I print_info: n_head           = 16
0.00.053.829 I print_info: n_head_kv        = 16
0.00.053.829 I print_info: n_rot            = 32
0.00.053.830 I print_info: n_swa            = 0
0.00.053.830 I print_info: n_embd_head_k    = 128
0.00.053.830 I print_info: n_embd_head_v    = 128
0.00.053.831 I print_info: n_gqa            = 1
0.00.053.832 I print_info: n_embd_k_gqa     = 2048
0.00.053.832 I print_info: n_embd_v_gqa     = 2048
0.00.053.833 I print_info: f_norm_eps       = 1.0e-05
0.00.053.833 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.833 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.833 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.834 I print_info: f_logit_scale    = 0.0e+00
0.00.053.834 I print_info: n_ff             = 8192
0.00.053.835 I print_info: n_expert         = 0
0.00.053.835 I print_info: n_expert_used    = 0
0.00.053.835 I print_info: causal attn      = 1
0.00.053.835 I print_info: pooling type     = 0
0.00.053.836 I print_info: rope type        = 2
0.00.053.838 I print_info: rope scaling     = linear
0.00.053.839 I print_info: freq_base_train  = 10000.0
0.00.053.839 I print_info: freq_scale_train = 1
0.00.053.839 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.839 I print_info: rope_finetuned   = unknown
0.00.053.840 I print_info: ssm_d_conv       = 0
0.00.053.842 I print_info: ssm_d_inner      = 0
0.00.053.842 I print_info: ssm_d_state      = 0
0.00.053.842 I print_info: ssm_dt_rank      = 0
0.00.053.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.843 I print_info: model type       = 1.4B
0.00.053.843 I print_info: model params     = 1.41 B
0.00.053.843 I print_info: general.name     = 1.4B
0.00.053.843 I print_info: vocab type       = BPE
0.00.053.844 I print_info: n_vocab          = 50304
0.00.053.844 I print_info: n_merges         = 50009
0.00.053.844 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.844 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.844 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.845 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.849 I print_info: LF token         = 128 'Ä'
0.00.053.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.850 I print_info: max token length = 1024
0.00.627.529 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.548 I load_tensors: offloading output layer to GPU
0.00.627.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.582 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.584 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.628.875 I llama_context: n_seq_max     = 1
0.00.628.881 I llama_context: n_ctx         = 2048
0.00.628.881 I llama_context: n_ctx_per_seq = 2048
0.00.628.882 I llama_context: n_batch       = 2048
0.00.628.882 I llama_context: n_ubatch      = 512
0.00.628.882 I llama_context: flash_attn    = 0
0.00.628.884 I llama_context: freq_base     = 10000.0
0.00.628.885 I llama_context: freq_scale    = 1
0.00.628.892 I ggml_metal_init: allocating
0.00.628.978 I ggml_metal_init: found device: Apple M4
0.00.628.988 I ggml_metal_init: picking default device: Apple M4
0.00.630.859 I ggml_metal_init: using embedded metal library
0.00.637.019 I ggml_metal_init: GPU name:   Apple M4
0.00.637.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.027 I ggml_metal_init: simdgroup reduction   = true
0.00.637.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.028 I ggml_metal_init: has residency sets    = true
0.00.637.028 I ggml_metal_init: has bfloat            = true
0.00.637.028 I ggml_metal_init: use bfloat            = true
0.00.637.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.437 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.784 I init:      Metal KV buffer size =   384.00 MiB
0.00.711.791 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.823 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.715.881 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.715.883 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.715.883 I llama_context: graph nodes  = 967
0.00.715.884 I llama_context: graph splits = 2
0.00.715.889 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.107 I main: llama threadpool init, n_threads = 4
0.00.771.150 I 
0.00.771.174 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.174 I 
0.00.771.391 I sampler seed: 1234
0.00.771.396 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.418 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.418 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.418 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.501.619 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.501.620 I llama_perf_context_print:        load time =     759.89 ms
0.01.501.622 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.36 tokens per second)
0.01.501.623 I llama_perf_context_print:        eval time =     680.30 ms /    63 runs   (   10.80 ms per token,    92.61 tokens per second)
0.01.501.623 I llama_perf_context_print:       total time =     731.42 ms /    70 tokens
0.01.505.314 I ggml_metal_free: deallocating

real	0m1.527s
user	0m0.121s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.664 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.198 I llama_model_loader: - type  f32:  194 tensors
0.00.025.199 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.200 I print_info: file format = GGUF V3 (latest)
0.00.025.200 I print_info: file type   = Q5_0
0.00.025.202 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.005 I load: special tokens cache size = 25
0.00.051.071 I load: token to piece cache size = 0.2984 MB
0.00.051.074 I print_info: arch             = gptneox
0.00.051.074 I print_info: vocab_only       = 0
0.00.051.075 I print_info: n_ctx_train      = 2048
0.00.051.075 I print_info: n_embd           = 2048
0.00.051.075 I print_info: n_layer          = 24
0.00.051.077 I print_info: n_head           = 16
0.00.051.078 I print_info: n_head_kv        = 16
0.00.051.079 I print_info: n_rot            = 32
0.00.051.079 I print_info: n_swa            = 0
0.00.051.079 I print_info: n_embd_head_k    = 128
0.00.051.079 I print_info: n_embd_head_v    = 128
0.00.051.080 I print_info: n_gqa            = 1
0.00.051.081 I print_info: n_embd_k_gqa     = 2048
0.00.051.081 I print_info: n_embd_v_gqa     = 2048
0.00.051.082 I print_info: f_norm_eps       = 1.0e-05
0.00.051.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.083 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.083 I print_info: f_logit_scale    = 0.0e+00
0.00.051.083 I print_info: n_ff             = 8192
0.00.051.084 I print_info: n_expert         = 0
0.00.051.084 I print_info: n_expert_used    = 0
0.00.051.084 I print_info: causal attn      = 1
0.00.051.084 I print_info: pooling type     = 0
0.00.051.084 I print_info: rope type        = 2
0.00.051.084 I print_info: rope scaling     = linear
0.00.051.085 I print_info: freq_base_train  = 10000.0
0.00.051.085 I print_info: freq_scale_train = 1
0.00.051.085 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.085 I print_info: rope_finetuned   = unknown
0.00.051.086 I print_info: ssm_d_conv       = 0
0.00.051.086 I print_info: ssm_d_inner      = 0
0.00.051.086 I print_info: ssm_d_state      = 0
0.00.051.086 I print_info: ssm_dt_rank      = 0
0.00.051.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.087 I print_info: model type       = 1.4B
0.00.051.087 I print_info: model params     = 1.41 B
0.00.051.087 I print_info: general.name     = 1.4B
0.00.051.088 I print_info: vocab type       = BPE
0.00.051.088 I print_info: n_vocab          = 50304
0.00.051.088 I print_info: n_merges         = 50009
0.00.051.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.090 I print_info: LF token         = 128 'Ä'
0.00.051.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.090 I print_info: max token length = 1024
0.00.662.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.055 I load_tensors: offloading output layer to GPU
0.00.662.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.090 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.662.091 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.663.412 I llama_context: n_seq_max     = 1
0.00.663.417 I llama_context: n_ctx         = 2048
0.00.663.418 I llama_context: n_ctx_per_seq = 2048
0.00.663.419 I llama_context: n_batch       = 2048
0.00.663.419 I llama_context: n_ubatch      = 512
0.00.663.419 I llama_context: flash_attn    = 0
0.00.663.421 I llama_context: freq_base     = 10000.0
0.00.663.422 I llama_context: freq_scale    = 1
0.00.663.424 I ggml_metal_init: allocating
0.00.663.508 I ggml_metal_init: found device: Apple M4
0.00.663.516 I ggml_metal_init: picking default device: Apple M4
0.00.665.143 I ggml_metal_init: using embedded metal library
0.00.671.597 I ggml_metal_init: GPU name:   Apple M4
0.00.671.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.603 I ggml_metal_init: simdgroup reduction   = true
0.00.671.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.604 I ggml_metal_init: has residency sets    = true
0.00.671.604 I ggml_metal_init: has bfloat            = true
0.00.671.604 I ggml_metal_init: use bfloat            = true
0.00.671.605 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.480 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.548 I init:      Metal KV buffer size =   384.00 MiB
0.00.743.555 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.634 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.748.163 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.748.165 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.748.165 I llama_context: graph nodes  = 967
0.00.748.165 I llama_context: graph splits = 2
0.00.748.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.869 I main: llama threadpool init, n_threads = 4
0.00.804.913 I 
0.00.804.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.942 I 
0.00.805.150 I sampler seed: 1234
0.00.805.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.207 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.207 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.590.633 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.590.634 I llama_perf_context_print:        load time =     795.23 ms
0.01.590.635 I llama_perf_context_print: prompt eval time =      43.10 ms /     7 tokens (    6.16 ms per token,   162.40 tokens per second)
0.01.590.636 I llama_perf_context_print:        eval time =     739.35 ms /    63 runs   (   11.74 ms per token,    85.21 tokens per second)
0.01.590.636 I llama_perf_context_print:       total time =     786.74 ms /    70 tokens
0.01.594.613 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.122s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.689 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.945 I llama_model_loader: - type  f32:  194 tensors
0.00.025.945 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.946 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.946 I print_info: file format = GGUF V3 (latest)
0.00.025.947 I print_info: file type   = Q5_1
0.00.025.947 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.161 I load: special tokens cache size = 25
0.00.051.165 I load: token to piece cache size = 0.2984 MB
0.00.051.168 I print_info: arch             = gptneox
0.00.051.168 I print_info: vocab_only       = 0
0.00.051.168 I print_info: n_ctx_train      = 2048
0.00.051.169 I print_info: n_embd           = 2048
0.00.051.169 I print_info: n_layer          = 24
0.00.051.172 I print_info: n_head           = 16
0.00.051.173 I print_info: n_head_kv        = 16
0.00.051.173 I print_info: n_rot            = 32
0.00.051.173 I print_info: n_swa            = 0
0.00.051.174 I print_info: n_embd_head_k    = 128
0.00.051.174 I print_info: n_embd_head_v    = 128
0.00.051.175 I print_info: n_gqa            = 1
0.00.051.176 I print_info: n_embd_k_gqa     = 2048
0.00.051.177 I print_info: n_embd_v_gqa     = 2048
0.00.051.177 I print_info: f_norm_eps       = 1.0e-05
0.00.051.178 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.178 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.178 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.178 I print_info: f_logit_scale    = 0.0e+00
0.00.051.179 I print_info: n_ff             = 8192
0.00.051.179 I print_info: n_expert         = 0
0.00.051.179 I print_info: n_expert_used    = 0
0.00.051.180 I print_info: causal attn      = 1
0.00.051.180 I print_info: pooling type     = 0
0.00.051.181 I print_info: rope type        = 2
0.00.051.183 I print_info: rope scaling     = linear
0.00.051.183 I print_info: freq_base_train  = 10000.0
0.00.051.183 I print_info: freq_scale_train = 1
0.00.051.184 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.184 I print_info: rope_finetuned   = unknown
0.00.051.184 I print_info: ssm_d_conv       = 0
0.00.051.184 I print_info: ssm_d_inner      = 0
0.00.051.184 I print_info: ssm_d_state      = 0
0.00.051.184 I print_info: ssm_dt_rank      = 0
0.00.051.184 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.185 I print_info: model type       = 1.4B
0.00.051.185 I print_info: model params     = 1.41 B
0.00.051.185 I print_info: general.name     = 1.4B
0.00.051.190 I print_info: vocab type       = BPE
0.00.051.190 I print_info: n_vocab          = 50304
0.00.051.190 I print_info: n_merges         = 50009
0.00.051.191 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.191 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.191 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.191 I print_info: LF token         = 128 'Ä'
0.00.051.192 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: max token length = 1024
0.00.683.128 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.144 I load_tensors: offloading output layer to GPU
0.00.683.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.187 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.683.189 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.684.509 I llama_context: n_seq_max     = 1
0.00.684.512 I llama_context: n_ctx         = 2048
0.00.684.513 I llama_context: n_ctx_per_seq = 2048
0.00.684.513 I llama_context: n_batch       = 2048
0.00.684.513 I llama_context: n_ubatch      = 512
0.00.684.514 I llama_context: flash_attn    = 0
0.00.684.515 I llama_context: freq_base     = 10000.0
0.00.684.515 I llama_context: freq_scale    = 1
0.00.684.520 I ggml_metal_init: allocating
0.00.684.544 I ggml_metal_init: found device: Apple M4
0.00.684.553 I ggml_metal_init: picking default device: Apple M4
0.00.686.017 I ggml_metal_init: using embedded metal library
0.00.692.136 I ggml_metal_init: GPU name:   Apple M4
0.00.692.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.142 I ggml_metal_init: simdgroup reduction   = true
0.00.692.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.143 I ggml_metal_init: has residency sets    = true
0.00.692.143 I ggml_metal_init: has bfloat            = true
0.00.692.143 I ggml_metal_init: use bfloat            = true
0.00.692.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.263 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.451 I init:      Metal KV buffer size =   384.00 MiB
0.00.763.458 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.763.537 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.767.928 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.767.930 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.767.930 I llama_context: graph nodes  = 967
0.00.767.930 I llama_context: graph splits = 2
0.00.767.937 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.768.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.768.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.095 I main: llama threadpool init, n_threads = 4
0.00.824.139 I 
0.00.824.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.165 I 
0.00.824.393 I sampler seed: 1234
0.00.824.398 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.824.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.824.423 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.824.423 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.662.078 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.662.079 I llama_perf_context_print:        load time =     813.52 ms
0.01.662.079 I llama_perf_context_print: prompt eval time =      42.21 ms /     7 tokens (    6.03 ms per token,   165.84 tokens per second)
0.01.662.080 I llama_perf_context_print:        eval time =     792.55 ms /    63 runs   (   12.58 ms per token,    79.49 tokens per second)
0.01.662.080 I llama_perf_context_print:       total time =     838.87 ms /    70 tokens
0.01.666.290 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.121s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.220 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.388 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.389 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.390 I llama_model_loader: - type  f32:  194 tensors
0.00.025.390 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.391 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.391 I print_info: file format = GGUF V3 (latest)
0.00.025.392 I print_info: file type   = Q2_K - Medium
0.00.025.393 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.373 I load: special tokens cache size = 25
0.00.050.297 I load: token to piece cache size = 0.2984 MB
0.00.050.300 I print_info: arch             = gptneox
0.00.050.301 I print_info: vocab_only       = 0
0.00.050.301 I print_info: n_ctx_train      = 2048
0.00.050.301 I print_info: n_embd           = 2048
0.00.050.301 I print_info: n_layer          = 24
0.00.050.304 I print_info: n_head           = 16
0.00.050.305 I print_info: n_head_kv        = 16
0.00.050.305 I print_info: n_rot            = 32
0.00.050.307 I print_info: n_swa            = 0
0.00.050.307 I print_info: n_embd_head_k    = 128
0.00.050.307 I print_info: n_embd_head_v    = 128
0.00.050.308 I print_info: n_gqa            = 1
0.00.050.309 I print_info: n_embd_k_gqa     = 2048
0.00.050.309 I print_info: n_embd_v_gqa     = 2048
0.00.050.310 I print_info: f_norm_eps       = 1.0e-05
0.00.050.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.320 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.321 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.321 I print_info: f_logit_scale    = 0.0e+00
0.00.050.323 I print_info: n_ff             = 8192
0.00.050.323 I print_info: n_expert         = 0
0.00.050.323 I print_info: n_expert_used    = 0
0.00.050.324 I print_info: causal attn      = 1
0.00.050.325 I print_info: pooling type     = 0
0.00.050.325 I print_info: rope type        = 2
0.00.050.325 I print_info: rope scaling     = linear
0.00.050.325 I print_info: freq_base_train  = 10000.0
0.00.050.326 I print_info: freq_scale_train = 1
0.00.050.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.326 I print_info: rope_finetuned   = unknown
0.00.050.326 I print_info: ssm_d_conv       = 0
0.00.050.326 I print_info: ssm_d_inner      = 0
0.00.050.329 I print_info: ssm_d_state      = 0
0.00.050.329 I print_info: ssm_dt_rank      = 0
0.00.050.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.329 I print_info: model type       = 1.4B
0.00.050.330 I print_info: model params     = 1.41 B
0.00.050.330 I print_info: general.name     = 1.4B
0.00.050.330 I print_info: vocab type       = BPE
0.00.050.332 I print_info: n_vocab          = 50304
0.00.050.332 I print_info: n_merges         = 50009
0.00.050.332 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.333 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.333 I print_info: LF token         = 128 'Ä'
0.00.050.334 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.334 I print_info: max token length = 1024
0.00.419.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.419.073 I load_tensors: offloading output layer to GPU
0.00.419.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.419.108 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.419.110 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.420.486 I llama_context: n_seq_max     = 1
0.00.420.491 I llama_context: n_ctx         = 2048
0.00.420.491 I llama_context: n_ctx_per_seq = 2048
0.00.420.492 I llama_context: n_batch       = 2048
0.00.420.492 I llama_context: n_ubatch      = 512
0.00.420.492 I llama_context: flash_attn    = 0
0.00.420.494 I llama_context: freq_base     = 10000.0
0.00.420.499 I llama_context: freq_scale    = 1
0.00.420.504 I ggml_metal_init: allocating
0.00.420.596 I ggml_metal_init: found device: Apple M4
0.00.420.607 I ggml_metal_init: picking default device: Apple M4
0.00.422.460 I ggml_metal_init: using embedded metal library
0.00.427.982 I ggml_metal_init: GPU name:   Apple M4
0.00.427.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.427.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.428.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.428.001 I ggml_metal_init: simdgroup reduction   = true
0.00.428.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.428.001 I ggml_metal_init: has residency sets    = true
0.00.428.002 I ggml_metal_init: has bfloat            = true
0.00.428.002 I ggml_metal_init: use bfloat            = true
0.00.428.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.428.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.448.695 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.503.115 I init:      Metal KV buffer size =   384.00 MiB
0.00.503.126 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.503.162 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.507.515 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.507.517 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.507.517 I llama_context: graph nodes  = 967
0.00.507.518 I llama_context: graph splits = 2
0.00.507.524 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.507.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.507.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.213 I main: llama threadpool init, n_threads = 4
0.00.567.256 I 
0.00.567.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.281 I 
0.00.567.508 I sampler seed: 1234
0.00.567.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.567.537 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.567.538 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.567.538 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.247.866 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.247.867 I llama_perf_context_print:        load time =     556.02 ms
0.01.247.868 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.46 tokens per second)
0.01.247.868 I llama_perf_context_print:        eval time =     634.32 ms /    63 runs   (   10.07 ms per token,    99.32 tokens per second)
0.01.247.869 I llama_perf_context_print:       total time =     681.62 ms /    70 tokens
0.01.251.948 I ggml_metal_free: deallocating

real	0m1.272s
user	0m0.122s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.662 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.399 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.022 I llama_model_loader: - type  f32:  194 tensors
0.00.025.023 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.023 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.023 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.024 I print_info: file format = GGUF V3 (latest)
0.00.025.024 I print_info: file type   = Q3_K - Medium
0.00.025.025 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.205 I load: special tokens cache size = 25
0.00.050.358 I load: token to piece cache size = 0.2984 MB
0.00.050.361 I print_info: arch             = gptneox
0.00.050.362 I print_info: vocab_only       = 0
0.00.050.362 I print_info: n_ctx_train      = 2048
0.00.050.362 I print_info: n_embd           = 2048
0.00.050.362 I print_info: n_layer          = 24
0.00.050.365 I print_info: n_head           = 16
0.00.050.366 I print_info: n_head_kv        = 16
0.00.050.366 I print_info: n_rot            = 32
0.00.050.368 I print_info: n_swa            = 0
0.00.050.369 I print_info: n_embd_head_k    = 128
0.00.050.369 I print_info: n_embd_head_v    = 128
0.00.050.369 I print_info: n_gqa            = 1
0.00.050.370 I print_info: n_embd_k_gqa     = 2048
0.00.050.371 I print_info: n_embd_v_gqa     = 2048
0.00.050.372 I print_info: f_norm_eps       = 1.0e-05
0.00.050.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.372 I print_info: f_logit_scale    = 0.0e+00
0.00.050.373 I print_info: n_ff             = 8192
0.00.050.373 I print_info: n_expert         = 0
0.00.050.373 I print_info: n_expert_used    = 0
0.00.050.375 I print_info: causal attn      = 1
0.00.050.376 I print_info: pooling type     = 0
0.00.050.376 I print_info: rope type        = 2
0.00.050.377 I print_info: rope scaling     = linear
0.00.050.377 I print_info: freq_base_train  = 10000.0
0.00.050.377 I print_info: freq_scale_train = 1
0.00.050.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.378 I print_info: rope_finetuned   = unknown
0.00.050.381 I print_info: ssm_d_conv       = 0
0.00.050.382 I print_info: ssm_d_inner      = 0
0.00.050.382 I print_info: ssm_d_state      = 0
0.00.050.382 I print_info: ssm_dt_rank      = 0
0.00.050.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.382 I print_info: model type       = 1.4B
0.00.050.383 I print_info: model params     = 1.41 B
0.00.050.383 I print_info: general.name     = 1.4B
0.00.050.385 I print_info: vocab type       = BPE
0.00.050.385 I print_info: n_vocab          = 50304
0.00.050.385 I print_info: n_merges         = 50009
0.00.050.385 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.385 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.385 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.385 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.386 I print_info: LF token         = 128 'Ä'
0.00.050.386 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.386 I print_info: max token length = 1024
0.00.527.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.637 I load_tensors: offloading output layer to GPU
0.00.527.638 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.672 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.527.673 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.528.967 I llama_context: n_seq_max     = 1
0.00.528.972 I llama_context: n_ctx         = 2048
0.00.528.972 I llama_context: n_ctx_per_seq = 2048
0.00.528.973 I llama_context: n_batch       = 2048
0.00.528.973 I llama_context: n_ubatch      = 512
0.00.528.974 I llama_context: flash_attn    = 0
0.00.528.980 I llama_context: freq_base     = 10000.0
0.00.528.985 I llama_context: freq_scale    = 1
0.00.528.987 I ggml_metal_init: allocating
0.00.529.069 I ggml_metal_init: found device: Apple M4
0.00.529.079 I ggml_metal_init: picking default device: Apple M4
0.00.530.873 I ggml_metal_init: using embedded metal library
0.00.536.922 I ggml_metal_init: GPU name:   Apple M4
0.00.536.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.930 I ggml_metal_init: simdgroup reduction   = true
0.00.536.930 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.931 I ggml_metal_init: has residency sets    = true
0.00.536.931 I ggml_metal_init: has bfloat            = true
0.00.536.931 I ggml_metal_init: use bfloat            = true
0.00.536.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.555.519 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.576 I init:      Metal KV buffer size =   384.00 MiB
0.00.608.583 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.628 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.613.223 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.613.225 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.613.226 I llama_context: graph nodes  = 967
0.00.613.226 I llama_context: graph splits = 2
0.00.613.232 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.712 I main: llama threadpool init, n_threads = 4
0.00.672.755 I 
0.00.672.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.780 I 
0.00.673.009 I sampler seed: 1234
0.00.673.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.063 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.063 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.418.548 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.418.549 I llama_perf_context_print:        load time =     663.17 ms
0.01.418.549 I llama_perf_context_print: prompt eval time =      50.26 ms /     7 tokens (    7.18 ms per token,   139.27 tokens per second)
0.01.418.550 I llama_perf_context_print:        eval time =     692.25 ms /    63 runs   (   10.99 ms per token,    91.01 tokens per second)
0.01.418.551 I llama_perf_context_print:       total time =     746.71 ms /    70 tokens
0.01.422.561 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.120s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.330 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.333 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.183 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.185 I llama_model_loader: - type  f32:  194 tensors
0.00.026.185 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.185 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.185 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.186 I print_info: file format = GGUF V3 (latest)
0.00.026.187 I print_info: file type   = Q4_K - Medium
0.00.026.187 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.146 I load: special tokens cache size = 25
0.00.052.349 I load: token to piece cache size = 0.2984 MB
0.00.052.352 I print_info: arch             = gptneox
0.00.052.352 I print_info: vocab_only       = 0
0.00.052.352 I print_info: n_ctx_train      = 2048
0.00.052.352 I print_info: n_embd           = 2048
0.00.052.352 I print_info: n_layer          = 24
0.00.052.355 I print_info: n_head           = 16
0.00.052.356 I print_info: n_head_kv        = 16
0.00.052.356 I print_info: n_rot            = 32
0.00.052.356 I print_info: n_swa            = 0
0.00.052.357 I print_info: n_embd_head_k    = 128
0.00.052.357 I print_info: n_embd_head_v    = 128
0.00.052.357 I print_info: n_gqa            = 1
0.00.052.358 I print_info: n_embd_k_gqa     = 2048
0.00.052.359 I print_info: n_embd_v_gqa     = 2048
0.00.052.359 I print_info: f_norm_eps       = 1.0e-05
0.00.052.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.360 I print_info: f_logit_scale    = 0.0e+00
0.00.052.361 I print_info: n_ff             = 8192
0.00.052.361 I print_info: n_expert         = 0
0.00.052.361 I print_info: n_expert_used    = 0
0.00.052.361 I print_info: causal attn      = 1
0.00.052.363 I print_info: pooling type     = 0
0.00.052.364 I print_info: rope type        = 2
0.00.052.364 I print_info: rope scaling     = linear
0.00.052.364 I print_info: freq_base_train  = 10000.0
0.00.052.365 I print_info: freq_scale_train = 1
0.00.052.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.365 I print_info: rope_finetuned   = unknown
0.00.052.365 I print_info: ssm_d_conv       = 0
0.00.052.365 I print_info: ssm_d_inner      = 0
0.00.052.365 I print_info: ssm_d_state      = 0
0.00.052.366 I print_info: ssm_dt_rank      = 0
0.00.052.367 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.367 I print_info: model type       = 1.4B
0.00.052.368 I print_info: model params     = 1.41 B
0.00.052.368 I print_info: general.name     = 1.4B
0.00.052.368 I print_info: vocab type       = BPE
0.00.052.369 I print_info: n_vocab          = 50304
0.00.052.369 I print_info: n_merges         = 50009
0.00.052.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.370 I print_info: LF token         = 128 'Ä'
0.00.052.374 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.375 I print_info: max token length = 1024
0.00.536.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.536.162 I load_tensors: offloading output layer to GPU
0.00.536.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.536.194 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.536.196 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.537.453 I llama_context: n_seq_max     = 1
0.00.537.455 I llama_context: n_ctx         = 2048
0.00.537.456 I llama_context: n_ctx_per_seq = 2048
0.00.537.456 I llama_context: n_batch       = 2048
0.00.537.457 I llama_context: n_ubatch      = 512
0.00.537.457 I llama_context: flash_attn    = 0
0.00.537.459 I llama_context: freq_base     = 10000.0
0.00.537.479 I llama_context: freq_scale    = 1
0.00.537.485 I ggml_metal_init: allocating
0.00.537.556 I ggml_metal_init: found device: Apple M4
0.00.537.564 I ggml_metal_init: picking default device: Apple M4
0.00.539.355 I ggml_metal_init: using embedded metal library
0.00.546.008 I ggml_metal_init: GPU name:   Apple M4
0.00.546.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.546.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.546.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.546.016 I ggml_metal_init: simdgroup reduction   = true
0.00.546.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.546.017 I ggml_metal_init: has residency sets    = true
0.00.546.018 I ggml_metal_init: has bfloat            = true
0.00.546.018 I ggml_metal_init: use bfloat            = true
0.00.546.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.546.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.564.082 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.261 I init:      Metal KV buffer size =   384.00 MiB
0.00.619.267 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.619.347 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.623.896 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.623.898 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.623.898 I llama_context: graph nodes  = 967
0.00.623.898 I llama_context: graph splits = 2
0.00.623.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.139 I main: llama threadpool init, n_threads = 4
0.00.684.177 I 
0.00.684.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.202 I 
0.00.684.425 I sampler seed: 1234
0.00.684.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.447 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.448 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.440.543 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.440.544 I llama_perf_context_print:        load time =     673.46 ms
0.01.440.545 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.45 tokens per second)
0.01.440.546 I llama_perf_context_print:        eval time =     701.54 ms /    63 runs   (   11.14 ms per token,    89.80 tokens per second)
0.01.440.547 I llama_perf_context_print:       total time =     757.30 ms /    70 tokens
0.01.444.601 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.122s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.726 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.135 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.797 I llama_model_loader: - type  f32:  194 tensors
0.00.024.797 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.797 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.798 I print_info: file format = GGUF V3 (latest)
0.00.024.799 I print_info: file type   = Q5_K - Medium
0.00.024.800 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.025 I load: special tokens cache size = 25
0.00.050.060 I load: token to piece cache size = 0.2984 MB
0.00.050.064 I print_info: arch             = gptneox
0.00.050.064 I print_info: vocab_only       = 0
0.00.050.064 I print_info: n_ctx_train      = 2048
0.00.050.064 I print_info: n_embd           = 2048
0.00.050.064 I print_info: n_layer          = 24
0.00.050.067 I print_info: n_head           = 16
0.00.050.068 I print_info: n_head_kv        = 16
0.00.050.068 I print_info: n_rot            = 32
0.00.050.068 I print_info: n_swa            = 0
0.00.050.069 I print_info: n_embd_head_k    = 128
0.00.050.071 I print_info: n_embd_head_v    = 128
0.00.050.071 I print_info: n_gqa            = 1
0.00.050.072 I print_info: n_embd_k_gqa     = 2048
0.00.050.073 I print_info: n_embd_v_gqa     = 2048
0.00.050.073 I print_info: f_norm_eps       = 1.0e-05
0.00.050.073 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.074 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.074 I print_info: f_logit_scale    = 0.0e+00
0.00.050.075 I print_info: n_ff             = 8192
0.00.050.075 I print_info: n_expert         = 0
0.00.050.075 I print_info: n_expert_used    = 0
0.00.050.077 I print_info: causal attn      = 1
0.00.050.077 I print_info: pooling type     = 0
0.00.050.082 I print_info: rope type        = 2
0.00.050.083 I print_info: rope scaling     = linear
0.00.050.083 I print_info: freq_base_train  = 10000.0
0.00.050.084 I print_info: freq_scale_train = 1
0.00.050.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.084 I print_info: rope_finetuned   = unknown
0.00.050.088 I print_info: ssm_d_conv       = 0
0.00.050.088 I print_info: ssm_d_inner      = 0
0.00.050.088 I print_info: ssm_d_state      = 0
0.00.050.088 I print_info: ssm_dt_rank      = 0
0.00.050.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.088 I print_info: model type       = 1.4B
0.00.050.089 I print_info: model params     = 1.41 B
0.00.050.089 I print_info: general.name     = 1.4B
0.00.050.089 I print_info: vocab type       = BPE
0.00.050.089 I print_info: n_vocab          = 50304
0.00.050.090 I print_info: n_merges         = 50009
0.00.050.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.092 I print_info: LF token         = 128 'Ä'
0.00.050.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.092 I print_info: max token length = 1024
0.00.614.477 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.481 I load_tensors: offloading output layer to GPU
0.00.614.483 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.507 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.614.508 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.615.463 I llama_context: n_seq_max     = 1
0.00.615.466 I llama_context: n_ctx         = 2048
0.00.615.466 I llama_context: n_ctx_per_seq = 2048
0.00.615.467 I llama_context: n_batch       = 2048
0.00.615.467 I llama_context: n_ubatch      = 512
0.00.615.467 I llama_context: flash_attn    = 0
0.00.615.468 I llama_context: freq_base     = 10000.0
0.00.615.469 I llama_context: freq_scale    = 1
0.00.615.473 I ggml_metal_init: allocating
0.00.615.492 I ggml_metal_init: found device: Apple M4
0.00.615.501 I ggml_metal_init: picking default device: Apple M4
0.00.616.947 I ggml_metal_init: using embedded metal library
0.00.622.963 I ggml_metal_init: GPU name:   Apple M4
0.00.622.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.969 I ggml_metal_init: simdgroup reduction   = true
0.00.622.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.970 I ggml_metal_init: has residency sets    = true
0.00.622.970 I ggml_metal_init: has bfloat            = true
0.00.622.970 I ggml_metal_init: use bfloat            = true
0.00.622.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.855 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.978 I init:      Metal KV buffer size =   384.00 MiB
0.00.697.985 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.029 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.702.502 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.702.505 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.702.506 I llama_context: graph nodes  = 967
0.00.702.506 I llama_context: graph splits = 2
0.00.702.516 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.664 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.667 I main: llama threadpool init, n_threads = 4
0.00.765.710 I 
0.00.765.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.734 I 
0.00.765.943 I sampler seed: 1234
0.00.765.948 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.978 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.981 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.981 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.504 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.01.610.504 I llama_perf_context_print:        load time =     756.04 ms
0.01.610.505 I llama_perf_context_print: prompt eval time =      51.53 ms /     7 tokens (    7.36 ms per token,   135.85 tokens per second)
0.01.610.506 I llama_perf_context_print:        eval time =     790.14 ms /    63 runs   (   12.54 ms per token,    79.73 tokens per second)
0.01.610.506 I llama_perf_context_print:       total time =     845.74 ms /    70 tokens
0.01.613.370 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.121s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.507 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.386 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.247 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.248 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.248 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.248 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.248 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.249 I llama_model_loader: - type  f32:  194 tensors
0.00.026.249 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.250 I print_info: file format = GGUF V3 (latest)
0.00.026.250 I print_info: file type   = Q6_K
0.00.026.254 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.448 I load: special tokens cache size = 25
0.00.051.501 I load: token to piece cache size = 0.2984 MB
0.00.051.504 I print_info: arch             = gptneox
0.00.051.504 I print_info: vocab_only       = 0
0.00.051.505 I print_info: n_ctx_train      = 2048
0.00.051.505 I print_info: n_embd           = 2048
0.00.051.505 I print_info: n_layer          = 24
0.00.051.508 I print_info: n_head           = 16
0.00.051.509 I print_info: n_head_kv        = 16
0.00.051.509 I print_info: n_rot            = 32
0.00.051.509 I print_info: n_swa            = 0
0.00.051.509 I print_info: n_embd_head_k    = 128
0.00.051.509 I print_info: n_embd_head_v    = 128
0.00.051.510 I print_info: n_gqa            = 1
0.00.051.511 I print_info: n_embd_k_gqa     = 2048
0.00.051.512 I print_info: n_embd_v_gqa     = 2048
0.00.051.512 I print_info: f_norm_eps       = 1.0e-05
0.00.051.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.513 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.513 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.513 I print_info: f_logit_scale    = 0.0e+00
0.00.051.514 I print_info: n_ff             = 8192
0.00.051.516 I print_info: n_expert         = 0
0.00.051.516 I print_info: n_expert_used    = 0
0.00.051.516 I print_info: causal attn      = 1
0.00.051.516 I print_info: pooling type     = 0
0.00.051.516 I print_info: rope type        = 2
0.00.051.516 I print_info: rope scaling     = linear
0.00.051.517 I print_info: freq_base_train  = 10000.0
0.00.051.517 I print_info: freq_scale_train = 1
0.00.051.517 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.517 I print_info: rope_finetuned   = unknown
0.00.051.518 I print_info: ssm_d_conv       = 0
0.00.051.518 I print_info: ssm_d_inner      = 0
0.00.051.518 I print_info: ssm_d_state      = 0
0.00.051.518 I print_info: ssm_dt_rank      = 0
0.00.051.518 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.519 I print_info: model type       = 1.4B
0.00.051.519 I print_info: model params     = 1.41 B
0.00.051.519 I print_info: general.name     = 1.4B
0.00.051.520 I print_info: vocab type       = BPE
0.00.051.520 I print_info: n_vocab          = 50304
0.00.051.520 I print_info: n_merges         = 50009
0.00.051.520 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.521 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.521 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.521 I print_info: LF token         = 128 'Ä'
0.00.051.522 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.523 I print_info: max token length = 1024
0.00.654.943 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.946 I load_tensors: offloading output layer to GPU
0.00.654.947 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.968 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.654.970 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.656.203 I llama_context: n_seq_max     = 1
0.00.656.206 I llama_context: n_ctx         = 2048
0.00.656.206 I llama_context: n_ctx_per_seq = 2048
0.00.656.206 I llama_context: n_batch       = 2048
0.00.656.207 I llama_context: n_ubatch      = 512
0.00.656.207 I llama_context: flash_attn    = 0
0.00.656.208 I llama_context: freq_base     = 10000.0
0.00.656.209 I llama_context: freq_scale    = 1
0.00.656.210 I ggml_metal_init: allocating
0.00.656.234 I ggml_metal_init: found device: Apple M4
0.00.656.240 I ggml_metal_init: picking default device: Apple M4
0.00.657.627 I ggml_metal_init: using embedded metal library
0.00.663.381 I ggml_metal_init: GPU name:   Apple M4
0.00.663.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.385 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.386 I ggml_metal_init: simdgroup reduction   = true
0.00.663.386 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.387 I ggml_metal_init: has residency sets    = true
0.00.663.387 I ggml_metal_init: has bfloat            = true
0.00.663.387 I ggml_metal_init: use bfloat            = true
0.00.663.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.785 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.910 I init:      Metal KV buffer size =   384.00 MiB
0.00.737.916 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.996 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.742.203 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.742.205 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.742.205 I llama_context: graph nodes  = 967
0.00.742.205 I llama_context: graph splits = 2
0.00.742.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.941 I main: llama threadpool init, n_threads = 4
0.00.806.001 I 
0.00.806.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.024 I 
0.00.806.238 I sampler seed: 1234
0.00.806.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.254 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.785 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.682.786 I llama_perf_context_print:        load time =     794.95 ms
0.01.682.787 I llama_perf_context_print: prompt eval time =      54.34 ms /     7 tokens (    7.76 ms per token,   128.83 tokens per second)
0.01.682.788 I llama_perf_context_print:        eval time =     819.22 ms /    63 runs   (   13.00 ms per token,    76.90 tokens per second)
0.01.682.788 I llama_perf_context_print:       total time =     877.73 ms /    70 tokens
0.01.686.612 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.121s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.498 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.167 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.177 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.180 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.183 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.187 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.027 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.873 I llama_model_loader: - type  f32:  194 tensors
0.00.049.873 I llama_model_loader: - type  f16:   98 tensors
0.00.049.874 I print_info: file format = GGUF V3 (latest)
0.00.049.875 I print_info: file type   = all F32 (guessed)
0.00.049.876 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.075.973 I load: special tokens cache size = 25
0.00.082.461 I load: token to piece cache size = 0.2984 MB
0.00.082.465 I print_info: arch             = gptneox
0.00.082.465 I print_info: vocab_only       = 0
0.00.082.465 I print_info: n_ctx_train      = 2048
0.00.082.465 I print_info: n_embd           = 2048
0.00.082.465 I print_info: n_layer          = 24
0.00.082.469 I print_info: n_head           = 16
0.00.082.470 I print_info: n_head_kv        = 16
0.00.082.470 I print_info: n_rot            = 32
0.00.082.470 I print_info: n_swa            = 0
0.00.082.470 I print_info: n_embd_head_k    = 128
0.00.082.471 I print_info: n_embd_head_v    = 128
0.00.082.471 I print_info: n_gqa            = 1
0.00.082.472 I print_info: n_embd_k_gqa     = 2048
0.00.082.472 I print_info: n_embd_v_gqa     = 2048
0.00.082.473 I print_info: f_norm_eps       = 1.0e-05
0.00.082.475 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.476 I print_info: f_logit_scale    = 0.0e+00
0.00.082.477 I print_info: n_ff             = 8192
0.00.082.477 I print_info: n_expert         = 0
0.00.082.478 I print_info: n_expert_used    = 0
0.00.082.478 I print_info: causal attn      = 1
0.00.082.478 I print_info: pooling type     = 0
0.00.082.478 I print_info: rope type        = 2
0.00.082.478 I print_info: rope scaling     = linear
0.00.082.479 I print_info: freq_base_train  = 10000.0
0.00.082.479 I print_info: freq_scale_train = 1
0.00.082.479 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.479 I print_info: rope_finetuned   = unknown
0.00.082.480 I print_info: ssm_d_conv       = 0
0.00.082.480 I print_info: ssm_d_inner      = 0
0.00.082.480 I print_info: ssm_d_state      = 0
0.00.082.480 I print_info: ssm_dt_rank      = 0
0.00.082.480 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.480 I print_info: model type       = 1.4B
0.00.082.481 I print_info: model params     = 1.41 B
0.00.082.481 I print_info: general.name     = 1.4B
0.00.082.482 I print_info: vocab type       = BPE
0.00.082.483 I print_info: n_vocab          = 50304
0.00.082.483 I print_info: n_merges         = 50009
0.00.082.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.487 I print_info: LF token         = 128 'Ä'
0.00.082.488 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.488 I print_info: max token length = 1024
0.00.962.365 I load_tensors: offloading 24 repeating layers to GPU
0.00.962.372 I load_tensors: offloading output layer to GPU
0.00.962.374 I load_tensors: offloaded 25/25 layers to GPU
0.00.962.400 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.962.402 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.963.388 I llama_context: n_seq_max     = 1
0.00.963.389 I llama_context: n_ctx         = 128
0.00.963.389 I llama_context: n_ctx_per_seq = 128
0.00.963.389 I llama_context: n_batch       = 128
0.00.963.389 I llama_context: n_ubatch      = 128
0.00.963.389 I llama_context: flash_attn    = 0
0.00.963.390 I llama_context: freq_base     = 10000.0
0.00.963.390 I llama_context: freq_scale    = 1
0.00.963.391 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.963.392 I ggml_metal_init: allocating
0.00.963.433 I ggml_metal_init: found device: Apple M4
0.00.963.437 I ggml_metal_init: picking default device: Apple M4
0.00.964.428 I ggml_metal_init: using embedded metal library
0.00.968.281 I ggml_metal_init: GPU name:   Apple M4
0.00.968.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.968.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.968.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.968.285 I ggml_metal_init: simdgroup reduction   = true
0.00.968.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.968.285 I ggml_metal_init: has residency sets    = true
0.00.968.285 I ggml_metal_init: has bfloat            = true
0.00.968.285 I ggml_metal_init: use bfloat            = true
0.00.968.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.968.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.978.857 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.980.507 I init:      Metal KV buffer size =    24.00 MiB
0.00.980.509 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.980.535 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.982.042 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.982.043 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.982.044 I llama_context: graph nodes  = 967
0.00.982.044 I llama_context: graph splits = 2
0.00.982.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.982.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.017.481 I 
0.01.017.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.017.535 I perplexity: tokenizing the input ..
0.01.026.722 I perplexity: tokenization took 9.184 ms
0.01.026.743 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.144.661 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.146.006 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.146.024 I llama_perf_context_print:        load time =     997.72 ms
0.01.146.029 I llama_perf_context_print: prompt eval time =     117.66 ms /   128 tokens (    0.92 ms per token,  1087.93 tokens per second)
0.01.146.030 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.146.031 I llama_perf_context_print:       total time =     128.54 ms /   129 tokens
0.01.146.615 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.112s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.446 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.217 I llama_model_loader: - type  f32:  194 tensors
0.00.025.218 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.219 I print_info: file format = GGUF V3 (latest)
0.00.025.219 I print_info: file type   = Q8_0
0.00.025.220 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.438 I load: special tokens cache size = 25
0.00.053.895 I load: token to piece cache size = 0.2984 MB
0.00.053.898 I print_info: arch             = gptneox
0.00.053.898 I print_info: vocab_only       = 0
0.00.053.898 I print_info: n_ctx_train      = 2048
0.00.053.899 I print_info: n_embd           = 2048
0.00.053.899 I print_info: n_layer          = 24
0.00.053.903 I print_info: n_head           = 16
0.00.053.903 I print_info: n_head_kv        = 16
0.00.053.905 I print_info: n_rot            = 32
0.00.053.905 I print_info: n_swa            = 0
0.00.053.906 I print_info: n_embd_head_k    = 128
0.00.053.906 I print_info: n_embd_head_v    = 128
0.00.053.907 I print_info: n_gqa            = 1
0.00.053.908 I print_info: n_embd_k_gqa     = 2048
0.00.053.908 I print_info: n_embd_v_gqa     = 2048
0.00.053.909 I print_info: f_norm_eps       = 1.0e-05
0.00.053.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.910 I print_info: f_logit_scale    = 0.0e+00
0.00.053.910 I print_info: n_ff             = 8192
0.00.053.910 I print_info: n_expert         = 0
0.00.053.910 I print_info: n_expert_used    = 0
0.00.053.911 I print_info: causal attn      = 1
0.00.053.911 I print_info: pooling type     = 0
0.00.053.911 I print_info: rope type        = 2
0.00.053.911 I print_info: rope scaling     = linear
0.00.053.911 I print_info: freq_base_train  = 10000.0
0.00.053.912 I print_info: freq_scale_train = 1
0.00.053.912 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.912 I print_info: rope_finetuned   = unknown
0.00.053.912 I print_info: ssm_d_conv       = 0
0.00.053.912 I print_info: ssm_d_inner      = 0
0.00.053.912 I print_info: ssm_d_state      = 0
0.00.053.912 I print_info: ssm_dt_rank      = 0
0.00.053.912 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.913 I print_info: model type       = 1.4B
0.00.053.913 I print_info: model params     = 1.41 B
0.00.053.913 I print_info: general.name     = 1.4B
0.00.053.913 I print_info: vocab type       = BPE
0.00.053.914 I print_info: n_vocab          = 50304
0.00.053.914 I print_info: n_merges         = 50009
0.00.053.914 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.914 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.914 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.914 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.915 I print_info: LF token         = 128 'Ä'
0.00.053.915 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.915 I print_info: max token length = 1024
0.00.774.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.774.457 I load_tensors: offloading output layer to GPU
0.00.774.457 I load_tensors: offloaded 25/25 layers to GPU
0.00.774.480 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.774.483 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.775.735 I llama_context: n_seq_max     = 1
0.00.775.737 I llama_context: n_ctx         = 128
0.00.775.738 I llama_context: n_ctx_per_seq = 128
0.00.775.738 I llama_context: n_batch       = 128
0.00.775.738 I llama_context: n_ubatch      = 128
0.00.775.739 I llama_context: flash_attn    = 0
0.00.775.740 I llama_context: freq_base     = 10000.0
0.00.775.740 I llama_context: freq_scale    = 1
0.00.775.741 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.775.742 I ggml_metal_init: allocating
0.00.775.770 I ggml_metal_init: found device: Apple M4
0.00.775.776 I ggml_metal_init: picking default device: Apple M4
0.00.777.009 I ggml_metal_init: using embedded metal library
0.00.782.590 I ggml_metal_init: GPU name:   Apple M4
0.00.782.594 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.782.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.782.595 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.782.596 I ggml_metal_init: simdgroup reduction   = true
0.00.782.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.782.596 I ggml_metal_init: has residency sets    = true
0.00.782.596 I ggml_metal_init: has bfloat            = true
0.00.782.596 I ggml_metal_init: use bfloat            = true
0.00.782.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.782.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.798.186 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.801.313 I init:      Metal KV buffer size =    24.00 MiB
0.00.801.318 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.801.353 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.804.007 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.804.008 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.804.009 I llama_context: graph nodes  = 967
0.00.804.009 I llama_context: graph splits = 2
0.00.804.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.804.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.833.434 I 
0.00.833.518 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.540 I perplexity: tokenizing the input ..
0.00.845.759 I perplexity: tokenization took 12.217 ms
0.00.845.775 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.977.015 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.978.321 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.978.335 I llama_perf_context_print:        load time =     824.23 ms
0.00.978.336 I llama_perf_context_print: prompt eval time =     131.01 ms /   128 tokens (    1.02 ms per token,   977.04 tokens per second)
0.00.978.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.978.337 I llama_perf_context_print:       total time =     144.91 ms /   129 tokens
0.00.978.909 I ggml_metal_free: deallocating

real	0m0.994s
user	0m0.096s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.196 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.137 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.830 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.831 I print_info: file type   = Q4_0
0.00.025.832 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.668 I load: special tokens cache size = 25
0.00.051.732 I load: token to piece cache size = 0.2984 MB
0.00.051.736 I print_info: arch             = gptneox
0.00.051.736 I print_info: vocab_only       = 0
0.00.051.736 I print_info: n_ctx_train      = 2048
0.00.051.736 I print_info: n_embd           = 2048
0.00.051.736 I print_info: n_layer          = 24
0.00.051.739 I print_info: n_head           = 16
0.00.051.740 I print_info: n_head_kv        = 16
0.00.051.740 I print_info: n_rot            = 32
0.00.051.740 I print_info: n_swa            = 0
0.00.051.740 I print_info: n_embd_head_k    = 128
0.00.051.742 I print_info: n_embd_head_v    = 128
0.00.051.742 I print_info: n_gqa            = 1
0.00.051.743 I print_info: n_embd_k_gqa     = 2048
0.00.051.744 I print_info: n_embd_v_gqa     = 2048
0.00.051.745 I print_info: f_norm_eps       = 1.0e-05
0.00.051.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.745 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.746 I print_info: f_logit_scale    = 0.0e+00
0.00.051.746 I print_info: n_ff             = 8192
0.00.051.746 I print_info: n_expert         = 0
0.00.051.747 I print_info: n_expert_used    = 0
0.00.051.747 I print_info: causal attn      = 1
0.00.051.747 I print_info: pooling type     = 0
0.00.051.747 I print_info: rope type        = 2
0.00.051.747 I print_info: rope scaling     = linear
0.00.051.748 I print_info: freq_base_train  = 10000.0
0.00.051.748 I print_info: freq_scale_train = 1
0.00.051.748 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.748 I print_info: rope_finetuned   = unknown
0.00.051.748 I print_info: ssm_d_conv       = 0
0.00.051.748 I print_info: ssm_d_inner      = 0
0.00.051.749 I print_info: ssm_d_state      = 0
0.00.051.749 I print_info: ssm_dt_rank      = 0
0.00.051.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.749 I print_info: model type       = 1.4B
0.00.051.750 I print_info: model params     = 1.41 B
0.00.051.750 I print_info: general.name     = 1.4B
0.00.051.750 I print_info: vocab type       = BPE
0.00.051.751 I print_info: n_vocab          = 50304
0.00.051.751 I print_info: n_merges         = 50009
0.00.051.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.754 I print_info: LF token         = 128 'Ä'
0.00.051.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.754 I print_info: max token length = 1024
0.00.576.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.576.448 I load_tensors: offloading output layer to GPU
0.00.576.448 I load_tensors: offloaded 25/25 layers to GPU
0.00.576.481 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.576.482 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.577.990 I llama_context: n_seq_max     = 1
0.00.577.994 I llama_context: n_ctx         = 128
0.00.577.995 I llama_context: n_ctx_per_seq = 128
0.00.577.996 I llama_context: n_batch       = 128
0.00.577.996 I llama_context: n_ubatch      = 128
0.00.577.996 I llama_context: flash_attn    = 0
0.00.577.998 I llama_context: freq_base     = 10000.0
0.00.577.999 I llama_context: freq_scale    = 1
0.00.578.000 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.002 I ggml_metal_init: allocating
0.00.578.083 I ggml_metal_init: found device: Apple M4
0.00.578.096 I ggml_metal_init: picking default device: Apple M4
0.00.579.862 I ggml_metal_init: using embedded metal library
0.00.585.346 I ggml_metal_init: GPU name:   Apple M4
0.00.585.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.585.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.585.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.585.370 I ggml_metal_init: simdgroup reduction   = true
0.00.585.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.585.371 I ggml_metal_init: has residency sets    = true
0.00.585.371 I ggml_metal_init: has bfloat            = true
0.00.585.371 I ggml_metal_init: use bfloat            = true
0.00.585.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.585.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.881 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.419 I init:      Metal KV buffer size =    24.00 MiB
0.00.608.433 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.608.502 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.611.776 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.611.778 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.611.778 I llama_context: graph nodes  = 967
0.00.611.779 I llama_context: graph splits = 2
0.00.611.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.611.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.235 I 
0.00.639.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.334 I perplexity: tokenizing the input ..
0.00.648.106 I perplexity: tokenization took 8.77 ms
0.00.648.120 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.470 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.771.805 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.771.827 I llama_perf_context_print:        load time =     629.03 ms
0.00.771.828 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.17 tokens per second)
0.00.771.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.829 I llama_perf_context_print:       total time =     132.60 ms /   129 tokens
0.00.772.394 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.093s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.440 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.440 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.441 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.441 I llama_model_loader: - type  f32:  194 tensors
0.00.024.442 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.442 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.443 I print_info: file format = GGUF V3 (latest)
0.00.024.443 I print_info: file type   = Q4_1
0.00.024.444 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.242 I load: special tokens cache size = 25
0.00.050.292 I load: token to piece cache size = 0.2984 MB
0.00.050.294 I print_info: arch             = gptneox
0.00.050.295 I print_info: vocab_only       = 0
0.00.050.295 I print_info: n_ctx_train      = 2048
0.00.050.295 I print_info: n_embd           = 2048
0.00.050.295 I print_info: n_layer          = 24
0.00.050.298 I print_info: n_head           = 16
0.00.050.299 I print_info: n_head_kv        = 16
0.00.050.299 I print_info: n_rot            = 32
0.00.050.300 I print_info: n_swa            = 0
0.00.050.300 I print_info: n_embd_head_k    = 128
0.00.050.300 I print_info: n_embd_head_v    = 128
0.00.050.301 I print_info: n_gqa            = 1
0.00.050.301 I print_info: n_embd_k_gqa     = 2048
0.00.050.302 I print_info: n_embd_v_gqa     = 2048
0.00.050.303 I print_info: f_norm_eps       = 1.0e-05
0.00.050.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.303 I print_info: f_logit_scale    = 0.0e+00
0.00.050.304 I print_info: n_ff             = 8192
0.00.050.304 I print_info: n_expert         = 0
0.00.050.304 I print_info: n_expert_used    = 0
0.00.050.305 I print_info: causal attn      = 1
0.00.050.305 I print_info: pooling type     = 0
0.00.050.305 I print_info: rope type        = 2
0.00.050.305 I print_info: rope scaling     = linear
0.00.050.305 I print_info: freq_base_train  = 10000.0
0.00.050.306 I print_info: freq_scale_train = 1
0.00.050.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.306 I print_info: rope_finetuned   = unknown
0.00.050.306 I print_info: ssm_d_conv       = 0
0.00.050.306 I print_info: ssm_d_inner      = 0
0.00.050.306 I print_info: ssm_d_state      = 0
0.00.050.307 I print_info: ssm_dt_rank      = 0
0.00.050.309 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.309 I print_info: model type       = 1.4B
0.00.050.310 I print_info: model params     = 1.41 B
0.00.050.310 I print_info: general.name     = 1.4B
0.00.050.310 I print_info: vocab type       = BPE
0.00.050.310 I print_info: n_vocab          = 50304
0.00.050.311 I print_info: n_merges         = 50009
0.00.050.311 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.311 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.311 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.311 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.312 I print_info: LF token         = 128 'Ä'
0.00.050.316 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.317 I print_info: max token length = 1024
0.00.602.424 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.441 I load_tensors: offloading output layer to GPU
0.00.602.442 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.476 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.602.477 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.604.106 I llama_context: n_seq_max     = 1
0.00.604.111 I llama_context: n_ctx         = 128
0.00.604.111 I llama_context: n_ctx_per_seq = 128
0.00.604.112 I llama_context: n_batch       = 128
0.00.604.113 I llama_context: n_ubatch      = 128
0.00.604.113 I llama_context: flash_attn    = 0
0.00.604.115 I llama_context: freq_base     = 10000.0
0.00.604.116 I llama_context: freq_scale    = 1
0.00.604.116 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.119 I ggml_metal_init: allocating
0.00.604.194 I ggml_metal_init: found device: Apple M4
0.00.604.203 I ggml_metal_init: picking default device: Apple M4
0.00.605.909 I ggml_metal_init: using embedded metal library
0.00.612.502 I ggml_metal_init: GPU name:   Apple M4
0.00.612.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.509 I ggml_metal_init: simdgroup reduction   = true
0.00.612.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.509 I ggml_metal_init: has residency sets    = true
0.00.612.509 I ggml_metal_init: has bfloat            = true
0.00.612.510 I ggml_metal_init: use bfloat            = true
0.00.612.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.590 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.123 I init:      Metal KV buffer size =    24.00 MiB
0.00.633.129 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.176 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.636.311 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.636.313 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.636.313 I llama_context: graph nodes  = 967
0.00.636.313 I llama_context: graph splits = 2
0.00.636.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.724 I 
0.00.658.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.784 I perplexity: tokenizing the input ..
0.00.668.049 I perplexity: tokenization took 9.263 ms
0.00.668.061 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.939 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.483 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.503 I llama_perf_context_print:        load time =     649.98 ms
0.00.791.504 I llama_perf_context_print: prompt eval time =     121.65 ms /   128 tokens (    0.95 ms per token,  1052.19 tokens per second)
0.00.791.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.505 I llama_perf_context_print:       total time =     132.78 ms /   129 tokens
0.00.792.059 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.092s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.232 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.332 I llama_model_loader: - type  f32:  194 tensors
0.00.026.332 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.332 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.333 I print_info: file format = GGUF V3 (latest)
0.00.026.333 I print_info: file type   = Q5_0
0.00.026.337 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.211 I load: special tokens cache size = 25
0.00.052.275 I load: token to piece cache size = 0.2984 MB
0.00.052.278 I print_info: arch             = gptneox
0.00.052.279 I print_info: vocab_only       = 0
0.00.052.279 I print_info: n_ctx_train      = 2048
0.00.052.279 I print_info: n_embd           = 2048
0.00.052.279 I print_info: n_layer          = 24
0.00.052.282 I print_info: n_head           = 16
0.00.052.283 I print_info: n_head_kv        = 16
0.00.052.283 I print_info: n_rot            = 32
0.00.052.284 I print_info: n_swa            = 0
0.00.052.284 I print_info: n_embd_head_k    = 128
0.00.052.284 I print_info: n_embd_head_v    = 128
0.00.052.285 I print_info: n_gqa            = 1
0.00.052.285 I print_info: n_embd_k_gqa     = 2048
0.00.052.287 I print_info: n_embd_v_gqa     = 2048
0.00.052.288 I print_info: f_norm_eps       = 1.0e-05
0.00.052.288 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.288 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.290 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.290 I print_info: f_logit_scale    = 0.0e+00
0.00.052.291 I print_info: n_ff             = 8192
0.00.052.291 I print_info: n_expert         = 0
0.00.052.291 I print_info: n_expert_used    = 0
0.00.052.291 I print_info: causal attn      = 1
0.00.052.297 I print_info: pooling type     = 0
0.00.052.298 I print_info: rope type        = 2
0.00.052.298 I print_info: rope scaling     = linear
0.00.052.298 I print_info: freq_base_train  = 10000.0
0.00.052.299 I print_info: freq_scale_train = 1
0.00.052.299 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.299 I print_info: rope_finetuned   = unknown
0.00.052.299 I print_info: ssm_d_conv       = 0
0.00.052.300 I print_info: ssm_d_inner      = 0
0.00.052.300 I print_info: ssm_d_state      = 0
0.00.052.300 I print_info: ssm_dt_rank      = 0
0.00.052.300 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.300 I print_info: model type       = 1.4B
0.00.052.301 I print_info: model params     = 1.41 B
0.00.052.301 I print_info: general.name     = 1.4B
0.00.052.301 I print_info: vocab type       = BPE
0.00.052.302 I print_info: n_vocab          = 50304
0.00.052.302 I print_info: n_merges         = 50009
0.00.052.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.302 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.302 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.304 I print_info: LF token         = 128 'Ä'
0.00.052.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.304 I print_info: max token length = 1024
0.00.665.591 I load_tensors: offloading 24 repeating layers to GPU
0.00.665.607 I load_tensors: offloading output layer to GPU
0.00.665.607 I load_tensors: offloaded 25/25 layers to GPU
0.00.665.641 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.665.643 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.666.889 I llama_context: n_seq_max     = 1
0.00.666.892 I llama_context: n_ctx         = 128
0.00.666.893 I llama_context: n_ctx_per_seq = 128
0.00.666.893 I llama_context: n_batch       = 128
0.00.666.893 I llama_context: n_ubatch      = 128
0.00.666.894 I llama_context: flash_attn    = 0
0.00.666.895 I llama_context: freq_base     = 10000.0
0.00.666.896 I llama_context: freq_scale    = 1
0.00.666.897 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.899 I ggml_metal_init: allocating
0.00.666.918 I ggml_metal_init: found device: Apple M4
0.00.666.927 I ggml_metal_init: picking default device: Apple M4
0.00.668.263 I ggml_metal_init: using embedded metal library
0.00.674.587 I ggml_metal_init: GPU name:   Apple M4
0.00.674.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.593 I ggml_metal_init: simdgroup reduction   = true
0.00.674.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.594 I ggml_metal_init: has residency sets    = true
0.00.674.594 I ggml_metal_init: has bfloat            = true
0.00.674.594 I ggml_metal_init: use bfloat            = true
0.00.674.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.302 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.818 I init:      Metal KV buffer size =    24.00 MiB
0.00.694.821 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.694.861 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.698.066 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.698.068 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.698.068 I llama_context: graph nodes  = 967
0.00.698.069 I llama_context: graph splits = 2
0.00.698.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.705 I 
0.00.729.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.811 I perplexity: tokenizing the input ..
0.00.739.326 I perplexity: tokenization took 9.513 ms
0.00.739.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.774 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.875.079 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.875.101 I llama_perf_context_print:        load time =     719.46 ms
0.00.875.102 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.75 tokens per second)
0.00.875.102 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.103 I llama_perf_context_print:       total time =     145.40 ms /   129 tokens
0.00.875.639 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.093s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.625 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.188 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.086 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.087 I llama_model_loader: - type  f32:  194 tensors
0.00.025.087 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.088 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.088 I print_info: file format = GGUF V3 (latest)
0.00.025.089 I print_info: file type   = Q5_1
0.00.025.090 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.278 I load: special tokens cache size = 25
0.00.052.531 I load: token to piece cache size = 0.2984 MB
0.00.052.536 I print_info: arch             = gptneox
0.00.052.536 I print_info: vocab_only       = 0
0.00.052.536 I print_info: n_ctx_train      = 2048
0.00.052.536 I print_info: n_embd           = 2048
0.00.052.536 I print_info: n_layer          = 24
0.00.052.540 I print_info: n_head           = 16
0.00.052.541 I print_info: n_head_kv        = 16
0.00.052.541 I print_info: n_rot            = 32
0.00.052.541 I print_info: n_swa            = 0
0.00.052.542 I print_info: n_embd_head_k    = 128
0.00.052.542 I print_info: n_embd_head_v    = 128
0.00.052.542 I print_info: n_gqa            = 1
0.00.052.543 I print_info: n_embd_k_gqa     = 2048
0.00.052.547 I print_info: n_embd_v_gqa     = 2048
0.00.052.547 I print_info: f_norm_eps       = 1.0e-05
0.00.052.547 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.549 I print_info: f_logit_scale    = 0.0e+00
0.00.052.549 I print_info: n_ff             = 8192
0.00.052.550 I print_info: n_expert         = 0
0.00.052.550 I print_info: n_expert_used    = 0
0.00.052.550 I print_info: causal attn      = 1
0.00.052.550 I print_info: pooling type     = 0
0.00.052.550 I print_info: rope type        = 2
0.00.052.550 I print_info: rope scaling     = linear
0.00.052.551 I print_info: freq_base_train  = 10000.0
0.00.052.551 I print_info: freq_scale_train = 1
0.00.052.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.551 I print_info: rope_finetuned   = unknown
0.00.052.552 I print_info: ssm_d_conv       = 0
0.00.052.552 I print_info: ssm_d_inner      = 0
0.00.052.552 I print_info: ssm_d_state      = 0
0.00.052.552 I print_info: ssm_dt_rank      = 0
0.00.052.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.552 I print_info: model type       = 1.4B
0.00.052.553 I print_info: model params     = 1.41 B
0.00.052.553 I print_info: general.name     = 1.4B
0.00.052.554 I print_info: vocab type       = BPE
0.00.052.554 I print_info: n_vocab          = 50304
0.00.052.555 I print_info: n_merges         = 50009
0.00.052.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.557 I print_info: LF token         = 128 'Ä'
0.00.052.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.557 I print_info: max token length = 1024
0.00.683.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.302 I load_tensors: offloading output layer to GPU
0.00.683.303 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.342 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.683.344 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.684.927 I llama_context: n_seq_max     = 1
0.00.684.932 I llama_context: n_ctx         = 128
0.00.684.933 I llama_context: n_ctx_per_seq = 128
0.00.684.933 I llama_context: n_batch       = 128
0.00.684.934 I llama_context: n_ubatch      = 128
0.00.684.934 I llama_context: flash_attn    = 0
0.00.684.936 I llama_context: freq_base     = 10000.0
0.00.684.937 I llama_context: freq_scale    = 1
0.00.684.937 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.944 I ggml_metal_init: allocating
0.00.685.028 I ggml_metal_init: found device: Apple M4
0.00.685.037 I ggml_metal_init: picking default device: Apple M4
0.00.686.580 I ggml_metal_init: using embedded metal library
0.00.693.278 I ggml_metal_init: GPU name:   Apple M4
0.00.693.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.289 I ggml_metal_init: simdgroup reduction   = true
0.00.693.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.290 I ggml_metal_init: has residency sets    = true
0.00.693.290 I ggml_metal_init: has bfloat            = true
0.00.693.290 I ggml_metal_init: use bfloat            = true
0.00.693.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.710.082 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.509 I init:      Metal KV buffer size =    24.00 MiB
0.00.713.514 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.713.563 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.716.759 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.716.761 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.716.761 I llama_context: graph nodes  = 967
0.00.716.762 I llama_context: graph splits = 2
0.00.716.765 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.716.765 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.948 I 
0.00.744.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.038 I perplexity: tokenizing the input ..
0.00.753.061 I perplexity: tokenization took 9.021 ms
0.00.753.073 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.679 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.888.112 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.888.126 I llama_perf_context_print:        load time =     735.31 ms
0.00.888.127 I llama_perf_context_print: prompt eval time =     133.37 ms /   128 tokens (    1.04 ms per token,   959.71 tokens per second)
0.00.888.128 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.128 I llama_perf_context_print:       total time =     144.18 ms /   129 tokens
0.00.888.651 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.093s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.362 I llama_model_loader: - type  f32:  194 tensors
0.00.025.362 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.363 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.364 I print_info: file format = GGUF V3 (latest)
0.00.025.364 I print_info: file type   = Q2_K - Medium
0.00.025.366 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.387 I load: special tokens cache size = 25
0.00.050.439 I load: token to piece cache size = 0.2984 MB
0.00.050.442 I print_info: arch             = gptneox
0.00.050.442 I print_info: vocab_only       = 0
0.00.050.442 I print_info: n_ctx_train      = 2048
0.00.050.443 I print_info: n_embd           = 2048
0.00.050.443 I print_info: n_layer          = 24
0.00.050.446 I print_info: n_head           = 16
0.00.050.446 I print_info: n_head_kv        = 16
0.00.050.447 I print_info: n_rot            = 32
0.00.050.447 I print_info: n_swa            = 0
0.00.050.447 I print_info: n_embd_head_k    = 128
0.00.050.447 I print_info: n_embd_head_v    = 128
0.00.050.448 I print_info: n_gqa            = 1
0.00.050.449 I print_info: n_embd_k_gqa     = 2048
0.00.050.449 I print_info: n_embd_v_gqa     = 2048
0.00.050.450 I print_info: f_norm_eps       = 1.0e-05
0.00.050.450 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.450 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.450 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.451 I print_info: f_logit_scale    = 0.0e+00
0.00.050.451 I print_info: n_ff             = 8192
0.00.050.451 I print_info: n_expert         = 0
0.00.050.452 I print_info: n_expert_used    = 0
0.00.050.452 I print_info: causal attn      = 1
0.00.050.452 I print_info: pooling type     = 0
0.00.050.452 I print_info: rope type        = 2
0.00.050.452 I print_info: rope scaling     = linear
0.00.050.453 I print_info: freq_base_train  = 10000.0
0.00.050.453 I print_info: freq_scale_train = 1
0.00.050.453 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.453 I print_info: rope_finetuned   = unknown
0.00.050.453 I print_info: ssm_d_conv       = 0
0.00.050.453 I print_info: ssm_d_inner      = 0
0.00.050.454 I print_info: ssm_d_state      = 0
0.00.050.454 I print_info: ssm_dt_rank      = 0
0.00.050.454 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.454 I print_info: model type       = 1.4B
0.00.050.456 I print_info: model params     = 1.41 B
0.00.050.456 I print_info: general.name     = 1.4B
0.00.050.457 I print_info: vocab type       = BPE
0.00.050.457 I print_info: n_vocab          = 50304
0.00.050.457 I print_info: n_merges         = 50009
0.00.050.457 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.458 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.458 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.458 I print_info: LF token         = 128 'Ä'
0.00.050.459 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.459 I print_info: max token length = 1024
0.00.440.595 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.608 I load_tensors: offloading output layer to GPU
0.00.440.609 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.637 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.440.638 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.442.019 I llama_context: n_seq_max     = 1
0.00.442.027 I llama_context: n_ctx         = 128
0.00.442.027 I llama_context: n_ctx_per_seq = 128
0.00.442.028 I llama_context: n_batch       = 128
0.00.442.028 I llama_context: n_ubatch      = 128
0.00.442.029 I llama_context: flash_attn    = 0
0.00.442.031 I llama_context: freq_base     = 10000.0
0.00.442.031 I llama_context: freq_scale    = 1
0.00.442.032 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.035 I ggml_metal_init: allocating
0.00.442.106 I ggml_metal_init: found device: Apple M4
0.00.442.115 I ggml_metal_init: picking default device: Apple M4
0.00.443.900 I ggml_metal_init: using embedded metal library
0.00.449.718 I ggml_metal_init: GPU name:   Apple M4
0.00.449.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.740 I ggml_metal_init: simdgroup reduction   = true
0.00.449.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.740 I ggml_metal_init: has residency sets    = true
0.00.449.740 I ggml_metal_init: has bfloat            = true
0.00.449.741 I ggml_metal_init: use bfloat            = true
0.00.449.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.579 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.475.402 I init:      Metal KV buffer size =    24.00 MiB
0.00.475.407 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.475.450 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.479.032 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.479.034 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.479.035 I llama_context: graph nodes  = 967
0.00.479.035 I llama_context: graph splits = 2
0.00.479.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.893 I 
0.00.506.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.992 I perplexity: tokenizing the input ..
0.00.515.151 I perplexity: tokenization took 8.158 ms
0.00.515.165 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.624 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.647.927 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.647.935 I llama_perf_context_print:        load time =     496.79 ms
0.00.647.937 I llama_perf_context_print: prompt eval time =     131.22 ms /   128 tokens (    1.03 ms per token,   975.43 tokens per second)
0.00.647.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.940 I llama_perf_context_print:       total time =     141.05 ms /   129 tokens
0.00.648.439 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.093s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.995 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.913 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.721 I llama_model_loader: - type  f32:  194 tensors
0.00.024.721 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.722 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.722 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.722 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.723 I print_info: file format = GGUF V3 (latest)
0.00.024.723 I print_info: file type   = Q3_K - Medium
0.00.024.725 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.724 I load: special tokens cache size = 25
0.00.051.005 I load: token to piece cache size = 0.2984 MB
0.00.051.012 I print_info: arch             = gptneox
0.00.051.013 I print_info: vocab_only       = 0
0.00.051.013 I print_info: n_ctx_train      = 2048
0.00.051.013 I print_info: n_embd           = 2048
0.00.051.013 I print_info: n_layer          = 24
0.00.051.018 I print_info: n_head           = 16
0.00.051.018 I print_info: n_head_kv        = 16
0.00.051.019 I print_info: n_rot            = 32
0.00.051.019 I print_info: n_swa            = 0
0.00.051.019 I print_info: n_embd_head_k    = 128
0.00.051.022 I print_info: n_embd_head_v    = 128
0.00.051.023 I print_info: n_gqa            = 1
0.00.051.023 I print_info: n_embd_k_gqa     = 2048
0.00.051.024 I print_info: n_embd_v_gqa     = 2048
0.00.051.024 I print_info: f_norm_eps       = 1.0e-05
0.00.051.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.025 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.025 I print_info: f_logit_scale    = 0.0e+00
0.00.051.025 I print_info: n_ff             = 8192
0.00.051.025 I print_info: n_expert         = 0
0.00.051.026 I print_info: n_expert_used    = 0
0.00.051.026 I print_info: causal attn      = 1
0.00.051.026 I print_info: pooling type     = 0
0.00.051.026 I print_info: rope type        = 2
0.00.051.027 I print_info: rope scaling     = linear
0.00.051.027 I print_info: freq_base_train  = 10000.0
0.00.051.028 I print_info: freq_scale_train = 1
0.00.051.028 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.028 I print_info: rope_finetuned   = unknown
0.00.051.028 I print_info: ssm_d_conv       = 0
0.00.051.028 I print_info: ssm_d_inner      = 0
0.00.051.028 I print_info: ssm_d_state      = 0
0.00.051.028 I print_info: ssm_dt_rank      = 0
0.00.051.029 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.029 I print_info: model type       = 1.4B
0.00.051.029 I print_info: model params     = 1.41 B
0.00.051.029 I print_info: general.name     = 1.4B
0.00.051.030 I print_info: vocab type       = BPE
0.00.051.030 I print_info: n_vocab          = 50304
0.00.051.030 I print_info: n_merges         = 50009
0.00.051.030 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.030 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.030 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.031 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.031 I print_info: LF token         = 128 'Ä'
0.00.051.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.031 I print_info: max token length = 1024
0.00.533.855 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.861 I load_tensors: offloading output layer to GPU
0.00.533.862 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.894 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.533.897 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.535.106 I llama_context: n_seq_max     = 1
0.00.535.120 I llama_context: n_ctx         = 128
0.00.535.121 I llama_context: n_ctx_per_seq = 128
0.00.535.121 I llama_context: n_batch       = 128
0.00.535.121 I llama_context: n_ubatch      = 128
0.00.535.122 I llama_context: flash_attn    = 0
0.00.535.123 I llama_context: freq_base     = 10000.0
0.00.535.124 I llama_context: freq_scale    = 1
0.00.535.124 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.535.127 I ggml_metal_init: allocating
0.00.535.202 I ggml_metal_init: found device: Apple M4
0.00.535.210 I ggml_metal_init: picking default device: Apple M4
0.00.536.814 I ggml_metal_init: using embedded metal library
0.00.542.158 I ggml_metal_init: GPU name:   Apple M4
0.00.542.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.173 I ggml_metal_init: simdgroup reduction   = true
0.00.542.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.174 I ggml_metal_init: has residency sets    = true
0.00.542.174 I ggml_metal_init: has bfloat            = true
0.00.542.175 I ggml_metal_init: use bfloat            = true
0.00.542.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.735 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.914 I init:      Metal KV buffer size =    24.00 MiB
0.00.564.917 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.963 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.567.390 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.567.391 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.567.391 I llama_context: graph nodes  = 967
0.00.567.391 I llama_context: graph splits = 2
0.00.567.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.567.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.030 I 
0.00.591.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.073 I perplexity: tokenizing the input ..
0.00.598.410 I perplexity: tokenization took 7.336 ms
0.00.598.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.658 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.730.994 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.731.003 I llama_perf_context_print:        load time =     582.03 ms
0.00.731.004 I llama_perf_context_print: prompt eval time =     130.99 ms /   128 tokens (    1.02 ms per token,   977.20 tokens per second)
0.00.731.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.007 I llama_perf_context_print:       total time =     139.97 ms /   129 tokens
0.00.731.536 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.090s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.376 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.381 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.155 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.156 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.156 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.157 I llama_model_loader: - type  f32:  194 tensors
0.00.027.157 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.158 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.158 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.158 I print_info: file format = GGUF V3 (latest)
0.00.027.159 I print_info: file type   = Q4_K - Medium
0.00.027.160 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.605 I load: special tokens cache size = 25
0.00.052.743 I load: token to piece cache size = 0.2984 MB
0.00.052.747 I print_info: arch             = gptneox
0.00.052.747 I print_info: vocab_only       = 0
0.00.052.748 I print_info: n_ctx_train      = 2048
0.00.052.748 I print_info: n_embd           = 2048
0.00.052.748 I print_info: n_layer          = 24
0.00.052.752 I print_info: n_head           = 16
0.00.052.753 I print_info: n_head_kv        = 16
0.00.052.753 I print_info: n_rot            = 32
0.00.052.753 I print_info: n_swa            = 0
0.00.052.753 I print_info: n_embd_head_k    = 128
0.00.052.753 I print_info: n_embd_head_v    = 128
0.00.052.754 I print_info: n_gqa            = 1
0.00.052.755 I print_info: n_embd_k_gqa     = 2048
0.00.052.756 I print_info: n_embd_v_gqa     = 2048
0.00.052.756 I print_info: f_norm_eps       = 1.0e-05
0.00.052.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.757 I print_info: f_logit_scale    = 0.0e+00
0.00.052.758 I print_info: n_ff             = 8192
0.00.052.758 I print_info: n_expert         = 0
0.00.052.758 I print_info: n_expert_used    = 0
0.00.052.758 I print_info: causal attn      = 1
0.00.052.758 I print_info: pooling type     = 0
0.00.052.758 I print_info: rope type        = 2
0.00.052.758 I print_info: rope scaling     = linear
0.00.052.759 I print_info: freq_base_train  = 10000.0
0.00.052.759 I print_info: freq_scale_train = 1
0.00.052.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.760 I print_info: rope_finetuned   = unknown
0.00.052.760 I print_info: ssm_d_conv       = 0
0.00.052.760 I print_info: ssm_d_inner      = 0
0.00.052.761 I print_info: ssm_d_state      = 0
0.00.052.763 I print_info: ssm_dt_rank      = 0
0.00.052.763 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.763 I print_info: model type       = 1.4B
0.00.052.763 I print_info: model params     = 1.41 B
0.00.052.764 I print_info: general.name     = 1.4B
0.00.052.764 I print_info: vocab type       = BPE
0.00.052.764 I print_info: n_vocab          = 50304
0.00.052.764 I print_info: n_merges         = 50009
0.00.052.764 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.765 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.765 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.765 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.765 I print_info: LF token         = 128 'Ä'
0.00.052.765 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.766 I print_info: max token length = 1024
0.00.552.120 I load_tensors: offloading 24 repeating layers to GPU
0.00.552.132 I load_tensors: offloading output layer to GPU
0.00.552.133 I load_tensors: offloaded 25/25 layers to GPU
0.00.552.167 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.552.169 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.553.666 I llama_context: n_seq_max     = 1
0.00.553.671 I llama_context: n_ctx         = 128
0.00.553.671 I llama_context: n_ctx_per_seq = 128
0.00.553.672 I llama_context: n_batch       = 128
0.00.553.672 I llama_context: n_ubatch      = 128
0.00.553.673 I llama_context: flash_attn    = 0
0.00.553.675 I llama_context: freq_base     = 10000.0
0.00.553.675 I llama_context: freq_scale    = 1
0.00.553.676 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.553.683 I ggml_metal_init: allocating
0.00.553.781 I ggml_metal_init: found device: Apple M4
0.00.553.790 I ggml_metal_init: picking default device: Apple M4
0.00.555.457 I ggml_metal_init: using embedded metal library
0.00.561.921 I ggml_metal_init: GPU name:   Apple M4
0.00.561.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.561.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.561.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.561.933 I ggml_metal_init: simdgroup reduction   = true
0.00.561.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.561.933 I ggml_metal_init: has residency sets    = true
0.00.561.933 I ggml_metal_init: has bfloat            = true
0.00.561.934 I ggml_metal_init: use bfloat            = true
0.00.561.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.561.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.127 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.583.611 I init:      Metal KV buffer size =    24.00 MiB
0.00.583.615 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.583.657 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.586.983 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.586.985 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.586.985 I llama_context: graph nodes  = 967
0.00.586.986 I llama_context: graph splits = 2
0.00.586.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.586.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.856 I 
0.00.617.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.951 I perplexity: tokenizing the input ..
0.00.627.820 I perplexity: tokenization took 9.867 ms
0.00.627.834 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.657 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.762.959 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.762.970 I llama_perf_context_print:        load time =     606.69 ms
0.00.762.971 I llama_perf_context_print: prompt eval time =     133.59 ms /   128 tokens (    1.04 ms per token,   958.14 tokens per second)
0.00.762.971 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.972 I llama_perf_context_print:       total time =     145.12 ms /   129 tokens
0.00.763.507 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.093s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.543 I llama_model_loader: - type  f32:  194 tensors
0.00.024.544 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.544 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.545 I print_info: file format = GGUF V3 (latest)
0.00.024.545 I print_info: file type   = Q5_K - Medium
0.00.024.546 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.639 I load: special tokens cache size = 25
0.00.050.932 I load: token to piece cache size = 0.2984 MB
0.00.050.936 I print_info: arch             = gptneox
0.00.050.936 I print_info: vocab_only       = 0
0.00.050.937 I print_info: n_ctx_train      = 2048
0.00.050.937 I print_info: n_embd           = 2048
0.00.050.937 I print_info: n_layer          = 24
0.00.050.941 I print_info: n_head           = 16
0.00.050.941 I print_info: n_head_kv        = 16
0.00.050.944 I print_info: n_rot            = 32
0.00.050.944 I print_info: n_swa            = 0
0.00.050.945 I print_info: n_embd_head_k    = 128
0.00.050.945 I print_info: n_embd_head_v    = 128
0.00.050.945 I print_info: n_gqa            = 1
0.00.050.946 I print_info: n_embd_k_gqa     = 2048
0.00.050.947 I print_info: n_embd_v_gqa     = 2048
0.00.050.947 I print_info: f_norm_eps       = 1.0e-05
0.00.050.948 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.948 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.948 I print_info: f_logit_scale    = 0.0e+00
0.00.050.949 I print_info: n_ff             = 8192
0.00.050.949 I print_info: n_expert         = 0
0.00.050.949 I print_info: n_expert_used    = 0
0.00.050.949 I print_info: causal attn      = 1
0.00.050.949 I print_info: pooling type     = 0
0.00.050.950 I print_info: rope type        = 2
0.00.050.950 I print_info: rope scaling     = linear
0.00.050.950 I print_info: freq_base_train  = 10000.0
0.00.050.951 I print_info: freq_scale_train = 1
0.00.050.951 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.951 I print_info: rope_finetuned   = unknown
0.00.050.951 I print_info: ssm_d_conv       = 0
0.00.050.951 I print_info: ssm_d_inner      = 0
0.00.050.951 I print_info: ssm_d_state      = 0
0.00.050.952 I print_info: ssm_dt_rank      = 0
0.00.050.952 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.952 I print_info: model type       = 1.4B
0.00.050.952 I print_info: model params     = 1.41 B
0.00.050.952 I print_info: general.name     = 1.4B
0.00.050.953 I print_info: vocab type       = BPE
0.00.050.953 I print_info: n_vocab          = 50304
0.00.050.953 I print_info: n_merges         = 50009
0.00.050.954 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.954 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: LF token         = 128 'Ä'
0.00.050.955 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: max token length = 1024
0.00.604.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.230 I load_tensors: offloading output layer to GPU
0.00.604.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.263 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.280 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.605.937 I llama_context: n_seq_max     = 1
0.00.605.941 I llama_context: n_ctx         = 128
0.00.605.941 I llama_context: n_ctx_per_seq = 128
0.00.605.942 I llama_context: n_batch       = 128
0.00.605.942 I llama_context: n_ubatch      = 128
0.00.605.943 I llama_context: flash_attn    = 0
0.00.605.944 I llama_context: freq_base     = 10000.0
0.00.605.944 I llama_context: freq_scale    = 1
0.00.605.945 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.947 I ggml_metal_init: allocating
0.00.605.966 I ggml_metal_init: found device: Apple M4
0.00.605.972 I ggml_metal_init: picking default device: Apple M4
0.00.607.299 I ggml_metal_init: using embedded metal library
0.00.613.705 I ggml_metal_init: GPU name:   Apple M4
0.00.613.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.711 I ggml_metal_init: simdgroup reduction   = true
0.00.613.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.712 I ggml_metal_init: has residency sets    = true
0.00.613.712 I ggml_metal_init: has bfloat            = true
0.00.613.712 I ggml_metal_init: use bfloat            = true
0.00.613.713 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.590 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.078 I init:      Metal KV buffer size =    24.00 MiB
0.00.634.087 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.140 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.637.239 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.637.241 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.637.241 I llama_context: graph nodes  = 967
0.00.637.241 I llama_context: graph splits = 2
0.00.637.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.420 I 
0.00.667.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.542 I perplexity: tokenizing the input ..
0.00.676.480 I perplexity: tokenization took 8.936 ms
0.00.676.494 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.024 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.817.363 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.817.376 I llama_perf_context_print:        load time =     658.68 ms
0.00.817.377 I llama_perf_context_print: prompt eval time =     139.30 ms /   128 tokens (    1.09 ms per token,   918.90 tokens per second)
0.00.817.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.378 I llama_perf_context_print:       total time =     149.96 ms /   129 tokens
0.00.817.941 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.092s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.657 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.441 I llama_model_loader: - type  f32:  194 tensors
0.00.025.441 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.441 I print_info: file format = GGUF V3 (latest)
0.00.025.442 I print_info: file type   = Q6_K
0.00.025.443 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.482 I load: special tokens cache size = 25
0.00.050.498 I load: token to piece cache size = 0.2984 MB
0.00.050.501 I print_info: arch             = gptneox
0.00.050.501 I print_info: vocab_only       = 0
0.00.050.502 I print_info: n_ctx_train      = 2048
0.00.050.502 I print_info: n_embd           = 2048
0.00.050.502 I print_info: n_layer          = 24
0.00.050.505 I print_info: n_head           = 16
0.00.050.505 I print_info: n_head_kv        = 16
0.00.050.506 I print_info: n_rot            = 32
0.00.050.506 I print_info: n_swa            = 0
0.00.050.506 I print_info: n_embd_head_k    = 128
0.00.050.506 I print_info: n_embd_head_v    = 128
0.00.050.507 I print_info: n_gqa            = 1
0.00.050.508 I print_info: n_embd_k_gqa     = 2048
0.00.050.509 I print_info: n_embd_v_gqa     = 2048
0.00.050.509 I print_info: f_norm_eps       = 1.0e-05
0.00.050.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.510 I print_info: f_logit_scale    = 0.0e+00
0.00.050.511 I print_info: n_ff             = 8192
0.00.050.511 I print_info: n_expert         = 0
0.00.050.511 I print_info: n_expert_used    = 0
0.00.050.511 I print_info: causal attn      = 1
0.00.050.511 I print_info: pooling type     = 0
0.00.050.511 I print_info: rope type        = 2
0.00.050.512 I print_info: rope scaling     = linear
0.00.050.512 I print_info: freq_base_train  = 10000.0
0.00.050.512 I print_info: freq_scale_train = 1
0.00.050.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.513 I print_info: rope_finetuned   = unknown
0.00.050.513 I print_info: ssm_d_conv       = 0
0.00.050.513 I print_info: ssm_d_inner      = 0
0.00.050.513 I print_info: ssm_d_state      = 0
0.00.050.513 I print_info: ssm_dt_rank      = 0
0.00.050.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.514 I print_info: model type       = 1.4B
0.00.050.514 I print_info: model params     = 1.41 B
0.00.050.514 I print_info: general.name     = 1.4B
0.00.050.515 I print_info: vocab type       = BPE
0.00.050.521 I print_info: n_vocab          = 50304
0.00.050.522 I print_info: n_merges         = 50009
0.00.050.522 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.522 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.523 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.523 I print_info: LF token         = 128 'Ä'
0.00.050.523 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.523 I print_info: max token length = 1024
0.00.295.156 I load_tensors: offloading 24 repeating layers to GPU
0.00.295.161 I load_tensors: offloading output layer to GPU
0.00.295.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.295.186 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.295.189 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.296.491 I llama_context: n_seq_max     = 1
0.00.296.494 I llama_context: n_ctx         = 128
0.00.296.495 I llama_context: n_ctx_per_seq = 128
0.00.296.495 I llama_context: n_batch       = 128
0.00.296.495 I llama_context: n_ubatch      = 128
0.00.296.496 I llama_context: flash_attn    = 0
0.00.296.497 I llama_context: freq_base     = 10000.0
0.00.296.497 I llama_context: freq_scale    = 1
0.00.296.498 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.296.500 I ggml_metal_init: allocating
0.00.296.535 I ggml_metal_init: found device: Apple M4
0.00.296.541 I ggml_metal_init: picking default device: Apple M4
0.00.297.903 I ggml_metal_init: using embedded metal library
0.00.303.934 I ggml_metal_init: GPU name:   Apple M4
0.00.303.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.303.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.303.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.303.940 I ggml_metal_init: simdgroup reduction   = true
0.00.303.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.303.940 I ggml_metal_init: has residency sets    = true
0.00.303.941 I ggml_metal_init: has bfloat            = true
0.00.303.941 I ggml_metal_init: use bfloat            = true
0.00.303.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.303.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.320.261 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.323.715 I init:      Metal KV buffer size =    24.00 MiB
0.00.323.719 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.323.759 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.327.168 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.327.170 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.327.171 I llama_context: graph nodes  = 967
0.00.327.171 I llama_context: graph splits = 2
0.00.327.175 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.327.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.818 I 
0.00.363.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.927 I perplexity: tokenizing the input ..
0.00.374.629 I perplexity: tokenization took 10.701 ms
0.00.374.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.370 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.515.776 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.515.788 I llama_perf_context_print:        load time =     353.73 ms
0.00.515.789 I llama_perf_context_print: prompt eval time =     139.49 ms /   128 tokens (    1.09 ms per token,   917.64 tokens per second)
0.00.515.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.515.791 I llama_perf_context_print:       total time =     151.98 ms /   129 tokens
0.00.516.320 I ggml_metal_free: deallocating

real	0m0.532s
user	0m0.091s
sys	0m0.091s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4580 (a0c500b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.475 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.665 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.669 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.671 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.528 I llama_model_loader: - type  f32:  194 tensors
0.00.052.529 I llama_model_loader: - type  f16:   98 tensors
0.00.052.529 I print_info: file format = GGUF V3 (latest)
0.00.052.530 I print_info: file type   = all F32 (guessed)
0.00.052.532 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.857 I load: special tokens cache size = 25
0.00.084.298 I load: token to piece cache size = 0.2984 MB
0.00.084.301 I print_info: arch             = gptneox
0.00.084.302 I print_info: vocab_only       = 0
0.00.084.302 I print_info: n_ctx_train      = 2048
0.00.084.302 I print_info: n_embd           = 2048
0.00.084.302 I print_info: n_layer          = 24
0.00.084.305 I print_info: n_head           = 16
0.00.084.306 I print_info: n_head_kv        = 16
0.00.084.306 I print_info: n_rot            = 32
0.00.084.306 I print_info: n_swa            = 0
0.00.084.306 I print_info: n_embd_head_k    = 128
0.00.084.307 I print_info: n_embd_head_v    = 128
0.00.084.307 I print_info: n_gqa            = 1
0.00.084.308 I print_info: n_embd_k_gqa     = 2048
0.00.084.308 I print_info: n_embd_v_gqa     = 2048
0.00.084.309 I print_info: f_norm_eps       = 1.0e-05
0.00.084.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.310 I print_info: f_logit_scale    = 0.0e+00
0.00.084.310 I print_info: n_ff             = 8192
0.00.084.310 I print_info: n_expert         = 0
0.00.084.310 I print_info: n_expert_used    = 0
0.00.084.311 I print_info: causal attn      = 1
0.00.084.311 I print_info: pooling type     = 0
0.00.084.311 I print_info: rope type        = 2
0.00.084.311 I print_info: rope scaling     = linear
0.00.084.311 I print_info: freq_base_train  = 10000.0
0.00.084.312 I print_info: freq_scale_train = 1
0.00.084.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.312 I print_info: rope_finetuned   = unknown
0.00.084.312 I print_info: ssm_d_conv       = 0
0.00.084.312 I print_info: ssm_d_inner      = 0
0.00.084.312 I print_info: ssm_d_state      = 0
0.00.084.313 I print_info: ssm_dt_rank      = 0
0.00.084.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.313 I print_info: model type       = 1.4B
0.00.084.313 I print_info: model params     = 1.41 B
0.00.084.313 I print_info: general.name     = 1.4B
0.00.084.315 I print_info: vocab type       = BPE
0.00.084.315 I print_info: n_vocab          = 50304
0.00.084.315 I print_info: n_merges         = 50009
0.00.084.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.316 I print_info: LF token         = 128 'Ä'
0.00.084.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.317 I print_info: max token length = 1024
0.01.197.132 I load_tensors: offloading 24 repeating layers to GPU
0.01.197.136 I load_tensors: offloading output layer to GPU
0.01.197.137 I load_tensors: offloaded 25/25 layers to GPU
0.01.197.159 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.197.161 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.198.164 I llama_context: n_seq_max     = 1
0.01.198.165 I llama_context: n_ctx         = 128
0.01.198.165 I llama_context: n_ctx_per_seq = 128
0.01.198.165 I llama_context: n_batch       = 128
0.01.198.166 I llama_context: n_ubatch      = 128
0.01.198.166 I llama_context: flash_attn    = 0
0.01.198.166 I llama_context: freq_base     = 10000.0
0.01.198.167 I llama_context: freq_scale    = 1
0.01.198.167 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.198.168 I ggml_metal_init: allocating
0.01.198.194 I ggml_metal_init: found device: Apple M4
0.01.198.197 I ggml_metal_init: picking default device: Apple M4
0.01.199.165 I ggml_metal_init: using embedded metal library
0.01.203.009 I ggml_metal_init: GPU name:   Apple M4
0.01.203.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.203.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.203.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.203.013 I ggml_metal_init: simdgroup reduction   = true
0.01.203.013 I ggml_metal_init: simdgroup matrix mul. = true
0.01.203.013 I ggml_metal_init: has residency sets    = true
0.01.203.013 I ggml_metal_init: has bfloat            = true
0.01.203.013 I ggml_metal_init: use bfloat            = true
0.01.203.014 I ggml_metal_init: hasUnifiedMemory      = true
0.01.203.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.213.681 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.215.424 I init:      Metal KV buffer size =    24.00 MiB
0.01.215.426 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.215.452 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.217.130 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.217.131 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.217.132 I llama_context: graph nodes  = 967
0.01.217.132 I llama_context: graph splits = 2
0.01.217.133 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.217.134 I 
0.01.217.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.217.174 I compute_imatrix: tokenizing the input ..
0.01.224.979 I compute_imatrix: tokenization took 7.804 ms
0.01.224.981 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.496.453 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.499.197 I llama_perf_context_print:        load time =    1474.98 ms
0.01.499.198 I llama_perf_context_print: prompt eval time =     269.73 ms /   128 tokens (    2.11 ms per token,   474.56 tokens per second)
0.01.499.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.499.199 I llama_perf_context_print:       total time =    1477.72 ms /   129 tokens
0.01.499.898 I ggml_metal_free: deallocating

real	0m1.684s
user	0m0.143s
sys	0m0.227s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4580 (a0c500b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11de04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11de08630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11de08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11de08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11de09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11de097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11de09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11de0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11de0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11de0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11de0ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11de0b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11de0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11de0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11de0cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11de0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11de0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11de0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11de0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11de0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11de0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11de10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11de10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11de111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11de11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11de11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11de11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11de12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11de12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11de12e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11de13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11de13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11de13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11de14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11de14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11de14970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11de14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11de15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11de156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11de15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11de15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11de16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11de16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11de16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11de17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11de175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11de17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11de17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11de18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11de18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11de18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11de19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11de19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11de19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11de1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11de1a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11de1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11de1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11de1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11de1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11de1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11de1c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11de1c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11de1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11de1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11de1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11de1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11de1df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11de1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11de1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11de1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11de1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11de1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11de1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11de20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11de208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11de20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11de21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11de219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11de21f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11de22530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11de22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11de23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11de23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11de23bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11de241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11de24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11de24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11de252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11de25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11de25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11de263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11de26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11de26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11de274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11de27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11de28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11de18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11de28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11de28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11de29070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11de29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11de29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11de2a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11de2a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11de2ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11de2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11de2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11de2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11de2c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11de2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11de2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11de2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11de2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11de2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11de2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11de2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11de2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11de2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11de2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11de2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11de30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11de30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11de30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11de31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11de31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11de31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11de32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11de32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11de32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11de32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11de33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11de33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11de33e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11de34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11de34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11de34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11de35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11de35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11de35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11de36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11de36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11de36b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11de37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11de37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11de37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11de37f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11de38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11de38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11de38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11de39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11de39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11de39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11de3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11de3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11de3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11de3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11de3b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11de3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11de3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11de3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11de3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11de3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11de3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11de3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11de3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11de3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11de3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11de3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11de3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11de3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11de3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11de40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11de40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11de40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11de41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11de41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11de41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11de41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11de42460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11de42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11de42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11de43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11de43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11de43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11de44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11de44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11de44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11de45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11de45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11de45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11de46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11de46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11de46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11de47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11de475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11de47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11de48120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11de48730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11de48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11de49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11de49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11de49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11de4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11de4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11de4aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11de4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11de4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11de4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11de4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11de4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11de4d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11de4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11de4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11de4e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11de4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11de4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11de4f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11de4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11de4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11de50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11de506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11de50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11de51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11de516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11de51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11de52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11de52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11de52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11de53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11de53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11de53bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11de54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11de54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11de54bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11de55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11de55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11de55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11de56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11de56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11de56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11de570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11de57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11de57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11de580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11de58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11de58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11de590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11de59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11de59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11de5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11de5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11de5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11de5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11de5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11de5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11de5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11de5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11de5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11de5d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11de5d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11de5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11de5e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11de5e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11de5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11de5f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11de5f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11de5fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11de5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11de603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11de60840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11de60ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11de61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11de61620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11de61ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11de61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11de62400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11de628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11de62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11de631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11de63680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11de63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11de64070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11de64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11de64eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11de655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11de65cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11de65fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11de667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11de66a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11de67070 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.723.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10df04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10df04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10df05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10df05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10df05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10df06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10df065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10df06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10df06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10df07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10df07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10df07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10df08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10df09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10df09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10df0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10df0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10df0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10df0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10df0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10df0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10df0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10df0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10df0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10df0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10df0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10df0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10df0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10df0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10df0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10df0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10df0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10df10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10df10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10df108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10df10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10df11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10df11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10df11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10df11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10df12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10df127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10df12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10df130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10df13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10df13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10df13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10df14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10df146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10df14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10df14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10df15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10df15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10df15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10df16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10df165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10df16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10df17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10df174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10df17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10df17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10df18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10df18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10df18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10df18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10df193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10df19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10df19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10df1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10df1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10df1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10df1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10df1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10df1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10df1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10df1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10df1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10df1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10df1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10df1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10df1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10df1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10df1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10df1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10df1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10df1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10df1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10df1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10df1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10df1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10df202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10df20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10df20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10df21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10df21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10df218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10df21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10df221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10df22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10df22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10df22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10df23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10df23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10df23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10df240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10df24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10df249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10df24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10df252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10df25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10df25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10df25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10df26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10df268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10df26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10df271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10df27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10df27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10df27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10df28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10df287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10df28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10df290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10df29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10df299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10df29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10df2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10df2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10df2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10df2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10df2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10df2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10df2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10df2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10df2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10df2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10df2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10df2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10df2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10df2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10df2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10df2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10df2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10df2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10df2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10df2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10df2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10df2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10df30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10df30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10df30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10df31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10df315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10df31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10df31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10df32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10df327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10df32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10df33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10df334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10df33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10df33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10df34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10df346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10df34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10df34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10df35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10df35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10df36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10df365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10df36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10df36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10df37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10df37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10df37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10df38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10df384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10df38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10df38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10df39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10df39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10df39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10df39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10df3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10df3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10df3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10df3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10df3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10df3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10df3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10df3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10df3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10df3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10df3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10df3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10df3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10df3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10df3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10df3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10df3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10df3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10df3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10df3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10df3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10df40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10df40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10df40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10df40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10df41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10df41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10df42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10df42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10df42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10df433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10df43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10df43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10df44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10df44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10df45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10df45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10df45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10df461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10df46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10df46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10df47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10df478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10df47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10df48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10df48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10df48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10df49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10df49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10df4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10df4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10df4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10df4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10df4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10df4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10df4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10df4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10df4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10df4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10df4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10df4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10df4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10df4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10df4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10df4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10df4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10df502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10df50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10df50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10df51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10df519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10df51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10df52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10df52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10df530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10df53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10df53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10df54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10df547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10df54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10df55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10df55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10df55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10df56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10df56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10df56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10df57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10df57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10df57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10df58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10df58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10df58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10df59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10df59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10df59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10df5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10df5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10df5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10df5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10df5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10df5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10df5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10df5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10df5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10df5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10df5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10df5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10df5e830 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10cf046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10cf04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10cf04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10cf05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10cf058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10cf05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10cf06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10cf065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10cf06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10cf06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10cf07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10cf07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10cf08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10cf08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10cf09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10cf09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10cf0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10cf0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10cf0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10cf0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10cf0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10cf0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10cf0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10cf0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10cf0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10cf0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10cf0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10cf0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10cf0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10cf0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10cf0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10cf0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10cf0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10cf10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10cf104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10cf10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10cf10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10cf111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10cf11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10cf11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10cf11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10cf123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10cf12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10cf12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10cf13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10cf13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10cf139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10cf13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10cf142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10cf14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10cf14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10cf15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10cf15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10cf158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10cf15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10cf161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10cf16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10cf16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10cf170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10cf17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10cf17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10cf17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10cf18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10cf186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10cf18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10cf18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10cf19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10cf198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10cf19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10cf1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10cf1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10cf1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10cf1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10cf1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10cf1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10cf1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10cf1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10cf1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10cf1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10cf1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10cf1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10cf1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10cf1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10cf1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10cf1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10cf1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10cf1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10cf1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10cf1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10cf1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10cf1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10cf20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10cf20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10cf20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10cf21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10cf214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10cf21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10cf21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10cf22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10cf226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10cf22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10cf22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10cf233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10cf23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10cf23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10cf243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10cf24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10cf24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10cf25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10cf25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10cf259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10cf25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10cf262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10cf26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10cf26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10cf27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10cf27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10cf278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10cf27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10cf281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10cf28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10cf28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10cf28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10cf29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10cf29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10cf29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10cf2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10cf2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10cf2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10cf2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10cf2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10cf2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10cf2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10cf2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10cf2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10cf2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10cf2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10cf2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10cf2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10cf2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10cf2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10cf2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10cf2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10cf2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10cf2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10cf2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10cf2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10cf2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10cf30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10cf306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10cf30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10cf30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10cf31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10cf318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10cf31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10cf32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10cf32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10cf32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10cf32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10cf33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10cf337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10cf33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10cf340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10cf34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10cf34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10cf34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10cf35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10cf356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10cf35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10cf35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10cf36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10cf36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10cf36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10cf37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10cf375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10cf37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10cf37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10cf38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10cf387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10cf38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10cf39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10cf394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10cf39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10cf39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10cf3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10cf3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10cf3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10cf3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10cf3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10cf3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10cf3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10cf3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10cf3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10cf3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10cf3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10cf3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10cf3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10cf3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10cf3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10cf3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10cf3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10cf3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10cf3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10cf3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10cf3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10cf3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10cf403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10cf40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10cf40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10cf41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10cf41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10cf41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10cf42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10cf426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10cf42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10cf42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10cf433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10cf43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10cf43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10cf44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10cf445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10cf44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10cf44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10cf45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10cf45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10cf45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10cf46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10cf464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10cf46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10cf46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10cf47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10cf47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10cf47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10cf47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10cf483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10cf48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10cf48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10cf49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10cf49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10cf49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10cf49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10cf4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10cf4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10cf4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10cf4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10cf4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10cf4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10cf4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10cf4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10cf4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10cf4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10cf4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10cf4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10cf4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10cf4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cf4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cf4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cf4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cf4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cf4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cf4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cf4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cf50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cf50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cf508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cf50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cf511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cf51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cf51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cf51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cf52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cf52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cf52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cf530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cf53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cf539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cf53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cf542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cf54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cf54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cf54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cf55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cf558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cf56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cf56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cf57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cf578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cf57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cf57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cf585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10cf58be0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.781s
user	0m0.300s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4580 (a0c500b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14470c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14470c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14470ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14470d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14470d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14470df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14470e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14470ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14470f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14470f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14470fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14470ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144710a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144711210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144711a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144712140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144712860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144712f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1447136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144713e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144714590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144714cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1447153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144715c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144716390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144716650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144716c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1447178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1447180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144718570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144718830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1447190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144719600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1447198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144719d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14471a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14471a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14471ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14471afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14471b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14471b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14471bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14471c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14471c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14471cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14471d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14471da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14471e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14471e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14471ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14471f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14471f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14471fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1447206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144720b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144720ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1447212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1447218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1447220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144722370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144722810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144722cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1447235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144723a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144723f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1447243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144724870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144724d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1447251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144725650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144725af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144726040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144726590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144726ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144727030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144727580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144727ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144728020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144728570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144728ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144729010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144729560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144729ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14472a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14472a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14472aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14472aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14472b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14472ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14472bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14472c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14472ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14472cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14472d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14472da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14471d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14472dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14472e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14472ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14472f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14472f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14472fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144730120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144730670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144730bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144731110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144731660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144731bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144732100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144732650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144732ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144733040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1447334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144733980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144733e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1447342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144734c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1447350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144735540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1447359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144735e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144736320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1447367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144736c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144737100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1447375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144737a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144737ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144738380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144738820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144738cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144739160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144739600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144739aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144739f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14473a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14473a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14473ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14473b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14473b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14473bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14473bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14473c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14473c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14473cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14473d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14473d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14473db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14473e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14473e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14473e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14473ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14473f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14473f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14473fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144740060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144740500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1447409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144740e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1447412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144741780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144741c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1447420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144742560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144742a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144742ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144743340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1447437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144743c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1447445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144744a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144744f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1447453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144745840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144745ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144746180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144746620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144746ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144746f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144747400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1447478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144747d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1447481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144748680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144748b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144748fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144749460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144749900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144749da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14474a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14474a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14474ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14474b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14474b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14474bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14474c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14474c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14474cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14474d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14474d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14474dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14474e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14474eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14474efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14474f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14474f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1447500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144750610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1447510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144751600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144751b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1447520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1447525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144753090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1447535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144753b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144754080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1447545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144754b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144755070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1447555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144755b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144756060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1447565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144756b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1447575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144757af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144758040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144758590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144758ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144759030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144759580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14475a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14475a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14475aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14475b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14475b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14475bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14475c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14475c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14475caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14475cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14475d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14475da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14475dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14475e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14475ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14475efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14475f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14475fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14475ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144760510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144760a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144760fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144761500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144761a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144761fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1447624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144762a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144762ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144763380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144763820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144763cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144764160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144764600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144764aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144764f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1447653e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144765880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144765d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1447661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144766660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144766fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1447674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144767c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144768330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144768a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144769170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144769430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144769c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144769ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14476a4f0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.110.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146c04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146c05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146c054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146c05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146c05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146c06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146c06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146c06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146c06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146c073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146c07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146c07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146c08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146c091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146c09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146c0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146c0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146c0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146c0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146c0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146c0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146c0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146c0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146c0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146c0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146c0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146c0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146c0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146c0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146c0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146c0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146c0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146c10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146c104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146c10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146c10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146c11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146c116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146c11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146c11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146c12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146c12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146c12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146c13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146c135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146c13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146c13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146c14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146c14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146c14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146c15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146c154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146c15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146c15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146c16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146c16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146c16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146c17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146c17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146c179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146c17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146c182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146c18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146c18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146c19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146c19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146c198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146c19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146c1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146c1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146c1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146c1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146c1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146c1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146c1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146c1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146c1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146c1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146c1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146c1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146c1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146c1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146c1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146c1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146c1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146c1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146c1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146c1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146c1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146c1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146c20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146c207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146c20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146c210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146c21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146c219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146c21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146c22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146c226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146c22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146c22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146c23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146c238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146c23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146c24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146c24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146c24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146c24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146c25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146c257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146c25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146c260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146c26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146c26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146c26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146c27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146c276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146c27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146c27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146c28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146c28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146c28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146c29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146c295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146c29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146c29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146c2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146c2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146c2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146c2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146c2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146c2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146c2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146c2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146c2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146c2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146c2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146c2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146c2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146c2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146c2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146c2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146c2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146c2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146c2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146c2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146c2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146c30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146c304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146c30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146c30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146c31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146c31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146c31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146c31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146c323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146c32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146c32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146c33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146c335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146c33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146c33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146c342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146c34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146c34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146c35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146c35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146c35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146c361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146c36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146c36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146c36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146c373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146c37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146c37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146c38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146c38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146c389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146c38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146c392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146c39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146c39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146c3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146c3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146c3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146c3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146c3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146c3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146c3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146c3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146c3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146c3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146c3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146c3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146c3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146c3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146c3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146c3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146c3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146c3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146c3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146c3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146c3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146c3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146c40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146c407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146c40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146c41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146c415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146c41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146c42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146c428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146c42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146c43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146c43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146c43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146c44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146c45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146c456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146c45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146c46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146c46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146c46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146c473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146c47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146c47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146c484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146c48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146c49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146c49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146c49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146c4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146c4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146c4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146c4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146c4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146c4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146c4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146c4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146c4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146c4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146c4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146c4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146c4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146c4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146c4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146c4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146c4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146c50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146c50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146c50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146c514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146c51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146c52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146c525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146c52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146c53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146c53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146c53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146c542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146c54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146c54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146c553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146c559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146c55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146c56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146c56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146c56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146c574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146c579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146c57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146c583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146c588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146c58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146c592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146c597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146c59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146c5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146c5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146c5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146c5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146c5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146c5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146c5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146c5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146c5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146c5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146c5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146c5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146c5e8e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144607990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144607e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144608270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1446086e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14460ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14460b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14460ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14460bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14460c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14460c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14460cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14460d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14460da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14460e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14460e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14460f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14460f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14460ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144610660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144611010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144612c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1446133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144613c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1446148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144615090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144615530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1446157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1446165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144616d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1446171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144617660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144617fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144618440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1446188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1446194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14461a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14461a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14461ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14461b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14461b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14461bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14461c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14461cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14461d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14461d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14461dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14461df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14461e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14461ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14461f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14461f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14461fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14461ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144620dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144621700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1446224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144622980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144622ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144623420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144623970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144623ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144624410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144624960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1446263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1446273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1446283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1446293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14462a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14462a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14462ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14462b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14462b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14462be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14462c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14462c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14462ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14462d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14462d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14462de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14462e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14462e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14462ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14462f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14462f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14462fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1446302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144630740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144631080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144631520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1446319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144631e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144632300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1446327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144632c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1446330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144633580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144633a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144633ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144634360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144634800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144634ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144635140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1446355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144635a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144635f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1446363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144636d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1446371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144637640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144637ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144637f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144638420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1446388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144638d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144639200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1446396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144639b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144639fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14463a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14463a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14463adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14463b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14463b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14463bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14463c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14463c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14463c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14463ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14463d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14463d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14463dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14463e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14463e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14463e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14463ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14463f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14463f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14463fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144640100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1446405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144640a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144640ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144641380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144641820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144641cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144642160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144642600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144642aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144642f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1446433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144643880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144643d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1446441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144644660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144644b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144644fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144645440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1446458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144645d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144646220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1446466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144646b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144647000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144648800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144648e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144649420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144649a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14464a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14464a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14464a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14464af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14464b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14464bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14464c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14464c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14464cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14464d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14464d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14464ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14464e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14464e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14464edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14464f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14464f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14464fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1446502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1446512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144651d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1446522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144652d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1446532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1446542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1446552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1446557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144656290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1446567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144656d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1446577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144657d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144658270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1446587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144658d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1446597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144659d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14465a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14465a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14465acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14465b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14465b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14465bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14465c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14465c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14465ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14465d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14465d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14465dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14465e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14465e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14465ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14465f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14465f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14465fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144660140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1446605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144660a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144660f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1446613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144661860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144661d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1446621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144662640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144662ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144662f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144663420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1446638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144663d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144664200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144664750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144664e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144665590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144665cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1446663d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144666690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144666e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144667140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144667750 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.968s
user	0m0.248s
sys	0m0.183s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
