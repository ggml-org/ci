Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.665s
user	0m0.886s
sys	0m1.269s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-sampling
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Built target llama-batched-bench
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-infill
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Built target llama-cli
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-quantize
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-run
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.042s
user	0m6.217s
sys	0m9.749s

main: quantize time =  2220.49 ms
main:    total time =  2220.49 ms

main: quantize time =  1469.04 ms
main:    total time =  1469.04 ms

main: quantize time =  1560.56 ms
main:    total time =  1560.56 ms

main: quantize time =  3204.92 ms
main:    total time =  3204.92 ms

main: quantize time =  1445.17 ms
main:    total time =  1445.17 ms

main: quantize time =  5606.80 ms
main:    total time =  5606.80 ms

main: quantize time =  5877.94 ms
main:    total time =  5877.94 ms

main: quantize time =  6836.04 ms
main:    total time =  6836.04 ms

main: quantize time =  6050.89 ms
main:    total time =  6050.89 ms

main: quantize time =  4386.47 ms
main:    total time =  4386.47 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.143 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.316 I main: llama backend init
0.00.000.322 I main: load the model and apply lora adapter, if any
0.00.049.760 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.395 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.081.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.069 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.070 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.071 I llama_model_loader: - type  f32:  194 tensors
0.00.081.072 I llama_model_loader: - type  f16:   98 tensors
0.00.081.073 I print_info: file format = GGUF V3 (latest)
0.00.081.075 I print_info: file type   = all F32 (guessed)
0.00.081.077 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.782 I load: special tokens cache size = 25
0.00.121.512 I load: token to piece cache size = 0.2984 MB
0.00.121.516 I print_info: arch             = gptneox
0.00.121.516 I print_info: vocab_only       = 0
0.00.121.516 I print_info: n_ctx_train      = 2048
0.00.121.516 I print_info: n_embd           = 2048
0.00.121.516 I print_info: n_layer          = 24
0.00.121.519 I print_info: n_head           = 16
0.00.121.520 I print_info: n_head_kv        = 16
0.00.121.520 I print_info: n_rot            = 32
0.00.121.520 I print_info: n_swa            = 0
0.00.121.520 I print_info: n_embd_head_k    = 128
0.00.121.523 I print_info: n_embd_head_v    = 128
0.00.121.523 I print_info: n_gqa            = 1
0.00.121.524 I print_info: n_embd_k_gqa     = 2048
0.00.121.525 I print_info: n_embd_v_gqa     = 2048
0.00.121.525 I print_info: f_norm_eps       = 1.0e-05
0.00.121.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.527 I print_info: f_logit_scale    = 0.0e+00
0.00.121.528 I print_info: n_ff             = 8192
0.00.121.528 I print_info: n_expert         = 0
0.00.121.528 I print_info: n_expert_used    = 0
0.00.121.528 I print_info: causal attn      = 1
0.00.121.528 I print_info: pooling type     = 0
0.00.121.529 I print_info: rope type        = 2
0.00.121.529 I print_info: rope scaling     = linear
0.00.121.529 I print_info: freq_base_train  = 10000.0
0.00.121.529 I print_info: freq_scale_train = 1
0.00.121.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.530 I print_info: rope_finetuned   = unknown
0.00.121.530 I print_info: ssm_d_conv       = 0
0.00.121.530 I print_info: ssm_d_inner      = 0
0.00.121.530 I print_info: ssm_d_state      = 0
0.00.121.530 I print_info: ssm_dt_rank      = 0
0.00.121.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.531 I print_info: model type       = 1.4B
0.00.121.531 I print_info: model params     = 1.41 B
0.00.121.531 I print_info: general.name     = 1.4B
0.00.121.532 I print_info: vocab type       = BPE
0.00.121.532 I print_info: n_vocab          = 50304
0.00.121.532 I print_info: n_merges         = 50009
0.00.121.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.533 I print_info: LF token         = 128 'Ä'
0.00.121.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.533 I print_info: max token length = 1024
0.00.124.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.124.246 I load_tensors: offloading output layer to GPU
0.00.124.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.124.265 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.124.266 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.124.588 I llama_init_from_model: n_seq_max     = 1
0.00.124.589 I llama_init_from_model: n_ctx         = 2048
0.00.124.589 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.124.590 I llama_init_from_model: n_batch       = 2048
0.00.124.590 I llama_init_from_model: n_ubatch      = 512
0.00.124.590 I llama_init_from_model: flash_attn    = 0
0.00.124.590 I llama_init_from_model: freq_base     = 10000.0
0.00.124.591 I llama_init_from_model: freq_scale    = 1
0.00.124.591 I ggml_metal_init: allocating
0.00.124.595 I ggml_metal_init: found device: Apple M4
0.00.124.597 I ggml_metal_init: picking default device: Apple M4
0.00.125.318 I ggml_metal_init: using embedded metal library
0.00.314.148 I ggml_metal_init: GPU name:   Apple M4
0.00.314.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.314.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.314.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.314.167 I ggml_metal_init: simdgroup reduction   = true
0.00.314.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.314.168 I ggml_metal_init: has bfloat            = true
0.00.314.168 I ggml_metal_init: use bfloat            = true
0.00.314.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.314.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.351.522 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.377.377 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.377.385 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.377.410 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.378.324 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.378.326 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.378.327 I llama_init_from_model: graph nodes  = 967
0.00.378.327 I llama_init_from_model: graph splits = 2
0.00.378.330 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.378.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.378.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.458.415 I main: llama threadpool init, n_threads = 4
0.00.458.459 I 
0.00.458.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.458.494 I 
0.00.458.573 I sampler seed: 1234
0.00.458.578 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.458.609 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.458.611 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.458.611 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.299.109 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.02.299.110 I llama_perf_context_print:        load time =     407.31 ms
0.02.299.111 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.85 tokens per second)
0.02.299.112 I llama_perf_context_print:        eval time =    1793.61 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.299.113 I llama_perf_context_print:       total time =    1842.03 ms /    70 tokens
0.02.299.292 I ggml_metal_free: deallocating

real	0m2.610s
user	0m0.163s
sys	0m0.113s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.265 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.346 I llama_model_loader: - type  f32:  194 tensors
0.00.035.346 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.347 I print_info: file format = GGUF V3 (latest)
0.00.035.352 I print_info: file type   = Q8_0
0.00.035.353 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.907 I load: special tokens cache size = 25
0.00.062.896 I load: token to piece cache size = 0.2984 MB
0.00.062.900 I print_info: arch             = gptneox
0.00.062.900 I print_info: vocab_only       = 0
0.00.062.900 I print_info: n_ctx_train      = 2048
0.00.062.902 I print_info: n_embd           = 2048
0.00.062.906 I print_info: n_layer          = 24
0.00.062.912 I print_info: n_head           = 16
0.00.062.913 I print_info: n_head_kv        = 16
0.00.062.914 I print_info: n_rot            = 32
0.00.062.916 I print_info: n_swa            = 0
0.00.062.916 I print_info: n_embd_head_k    = 128
0.00.062.916 I print_info: n_embd_head_v    = 128
0.00.062.917 I print_info: n_gqa            = 1
0.00.062.918 I print_info: n_embd_k_gqa     = 2048
0.00.062.918 I print_info: n_embd_v_gqa     = 2048
0.00.062.919 I print_info: f_norm_eps       = 1.0e-05
0.00.062.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.920 I print_info: f_logit_scale    = 0.0e+00
0.00.062.921 I print_info: n_ff             = 8192
0.00.062.921 I print_info: n_expert         = 0
0.00.062.921 I print_info: n_expert_used    = 0
0.00.062.921 I print_info: causal attn      = 1
0.00.062.921 I print_info: pooling type     = 0
0.00.062.921 I print_info: rope type        = 2
0.00.062.921 I print_info: rope scaling     = linear
0.00.062.922 I print_info: freq_base_train  = 10000.0
0.00.062.922 I print_info: freq_scale_train = 1
0.00.062.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.922 I print_info: rope_finetuned   = unknown
0.00.062.922 I print_info: ssm_d_conv       = 0
0.00.062.923 I print_info: ssm_d_inner      = 0
0.00.062.923 I print_info: ssm_d_state      = 0
0.00.062.923 I print_info: ssm_dt_rank      = 0
0.00.062.923 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.925 I print_info: model type       = 1.4B
0.00.062.925 I print_info: model params     = 1.41 B
0.00.062.925 I print_info: general.name     = 1.4B
0.00.062.926 I print_info: vocab type       = BPE
0.00.062.926 I print_info: n_vocab          = 50304
0.00.062.926 I print_info: n_merges         = 50009
0.00.062.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.927 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.927 I print_info: LF token         = 128 'Ä'
0.00.062.930 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.930 I print_info: max token length = 1024
0.00.065.425 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.426 I load_tensors: offloading output layer to GPU
0.00.065.426 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.438 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.439 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.763 I llama_init_from_model: n_seq_max     = 1
0.00.065.764 I llama_init_from_model: n_ctx         = 2048
0.00.065.764 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.765 I llama_init_from_model: n_batch       = 2048
0.00.065.765 I llama_init_from_model: n_ubatch      = 512
0.00.065.765 I llama_init_from_model: flash_attn    = 0
0.00.065.765 I llama_init_from_model: freq_base     = 10000.0
0.00.065.766 I llama_init_from_model: freq_scale    = 1
0.00.065.766 I ggml_metal_init: allocating
0.00.065.769 I ggml_metal_init: found device: Apple M4
0.00.065.772 I ggml_metal_init: picking default device: Apple M4
0.00.066.534 I ggml_metal_init: using embedded metal library
0.00.069.305 I ggml_metal_init: GPU name:   Apple M4
0.00.069.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.308 I ggml_metal_init: simdgroup reduction   = true
0.00.069.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.308 I ggml_metal_init: has bfloat            = true
0.00.069.309 I ggml_metal_init: use bfloat            = true
0.00.069.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.699 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.201 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.224 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.453 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.106.454 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.106.455 I llama_init_from_model: graph nodes  = 967
0.00.106.455 I llama_init_from_model: graph splits = 2
0.00.106.459 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.589 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.332.317 I main: llama threadpool init, n_threads = 4
0.01.332.350 I 
0.01.332.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.332.375 I 
0.01.332.602 I sampler seed: 1234
0.01.332.606 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.332.616 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.332.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.332.617 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.439.735 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48965.52 tokens per second)
0.02.439.735 I llama_perf_context_print:        load time =    1321.60 ms
0.02.439.736 I llama_perf_context_print: prompt eval time =      45.62 ms /     7 tokens (    6.52 ms per token,   153.43 tokens per second)
0.02.439.737 I llama_perf_context_print:        eval time =    1058.92 ms /    63 runs   (   16.81 ms per token,    59.49 tokens per second)
0.02.439.737 I llama_perf_context_print:       total time =    1108.28 ms /    70 tokens
0.02.440.015 I ggml_metal_free: deallocating

real	0m2.457s
user	0m0.115s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.014.400 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.567 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.064 I llama_model_loader: - type  f32:  194 tensors
0.00.042.064 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.064 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.065 I print_info: file format = GGUF V3 (latest)
0.00.042.065 I print_info: file type   = Q4_0
0.00.042.066 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.068.864 I load: special tokens cache size = 25
0.00.079.001 I load: token to piece cache size = 0.2984 MB
0.00.079.006 I print_info: arch             = gptneox
0.00.079.006 I print_info: vocab_only       = 0
0.00.079.006 I print_info: n_ctx_train      = 2048
0.00.079.007 I print_info: n_embd           = 2048
0.00.079.007 I print_info: n_layer          = 24
0.00.079.010 I print_info: n_head           = 16
0.00.079.011 I print_info: n_head_kv        = 16
0.00.079.011 I print_info: n_rot            = 32
0.00.079.011 I print_info: n_swa            = 0
0.00.079.011 I print_info: n_embd_head_k    = 128
0.00.079.012 I print_info: n_embd_head_v    = 128
0.00.079.012 I print_info: n_gqa            = 1
0.00.079.013 I print_info: n_embd_k_gqa     = 2048
0.00.079.014 I print_info: n_embd_v_gqa     = 2048
0.00.079.015 I print_info: f_norm_eps       = 1.0e-05
0.00.079.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.016 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.016 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.016 I print_info: f_logit_scale    = 0.0e+00
0.00.079.017 I print_info: n_ff             = 8192
0.00.079.017 I print_info: n_expert         = 0
0.00.079.017 I print_info: n_expert_used    = 0
0.00.079.017 I print_info: causal attn      = 1
0.00.079.017 I print_info: pooling type     = 0
0.00.079.018 I print_info: rope type        = 2
0.00.079.018 I print_info: rope scaling     = linear
0.00.079.018 I print_info: freq_base_train  = 10000.0
0.00.079.019 I print_info: freq_scale_train = 1
0.00.079.019 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.019 I print_info: rope_finetuned   = unknown
0.00.079.019 I print_info: ssm_d_conv       = 0
0.00.079.020 I print_info: ssm_d_inner      = 0
0.00.079.020 I print_info: ssm_d_state      = 0
0.00.079.020 I print_info: ssm_dt_rank      = 0
0.00.079.020 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.021 I print_info: model type       = 1.4B
0.00.079.021 I print_info: model params     = 1.41 B
0.00.079.021 I print_info: general.name     = 1.4B
0.00.079.022 I print_info: vocab type       = BPE
0.00.079.022 I print_info: n_vocab          = 50304
0.00.079.022 I print_info: n_merges         = 50009
0.00.079.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.023 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.023 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.026 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.026 I print_info: LF token         = 128 'Ä'
0.00.079.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.027 I print_info: max token length = 1024
0.00.081.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.081.532 I load_tensors: offloading output layer to GPU
0.00.081.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.081.544 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.546 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.081.995 I llama_init_from_model: n_seq_max     = 1
0.00.081.997 I llama_init_from_model: n_ctx         = 2048
0.00.081.997 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.081.997 I llama_init_from_model: n_batch       = 2048
0.00.081.997 I llama_init_from_model: n_ubatch      = 512
0.00.081.998 I llama_init_from_model: flash_attn    = 0
0.00.081.998 I llama_init_from_model: freq_base     = 10000.0
0.00.081.999 I llama_init_from_model: freq_scale    = 1
0.00.081.999 I ggml_metal_init: allocating
0.00.082.004 I ggml_metal_init: found device: Apple M4
0.00.082.007 I ggml_metal_init: picking default device: Apple M4
0.00.082.862 I ggml_metal_init: using embedded metal library
0.00.086.589 I ggml_metal_init: GPU name:   Apple M4
0.00.086.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.593 I ggml_metal_init: simdgroup reduction   = true
0.00.086.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.593 I ggml_metal_init: has bfloat            = true
0.00.086.594 I ggml_metal_init: use bfloat            = true
0.00.086.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.821 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.337 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.347 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.369 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.123.412 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.123.414 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.123.414 I llama_init_from_model: graph nodes  = 967
0.00.123.414 I llama_init_from_model: graph splits = 2
0.00.123.417 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.593 I main: llama threadpool init, n_threads = 4
0.00.748.629 I 
0.00.748.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.655 I 
0.00.748.868 I sampler seed: 1234
0.00.748.873 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.905 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.906 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.425.334 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.425.335 I llama_perf_context_print:        load time =     733.17 ms
0.01.425.335 I llama_perf_context_print: prompt eval time =      39.88 ms /     7 tokens (    5.70 ms per token,   175.51 tokens per second)
0.01.425.336 I llama_perf_context_print:        eval time =     633.57 ms /    63 runs   (   10.06 ms per token,    99.44 tokens per second)
0.01.425.336 I llama_perf_context_print:       total time =     677.76 ms /    70 tokens
0.01.425.583 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.131s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.210 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.210 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.211 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.211 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.211 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.217 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.304 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.315 I llama_model_loader: - type  f32:  194 tensors
0.00.027.315 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.316 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.316 I print_info: file format = GGUF V3 (latest)
0.00.027.317 I print_info: file type   = Q4_1
0.00.027.318 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.760 I load: special tokens cache size = 25
0.00.052.749 I load: token to piece cache size = 0.2984 MB
0.00.052.752 I print_info: arch             = gptneox
0.00.052.753 I print_info: vocab_only       = 0
0.00.052.753 I print_info: n_ctx_train      = 2048
0.00.052.753 I print_info: n_embd           = 2048
0.00.052.753 I print_info: n_layer          = 24
0.00.052.756 I print_info: n_head           = 16
0.00.052.757 I print_info: n_head_kv        = 16
0.00.052.757 I print_info: n_rot            = 32
0.00.052.757 I print_info: n_swa            = 0
0.00.052.757 I print_info: n_embd_head_k    = 128
0.00.052.758 I print_info: n_embd_head_v    = 128
0.00.052.758 I print_info: n_gqa            = 1
0.00.052.759 I print_info: n_embd_k_gqa     = 2048
0.00.052.760 I print_info: n_embd_v_gqa     = 2048
0.00.052.762 I print_info: f_norm_eps       = 1.0e-05
0.00.052.762 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.762 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.764 I print_info: f_logit_scale    = 0.0e+00
0.00.052.764 I print_info: n_ff             = 8192
0.00.052.764 I print_info: n_expert         = 0
0.00.052.765 I print_info: n_expert_used    = 0
0.00.052.765 I print_info: causal attn      = 1
0.00.052.765 I print_info: pooling type     = 0
0.00.052.766 I print_info: rope type        = 2
0.00.052.768 I print_info: rope scaling     = linear
0.00.052.768 I print_info: freq_base_train  = 10000.0
0.00.052.769 I print_info: freq_scale_train = 1
0.00.052.769 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.769 I print_info: rope_finetuned   = unknown
0.00.052.769 I print_info: ssm_d_conv       = 0
0.00.052.769 I print_info: ssm_d_inner      = 0
0.00.052.770 I print_info: ssm_d_state      = 0
0.00.052.770 I print_info: ssm_dt_rank      = 0
0.00.052.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.770 I print_info: model type       = 1.4B
0.00.052.771 I print_info: model params     = 1.41 B
0.00.052.771 I print_info: general.name     = 1.4B
0.00.052.775 I print_info: vocab type       = BPE
0.00.052.775 I print_info: n_vocab          = 50304
0.00.052.775 I print_info: n_merges         = 50009
0.00.052.776 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.776 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.776 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.776 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.776 I print_info: LF token         = 128 'Ä'
0.00.052.777 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.777 I print_info: max token length = 1024
0.00.054.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.768 I load_tensors: offloading output layer to GPU
0.00.054.768 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.779 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.780 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.059 I llama_init_from_model: n_seq_max     = 1
0.00.055.060 I llama_init_from_model: n_ctx         = 2048
0.00.055.060 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.060 I llama_init_from_model: n_batch       = 2048
0.00.055.060 I llama_init_from_model: n_ubatch      = 512
0.00.055.061 I llama_init_from_model: flash_attn    = 0
0.00.055.061 I llama_init_from_model: freq_base     = 10000.0
0.00.055.061 I llama_init_from_model: freq_scale    = 1
0.00.055.062 I ggml_metal_init: allocating
0.00.055.065 I ggml_metal_init: found device: Apple M4
0.00.055.067 I ggml_metal_init: picking default device: Apple M4
0.00.055.681 I ggml_metal_init: using embedded metal library
0.00.058.090 I ggml_metal_init: GPU name:   Apple M4
0.00.058.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.092 I ggml_metal_init: simdgroup reduction   = true
0.00.058.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.092 I ggml_metal_init: has bfloat            = true
0.00.058.093 I ggml_metal_init: use bfloat            = true
0.00.058.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.028 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.715 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.721 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.739 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.786 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.787 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.787 I llama_init_from_model: graph nodes  = 967
0.00.088.788 I llama_init_from_model: graph splits = 2
0.00.088.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.907 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.908 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.865.026 I main: llama threadpool init, n_threads = 4
0.00.865.074 I 
0.00.865.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.865.103 I 
0.00.865.333 I sampler seed: 1234
0.00.865.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.865.381 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.865.381 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.865.381 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.590.635 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62390.16 tokens per second)
0.01.590.636 I llama_perf_context_print:        load time =     855.26 ms
0.01.590.636 I llama_perf_context_print: prompt eval time =      44.02 ms /     7 tokens (    6.29 ms per token,   159.03 tokens per second)
0.01.590.637 I llama_perf_context_print:        eval time =     678.31 ms /    63 runs   (   10.77 ms per token,    92.88 tokens per second)
0.01.590.638 I llama_perf_context_print:       total time =     726.47 ms /    70 tokens
0.01.590.846 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.444 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.450 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.452 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.453 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.455 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.730 I llama_model_loader: - type  f32:  194 tensors
0.00.026.731 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.732 I print_info: file format = GGUF V3 (latest)
0.00.026.732 I print_info: file type   = Q5_0
0.00.026.733 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.116 I load: special tokens cache size = 25
0.00.052.208 I load: token to piece cache size = 0.2984 MB
0.00.052.212 I print_info: arch             = gptneox
0.00.052.212 I print_info: vocab_only       = 0
0.00.052.212 I print_info: n_ctx_train      = 2048
0.00.052.212 I print_info: n_embd           = 2048
0.00.052.213 I print_info: n_layer          = 24
0.00.052.216 I print_info: n_head           = 16
0.00.052.217 I print_info: n_head_kv        = 16
0.00.052.217 I print_info: n_rot            = 32
0.00.052.217 I print_info: n_swa            = 0
0.00.052.217 I print_info: n_embd_head_k    = 128
0.00.052.220 I print_info: n_embd_head_v    = 128
0.00.052.220 I print_info: n_gqa            = 1
0.00.052.221 I print_info: n_embd_k_gqa     = 2048
0.00.052.222 I print_info: n_embd_v_gqa     = 2048
0.00.052.223 I print_info: f_norm_eps       = 1.0e-05
0.00.052.223 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.223 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.223 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.224 I print_info: f_logit_scale    = 0.0e+00
0.00.052.224 I print_info: n_ff             = 8192
0.00.052.225 I print_info: n_expert         = 0
0.00.052.225 I print_info: n_expert_used    = 0
0.00.052.225 I print_info: causal attn      = 1
0.00.052.225 I print_info: pooling type     = 0
0.00.052.228 I print_info: rope type        = 2
0.00.052.228 I print_info: rope scaling     = linear
0.00.052.229 I print_info: freq_base_train  = 10000.0
0.00.052.229 I print_info: freq_scale_train = 1
0.00.052.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.229 I print_info: rope_finetuned   = unknown
0.00.052.229 I print_info: ssm_d_conv       = 0
0.00.052.229 I print_info: ssm_d_inner      = 0
0.00.052.230 I print_info: ssm_d_state      = 0
0.00.052.230 I print_info: ssm_dt_rank      = 0
0.00.052.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.234 I print_info: model type       = 1.4B
0.00.052.234 I print_info: model params     = 1.41 B
0.00.052.235 I print_info: general.name     = 1.4B
0.00.052.235 I print_info: vocab type       = BPE
0.00.052.235 I print_info: n_vocab          = 50304
0.00.052.235 I print_info: n_merges         = 50009
0.00.052.236 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: LF token         = 128 'Ä'
0.00.052.237 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.238 I print_info: max token length = 1024
0.00.054.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.232 I load_tensors: offloading output layer to GPU
0.00.054.232 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.243 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.244 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.534 I llama_init_from_model: n_seq_max     = 1
0.00.054.534 I llama_init_from_model: n_ctx         = 2048
0.00.054.534 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.535 I llama_init_from_model: n_batch       = 2048
0.00.054.535 I llama_init_from_model: n_ubatch      = 512
0.00.054.535 I llama_init_from_model: flash_attn    = 0
0.00.054.535 I llama_init_from_model: freq_base     = 10000.0
0.00.054.536 I llama_init_from_model: freq_scale    = 1
0.00.054.536 I ggml_metal_init: allocating
0.00.054.539 I ggml_metal_init: found device: Apple M4
0.00.054.541 I ggml_metal_init: picking default device: Apple M4
0.00.055.162 I ggml_metal_init: using embedded metal library
0.00.057.526 I ggml_metal_init: GPU name:   Apple M4
0.00.057.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.528 I ggml_metal_init: simdgroup reduction   = true
0.00.057.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.528 I ggml_metal_init: has bfloat            = true
0.00.057.529 I ggml_metal_init: use bfloat            = true
0.00.057.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.530 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.469 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.635 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.726 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.727 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.728 I llama_init_from_model: graph nodes  = 967
0.00.088.728 I llama_init_from_model: graph splits = 2
0.00.088.731 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.046 I main: llama threadpool init, n_threads = 4
0.00.809.080 I 
0.00.809.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.102 I 
0.00.809.322 I sampler seed: 1234
0.00.809.326 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.348 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.587.885 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.587.886 I llama_perf_context_print:        load time =     798.44 ms
0.01.587.887 I llama_perf_context_print: prompt eval time =      43.18 ms /     7 tokens (    6.17 ms per token,   162.10 tokens per second)
0.01.587.887 I llama_perf_context_print:        eval time =     732.40 ms /    63 runs   (   11.63 ms per token,    86.02 tokens per second)
0.01.587.888 I llama_perf_context_print:       total time =     779.69 ms /    70 tokens
0.01.588.128 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.114 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.597 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.600 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.603 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.323 I llama_model_loader: - type  f32:  194 tensors
0.00.025.323 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.324 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.324 I print_info: file format = GGUF V3 (latest)
0.00.025.324 I print_info: file type   = Q5_1
0.00.025.325 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.129 I load: special tokens cache size = 25
0.00.050.312 I load: token to piece cache size = 0.2984 MB
0.00.050.315 I print_info: arch             = gptneox
0.00.050.315 I print_info: vocab_only       = 0
0.00.050.315 I print_info: n_ctx_train      = 2048
0.00.050.315 I print_info: n_embd           = 2048
0.00.050.315 I print_info: n_layer          = 24
0.00.050.318 I print_info: n_head           = 16
0.00.050.319 I print_info: n_head_kv        = 16
0.00.050.319 I print_info: n_rot            = 32
0.00.050.319 I print_info: n_swa            = 0
0.00.050.320 I print_info: n_embd_head_k    = 128
0.00.050.320 I print_info: n_embd_head_v    = 128
0.00.050.321 I print_info: n_gqa            = 1
0.00.050.321 I print_info: n_embd_k_gqa     = 2048
0.00.050.322 I print_info: n_embd_v_gqa     = 2048
0.00.050.323 I print_info: f_norm_eps       = 1.0e-05
0.00.050.323 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.324 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.328 I print_info: f_logit_scale    = 0.0e+00
0.00.050.329 I print_info: n_ff             = 8192
0.00.050.329 I print_info: n_expert         = 0
0.00.050.329 I print_info: n_expert_used    = 0
0.00.050.330 I print_info: causal attn      = 1
0.00.050.330 I print_info: pooling type     = 0
0.00.050.330 I print_info: rope type        = 2
0.00.050.330 I print_info: rope scaling     = linear
0.00.050.330 I print_info: freq_base_train  = 10000.0
0.00.050.331 I print_info: freq_scale_train = 1
0.00.050.331 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.331 I print_info: rope_finetuned   = unknown
0.00.050.331 I print_info: ssm_d_conv       = 0
0.00.050.331 I print_info: ssm_d_inner      = 0
0.00.050.333 I print_info: ssm_d_state      = 0
0.00.050.333 I print_info: ssm_dt_rank      = 0
0.00.050.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.333 I print_info: model type       = 1.4B
0.00.050.334 I print_info: model params     = 1.41 B
0.00.050.334 I print_info: general.name     = 1.4B
0.00.050.334 I print_info: vocab type       = BPE
0.00.050.335 I print_info: n_vocab          = 50304
0.00.050.336 I print_info: n_merges         = 50009
0.00.050.336 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.336 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.336 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.341 I print_info: LF token         = 128 'Ä'
0.00.050.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.342 I print_info: max token length = 1024
0.00.051.896 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.896 I load_tensors: offloading output layer to GPU
0.00.051.897 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.906 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.907 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.179 I llama_init_from_model: n_seq_max     = 1
0.00.052.180 I llama_init_from_model: n_ctx         = 2048
0.00.052.180 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.180 I llama_init_from_model: n_batch       = 2048
0.00.052.180 I llama_init_from_model: n_ubatch      = 512
0.00.052.180 I llama_init_from_model: flash_attn    = 0
0.00.052.181 I llama_init_from_model: freq_base     = 10000.0
0.00.052.181 I llama_init_from_model: freq_scale    = 1
0.00.052.181 I ggml_metal_init: allocating
0.00.052.185 I ggml_metal_init: found device: Apple M4
0.00.052.187 I ggml_metal_init: picking default device: Apple M4
0.00.052.750 I ggml_metal_init: using embedded metal library
0.00.055.076 I ggml_metal_init: GPU name:   Apple M4
0.00.055.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.079 I ggml_metal_init: simdgroup reduction   = true
0.00.055.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.079 I ggml_metal_init: has bfloat            = true
0.00.055.079 I ggml_metal_init: use bfloat            = true
0.00.055.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.703 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.990 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.011 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.975 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.976 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.976 I llama_init_from_model: graph nodes  = 967
0.00.084.977 I llama_init_from_model: graph splits = 2
0.00.084.980 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.797 I main: llama threadpool init, n_threads = 4
0.00.721.841 I 
0.00.721.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.887 I 
0.00.722.115 I sampler seed: 1234
0.00.722.121 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.152 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.152 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.551.504 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.551.505 I llama_perf_context_print:        load time =     711.82 ms
0.01.551.505 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.57 tokens per second)
0.01.551.506 I llama_perf_context_print:        eval time =     784.09 ms /    63 runs   (   12.45 ms per token,    80.35 tokens per second)
0.01.551.506 I llama_perf_context_print:       total time =     830.57 ms /    70 tokens
0.01.551.711 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.714 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.605 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.607 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.771 I llama_model_loader: - type  f32:  194 tensors
0.00.025.771 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.771 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.772 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.772 I print_info: file format = GGUF V3 (latest)
0.00.025.773 I print_info: file type   = Q2_K - Medium
0.00.025.778 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.295 I load: special tokens cache size = 25
0.00.051.441 I load: token to piece cache size = 0.2984 MB
0.00.051.444 I print_info: arch             = gptneox
0.00.051.444 I print_info: vocab_only       = 0
0.00.051.444 I print_info: n_ctx_train      = 2048
0.00.051.445 I print_info: n_embd           = 2048
0.00.051.445 I print_info: n_layer          = 24
0.00.051.448 I print_info: n_head           = 16
0.00.051.449 I print_info: n_head_kv        = 16
0.00.051.449 I print_info: n_rot            = 32
0.00.051.449 I print_info: n_swa            = 0
0.00.051.449 I print_info: n_embd_head_k    = 128
0.00.051.449 I print_info: n_embd_head_v    = 128
0.00.051.452 I print_info: n_gqa            = 1
0.00.051.453 I print_info: n_embd_k_gqa     = 2048
0.00.051.454 I print_info: n_embd_v_gqa     = 2048
0.00.051.454 I print_info: f_norm_eps       = 1.0e-05
0.00.051.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.456 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.456 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.456 I print_info: f_logit_scale    = 0.0e+00
0.00.051.457 I print_info: n_ff             = 8192
0.00.051.457 I print_info: n_expert         = 0
0.00.051.458 I print_info: n_expert_used    = 0
0.00.051.458 I print_info: causal attn      = 1
0.00.051.458 I print_info: pooling type     = 0
0.00.051.458 I print_info: rope type        = 2
0.00.051.458 I print_info: rope scaling     = linear
0.00.051.459 I print_info: freq_base_train  = 10000.0
0.00.051.459 I print_info: freq_scale_train = 1
0.00.051.459 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.459 I print_info: rope_finetuned   = unknown
0.00.051.460 I print_info: ssm_d_conv       = 0
0.00.051.460 I print_info: ssm_d_inner      = 0
0.00.051.460 I print_info: ssm_d_state      = 0
0.00.051.460 I print_info: ssm_dt_rank      = 0
0.00.051.460 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.460 I print_info: model type       = 1.4B
0.00.051.461 I print_info: model params     = 1.41 B
0.00.051.461 I print_info: general.name     = 1.4B
0.00.051.461 I print_info: vocab type       = BPE
0.00.051.462 I print_info: n_vocab          = 50304
0.00.051.462 I print_info: n_merges         = 50009
0.00.051.462 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.463 I print_info: LF token         = 128 'Ä'
0.00.051.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.463 I print_info: max token length = 1024
0.00.053.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.385 I load_tensors: offloading output layer to GPU
0.00.053.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.396 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.397 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.677 I llama_init_from_model: n_seq_max     = 1
0.00.053.678 I llama_init_from_model: n_ctx         = 2048
0.00.053.678 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.679 I llama_init_from_model: n_batch       = 2048
0.00.053.679 I llama_init_from_model: n_ubatch      = 512
0.00.053.679 I llama_init_from_model: flash_attn    = 0
0.00.053.679 I llama_init_from_model: freq_base     = 10000.0
0.00.053.680 I llama_init_from_model: freq_scale    = 1
0.00.053.680 I ggml_metal_init: allocating
0.00.053.683 I ggml_metal_init: found device: Apple M4
0.00.053.685 I ggml_metal_init: picking default device: Apple M4
0.00.054.301 I ggml_metal_init: using embedded metal library
0.00.056.672 I ggml_metal_init: GPU name:   Apple M4
0.00.056.673 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.674 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.674 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.674 I ggml_metal_init: simdgroup reduction   = true
0.00.056.675 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.675 I ggml_metal_init: has bfloat            = true
0.00.056.675 I ggml_metal_init: use bfloat            = true
0.00.056.675 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.676 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.619 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.813 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.821 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.849 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.920 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.922 I llama_init_from_model: graph nodes  = 967
0.00.086.922 I llama_init_from_model: graph splits = 2
0.00.086.925 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.856 I main: llama threadpool init, n_threads = 4
0.00.435.897 I 
0.00.435.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.435.923 I 
0.00.436.149 I sampler seed: 1234
0.00.436.155 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.436.166 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.436.166 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.436.166 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.115.033 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63563.12 tokens per second)
0.01.115.034 I llama_perf_context_print:        load time =     425.29 ms
0.01.115.035 I llama_perf_context_print: prompt eval time =      41.53 ms /     7 tokens (    5.93 ms per token,   168.54 tokens per second)
0.01.115.035 I llama_perf_context_print:        eval time =     634.48 ms /    63 runs   (   10.07 ms per token,    99.29 tokens per second)
0.01.115.036 I llama_perf_context_print:       total time =     680.03 ms /    70 tokens
0.01.115.281 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.111s
sys	0m0.105s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.507 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.511 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.726 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.727 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.727 I print_info: file format = GGUF V3 (latest)
0.00.024.728 I print_info: file type   = Q3_K - Medium
0.00.024.729 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.462 I load: special tokens cache size = 25
0.00.049.554 I load: token to piece cache size = 0.2984 MB
0.00.049.557 I print_info: arch             = gptneox
0.00.049.557 I print_info: vocab_only       = 0
0.00.049.557 I print_info: n_ctx_train      = 2048
0.00.049.557 I print_info: n_embd           = 2048
0.00.049.557 I print_info: n_layer          = 24
0.00.049.560 I print_info: n_head           = 16
0.00.049.561 I print_info: n_head_kv        = 16
0.00.049.561 I print_info: n_rot            = 32
0.00.049.561 I print_info: n_swa            = 0
0.00.049.561 I print_info: n_embd_head_k    = 128
0.00.049.562 I print_info: n_embd_head_v    = 128
0.00.049.563 I print_info: n_gqa            = 1
0.00.049.564 I print_info: n_embd_k_gqa     = 2048
0.00.049.565 I print_info: n_embd_v_gqa     = 2048
0.00.049.565 I print_info: f_norm_eps       = 1.0e-05
0.00.049.566 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.568 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.568 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.568 I print_info: f_logit_scale    = 0.0e+00
0.00.049.569 I print_info: n_ff             = 8192
0.00.049.569 I print_info: n_expert         = 0
0.00.049.569 I print_info: n_expert_used    = 0
0.00.049.569 I print_info: causal attn      = 1
0.00.049.569 I print_info: pooling type     = 0
0.00.049.570 I print_info: rope type        = 2
0.00.049.570 I print_info: rope scaling     = linear
0.00.049.570 I print_info: freq_base_train  = 10000.0
0.00.049.571 I print_info: freq_scale_train = 1
0.00.049.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.571 I print_info: rope_finetuned   = unknown
0.00.049.571 I print_info: ssm_d_conv       = 0
0.00.049.571 I print_info: ssm_d_inner      = 0
0.00.049.571 I print_info: ssm_d_state      = 0
0.00.049.572 I print_info: ssm_dt_rank      = 0
0.00.049.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.572 I print_info: model type       = 1.4B
0.00.049.572 I print_info: model params     = 1.41 B
0.00.049.573 I print_info: general.name     = 1.4B
0.00.049.573 I print_info: vocab type       = BPE
0.00.049.573 I print_info: n_vocab          = 50304
0.00.049.574 I print_info: n_merges         = 50009
0.00.049.574 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.574 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.575 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.576 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.576 I print_info: LF token         = 128 'Ä'
0.00.049.577 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.577 I print_info: max token length = 1024
0.00.051.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.281 I load_tensors: offloading output layer to GPU
0.00.051.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.287 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.287 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.556 I llama_init_from_model: n_seq_max     = 1
0.00.051.557 I llama_init_from_model: n_ctx         = 2048
0.00.051.557 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.557 I llama_init_from_model: n_batch       = 2048
0.00.051.558 I llama_init_from_model: n_ubatch      = 512
0.00.051.558 I llama_init_from_model: flash_attn    = 0
0.00.051.558 I llama_init_from_model: freq_base     = 10000.0
0.00.051.558 I llama_init_from_model: freq_scale    = 1
0.00.051.559 I ggml_metal_init: allocating
0.00.051.562 I ggml_metal_init: found device: Apple M4
0.00.051.564 I ggml_metal_init: picking default device: Apple M4
0.00.052.165 I ggml_metal_init: using embedded metal library
0.00.054.539 I ggml_metal_init: GPU name:   Apple M4
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.543 I ggml_metal_init: simdgroup reduction   = true
0.00.054.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.543 I ggml_metal_init: has bfloat            = true
0.00.054.543 I ggml_metal_init: use bfloat            = true
0.00.054.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.165 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.602 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.608 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.626 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.620 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.621 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.621 I llama_init_from_model: graph nodes  = 967
0.00.083.622 I llama_init_from_model: graph splits = 2
0.00.083.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.737 I main: llama threadpool init, n_threads = 4
0.00.521.774 I 
0.00.521.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.803 I 
0.00.522.026 I sampler seed: 1234
0.00.522.030 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.073 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.075 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.257.574 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.257.575 I llama_perf_context_print:        load time =     511.98 ms
0.01.257.576 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.03 tokens per second)
0.01.257.577 I llama_perf_context_print:        eval time =     692.28 ms /    63 runs   (   10.99 ms per token,    91.00 tokens per second)
0.01.257.578 I llama_perf_context_print:       total time =     736.70 ms /    70 tokens
0.01.257.787 I ggml_metal_free: deallocating

real	0m1.274s
user	0m0.109s
sys	0m0.118s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.475 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.484 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.485 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.381 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.376 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.377 I llama_model_loader: - type  f32:  194 tensors
0.00.024.377 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.377 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.377 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.378 I print_info: file format = GGUF V3 (latest)
0.00.024.378 I print_info: file type   = Q4_K - Medium
0.00.024.379 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.165 I load: special tokens cache size = 25
0.00.049.440 I load: token to piece cache size = 0.2984 MB
0.00.049.443 I print_info: arch             = gptneox
0.00.049.444 I print_info: vocab_only       = 0
0.00.049.444 I print_info: n_ctx_train      = 2048
0.00.049.444 I print_info: n_embd           = 2048
0.00.049.444 I print_info: n_layer          = 24
0.00.049.447 I print_info: n_head           = 16
0.00.049.448 I print_info: n_head_kv        = 16
0.00.049.448 I print_info: n_rot            = 32
0.00.049.448 I print_info: n_swa            = 0
0.00.049.448 I print_info: n_embd_head_k    = 128
0.00.049.448 I print_info: n_embd_head_v    = 128
0.00.049.449 I print_info: n_gqa            = 1
0.00.049.452 I print_info: n_embd_k_gqa     = 2048
0.00.049.453 I print_info: n_embd_v_gqa     = 2048
0.00.049.454 I print_info: f_norm_eps       = 1.0e-05
0.00.049.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.454 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.454 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.455 I print_info: f_logit_scale    = 0.0e+00
0.00.049.455 I print_info: n_ff             = 8192
0.00.049.455 I print_info: n_expert         = 0
0.00.049.456 I print_info: n_expert_used    = 0
0.00.049.456 I print_info: causal attn      = 1
0.00.049.456 I print_info: pooling type     = 0
0.00.049.458 I print_info: rope type        = 2
0.00.049.458 I print_info: rope scaling     = linear
0.00.049.458 I print_info: freq_base_train  = 10000.0
0.00.049.458 I print_info: freq_scale_train = 1
0.00.049.459 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.459 I print_info: rope_finetuned   = unknown
0.00.049.459 I print_info: ssm_d_conv       = 0
0.00.049.459 I print_info: ssm_d_inner      = 0
0.00.049.459 I print_info: ssm_d_state      = 0
0.00.049.459 I print_info: ssm_dt_rank      = 0
0.00.049.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.460 I print_info: model type       = 1.4B
0.00.049.460 I print_info: model params     = 1.41 B
0.00.049.460 I print_info: general.name     = 1.4B
0.00.049.461 I print_info: vocab type       = BPE
0.00.049.461 I print_info: n_vocab          = 50304
0.00.049.461 I print_info: n_merges         = 50009
0.00.049.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.462 I print_info: LF token         = 128 'Ä'
0.00.049.466 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.469 I print_info: max token length = 1024
0.00.051.724 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.724 I load_tensors: offloading output layer to GPU
0.00.051.725 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.732 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.733 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.136 I llama_init_from_model: n_seq_max     = 1
0.00.052.137 I llama_init_from_model: n_ctx         = 2048
0.00.052.137 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.137 I llama_init_from_model: n_batch       = 2048
0.00.052.137 I llama_init_from_model: n_ubatch      = 512
0.00.052.137 I llama_init_from_model: flash_attn    = 0
0.00.052.138 I llama_init_from_model: freq_base     = 10000.0
0.00.052.138 I llama_init_from_model: freq_scale    = 1
0.00.052.139 I ggml_metal_init: allocating
0.00.052.142 I ggml_metal_init: found device: Apple M4
0.00.052.143 I ggml_metal_init: picking default device: Apple M4
0.00.052.704 I ggml_metal_init: using embedded metal library
0.00.055.030 I ggml_metal_init: GPU name:   Apple M4
0.00.055.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.032 I ggml_metal_init: simdgroup reduction   = true
0.00.055.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.033 I ggml_metal_init: has bfloat            = true
0.00.055.033 I ggml_metal_init: use bfloat            = true
0.00.055.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.595 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.498 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.507 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.533 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.534 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.535 I llama_init_from_model: graph nodes  = 967
0.00.085.535 I llama_init_from_model: graph splits = 2
0.00.085.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.313 I main: llama threadpool init, n_threads = 4
0.00.596.359 I 
0.00.596.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.384 I 
0.00.596.611 I sampler seed: 1234
0.00.596.616 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.627 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.627 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.348.977 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49168.98 tokens per second)
0.01.348.978 I llama_perf_context_print:        load time =     586.59 ms
0.01.348.979 I llama_perf_context_print: prompt eval time =      46.74 ms /     7 tokens (    6.68 ms per token,   149.75 tokens per second)
0.01.348.980 I llama_perf_context_print:        eval time =     702.89 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.348.980 I llama_perf_context_print:       total time =     753.62 ms /    70 tokens
0.01.349.216 I ggml_metal_free: deallocating

real	0m1.366s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.429 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.430 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.563 I llama_model_loader: - type  f32:  194 tensors
0.00.026.563 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.563 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.564 I print_info: file format = GGUF V3 (latest)
0.00.026.564 I print_info: file type   = Q5_K - Medium
0.00.026.565 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.218 I load: special tokens cache size = 25
0.00.052.358 I load: token to piece cache size = 0.2984 MB
0.00.052.361 I print_info: arch             = gptneox
0.00.052.361 I print_info: vocab_only       = 0
0.00.052.361 I print_info: n_ctx_train      = 2048
0.00.052.361 I print_info: n_embd           = 2048
0.00.052.361 I print_info: n_layer          = 24
0.00.052.364 I print_info: n_head           = 16
0.00.052.365 I print_info: n_head_kv        = 16
0.00.052.367 I print_info: n_rot            = 32
0.00.052.367 I print_info: n_swa            = 0
0.00.052.367 I print_info: n_embd_head_k    = 128
0.00.052.368 I print_info: n_embd_head_v    = 128
0.00.052.368 I print_info: n_gqa            = 1
0.00.052.369 I print_info: n_embd_k_gqa     = 2048
0.00.052.371 I print_info: n_embd_v_gqa     = 2048
0.00.052.371 I print_info: f_norm_eps       = 1.0e-05
0.00.052.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.372 I print_info: f_logit_scale    = 0.0e+00
0.00.052.373 I print_info: n_ff             = 8192
0.00.052.373 I print_info: n_expert         = 0
0.00.052.373 I print_info: n_expert_used    = 0
0.00.052.373 I print_info: causal attn      = 1
0.00.052.373 I print_info: pooling type     = 0
0.00.052.375 I print_info: rope type        = 2
0.00.052.376 I print_info: rope scaling     = linear
0.00.052.377 I print_info: freq_base_train  = 10000.0
0.00.052.377 I print_info: freq_scale_train = 1
0.00.052.377 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.378 I print_info: rope_finetuned   = unknown
0.00.052.378 I print_info: ssm_d_conv       = 0
0.00.052.378 I print_info: ssm_d_inner      = 0
0.00.052.378 I print_info: ssm_d_state      = 0
0.00.052.378 I print_info: ssm_dt_rank      = 0
0.00.052.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.383 I print_info: model type       = 1.4B
0.00.052.383 I print_info: model params     = 1.41 B
0.00.052.383 I print_info: general.name     = 1.4B
0.00.052.384 I print_info: vocab type       = BPE
0.00.052.384 I print_info: n_vocab          = 50304
0.00.052.385 I print_info: n_merges         = 50009
0.00.052.385 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.385 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.385 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.386 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.386 I print_info: LF token         = 128 'Ä'
0.00.052.386 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.387 I print_info: max token length = 1024
0.00.054.400 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.400 I load_tensors: offloading output layer to GPU
0.00.054.400 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.411 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.412 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.758 I llama_init_from_model: n_seq_max     = 1
0.00.054.759 I llama_init_from_model: n_ctx         = 2048
0.00.054.759 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.760 I llama_init_from_model: n_batch       = 2048
0.00.054.760 I llama_init_from_model: n_ubatch      = 512
0.00.054.760 I llama_init_from_model: flash_attn    = 0
0.00.054.760 I llama_init_from_model: freq_base     = 10000.0
0.00.054.760 I llama_init_from_model: freq_scale    = 1
0.00.054.761 I ggml_metal_init: allocating
0.00.054.764 I ggml_metal_init: found device: Apple M4
0.00.054.766 I ggml_metal_init: picking default device: Apple M4
0.00.055.376 I ggml_metal_init: using embedded metal library
0.00.057.739 I ggml_metal_init: GPU name:   Apple M4
0.00.057.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.741 I ggml_metal_init: simdgroup reduction   = true
0.00.057.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.742 I ggml_metal_init: has bfloat            = true
0.00.057.742 I ggml_metal_init: use bfloat            = true
0.00.057.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.711 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.858 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.894 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.921 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.923 I llama_init_from_model: graph nodes  = 967
0.00.087.923 I llama_init_from_model: graph splits = 2
0.00.087.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.294 I main: llama threadpool init, n_threads = 4
0.00.697.328 I 
0.00.697.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.360 I 
0.00.697.583 I sampler seed: 1234
0.00.697.588 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.633 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.633 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.545.564 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.545.564 I llama_perf_context_print:        load time =     686.76 ms
0.01.545.565 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.545.566 I llama_perf_context_print:        eval time =     793.37 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.545.566 I llama_perf_context_print:       total time =     849.14 ms /    70 tokens
0.01.545.822 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.134 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.457 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.464 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.470 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.470 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.713 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.716 I llama_model_loader: - type  f32:  194 tensors
0.00.026.716 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.717 I print_info: file format = GGUF V3 (latest)
0.00.026.717 I print_info: file type   = Q6_K
0.00.026.718 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.230 I load: special tokens cache size = 25
0.00.052.277 I load: token to piece cache size = 0.2984 MB
0.00.052.280 I print_info: arch             = gptneox
0.00.052.280 I print_info: vocab_only       = 0
0.00.052.280 I print_info: n_ctx_train      = 2048
0.00.052.280 I print_info: n_embd           = 2048
0.00.052.281 I print_info: n_layer          = 24
0.00.052.283 I print_info: n_head           = 16
0.00.052.284 I print_info: n_head_kv        = 16
0.00.052.284 I print_info: n_rot            = 32
0.00.052.284 I print_info: n_swa            = 0
0.00.052.285 I print_info: n_embd_head_k    = 128
0.00.052.285 I print_info: n_embd_head_v    = 128
0.00.052.286 I print_info: n_gqa            = 1
0.00.052.286 I print_info: n_embd_k_gqa     = 2048
0.00.052.288 I print_info: n_embd_v_gqa     = 2048
0.00.052.289 I print_info: f_norm_eps       = 1.0e-05
0.00.052.289 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.289 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.291 I print_info: f_logit_scale    = 0.0e+00
0.00.052.292 I print_info: n_ff             = 8192
0.00.052.292 I print_info: n_expert         = 0
0.00.052.292 I print_info: n_expert_used    = 0
0.00.052.293 I print_info: causal attn      = 1
0.00.052.293 I print_info: pooling type     = 0
0.00.052.294 I print_info: rope type        = 2
0.00.052.294 I print_info: rope scaling     = linear
0.00.052.294 I print_info: freq_base_train  = 10000.0
0.00.052.295 I print_info: freq_scale_train = 1
0.00.052.295 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.295 I print_info: rope_finetuned   = unknown
0.00.052.295 I print_info: ssm_d_conv       = 0
0.00.052.295 I print_info: ssm_d_inner      = 0
0.00.052.296 I print_info: ssm_d_state      = 0
0.00.052.296 I print_info: ssm_dt_rank      = 0
0.00.052.296 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.296 I print_info: model type       = 1.4B
0.00.052.296 I print_info: model params     = 1.41 B
0.00.052.297 I print_info: general.name     = 1.4B
0.00.052.297 I print_info: vocab type       = BPE
0.00.052.297 I print_info: n_vocab          = 50304
0.00.052.297 I print_info: n_merges         = 50009
0.00.052.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: LF token         = 128 'Ä'
0.00.052.303 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: max token length = 1024
0.00.054.091 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.091 I load_tensors: offloading output layer to GPU
0.00.054.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.097 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.097 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.373 I llama_init_from_model: n_seq_max     = 1
0.00.054.374 I llama_init_from_model: n_ctx         = 2048
0.00.054.374 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.374 I llama_init_from_model: n_batch       = 2048
0.00.054.374 I llama_init_from_model: n_ubatch      = 512
0.00.054.375 I llama_init_from_model: flash_attn    = 0
0.00.054.375 I llama_init_from_model: freq_base     = 10000.0
0.00.054.375 I llama_init_from_model: freq_scale    = 1
0.00.054.376 I ggml_metal_init: allocating
0.00.054.379 I ggml_metal_init: found device: Apple M4
0.00.054.381 I ggml_metal_init: picking default device: Apple M4
0.00.054.965 I ggml_metal_init: using embedded metal library
0.00.057.309 I ggml_metal_init: GPU name:   Apple M4
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.311 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.311 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.311 I ggml_metal_init: simdgroup reduction   = true
0.00.057.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.312 I ggml_metal_init: has bfloat            = true
0.00.057.312 I ggml_metal_init: use bfloat            = true
0.00.057.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.182 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.125 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.149 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.199 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.200 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.201 I llama_init_from_model: graph nodes  = 967
0.00.088.201 I llama_init_from_model: graph splits = 2
0.00.088.204 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.868 I main: llama threadpool init, n_threads = 4
0.00.737.906 I 
0.00.737.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.950 I 
0.00.738.188 I sampler seed: 1234
0.00.738.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.227 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.231 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.231 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.623.684 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.623.684 I llama_perf_context_print:        load time =     727.85 ms
0.01.623.685 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.48 tokens per second)
0.01.623.686 I llama_perf_context_print:        eval time =     828.15 ms /    63 runs   (   13.15 ms per token,    76.07 tokens per second)
0.01.623.687 I llama_perf_context_print:       total time =     886.70 ms /    70 tokens
0.01.623.975 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.787 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.785 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.798 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.952 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.956 I llama_model_loader: - type  f32:  194 tensors
0.00.055.957 I llama_model_loader: - type  f16:   98 tensors
0.00.055.958 I print_info: file format = GGUF V3 (latest)
0.00.055.958 I print_info: file type   = all F32 (guessed)
0.00.055.960 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.157 I load: special tokens cache size = 25
0.00.091.086 I load: token to piece cache size = 0.2984 MB
0.00.091.089 I print_info: arch             = gptneox
0.00.091.089 I print_info: vocab_only       = 0
0.00.091.090 I print_info: n_ctx_train      = 2048
0.00.091.090 I print_info: n_embd           = 2048
0.00.091.090 I print_info: n_layer          = 24
0.00.091.093 I print_info: n_head           = 16
0.00.091.095 I print_info: n_head_kv        = 16
0.00.091.095 I print_info: n_rot            = 32
0.00.091.095 I print_info: n_swa            = 0
0.00.091.095 I print_info: n_embd_head_k    = 128
0.00.091.096 I print_info: n_embd_head_v    = 128
0.00.091.096 I print_info: n_gqa            = 1
0.00.091.097 I print_info: n_embd_k_gqa     = 2048
0.00.091.097 I print_info: n_embd_v_gqa     = 2048
0.00.091.098 I print_info: f_norm_eps       = 1.0e-05
0.00.091.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.099 I print_info: f_logit_scale    = 0.0e+00
0.00.091.099 I print_info: n_ff             = 8192
0.00.091.099 I print_info: n_expert         = 0
0.00.091.100 I print_info: n_expert_used    = 0
0.00.091.100 I print_info: causal attn      = 1
0.00.091.100 I print_info: pooling type     = 0
0.00.091.100 I print_info: rope type        = 2
0.00.091.100 I print_info: rope scaling     = linear
0.00.091.100 I print_info: freq_base_train  = 10000.0
0.00.091.101 I print_info: freq_scale_train = 1
0.00.091.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.101 I print_info: rope_finetuned   = unknown
0.00.091.101 I print_info: ssm_d_conv       = 0
0.00.091.101 I print_info: ssm_d_inner      = 0
0.00.091.102 I print_info: ssm_d_state      = 0
0.00.091.102 I print_info: ssm_dt_rank      = 0
0.00.091.102 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.102 I print_info: model type       = 1.4B
0.00.091.102 I print_info: model params     = 1.41 B
0.00.091.103 I print_info: general.name     = 1.4B
0.00.091.103 I print_info: vocab type       = BPE
0.00.091.103 I print_info: n_vocab          = 50304
0.00.091.103 I print_info: n_merges         = 50009
0.00.091.104 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.104 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.104 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.104 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.104 I print_info: LF token         = 128 'Ä'
0.00.091.105 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.105 I print_info: max token length = 1024
0.00.093.779 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.779 I load_tensors: offloading output layer to GPU
0.00.093.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.790 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.791 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.050 I llama_init_from_model: n_seq_max     = 1
0.00.094.050 I llama_init_from_model: n_ctx         = 128
0.00.094.051 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.051 I llama_init_from_model: n_batch       = 128
0.00.094.051 I llama_init_from_model: n_ubatch      = 128
0.00.094.051 I llama_init_from_model: flash_attn    = 0
0.00.094.052 I llama_init_from_model: freq_base     = 10000.0
0.00.094.052 I llama_init_from_model: freq_scale    = 1
0.00.094.052 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.053 I ggml_metal_init: allocating
0.00.094.055 I ggml_metal_init: found device: Apple M4
0.00.094.057 I ggml_metal_init: picking default device: Apple M4
0.00.094.691 I ggml_metal_init: using embedded metal library
0.00.097.307 I ggml_metal_init: GPU name:   Apple M4
0.00.097.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.310 I ggml_metal_init: simdgroup reduction   = true
0.00.097.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.310 I ggml_metal_init: has bfloat            = true
0.00.097.310 I ggml_metal_init: use bfloat            = true
0.00.097.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.889 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.057 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.058 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.058 I llama_init_from_model: graph nodes  = 967
0.00.109.058 I llama_init_from_model: graph splits = 2
0.00.109.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.121.494 I 
0.01.121.557 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.121.589 I perplexity: tokenizing the input ..
0.01.135.155 I perplexity: tokenization took 13.563 ms
0.01.135.195 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.258.346 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.260.238 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.260.265 I llama_perf_context_print:        load time =    1097.54 ms
0.01.260.269 I llama_perf_context_print: prompt eval time =     122.13 ms /   128 tokens (    0.95 ms per token,  1048.05 tokens per second)
0.01.260.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.260.271 I llama_perf_context_print:       total time =     138.77 ms /   129 tokens
0.01.261.020 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.125s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.140 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.768 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.507 I llama_model_loader: - type  f32:  194 tensors
0.00.033.507 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.508 I print_info: file format = GGUF V3 (latest)
0.00.033.508 I print_info: file type   = Q8_0
0.00.033.509 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.427 I load: special tokens cache size = 25
0.00.063.629 I load: token to piece cache size = 0.2984 MB
0.00.063.632 I print_info: arch             = gptneox
0.00.063.632 I print_info: vocab_only       = 0
0.00.063.632 I print_info: n_ctx_train      = 2048
0.00.063.632 I print_info: n_embd           = 2048
0.00.063.632 I print_info: n_layer          = 24
0.00.063.635 I print_info: n_head           = 16
0.00.063.636 I print_info: n_head_kv        = 16
0.00.063.636 I print_info: n_rot            = 32
0.00.063.636 I print_info: n_swa            = 0
0.00.063.636 I print_info: n_embd_head_k    = 128
0.00.063.636 I print_info: n_embd_head_v    = 128
0.00.063.637 I print_info: n_gqa            = 1
0.00.063.638 I print_info: n_embd_k_gqa     = 2048
0.00.063.640 I print_info: n_embd_v_gqa     = 2048
0.00.063.641 I print_info: f_norm_eps       = 1.0e-05
0.00.063.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.642 I print_info: f_logit_scale    = 0.0e+00
0.00.063.643 I print_info: n_ff             = 8192
0.00.063.643 I print_info: n_expert         = 0
0.00.063.643 I print_info: n_expert_used    = 0
0.00.063.643 I print_info: causal attn      = 1
0.00.063.643 I print_info: pooling type     = 0
0.00.063.643 I print_info: rope type        = 2
0.00.063.644 I print_info: rope scaling     = linear
0.00.063.644 I print_info: freq_base_train  = 10000.0
0.00.063.644 I print_info: freq_scale_train = 1
0.00.063.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.645 I print_info: rope_finetuned   = unknown
0.00.063.645 I print_info: ssm_d_conv       = 0
0.00.063.645 I print_info: ssm_d_inner      = 0
0.00.063.645 I print_info: ssm_d_state      = 0
0.00.063.645 I print_info: ssm_dt_rank      = 0
0.00.063.645 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.646 I print_info: model type       = 1.4B
0.00.063.646 I print_info: model params     = 1.41 B
0.00.063.646 I print_info: general.name     = 1.4B
0.00.063.647 I print_info: vocab type       = BPE
0.00.063.647 I print_info: n_vocab          = 50304
0.00.063.647 I print_info: n_merges         = 50009
0.00.063.647 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.647 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.648 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.648 I print_info: LF token         = 128 'Ä'
0.00.063.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.648 I print_info: max token length = 1024
0.00.065.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.895 I load_tensors: offloading output layer to GPU
0.00.065.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.906 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.908 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.192 I llama_init_from_model: n_seq_max     = 1
0.00.066.193 I llama_init_from_model: n_ctx         = 128
0.00.066.193 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.193 I llama_init_from_model: n_batch       = 128
0.00.066.193 I llama_init_from_model: n_ubatch      = 128
0.00.066.193 I llama_init_from_model: flash_attn    = 0
0.00.066.194 I llama_init_from_model: freq_base     = 10000.0
0.00.066.194 I llama_init_from_model: freq_scale    = 1
0.00.066.194 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.194 I ggml_metal_init: allocating
0.00.066.197 I ggml_metal_init: found device: Apple M4
0.00.066.199 I ggml_metal_init: picking default device: Apple M4
0.00.066.798 I ggml_metal_init: using embedded metal library
0.00.069.396 I ggml_metal_init: GPU name:   Apple M4
0.00.069.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.399 I ggml_metal_init: simdgroup reduction   = true
0.00.069.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.399 I ggml_metal_init: has bfloat            = true
0.00.069.399 I ggml_metal_init: use bfloat            = true
0.00.069.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.805 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.037 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.040 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.055 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.865 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.866 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.866 I llama_init_from_model: graph nodes  = 967
0.00.080.867 I llama_init_from_model: graph splits = 2
0.00.080.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.903.549 I 
0.00.903.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.903.617 I perplexity: tokenizing the input ..
0.00.911.868 I perplexity: tokenization took 8.249 ms
0.00.911.878 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.036.328 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.037.496 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.037.518 I llama_perf_context_print:        load time =     892.40 ms
0.01.037.519 I llama_perf_context_print: prompt eval time =     124.22 ms /   128 tokens (    0.97 ms per token,  1030.40 tokens per second)
0.01.037.519 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.037.520 I llama_perf_context_print:       total time =     133.97 ms /   129 tokens
0.01.038.000 I ggml_metal_free: deallocating

real	0m1.056s
user	0m0.092s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.036 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.476 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.501 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.503 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.504 I llama_model_loader: - type  f32:  194 tensors
0.00.026.504 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.504 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.505 I print_info: file format = GGUF V3 (latest)
0.00.026.505 I print_info: file type   = Q4_0
0.00.026.506 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.917 I load: special tokens cache size = 25
0.00.051.929 I load: token to piece cache size = 0.2984 MB
0.00.051.932 I print_info: arch             = gptneox
0.00.051.933 I print_info: vocab_only       = 0
0.00.051.933 I print_info: n_ctx_train      = 2048
0.00.051.933 I print_info: n_embd           = 2048
0.00.051.933 I print_info: n_layer          = 24
0.00.051.936 I print_info: n_head           = 16
0.00.051.937 I print_info: n_head_kv        = 16
0.00.051.937 I print_info: n_rot            = 32
0.00.051.937 I print_info: n_swa            = 0
0.00.051.938 I print_info: n_embd_head_k    = 128
0.00.051.938 I print_info: n_embd_head_v    = 128
0.00.051.939 I print_info: n_gqa            = 1
0.00.051.939 I print_info: n_embd_k_gqa     = 2048
0.00.051.940 I print_info: n_embd_v_gqa     = 2048
0.00.051.940 I print_info: f_norm_eps       = 1.0e-05
0.00.051.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.941 I print_info: f_logit_scale    = 0.0e+00
0.00.051.942 I print_info: n_ff             = 8192
0.00.051.942 I print_info: n_expert         = 0
0.00.051.942 I print_info: n_expert_used    = 0
0.00.051.942 I print_info: causal attn      = 1
0.00.051.942 I print_info: pooling type     = 0
0.00.051.943 I print_info: rope type        = 2
0.00.051.943 I print_info: rope scaling     = linear
0.00.051.943 I print_info: freq_base_train  = 10000.0
0.00.051.944 I print_info: freq_scale_train = 1
0.00.051.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.944 I print_info: rope_finetuned   = unknown
0.00.051.944 I print_info: ssm_d_conv       = 0
0.00.051.944 I print_info: ssm_d_inner      = 0
0.00.051.945 I print_info: ssm_d_state      = 0
0.00.051.945 I print_info: ssm_dt_rank      = 0
0.00.051.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.945 I print_info: model type       = 1.4B
0.00.051.945 I print_info: model params     = 1.41 B
0.00.051.946 I print_info: general.name     = 1.4B
0.00.051.946 I print_info: vocab type       = BPE
0.00.051.946 I print_info: n_vocab          = 50304
0.00.051.947 I print_info: n_merges         = 50009
0.00.051.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.948 I print_info: LF token         = 128 'Ä'
0.00.051.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.948 I print_info: max token length = 1024
0.00.053.911 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.912 I load_tensors: offloading output layer to GPU
0.00.053.912 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.922 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.924 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.204 I llama_init_from_model: n_seq_max     = 1
0.00.054.205 I llama_init_from_model: n_ctx         = 128
0.00.054.205 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.206 I llama_init_from_model: n_batch       = 128
0.00.054.206 I llama_init_from_model: n_ubatch      = 128
0.00.054.206 I llama_init_from_model: flash_attn    = 0
0.00.054.206 I llama_init_from_model: freq_base     = 10000.0
0.00.054.206 I llama_init_from_model: freq_scale    = 1
0.00.054.207 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.207 I ggml_metal_init: allocating
0.00.054.210 I ggml_metal_init: found device: Apple M4
0.00.054.212 I ggml_metal_init: picking default device: Apple M4
0.00.054.792 I ggml_metal_init: using embedded metal library
0.00.057.138 I ggml_metal_init: GPU name:   Apple M4
0.00.057.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.140 I ggml_metal_init: simdgroup reduction   = true
0.00.057.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.140 I ggml_metal_init: has bfloat            = true
0.00.057.140 I ggml_metal_init: use bfloat            = true
0.00.057.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.054 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.323 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.276 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.277 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.278 I llama_init_from_model: graph nodes  = 967
0.00.069.278 I llama_init_from_model: graph splits = 2
0.00.069.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.187 I 
0.00.667.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.275 I perplexity: tokenizing the input ..
0.00.675.005 I perplexity: tokenization took 7.727 ms
0.00.675.015 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.822 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.978 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.799.002 I llama_perf_context_print:        load time =     657.14 ms
0.00.799.004 I llama_perf_context_print: prompt eval time =     122.56 ms /   128 tokens (    0.96 ms per token,  1044.43 tokens per second)
0.00.799.005 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.005 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.799.519 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.112 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.113 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.113 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.117 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.118 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.122 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.821 I print_info: file format = GGUF V3 (latest)
0.00.024.821 I print_info: file type   = Q4_1
0.00.024.822 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.553 I load: special tokens cache size = 25
0.00.049.306 I load: token to piece cache size = 0.2984 MB
0.00.049.309 I print_info: arch             = gptneox
0.00.049.309 I print_info: vocab_only       = 0
0.00.049.310 I print_info: n_ctx_train      = 2048
0.00.049.310 I print_info: n_embd           = 2048
0.00.049.310 I print_info: n_layer          = 24
0.00.049.313 I print_info: n_head           = 16
0.00.049.314 I print_info: n_head_kv        = 16
0.00.049.314 I print_info: n_rot            = 32
0.00.049.314 I print_info: n_swa            = 0
0.00.049.314 I print_info: n_embd_head_k    = 128
0.00.049.315 I print_info: n_embd_head_v    = 128
0.00.049.315 I print_info: n_gqa            = 1
0.00.049.316 I print_info: n_embd_k_gqa     = 2048
0.00.049.317 I print_info: n_embd_v_gqa     = 2048
0.00.049.317 I print_info: f_norm_eps       = 1.0e-05
0.00.049.320 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.320 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.320 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.320 I print_info: f_logit_scale    = 0.0e+00
0.00.049.321 I print_info: n_ff             = 8192
0.00.049.321 I print_info: n_expert         = 0
0.00.049.321 I print_info: n_expert_used    = 0
0.00.049.321 I print_info: causal attn      = 1
0.00.049.321 I print_info: pooling type     = 0
0.00.049.322 I print_info: rope type        = 2
0.00.049.322 I print_info: rope scaling     = linear
0.00.049.322 I print_info: freq_base_train  = 10000.0
0.00.049.323 I print_info: freq_scale_train = 1
0.00.049.323 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.323 I print_info: rope_finetuned   = unknown
0.00.049.323 I print_info: ssm_d_conv       = 0
0.00.049.323 I print_info: ssm_d_inner      = 0
0.00.049.323 I print_info: ssm_d_state      = 0
0.00.049.324 I print_info: ssm_dt_rank      = 0
0.00.049.324 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.324 I print_info: model type       = 1.4B
0.00.049.324 I print_info: model params     = 1.41 B
0.00.049.325 I print_info: general.name     = 1.4B
0.00.049.325 I print_info: vocab type       = BPE
0.00.049.325 I print_info: n_vocab          = 50304
0.00.049.325 I print_info: n_merges         = 50009
0.00.049.326 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.327 I print_info: LF token         = 128 'Ä'
0.00.049.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.327 I print_info: max token length = 1024
0.00.051.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.265 I load_tensors: offloading output layer to GPU
0.00.051.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.276 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.277 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.552 I llama_init_from_model: n_seq_max     = 1
0.00.051.553 I llama_init_from_model: n_ctx         = 128
0.00.051.553 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.553 I llama_init_from_model: n_batch       = 128
0.00.051.553 I llama_init_from_model: n_ubatch      = 128
0.00.051.553 I llama_init_from_model: flash_attn    = 0
0.00.051.554 I llama_init_from_model: freq_base     = 10000.0
0.00.051.554 I llama_init_from_model: freq_scale    = 1
0.00.051.554 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.555 I ggml_metal_init: allocating
0.00.051.557 I ggml_metal_init: found device: Apple M4
0.00.051.559 I ggml_metal_init: picking default device: Apple M4
0.00.052.128 I ggml_metal_init: using embedded metal library
0.00.054.462 I ggml_metal_init: GPU name:   Apple M4
0.00.054.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.464 I ggml_metal_init: simdgroup reduction   = true
0.00.054.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.464 I ggml_metal_init: has bfloat            = true
0.00.054.464 I ggml_metal_init: use bfloat            = true
0.00.054.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.220 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.528 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.530 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.544 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.431 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.432 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.432 I llama_init_from_model: graph nodes  = 967
0.00.066.433 I llama_init_from_model: graph splits = 2
0.00.066.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.741 I 
0.00.746.797 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.825 I perplexity: tokenizing the input ..
0.00.754.967 I perplexity: tokenization took 8.141 ms
0.00.754.978 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.942 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.879.116 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.879.136 I llama_perf_context_print:        load time =     737.77 ms
0.00.879.137 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.88 tokens per second)
0.00.879.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.138 I llama_perf_context_print:       total time =     132.40 ms /   129 tokens
0.00.879.650 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.642 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.865 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.865 I llama_model_loader: - type  f32:  194 tensors
0.00.025.866 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.867 I print_info: file format = GGUF V3 (latest)
0.00.025.867 I print_info: file type   = Q5_0
0.00.025.868 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.306 I load: special tokens cache size = 25
0.00.051.306 I load: token to piece cache size = 0.2984 MB
0.00.051.309 I print_info: arch             = gptneox
0.00.051.309 I print_info: vocab_only       = 0
0.00.051.309 I print_info: n_ctx_train      = 2048
0.00.051.310 I print_info: n_embd           = 2048
0.00.051.310 I print_info: n_layer          = 24
0.00.051.313 I print_info: n_head           = 16
0.00.051.313 I print_info: n_head_kv        = 16
0.00.051.313 I print_info: n_rot            = 32
0.00.051.314 I print_info: n_swa            = 0
0.00.051.314 I print_info: n_embd_head_k    = 128
0.00.051.314 I print_info: n_embd_head_v    = 128
0.00.051.315 I print_info: n_gqa            = 1
0.00.051.316 I print_info: n_embd_k_gqa     = 2048
0.00.051.316 I print_info: n_embd_v_gqa     = 2048
0.00.051.317 I print_info: f_norm_eps       = 1.0e-05
0.00.051.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.318 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.318 I print_info: f_logit_scale    = 0.0e+00
0.00.051.319 I print_info: n_ff             = 8192
0.00.051.320 I print_info: n_expert         = 0
0.00.051.320 I print_info: n_expert_used    = 0
0.00.051.320 I print_info: causal attn      = 1
0.00.051.321 I print_info: pooling type     = 0
0.00.051.321 I print_info: rope type        = 2
0.00.051.321 I print_info: rope scaling     = linear
0.00.051.321 I print_info: freq_base_train  = 10000.0
0.00.051.322 I print_info: freq_scale_train = 1
0.00.051.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.322 I print_info: rope_finetuned   = unknown
0.00.051.322 I print_info: ssm_d_conv       = 0
0.00.051.323 I print_info: ssm_d_inner      = 0
0.00.051.323 I print_info: ssm_d_state      = 0
0.00.051.323 I print_info: ssm_dt_rank      = 0
0.00.051.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.323 I print_info: model type       = 1.4B
0.00.051.324 I print_info: model params     = 1.41 B
0.00.051.324 I print_info: general.name     = 1.4B
0.00.051.324 I print_info: vocab type       = BPE
0.00.051.325 I print_info: n_vocab          = 50304
0.00.051.325 I print_info: n_merges         = 50009
0.00.051.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: LF token         = 128 'Ä'
0.00.051.326 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: max token length = 1024
0.00.053.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.338 I load_tensors: offloading output layer to GPU
0.00.053.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.349 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.350 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.642 I llama_init_from_model: n_seq_max     = 1
0.00.053.643 I llama_init_from_model: n_ctx         = 128
0.00.053.643 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.644 I llama_init_from_model: n_batch       = 128
0.00.053.644 I llama_init_from_model: n_ubatch      = 128
0.00.053.644 I llama_init_from_model: flash_attn    = 0
0.00.053.644 I llama_init_from_model: freq_base     = 10000.0
0.00.053.644 I llama_init_from_model: freq_scale    = 1
0.00.053.645 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.645 I ggml_metal_init: allocating
0.00.053.648 I ggml_metal_init: found device: Apple M4
0.00.053.650 I ggml_metal_init: picking default device: Apple M4
0.00.054.223 I ggml_metal_init: using embedded metal library
0.00.056.600 I ggml_metal_init: GPU name:   Apple M4
0.00.056.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.603 I ggml_metal_init: simdgroup reduction   = true
0.00.056.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.603 I ggml_metal_init: has bfloat            = true
0.00.056.603 I ggml_metal_init: use bfloat            = true
0.00.056.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.441 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.740 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.745 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.761 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.636 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.637 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.638 I llama_init_from_model: graph nodes  = 967
0.00.068.638 I llama_init_from_model: graph splits = 2
0.00.068.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.544 I 
0.00.829.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.587 I perplexity: tokenizing the input ..
0.00.838.218 I perplexity: tokenization took 8.629 ms
0.00.838.231 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.972.719 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.974.074 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.974.088 I llama_perf_context_print:        load time =     819.90 ms
0.00.974.089 I llama_perf_context_print: prompt eval time =     134.25 ms /   128 tokens (    1.05 ms per token,   953.47 tokens per second)
0.00.974.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.974.090 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.974.428 I ggml_metal_free: deallocating

real	0m0.990s
user	0m0.080s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.825 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.029.270 I llama_model_loader: - type  f32:  194 tensors
0.00.029.270 I llama_model_loader: - type q5_1:   97 tensors
0.00.029.270 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.271 I print_info: file format = GGUF V3 (latest)
0.00.029.273 I print_info: file type   = Q5_1
0.00.029.275 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.049.591 I load: special tokens cache size = 25
0.00.055.823 I load: token to piece cache size = 0.2984 MB
0.00.055.827 I print_info: arch             = gptneox
0.00.055.828 I print_info: vocab_only       = 0
0.00.055.828 I print_info: n_ctx_train      = 2048
0.00.055.828 I print_info: n_embd           = 2048
0.00.055.828 I print_info: n_layer          = 24
0.00.055.833 I print_info: n_head           = 16
0.00.055.833 I print_info: n_head_kv        = 16
0.00.055.833 I print_info: n_rot            = 32
0.00.055.834 I print_info: n_swa            = 0
0.00.055.834 I print_info: n_embd_head_k    = 128
0.00.055.834 I print_info: n_embd_head_v    = 128
0.00.055.835 I print_info: n_gqa            = 1
0.00.055.835 I print_info: n_embd_k_gqa     = 2048
0.00.055.836 I print_info: n_embd_v_gqa     = 2048
0.00.055.837 I print_info: f_norm_eps       = 1.0e-05
0.00.055.837 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.837 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.837 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.837 I print_info: f_logit_scale    = 0.0e+00
0.00.055.838 I print_info: n_ff             = 8192
0.00.055.838 I print_info: n_expert         = 0
0.00.055.838 I print_info: n_expert_used    = 0
0.00.055.838 I print_info: causal attn      = 1
0.00.055.839 I print_info: pooling type     = 0
0.00.055.839 I print_info: rope type        = 2
0.00.055.839 I print_info: rope scaling     = linear
0.00.055.839 I print_info: freq_base_train  = 10000.0
0.00.055.839 I print_info: freq_scale_train = 1
0.00.055.840 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.840 I print_info: rope_finetuned   = unknown
0.00.055.840 I print_info: ssm_d_conv       = 0
0.00.055.840 I print_info: ssm_d_inner      = 0
0.00.055.840 I print_info: ssm_d_state      = 0
0.00.055.840 I print_info: ssm_dt_rank      = 0
0.00.055.840 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.841 I print_info: model type       = 1.4B
0.00.055.841 I print_info: model params     = 1.41 B
0.00.055.841 I print_info: general.name     = 1.4B
0.00.055.842 I print_info: vocab type       = BPE
0.00.055.842 I print_info: n_vocab          = 50304
0.00.055.842 I print_info: n_merges         = 50009
0.00.055.842 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.842 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.842 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.843 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.843 I print_info: LF token         = 128 'Ä'
0.00.055.843 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.843 I print_info: max token length = 1024
0.00.057.861 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.861 I load_tensors: offloading output layer to GPU
0.00.057.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.872 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.057.874 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.058.140 I llama_init_from_model: n_seq_max     = 1
0.00.058.141 I llama_init_from_model: n_ctx         = 128
0.00.058.141 I llama_init_from_model: n_ctx_per_seq = 128
0.00.058.141 I llama_init_from_model: n_batch       = 128
0.00.058.141 I llama_init_from_model: n_ubatch      = 128
0.00.058.141 I llama_init_from_model: flash_attn    = 0
0.00.058.142 I llama_init_from_model: freq_base     = 10000.0
0.00.058.142 I llama_init_from_model: freq_scale    = 1
0.00.058.142 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.143 I ggml_metal_init: allocating
0.00.058.147 I ggml_metal_init: found device: Apple M4
0.00.058.150 I ggml_metal_init: picking default device: Apple M4
0.00.058.753 I ggml_metal_init: using embedded metal library
0.00.061.194 I ggml_metal_init: GPU name:   Apple M4
0.00.061.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.197 I ggml_metal_init: simdgroup reduction   = true
0.00.061.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.197 I ggml_metal_init: has bfloat            = true
0.00.061.197 I ggml_metal_init: use bfloat            = true
0.00.061.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.256 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.711 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.718 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.737 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.706 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.708 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.708 I llama_init_from_model: graph nodes  = 967
0.00.072.708 I llama_init_from_model: graph splits = 2
0.00.072.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.799 I 
0.00.678.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.854 I perplexity: tokenizing the input ..
0.00.686.368 I perplexity: tokenization took 7.513 ms
0.00.686.382 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.362 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.967 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.980 I llama_perf_context_print:        load time =     668.18 ms
0.00.821.981 I llama_perf_context_print: prompt eval time =     133.75 ms /   128 tokens (    1.04 ms per token,   957.01 tokens per second)
0.00.821.982 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.982 I llama_perf_context_print:       total time =     143.18 ms /   129 tokens
0.00.822.338 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.247 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.468 I llama_model_loader: - type  f32:  194 tensors
0.00.027.469 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.469 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.469 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.470 I print_info: file format = GGUF V3 (latest)
0.00.027.470 I print_info: file type   = Q2_K - Medium
0.00.027.471 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.892 I load: special tokens cache size = 25
0.00.052.833 I load: token to piece cache size = 0.2984 MB
0.00.052.837 I print_info: arch             = gptneox
0.00.052.837 I print_info: vocab_only       = 0
0.00.052.837 I print_info: n_ctx_train      = 2048
0.00.052.837 I print_info: n_embd           = 2048
0.00.052.837 I print_info: n_layer          = 24
0.00.052.841 I print_info: n_head           = 16
0.00.052.842 I print_info: n_head_kv        = 16
0.00.052.842 I print_info: n_rot            = 32
0.00.052.843 I print_info: n_swa            = 0
0.00.052.843 I print_info: n_embd_head_k    = 128
0.00.052.843 I print_info: n_embd_head_v    = 128
0.00.052.844 I print_info: n_gqa            = 1
0.00.052.845 I print_info: n_embd_k_gqa     = 2048
0.00.052.846 I print_info: n_embd_v_gqa     = 2048
0.00.052.846 I print_info: f_norm_eps       = 1.0e-05
0.00.052.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.847 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.847 I print_info: f_logit_scale    = 0.0e+00
0.00.052.848 I print_info: n_ff             = 8192
0.00.052.848 I print_info: n_expert         = 0
0.00.052.848 I print_info: n_expert_used    = 0
0.00.052.848 I print_info: causal attn      = 1
0.00.052.848 I print_info: pooling type     = 0
0.00.052.848 I print_info: rope type        = 2
0.00.052.848 I print_info: rope scaling     = linear
0.00.052.849 I print_info: freq_base_train  = 10000.0
0.00.052.849 I print_info: freq_scale_train = 1
0.00.052.850 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.850 I print_info: rope_finetuned   = unknown
0.00.052.850 I print_info: ssm_d_conv       = 0
0.00.052.850 I print_info: ssm_d_inner      = 0
0.00.052.850 I print_info: ssm_d_state      = 0
0.00.052.850 I print_info: ssm_dt_rank      = 0
0.00.052.851 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.851 I print_info: model type       = 1.4B
0.00.052.851 I print_info: model params     = 1.41 B
0.00.052.851 I print_info: general.name     = 1.4B
0.00.052.853 I print_info: vocab type       = BPE
0.00.052.853 I print_info: n_vocab          = 50304
0.00.052.853 I print_info: n_merges         = 50009
0.00.052.853 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.853 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.854 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.854 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.854 I print_info: LF token         = 128 'Ä'
0.00.052.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.854 I print_info: max token length = 1024
0.00.055.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.983 I load_tensors: offloading output layer to GPU
0.00.055.984 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.994 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.995 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.056.271 I llama_init_from_model: n_seq_max     = 1
0.00.056.272 I llama_init_from_model: n_ctx         = 128
0.00.056.272 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.272 I llama_init_from_model: n_batch       = 128
0.00.056.272 I llama_init_from_model: n_ubatch      = 128
0.00.056.272 I llama_init_from_model: flash_attn    = 0
0.00.056.273 I llama_init_from_model: freq_base     = 10000.0
0.00.056.273 I llama_init_from_model: freq_scale    = 1
0.00.056.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.274 I ggml_metal_init: allocating
0.00.056.277 I ggml_metal_init: found device: Apple M4
0.00.056.279 I ggml_metal_init: picking default device: Apple M4
0.00.056.890 I ggml_metal_init: using embedded metal library
0.00.059.498 I ggml_metal_init: GPU name:   Apple M4
0.00.059.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.500 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.500 I ggml_metal_init: simdgroup reduction   = true
0.00.059.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.501 I ggml_metal_init: has bfloat            = true
0.00.059.501 I ggml_metal_init: use bfloat            = true
0.00.059.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.417 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.748 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.750 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.765 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.689 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.690 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.690 I llama_init_from_model: graph nodes  = 967
0.00.070.690 I llama_init_from_model: graph splits = 2
0.00.070.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.879 I 
0.00.484.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.934 I perplexity: tokenizing the input ..
0.00.492.172 I perplexity: tokenization took 7.236 ms
0.00.492.184 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.624.774 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.625.944 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.625.964 I llama_perf_context_print:        load time =     473.62 ms
0.00.625.965 I llama_perf_context_print: prompt eval time =     132.35 ms /   128 tokens (    1.03 ms per token,   967.11 tokens per second)
0.00.625.967 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.968 I llama_perf_context_print:       total time =     141.09 ms /   129 tokens
0.00.626.436 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.078s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.486 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.252 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.255 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.740 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.741 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.741 I llama_model_loader: - type  f32:  194 tensors
0.00.023.742 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.742 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.742 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.743 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.743 I print_info: file format = GGUF V3 (latest)
0.00.023.743 I print_info: file type   = Q3_K - Medium
0.00.023.744 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.456 I load: special tokens cache size = 25
0.00.048.324 I load: token to piece cache size = 0.2984 MB
0.00.048.327 I print_info: arch             = gptneox
0.00.048.327 I print_info: vocab_only       = 0
0.00.048.327 I print_info: n_ctx_train      = 2048
0.00.048.328 I print_info: n_embd           = 2048
0.00.048.328 I print_info: n_layer          = 24
0.00.048.330 I print_info: n_head           = 16
0.00.048.331 I print_info: n_head_kv        = 16
0.00.048.331 I print_info: n_rot            = 32
0.00.048.332 I print_info: n_swa            = 0
0.00.048.332 I print_info: n_embd_head_k    = 128
0.00.048.332 I print_info: n_embd_head_v    = 128
0.00.048.333 I print_info: n_gqa            = 1
0.00.048.334 I print_info: n_embd_k_gqa     = 2048
0.00.048.334 I print_info: n_embd_v_gqa     = 2048
0.00.048.335 I print_info: f_norm_eps       = 1.0e-05
0.00.048.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.336 I print_info: f_logit_scale    = 0.0e+00
0.00.048.336 I print_info: n_ff             = 8192
0.00.048.337 I print_info: n_expert         = 0
0.00.048.337 I print_info: n_expert_used    = 0
0.00.048.337 I print_info: causal attn      = 1
0.00.048.337 I print_info: pooling type     = 0
0.00.048.337 I print_info: rope type        = 2
0.00.048.338 I print_info: rope scaling     = linear
0.00.048.338 I print_info: freq_base_train  = 10000.0
0.00.048.338 I print_info: freq_scale_train = 1
0.00.048.338 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.339 I print_info: rope_finetuned   = unknown
0.00.048.339 I print_info: ssm_d_conv       = 0
0.00.048.339 I print_info: ssm_d_inner      = 0
0.00.048.339 I print_info: ssm_d_state      = 0
0.00.048.339 I print_info: ssm_dt_rank      = 0
0.00.048.339 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.340 I print_info: model type       = 1.4B
0.00.048.340 I print_info: model params     = 1.41 B
0.00.048.340 I print_info: general.name     = 1.4B
0.00.048.341 I print_info: vocab type       = BPE
0.00.048.341 I print_info: n_vocab          = 50304
0.00.048.341 I print_info: n_merges         = 50009
0.00.048.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.342 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.342 I print_info: LF token         = 128 'Ä'
0.00.048.343 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.343 I print_info: max token length = 1024
0.00.050.231 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.231 I load_tensors: offloading output layer to GPU
0.00.050.231 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.241 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.243 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.519 I llama_init_from_model: n_seq_max     = 1
0.00.050.519 I llama_init_from_model: n_ctx         = 128
0.00.050.520 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.520 I llama_init_from_model: n_batch       = 128
0.00.050.520 I llama_init_from_model: n_ubatch      = 128
0.00.050.520 I llama_init_from_model: flash_attn    = 0
0.00.050.520 I llama_init_from_model: freq_base     = 10000.0
0.00.050.521 I llama_init_from_model: freq_scale    = 1
0.00.050.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.521 I ggml_metal_init: allocating
0.00.050.524 I ggml_metal_init: found device: Apple M4
0.00.050.526 I ggml_metal_init: picking default device: Apple M4
0.00.051.095 I ggml_metal_init: using embedded metal library
0.00.053.440 I ggml_metal_init: GPU name:   Apple M4
0.00.053.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.442 I ggml_metal_init: simdgroup reduction   = true
0.00.053.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.442 I ggml_metal_init: has bfloat            = true
0.00.053.443 I ggml_metal_init: use bfloat            = true
0.00.053.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.082 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.350 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.366 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.312 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.313 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.314 I llama_init_from_model: graph nodes  = 967
0.00.065.314 I llama_init_from_model: graph splits = 2
0.00.065.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.129 I 
0.00.459.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.186 I perplexity: tokenizing the input ..
0.00.466.920 I perplexity: tokenization took 7.731 ms
0.00.466.930 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.209 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.600.540 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.600.559 I llama_perf_context_print:        load time =     450.64 ms
0.00.600.560 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.33 tokens per second)
0.00.600.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.562 I llama_perf_context_print:       total time =     141.43 ms /   129 tokens
0.00.601.087 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.076s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.426 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.459 I llama_model_loader: - type  f32:  194 tensors
0.00.025.460 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.460 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.460 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.460 I print_info: file format = GGUF V3 (latest)
0.00.025.461 I print_info: file type   = Q4_K - Medium
0.00.025.462 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.249 I load: special tokens cache size = 25
0.00.050.382 I load: token to piece cache size = 0.2984 MB
0.00.050.385 I print_info: arch             = gptneox
0.00.050.386 I print_info: vocab_only       = 0
0.00.050.386 I print_info: n_ctx_train      = 2048
0.00.050.386 I print_info: n_embd           = 2048
0.00.050.386 I print_info: n_layer          = 24
0.00.050.389 I print_info: n_head           = 16
0.00.050.390 I print_info: n_head_kv        = 16
0.00.050.390 I print_info: n_rot            = 32
0.00.050.393 I print_info: n_swa            = 0
0.00.050.393 I print_info: n_embd_head_k    = 128
0.00.050.393 I print_info: n_embd_head_v    = 128
0.00.050.394 I print_info: n_gqa            = 1
0.00.050.394 I print_info: n_embd_k_gqa     = 2048
0.00.050.395 I print_info: n_embd_v_gqa     = 2048
0.00.050.396 I print_info: f_norm_eps       = 1.0e-05
0.00.050.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.396 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.396 I print_info: f_logit_scale    = 0.0e+00
0.00.050.397 I print_info: n_ff             = 8192
0.00.050.397 I print_info: n_expert         = 0
0.00.050.397 I print_info: n_expert_used    = 0
0.00.050.397 I print_info: causal attn      = 1
0.00.050.398 I print_info: pooling type     = 0
0.00.050.398 I print_info: rope type        = 2
0.00.050.398 I print_info: rope scaling     = linear
0.00.050.398 I print_info: freq_base_train  = 10000.0
0.00.050.399 I print_info: freq_scale_train = 1
0.00.050.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.399 I print_info: rope_finetuned   = unknown
0.00.050.399 I print_info: ssm_d_conv       = 0
0.00.050.399 I print_info: ssm_d_inner      = 0
0.00.050.400 I print_info: ssm_d_state      = 0
0.00.050.400 I print_info: ssm_dt_rank      = 0
0.00.050.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.400 I print_info: model type       = 1.4B
0.00.050.400 I print_info: model params     = 1.41 B
0.00.050.401 I print_info: general.name     = 1.4B
0.00.050.401 I print_info: vocab type       = BPE
0.00.050.401 I print_info: n_vocab          = 50304
0.00.050.402 I print_info: n_merges         = 50009
0.00.050.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.403 I print_info: LF token         = 128 'Ä'
0.00.050.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.403 I print_info: max token length = 1024
0.00.052.399 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.399 I load_tensors: offloading output layer to GPU
0.00.052.400 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.410 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.411 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.785 I llama_init_from_model: n_seq_max     = 1
0.00.052.786 I llama_init_from_model: n_ctx         = 128
0.00.052.786 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.787 I llama_init_from_model: n_batch       = 128
0.00.052.787 I llama_init_from_model: n_ubatch      = 128
0.00.052.787 I llama_init_from_model: flash_attn    = 0
0.00.052.787 I llama_init_from_model: freq_base     = 10000.0
0.00.052.787 I llama_init_from_model: freq_scale    = 1
0.00.052.788 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.788 I ggml_metal_init: allocating
0.00.052.791 I ggml_metal_init: found device: Apple M4
0.00.052.793 I ggml_metal_init: picking default device: Apple M4
0.00.053.389 I ggml_metal_init: using embedded metal library
0.00.055.710 I ggml_metal_init: GPU name:   Apple M4
0.00.055.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.712 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.713 I ggml_metal_init: simdgroup reduction   = true
0.00.055.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.713 I ggml_metal_init: has bfloat            = true
0.00.055.713 I ggml_metal_init: use bfloat            = true
0.00.055.713 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.327 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.620 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.634 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.565 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.566 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.566 I llama_init_from_model: graph nodes  = 967
0.00.067.566 I llama_init_from_model: graph splits = 2
0.00.067.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.343 I 
0.00.539.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.534 I perplexity: tokenizing the input ..
0.00.547.471 I perplexity: tokenization took 7.936 ms
0.00.547.482 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.012 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.683.191 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.683.206 I llama_perf_context_print:        load time =     529.91 ms
0.00.683.206 I llama_perf_context_print: prompt eval time =     134.30 ms /   128 tokens (    1.05 ms per token,   953.07 tokens per second)
0.00.683.211 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.683.211 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.683.561 I ggml_metal_free: deallocating

real	0m0.697s
user	0m0.078s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.962 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.963 I llama_model_loader: - type  f32:  194 tensors
0.00.025.963 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.963 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.964 I print_info: file format = GGUF V3 (latest)
0.00.025.964 I print_info: file type   = Q5_K - Medium
0.00.025.965 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.424 I load: special tokens cache size = 25
0.00.051.375 I load: token to piece cache size = 0.2984 MB
0.00.051.378 I print_info: arch             = gptneox
0.00.051.378 I print_info: vocab_only       = 0
0.00.051.378 I print_info: n_ctx_train      = 2048
0.00.051.378 I print_info: n_embd           = 2048
0.00.051.378 I print_info: n_layer          = 24
0.00.051.381 I print_info: n_head           = 16
0.00.051.381 I print_info: n_head_kv        = 16
0.00.051.382 I print_info: n_rot            = 32
0.00.051.382 I print_info: n_swa            = 0
0.00.051.382 I print_info: n_embd_head_k    = 128
0.00.051.382 I print_info: n_embd_head_v    = 128
0.00.051.383 I print_info: n_gqa            = 1
0.00.051.384 I print_info: n_embd_k_gqa     = 2048
0.00.051.384 I print_info: n_embd_v_gqa     = 2048
0.00.051.385 I print_info: f_norm_eps       = 1.0e-05
0.00.051.385 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.386 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.386 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.386 I print_info: f_logit_scale    = 0.0e+00
0.00.051.387 I print_info: n_ff             = 8192
0.00.051.387 I print_info: n_expert         = 0
0.00.051.387 I print_info: n_expert_used    = 0
0.00.051.387 I print_info: causal attn      = 1
0.00.051.387 I print_info: pooling type     = 0
0.00.051.387 I print_info: rope type        = 2
0.00.051.387 I print_info: rope scaling     = linear
0.00.051.388 I print_info: freq_base_train  = 10000.0
0.00.051.388 I print_info: freq_scale_train = 1
0.00.051.388 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.388 I print_info: rope_finetuned   = unknown
0.00.051.389 I print_info: ssm_d_conv       = 0
0.00.051.389 I print_info: ssm_d_inner      = 0
0.00.051.389 I print_info: ssm_d_state      = 0
0.00.051.389 I print_info: ssm_dt_rank      = 0
0.00.051.392 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.392 I print_info: model type       = 1.4B
0.00.051.392 I print_info: model params     = 1.41 B
0.00.051.393 I print_info: general.name     = 1.4B
0.00.051.393 I print_info: vocab type       = BPE
0.00.051.393 I print_info: n_vocab          = 50304
0.00.051.393 I print_info: n_merges         = 50009
0.00.051.394 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.394 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.394 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.394 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: LF token         = 128 'Ä'
0.00.051.399 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.399 I print_info: max token length = 1024
0.00.053.440 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.440 I load_tensors: offloading output layer to GPU
0.00.053.440 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.451 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.452 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.728 I llama_init_from_model: n_seq_max     = 1
0.00.053.729 I llama_init_from_model: n_ctx         = 128
0.00.053.729 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.729 I llama_init_from_model: n_batch       = 128
0.00.053.729 I llama_init_from_model: n_ubatch      = 128
0.00.053.730 I llama_init_from_model: flash_attn    = 0
0.00.053.730 I llama_init_from_model: freq_base     = 10000.0
0.00.053.730 I llama_init_from_model: freq_scale    = 1
0.00.053.731 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.731 I ggml_metal_init: allocating
0.00.053.734 I ggml_metal_init: found device: Apple M4
0.00.053.736 I ggml_metal_init: picking default device: Apple M4
0.00.054.317 I ggml_metal_init: using embedded metal library
0.00.056.696 I ggml_metal_init: GPU name:   Apple M4
0.00.056.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.698 I ggml_metal_init: simdgroup reduction   = true
0.00.056.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.699 I ggml_metal_init: has bfloat            = true
0.00.056.699 I ggml_metal_init: use bfloat            = true
0.00.056.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.556 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.863 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.782 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.783 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.783 I llama_init_from_model: graph nodes  = 967
0.00.068.784 I llama_init_from_model: graph splits = 2
0.00.068.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.370 I 
0.00.628.412 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.428 I perplexity: tokenizing the input ..
0.00.636.396 I perplexity: tokenization took 7.966 ms
0.00.636.407 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.190 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.778.465 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.778.487 I llama_perf_context_print:        load time =     618.34 ms
0.00.778.488 I llama_perf_context_print: prompt eval time =     140.55 ms /   128 tokens (    1.10 ms per token,   910.73 tokens per second)
0.00.778.488 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.489 I llama_perf_context_print:       total time =     150.12 ms /   129 tokens
0.00.779.043 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.318 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.189 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.193 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.061 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.062 I print_info: file format = GGUF V3 (latest)
0.00.025.062 I print_info: file type   = Q6_K
0.00.025.063 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.775 I load: special tokens cache size = 25
0.00.049.666 I load: token to piece cache size = 0.2984 MB
0.00.049.669 I print_info: arch             = gptneox
0.00.049.669 I print_info: vocab_only       = 0
0.00.049.669 I print_info: n_ctx_train      = 2048
0.00.049.669 I print_info: n_embd           = 2048
0.00.049.670 I print_info: n_layer          = 24
0.00.049.672 I print_info: n_head           = 16
0.00.049.673 I print_info: n_head_kv        = 16
0.00.049.673 I print_info: n_rot            = 32
0.00.049.673 I print_info: n_swa            = 0
0.00.049.673 I print_info: n_embd_head_k    = 128
0.00.049.674 I print_info: n_embd_head_v    = 128
0.00.049.674 I print_info: n_gqa            = 1
0.00.049.675 I print_info: n_embd_k_gqa     = 2048
0.00.049.678 I print_info: n_embd_v_gqa     = 2048
0.00.049.678 I print_info: f_norm_eps       = 1.0e-05
0.00.049.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.679 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.679 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.679 I print_info: f_logit_scale    = 0.0e+00
0.00.049.680 I print_info: n_ff             = 8192
0.00.049.680 I print_info: n_expert         = 0
0.00.049.680 I print_info: n_expert_used    = 0
0.00.049.680 I print_info: causal attn      = 1
0.00.049.681 I print_info: pooling type     = 0
0.00.049.681 I print_info: rope type        = 2
0.00.049.681 I print_info: rope scaling     = linear
0.00.049.681 I print_info: freq_base_train  = 10000.0
0.00.049.683 I print_info: freq_scale_train = 1
0.00.049.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.684 I print_info: rope_finetuned   = unknown
0.00.049.684 I print_info: ssm_d_conv       = 0
0.00.049.685 I print_info: ssm_d_inner      = 0
0.00.049.685 I print_info: ssm_d_state      = 0
0.00.049.685 I print_info: ssm_dt_rank      = 0
0.00.049.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.685 I print_info: model type       = 1.4B
0.00.049.686 I print_info: model params     = 1.41 B
0.00.049.686 I print_info: general.name     = 1.4B
0.00.049.686 I print_info: vocab type       = BPE
0.00.049.686 I print_info: n_vocab          = 50304
0.00.049.687 I print_info: n_merges         = 50009
0.00.049.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.687 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.688 I print_info: LF token         = 128 'Ä'
0.00.049.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.688 I print_info: max token length = 1024
0.00.051.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.732 I load_tensors: offloading output layer to GPU
0.00.051.732 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.743 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.744 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.101 I llama_init_from_model: n_seq_max     = 1
0.00.052.102 I llama_init_from_model: n_ctx         = 128
0.00.052.102 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.102 I llama_init_from_model: n_batch       = 128
0.00.052.102 I llama_init_from_model: n_ubatch      = 128
0.00.052.102 I llama_init_from_model: flash_attn    = 0
0.00.052.103 I llama_init_from_model: freq_base     = 10000.0
0.00.052.103 I llama_init_from_model: freq_scale    = 1
0.00.052.103 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.104 I ggml_metal_init: allocating
0.00.052.110 I ggml_metal_init: found device: Apple M4
0.00.052.112 I ggml_metal_init: picking default device: Apple M4
0.00.052.710 I ggml_metal_init: using embedded metal library
0.00.055.062 I ggml_metal_init: GPU name:   Apple M4
0.00.055.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.064 I ggml_metal_init: simdgroup reduction   = true
0.00.055.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.065 I ggml_metal_init: has bfloat            = true
0.00.055.065 I ggml_metal_init: use bfloat            = true
0.00.055.065 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.687 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.952 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.954 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.850 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.851 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.851 I llama_init_from_model: graph nodes  = 967
0.00.066.851 I llama_init_from_model: graph splits = 2
0.00.066.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.229 I 
0.00.385.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.279 I perplexity: tokenizing the input ..
0.00.392.830 I perplexity: tokenization took 7.55 ms
0.00.392.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.532.559 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.533.736 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.533.757 I llama_perf_context_print:        load time =     375.90 ms
0.00.533.758 I llama_perf_context_print: prompt eval time =     139.49 ms /   128 tokens (    1.09 ms per token,   917.62 tokens per second)
0.00.533.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.533.759 I llama_perf_context_print:       total time =     148.53 ms /   129 tokens
0.00.534.201 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.077s
sys	0m0.075s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.244 I build: 4544 (a07c2c8a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.606 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.622 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.622 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.623 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.624 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.628 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.369 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.845 I llama_model_loader: - type  f32:  194 tensors
0.00.054.845 I llama_model_loader: - type  f16:   98 tensors
0.00.054.846 I print_info: file format = GGUF V3 (latest)
0.00.054.846 I print_info: file type   = all F32 (guessed)
0.00.054.853 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.258 I load: special tokens cache size = 25
0.00.086.836 I load: token to piece cache size = 0.2984 MB
0.00.086.839 I print_info: arch             = gptneox
0.00.086.839 I print_info: vocab_only       = 0
0.00.086.839 I print_info: n_ctx_train      = 2048
0.00.086.839 I print_info: n_embd           = 2048
0.00.086.840 I print_info: n_layer          = 24
0.00.086.842 I print_info: n_head           = 16
0.00.086.843 I print_info: n_head_kv        = 16
0.00.086.843 I print_info: n_rot            = 32
0.00.086.843 I print_info: n_swa            = 0
0.00.086.844 I print_info: n_embd_head_k    = 128
0.00.086.844 I print_info: n_embd_head_v    = 128
0.00.086.844 I print_info: n_gqa            = 1
0.00.086.845 I print_info: n_embd_k_gqa     = 2048
0.00.086.846 I print_info: n_embd_v_gqa     = 2048
0.00.086.846 I print_info: f_norm_eps       = 1.0e-05
0.00.086.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.847 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.849 I print_info: f_logit_scale    = 0.0e+00
0.00.086.849 I print_info: n_ff             = 8192
0.00.086.849 I print_info: n_expert         = 0
0.00.086.849 I print_info: n_expert_used    = 0
0.00.086.850 I print_info: causal attn      = 1
0.00.086.850 I print_info: pooling type     = 0
0.00.086.851 I print_info: rope type        = 2
0.00.086.851 I print_info: rope scaling     = linear
0.00.086.851 I print_info: freq_base_train  = 10000.0
0.00.086.852 I print_info: freq_scale_train = 1
0.00.086.852 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.852 I print_info: rope_finetuned   = unknown
0.00.086.852 I print_info: ssm_d_conv       = 0
0.00.086.852 I print_info: ssm_d_inner      = 0
0.00.086.852 I print_info: ssm_d_state      = 0
0.00.086.852 I print_info: ssm_dt_rank      = 0
0.00.086.852 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.854 I print_info: model type       = 1.4B
0.00.086.854 I print_info: model params     = 1.41 B
0.00.086.854 I print_info: general.name     = 1.4B
0.00.086.855 I print_info: vocab type       = BPE
0.00.086.858 I print_info: n_vocab          = 50304
0.00.086.858 I print_info: n_merges         = 50009
0.00.086.858 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.859 I print_info: LF token         = 128 'Ä'
0.00.086.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.859 I print_info: max token length = 1024
0.00.089.334 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.334 I load_tensors: offloading output layer to GPU
0.00.089.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.344 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.346 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.657 I llama_init_from_model: n_seq_max     = 1
0.00.089.658 I llama_init_from_model: n_ctx         = 128
0.00.089.658 I llama_init_from_model: n_ctx_per_seq = 128
0.00.089.658 I llama_init_from_model: n_batch       = 128
0.00.089.658 I llama_init_from_model: n_ubatch      = 128
0.00.089.658 I llama_init_from_model: flash_attn    = 0
0.00.089.659 I llama_init_from_model: freq_base     = 10000.0
0.00.089.659 I llama_init_from_model: freq_scale    = 1
0.00.089.659 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.660 I ggml_metal_init: allocating
0.00.089.663 I ggml_metal_init: found device: Apple M4
0.00.089.665 I ggml_metal_init: picking default device: Apple M4
0.00.090.252 I ggml_metal_init: using embedded metal library
0.00.092.757 I ggml_metal_init: GPU name:   Apple M4
0.00.092.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.760 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.760 I ggml_metal_init: simdgroup reduction   = true
0.00.092.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.760 I ggml_metal_init: has bfloat            = true
0.00.092.760 I ggml_metal_init: use bfloat            = true
0.00.092.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.796 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.057 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.996 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.998 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.998 I llama_init_from_model: graph nodes  = 967
0.00.103.998 I llama_init_from_model: graph splits = 2
0.00.103.999 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.000 I 
0.00.104.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.033 I compute_imatrix: tokenizing the input ..
0.00.110.586 I compute_imatrix: tokenization took 6.553 ms
0.00.110.588 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.663.672 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.666.516 I llama_perf_context_print:        load time =    1640.84 ms
0.01.666.517 I llama_perf_context_print: prompt eval time =    1552.47 ms /   128 tokens (   12.13 ms per token,    82.45 tokens per second)
0.01.666.518 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.666.520 I llama_perf_context_print:       total time =    1643.67 ms /   129 tokens
0.01.667.103 I ggml_metal_free: deallocating

real	0m1.867s
user	0m0.166s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4544 (a07c2c8a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d60a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d60a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d60af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d60b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d60ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d60c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d60c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d60cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d60d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d60d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d60db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d60e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d60eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d60f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d60fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d610250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d610970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d611090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d6117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d6126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d612dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d6134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d6144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d614760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d614d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d6159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d615f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d6161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d6171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d617710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d6179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d617e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d6187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d6190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d619590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d619a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d619ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d61a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d61a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d61ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d61b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d61bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d61c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d61c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d61cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d61d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d61d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d61dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d61e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d61ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d61f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d61f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d61f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d6201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d620dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d621700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d6224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d622980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d6232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d623760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d623c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d624150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d6246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d624bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d625140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d625690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d625be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d626130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d626680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d626bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d627120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d627670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d627bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d628110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d628bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d629100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d629650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d629ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d62a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d62a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d62ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d62b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d62b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d62bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d61b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d62bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d62c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d62ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d62d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d62d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d62dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d62e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d62e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d62ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d62f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d62f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d62fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d630210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d630760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d630cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d631150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d6315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d631f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d6323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d632870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d6331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d633af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d633f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d634430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d6348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d634d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d635210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d6356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d635b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d635ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d636490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d636dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d637270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d637bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d638050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d6384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d638990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d638e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d6392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d639770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d639c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d63a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d63a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d63a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d63ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d63b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d63b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d63bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d63c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d63c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d63ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d63cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d63d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d63dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d63e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d63e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d63eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d63ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d63f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d63f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d63fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d6401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d640670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d640b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d641450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d6418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d641d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d6426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d642b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d6434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d643950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d643df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d644290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d644730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d644bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d645070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d645510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d6459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d645e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d6462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d646790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d6470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d647570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d647a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d648950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d6493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d6496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d649cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d64a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d64a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d64b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d64b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d64b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d64be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d64c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d64cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d64d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d64d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d64da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d64e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d64e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d64ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d64f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d64f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d64fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d6501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d650700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d650c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d6511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d6516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d651c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d652190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d6526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d652c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d653180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d6536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d653c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d654170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d6546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d654c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d6556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d655c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d656150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d6566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d656bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d657690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d657be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d658680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d658bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d659120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d659670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d659bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d65a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d65a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d65abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d65b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d65b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d65bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d65c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d65c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d65cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d65d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d65d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d65db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d65e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d65e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d65eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d65f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d65f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d65fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d6600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d660600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d660b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d660ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d661490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d661930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d661dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d662270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d662710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d662bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d663050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d6634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d663e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d6642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d664770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d664c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d6650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d665d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d666440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d666b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d667280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d667540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d667d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d667ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d668600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.297 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e104b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e104fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e105430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e1058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e105d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e106180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e1065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e106a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e106ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e107340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e1077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e107ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e1089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e109170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e109980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e10a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e10a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e10aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e10b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e10bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e10c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e10cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e10d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e10d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e10e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e10e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e10e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e10eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e10ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e10f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e10f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e10fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e1101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e110470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e1108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e110d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e1111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e111630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e111aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e111f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e112380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e1127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e112c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e1130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e113540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e1139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e113e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e114290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e114700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e114b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e114fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e115450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e1158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e115d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e1161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e116610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e116b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e117080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e1174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e117960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e117dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e118240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e1186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e118b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e118f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e119400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e119870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e119ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e11a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e11a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e11aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e11aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e11b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e11b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e11bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e11c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e11c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e11c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e11cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e11d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e11d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e11db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e11df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e11e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e11e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e11ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e11f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e11f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e11fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e11fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e1202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e120760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e120bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e121040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e1214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e121920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e121d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e122200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e122670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e122ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e122f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e1233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e123830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e123ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e124110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e124580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e1249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e124e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e1252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e125740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e125bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e126020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e126490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e126900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e126d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e1271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e127650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e127ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e127f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e1283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e128810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e128c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e1290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e129560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e1299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e129e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e12a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e12a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e12ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e12b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e12b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e12b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e12bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e12c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e12c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e12caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e12cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e12d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e12d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e12dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e12e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e12e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e12e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e12ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e12f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e12f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e12fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e12ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e130450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e1308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e130d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e1311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e131610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e131a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e131ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e132360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e1327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e132c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e1330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e133520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e133990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e133e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e134270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e1346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e134b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e134fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e135bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e135eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e136170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e1365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e136a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e136ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e137330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e1377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e137c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e138080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e1384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e138960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e138dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e139240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e1396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e139b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e139f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e13a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e13a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e13ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e13b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e13b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e13ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e13bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e13c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e13c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e13cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e13d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e13d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e13d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e13ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e13e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e13e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e13eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e13ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e13f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e13f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e13fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e1402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e140730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e140ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e141010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e141530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e141a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e1425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e142870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e142e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e1433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e1439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e143f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e144530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e144af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e1450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e145670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e145c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e1461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e1467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e146d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e147330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e1478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e147eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e148470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e148a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e148ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e1495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e149b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e14a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e14a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e14acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e14b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e14b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e14bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e14c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e14c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e14cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e14d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e14dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e14e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e14e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e14ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e14f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e14f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e14fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e1502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e1508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e150e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e151430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e1519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e151fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e152570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e152b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e1530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e1536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e153c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e154230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e1547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e154db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e155370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e155930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e155ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e1564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e156a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e156f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e157470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e157970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e157e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e158370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e158870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e158d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e159270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e159770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e159c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e15a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e15a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e15ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e15b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e15b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e15bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e15c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e15cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e15d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e15d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e15df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e15e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e15e860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e006db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e007220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e0078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e0083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e008b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e009380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e009aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e00a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e00a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e00b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e00b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e00bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e00c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e00cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e00d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e00db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e00de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e00e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e00e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e00e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e00ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e00f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e00f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e00fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e00ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e010380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e0107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e010c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e0110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e011540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e0119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e011e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e012290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e012700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e012b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e012fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e013450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e0138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e013d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e0141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e014a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e014ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e015360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e0157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e015c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e0160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e016620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e016b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e016f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e017400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e017870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e017ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e018150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e0185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e018a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e018ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e019310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e019780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e019bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e01a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e01a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e01a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e01adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e01b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e01b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e01bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e01bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e01c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e01c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e01ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e01d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e01d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e01da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e01de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e01e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e01e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e01ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e01f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e01f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e01f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e01fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e020200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e020670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e020ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e020f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e0213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e021830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e021ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e022110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e022580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e0229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e022e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e0232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e023e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e024290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e024700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e024b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e024fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e025450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e0258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e025d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e0261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e026610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e026a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e026ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e027360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e0277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e027c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e0280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e028520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e028990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e028e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e029270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e0296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e029b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e029fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e02a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e02a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e02ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e02b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e02b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e02ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e02bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e02c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e02c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e02cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e02d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e02d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e02d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e02dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e02e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e02e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e02eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e02efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e02f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e02f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e02fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e030160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e0305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e030a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e030eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e031320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e031790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e031c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e032070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e0324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e032950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e033230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e0336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e033b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e033f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e0343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e034860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e034cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e035140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e0355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e035a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e035e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e036300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e036770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e036be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e037050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e0374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e037930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e037da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e038210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e038680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e038af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e038f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e0393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e039840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e039cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e03a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e03a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e03aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e03ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e03b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e03b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e03bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e03c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e03c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e03c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e03cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e03d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e03d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e03dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e03df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e03e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e03e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e03ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e03f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e03f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e03f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e03fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e0402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e040730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e040ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e041010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e041b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e041e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e042110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e0429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e042e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e0432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e043740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e043bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e044020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e044490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e044900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e0451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e045650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e045ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e045f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e0463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e046810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e046c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e0470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e047560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e0479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e047e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e0482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e048720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e048b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e049000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e049470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e0498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e049d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e04a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e04a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e04aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e04af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e04b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e04b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e04bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e04c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e04c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e04c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e04ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e04d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e04d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e04db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e04dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e04e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e04e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e04ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e04f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e04f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e04fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e04fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e050360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e0507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e050c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e0510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e051520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e051990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e051e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e052270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e0526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e052fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e053430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e0538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e053d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e054180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e0545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e054a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e054ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e055340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e0557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e056220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e056940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e057060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e057780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e057a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e057eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e0584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e058ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.856s
user	0m0.299s
sys	0m0.311s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4544 (a07c2c8a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145710460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145710b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145711120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1457116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145711c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145712230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1457127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145712d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145713340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145713840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145713d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145714240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145714d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145715510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145715d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145716440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145716b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145717280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1457179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145718170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145718890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145718fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1457196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145719f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14571a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14571a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14571af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14571bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14571c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14571c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14571c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14571cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14571d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14571d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14571dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14571e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14571e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14571e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14571ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14571f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14571f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14571fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1457200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145720560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145720820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145720e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145721440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145722370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145722980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145722f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1457235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1457241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1457249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145724e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1457252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1457255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145725bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1457263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145726670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145726b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145726fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145727450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1457278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145727d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145728230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1457286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145728b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145729010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1457294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145729950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145729df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14572a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14572a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14572ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14572b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14572b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14572bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14572c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14572c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14572cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14572d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14572d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14572ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14572e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14572e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14572eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14572f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14572f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14572fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1457302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145730830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145730d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1457312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145731820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145731d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145721a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1457321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145732990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145732ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145733430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145733980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145734420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145734970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145734ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145735410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145735960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145735eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145736400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145736950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145736ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145737340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1457377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145737c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145738120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1457385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145738a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145738f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1457393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145739ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14573a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14573a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14573aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14573af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14573b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14573b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14573bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14573c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14573c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14573cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14573cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14573d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14573d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14573dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14573e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14573e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14573eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14573f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14573f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14573f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14573fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1457402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145740740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145740be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145741080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145741520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1457419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145742300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1457427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145742c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1457430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145743580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145743a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145743ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145744360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145744800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145744ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145745140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1457455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145745f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1457463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145746860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145746d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1457471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145747640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145747ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145748420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1457488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145749200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1457496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14574a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14574a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14574adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14574b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14574b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14574bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14574c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14574c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14574c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14574ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14574d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14574d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14574dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14574e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14574e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14574eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14574f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14574f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14574f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14574feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1457504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145750ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1457512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145751760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145751a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145752030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145752e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1457532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145753770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145753c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1457543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145754910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145754e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1457553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145755900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145755e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1457563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1457568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145756e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145757390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1457578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145757e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145758380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1457588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145758e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145759370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1457598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145759e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14575a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14575a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14575ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14575b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14575b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14575bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14575c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14575c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14575cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14575d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14575d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14575ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14575e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14575e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14575edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14575f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14575f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14575fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145760300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145760850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145760da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1457612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145761840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145761d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1457622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145762830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145762d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1457632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145763820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145763d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1457642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145764810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145764d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1457652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145765800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145765d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1457662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1457667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145766d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1457671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145767680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145767b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145767fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145768460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145768900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145768da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145769240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1457696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145769b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14576a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14576a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14576a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14576ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14576b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14576b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14576bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14576c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14576cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14576d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14576d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14576df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14576e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14576e7f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.011 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146a35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146a363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146a36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146a36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146a37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146a375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146a37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146a38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146a38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146a38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146a39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146a394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146a39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146a3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146a3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146a3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146a3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146a3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146a3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146a3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146a3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146a3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146a3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146a3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146a3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146a3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146a3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146a3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146a400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146a40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146a409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146a41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146a417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146a41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146a42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146a430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146a43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146a441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146a447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146a44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146a46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146a46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146a475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146a47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146a486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146a48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146a49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146a49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146a49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146a4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146a4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146a4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146a4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146a4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146a4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146a4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146a4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146a510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146a51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146a527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146a52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146a53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146a53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146a53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146a54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146a55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146a55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146a56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146a56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146a58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146a58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146a594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146a59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146a5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146a5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146a5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146a5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146a5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146a5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146a5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146a5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146a5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146a5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146a5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1468044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1468056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1468063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1468092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14680a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14680a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14680af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14680b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14680be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14680c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14680cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14680d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14680dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14680dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14680e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14680e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14680e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14680edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14680f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14680f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14680fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14680fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1468102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1468114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1468133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1468149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1468152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1468177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1468180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1468189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1468196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14681a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14681a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14681ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14681b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14681b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14681ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14681bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14681c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14681c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14681cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14681d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14681d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14681d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14681ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14681e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14681e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14681eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14681efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14681f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14681f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14681fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1468205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1468217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1468224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1468253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1468269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1468272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1468291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14682a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14682a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14682ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14682b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14682b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14682b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14682be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14682c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14682c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14682cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14682d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14682d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14682d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14682dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14682e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14682e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14682eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14682ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14682f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14682f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14682fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1468300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1468309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1468328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1468331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1468347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1468350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1468366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1468378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1468385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1468397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14683a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14683a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14683a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14683ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14683b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14683b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14683bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14683bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14683c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14683c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14683ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14683d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14683d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14683da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14683deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14683e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14683e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14683ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14683f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14683f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14683f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14683fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1468406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1468424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1468436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1468455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1468474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1468493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14684a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14684a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14684aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14684ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14684b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14684b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14684bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14684c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14684c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14684c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14684cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14684d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14684d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14684dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14684df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14684e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14684e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14684eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14684f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14684f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14684f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14684fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1468502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1468521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1468533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1468540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1468549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1468552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1468568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1468576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1468579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.929s
user	0m0.244s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
