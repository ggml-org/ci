Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.101s
user	0m1.009s
sys	0m1.469s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-chat
[ 50%] Built target test-llama-grammar
[ 52%] Linking CXX executable ../bin/test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-barrier
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-quantize-perf
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-batched-bench
[ 70%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Built target llama-imatrix
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-create
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-parallel
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-passkey
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-tts
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.239s
user	0m6.551s
sys	0m10.093s

main: quantize time =  4743.60 ms
main:    total time =  4743.60 ms

main: quantize time =  4237.88 ms
main:    total time =  4237.88 ms

main: quantize time =  2390.40 ms
main:    total time =  2390.40 ms

main: quantize time =  2579.90 ms
main:    total time =  2579.90 ms

main: quantize time =  2831.66 ms
main:    total time =  2831.66 ms

main: quantize time =  5686.46 ms
main:    total time =  5686.46 ms

main: quantize time =  6711.69 ms
main:    total time =  6711.69 ms

main: quantize time =  7039.52 ms
main:    total time =  7039.52 ms

main: quantize time =  6044.64 ms
main:    total time =  6044.64 ms

main: quantize time =  4439.80 ms
main:    total time =  4439.80 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.210 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.364 I main: llama backend init
0.00.000.370 I main: load the model and apply lora adapter, if any
0.00.048.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.388 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.424 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.429 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.432 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.433 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.822 I llama_model_loader: - type  f32:  194 tensors
0.00.079.823 I llama_model_loader: - type  f16:   98 tensors
0.00.079.824 I print_info: file format = GGUF V3 (latest)
0.00.079.825 I print_info: file type   = all F32 (guessed)
0.00.079.829 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.095.979 I load: special tokens cache size = 25
0.00.106.413 I load: token to piece cache size = 0.2984 MB
0.00.106.434 I print_info: arch             = gptneox
0.00.106.435 I print_info: vocab_only       = 0
0.00.106.435 I print_info: n_ctx_train      = 2048
0.00.106.436 I print_info: n_embd           = 2048
0.00.106.436 I print_info: n_layer          = 24
0.00.106.441 I print_info: n_head           = 16
0.00.106.442 I print_info: n_head_kv        = 16
0.00.106.443 I print_info: n_rot            = 32
0.00.106.443 I print_info: n_swa            = 0
0.00.106.443 I print_info: n_embd_head_k    = 128
0.00.106.443 I print_info: n_embd_head_v    = 128
0.00.106.444 I print_info: n_gqa            = 1
0.00.106.445 I print_info: n_embd_k_gqa     = 2048
0.00.106.446 I print_info: n_embd_v_gqa     = 2048
0.00.106.447 I print_info: f_norm_eps       = 1.0e-05
0.00.106.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.448 I print_info: f_logit_scale    = 0.0e+00
0.00.106.449 I print_info: n_ff             = 8192
0.00.106.450 I print_info: n_expert         = 0
0.00.106.450 I print_info: n_expert_used    = 0
0.00.106.450 I print_info: causal attn      = 1
0.00.106.450 I print_info: pooling type     = 0
0.00.106.450 I print_info: rope type        = 2
0.00.106.451 I print_info: rope scaling     = linear
0.00.106.451 I print_info: freq_base_train  = 10000.0
0.00.106.452 I print_info: freq_scale_train = 1
0.00.106.452 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.454 I print_info: rope_finetuned   = unknown
0.00.106.454 I print_info: ssm_d_conv       = 0
0.00.106.454 I print_info: ssm_d_inner      = 0
0.00.106.454 I print_info: ssm_d_state      = 0
0.00.106.454 I print_info: ssm_dt_rank      = 0
0.00.106.454 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.455 I print_info: model type       = 1.4B
0.00.106.455 I print_info: model params     = 1.41 B
0.00.106.456 I print_info: general.name     = 1.4B
0.00.106.456 I print_info: vocab type       = BPE
0.00.106.457 I print_info: n_vocab          = 50304
0.00.106.457 I print_info: n_merges         = 50009
0.00.106.457 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.457 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.457 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.458 I print_info: LF token         = 187 'Ċ'
0.00.106.458 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.458 I print_info: max token length = 1024
0.00.106.465 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.156.247 I load_tensors: offloading 24 repeating layers to GPU
0.00.156.250 I load_tensors: offloading output layer to GPU
0.00.156.250 I load_tensors: offloaded 25/25 layers to GPU
0.00.156.279 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.156.280 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.156.828 I llama_init_from_model: n_seq_max     = 1
0.00.156.829 I llama_init_from_model: n_ctx         = 2048
0.00.156.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.156.829 I llama_init_from_model: n_batch       = 2048
0.00.156.829 I llama_init_from_model: n_ubatch      = 512
0.00.156.829 I llama_init_from_model: flash_attn    = 0
0.00.156.830 I llama_init_from_model: freq_base     = 10000.0
0.00.156.830 I llama_init_from_model: freq_scale    = 1
0.00.156.832 I ggml_metal_init: allocating
0.00.156.879 I ggml_metal_init: found device: Apple M4
0.00.156.889 I ggml_metal_init: picking default device: Apple M4
0.00.157.452 I ggml_metal_init: using embedded metal library
0.00.166.655 I ggml_metal_init: GPU name:   Apple M4
0.00.166.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.166.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.166.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.166.658 I ggml_metal_init: simdgroup reduction   = true
0.00.166.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.166.659 I ggml_metal_init: has residency sets    = true
0.00.166.659 I ggml_metal_init: has bfloat            = true
0.00.166.659 I ggml_metal_init: use bfloat            = true
0.00.166.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.166.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.211.054 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.239.366 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.239.371 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.239.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.243.319 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.243.321 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.243.322 I llama_init_from_model: graph nodes  = 967
0.00.243.322 I llama_init_from_model: graph splits = 2
0.00.243.327 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.243.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.243.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.445 I main: llama threadpool init, n_threads = 4
0.00.308.486 I 
0.00.308.519 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.521 I 
0.00.308.708 I sampler seed: 1234
0.00.308.714 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.308.747 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.308.748 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.308.748 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.152.565 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.152.566 I llama_perf_context_print:        load time =     259.17 ms
0.02.152.567 I llama_perf_context_print: prompt eval time =      53.53 ms /     7 tokens (    7.65 ms per token,   130.77 tokens per second)
0.02.152.568 I llama_perf_context_print:        eval time =    1787.45 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.152.569 I llama_perf_context_print:       total time =    1845.00 ms /    70 tokens
0.02.152.808 I ggml_metal_free: deallocating

real	0m2.498s
user	0m0.137s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.009.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.316 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.329 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.330 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.332 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.335 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.271 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.272 I llama_model_loader: - type  f32:  194 tensors
0.00.033.272 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.273 I print_info: file format = GGUF V3 (latest)
0.00.033.279 I print_info: file type   = Q8_0
0.00.033.280 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.821 I load: special tokens cache size = 25
0.00.048.283 I load: token to piece cache size = 0.2984 MB
0.00.048.302 I print_info: arch             = gptneox
0.00.048.303 I print_info: vocab_only       = 0
0.00.048.304 I print_info: n_ctx_train      = 2048
0.00.048.304 I print_info: n_embd           = 2048
0.00.048.304 I print_info: n_layer          = 24
0.00.048.310 I print_info: n_head           = 16
0.00.048.311 I print_info: n_head_kv        = 16
0.00.048.311 I print_info: n_rot            = 32
0.00.048.311 I print_info: n_swa            = 0
0.00.048.311 I print_info: n_embd_head_k    = 128
0.00.048.312 I print_info: n_embd_head_v    = 128
0.00.048.312 I print_info: n_gqa            = 1
0.00.048.313 I print_info: n_embd_k_gqa     = 2048
0.00.048.314 I print_info: n_embd_v_gqa     = 2048
0.00.048.315 I print_info: f_norm_eps       = 1.0e-05
0.00.048.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.316 I print_info: f_logit_scale    = 0.0e+00
0.00.048.316 I print_info: n_ff             = 8192
0.00.048.320 I print_info: n_expert         = 0
0.00.048.320 I print_info: n_expert_used    = 0
0.00.048.320 I print_info: causal attn      = 1
0.00.048.320 I print_info: pooling type     = 0
0.00.048.320 I print_info: rope type        = 2
0.00.048.321 I print_info: rope scaling     = linear
0.00.048.321 I print_info: freq_base_train  = 10000.0
0.00.048.321 I print_info: freq_scale_train = 1
0.00.048.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.322 I print_info: rope_finetuned   = unknown
0.00.048.322 I print_info: ssm_d_conv       = 0
0.00.048.322 I print_info: ssm_d_inner      = 0
0.00.048.322 I print_info: ssm_d_state      = 0
0.00.048.322 I print_info: ssm_dt_rank      = 0
0.00.048.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.322 I print_info: model type       = 1.4B
0.00.048.323 I print_info: model params     = 1.41 B
0.00.048.323 I print_info: general.name     = 1.4B
0.00.048.324 I print_info: vocab type       = BPE
0.00.048.324 I print_info: n_vocab          = 50304
0.00.048.324 I print_info: n_merges         = 50009
0.00.048.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.325 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.325 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.325 I print_info: LF token         = 187 'Ċ'
0.00.048.325 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.325 I print_info: max token length = 1024
0.00.048.326 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.118.136 I load_tensors: offloading 24 repeating layers to GPU
0.01.118.141 I load_tensors: offloading output layer to GPU
0.01.118.142 I load_tensors: offloaded 25/25 layers to GPU
0.01.118.163 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.118.165 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.118.865 I llama_init_from_model: n_seq_max     = 1
0.01.118.866 I llama_init_from_model: n_ctx         = 2048
0.01.118.867 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.118.867 I llama_init_from_model: n_batch       = 2048
0.01.118.867 I llama_init_from_model: n_ubatch      = 512
0.01.118.868 I llama_init_from_model: flash_attn    = 0
0.01.118.869 I llama_init_from_model: freq_base     = 10000.0
0.01.118.869 I llama_init_from_model: freq_scale    = 1
0.01.118.870 I ggml_metal_init: allocating
0.01.118.879 I ggml_metal_init: found device: Apple M4
0.01.118.886 I ggml_metal_init: picking default device: Apple M4
0.01.119.933 I ggml_metal_init: using embedded metal library
0.01.125.181 I ggml_metal_init: GPU name:   Apple M4
0.01.125.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.125.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.125.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.125.186 I ggml_metal_init: simdgroup reduction   = true
0.01.125.186 I ggml_metal_init: simdgroup matrix mul. = true
0.01.125.186 I ggml_metal_init: has residency sets    = true
0.01.125.186 I ggml_metal_init: has bfloat            = true
0.01.125.186 I ggml_metal_init: use bfloat            = true
0.01.125.187 I ggml_metal_init: hasUnifiedMemory      = true
0.01.125.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.140.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.181.351 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.181.358 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.181.391 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.185.674 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.185.676 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.185.677 I llama_init_from_model: graph nodes  = 967
0.01.185.677 I llama_init_from_model: graph splits = 2
0.01.185.682 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.185.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.185.814 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.240.217 I main: llama threadpool init, n_threads = 4
0.01.240.268 I 
0.01.240.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.240.291 I 
0.01.240.472 I sampler seed: 1234
0.01.240.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.240.522 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.240.524 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.240.524 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.324.925 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.02.324.925 I llama_perf_context_print:        load time =    1229.45 ms
0.02.324.926 I llama_perf_context_print: prompt eval time =      39.89 ms /     7 tokens (    5.70 ms per token,   175.50 tokens per second)
0.02.324.927 I llama_perf_context_print:        eval time =    1041.64 ms /    63 runs   (   16.53 ms per token,    60.48 tokens per second)
0.02.324.927 I llama_perf_context_print:       total time =    1085.48 ms /    70 tokens
0.02.325.159 I ggml_metal_free: deallocating

real	0m2.344s
user	0m0.108s
sys	0m0.248s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.300 I llama_model_loader: - type  f32:  194 tensors
0.00.028.300 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.300 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.301 I print_info: file format = GGUF V3 (latest)
0.00.028.304 I print_info: file type   = Q4_0
0.00.028.305 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.854 I load: special tokens cache size = 25
0.00.043.074 I load: token to piece cache size = 0.2984 MB
0.00.043.089 I print_info: arch             = gptneox
0.00.043.090 I print_info: vocab_only       = 0
0.00.043.091 I print_info: n_ctx_train      = 2048
0.00.043.091 I print_info: n_embd           = 2048
0.00.043.091 I print_info: n_layer          = 24
0.00.043.096 I print_info: n_head           = 16
0.00.043.097 I print_info: n_head_kv        = 16
0.00.043.097 I print_info: n_rot            = 32
0.00.043.097 I print_info: n_swa            = 0
0.00.043.097 I print_info: n_embd_head_k    = 128
0.00.043.098 I print_info: n_embd_head_v    = 128
0.00.043.098 I print_info: n_gqa            = 1
0.00.043.099 I print_info: n_embd_k_gqa     = 2048
0.00.043.100 I print_info: n_embd_v_gqa     = 2048
0.00.043.100 I print_info: f_norm_eps       = 1.0e-05
0.00.043.101 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.101 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.101 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.101 I print_info: f_logit_scale    = 0.0e+00
0.00.043.102 I print_info: n_ff             = 8192
0.00.043.102 I print_info: n_expert         = 0
0.00.043.102 I print_info: n_expert_used    = 0
0.00.043.102 I print_info: causal attn      = 1
0.00.043.102 I print_info: pooling type     = 0
0.00.043.103 I print_info: rope type        = 2
0.00.043.103 I print_info: rope scaling     = linear
0.00.043.103 I print_info: freq_base_train  = 10000.0
0.00.043.103 I print_info: freq_scale_train = 1
0.00.043.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.104 I print_info: rope_finetuned   = unknown
0.00.043.104 I print_info: ssm_d_conv       = 0
0.00.043.104 I print_info: ssm_d_inner      = 0
0.00.043.107 I print_info: ssm_d_state      = 0
0.00.043.107 I print_info: ssm_dt_rank      = 0
0.00.043.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.107 I print_info: model type       = 1.4B
0.00.043.107 I print_info: model params     = 1.41 B
0.00.043.107 I print_info: general.name     = 1.4B
0.00.043.108 I print_info: vocab type       = BPE
0.00.043.108 I print_info: n_vocab          = 50304
0.00.043.109 I print_info: n_merges         = 50009
0.00.043.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.110 I print_info: LF token         = 187 'Ċ'
0.00.043.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.111 I print_info: max token length = 1024
0.00.043.111 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.954 I load_tensors: offloading output layer to GPU
0.00.613.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.988 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.613.989 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.615.680 I llama_init_from_model: n_seq_max     = 1
0.00.615.684 I llama_init_from_model: n_ctx         = 2048
0.00.615.684 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.685 I llama_init_from_model: n_batch       = 2048
0.00.615.685 I llama_init_from_model: n_ubatch      = 512
0.00.615.685 I llama_init_from_model: flash_attn    = 0
0.00.615.688 I llama_init_from_model: freq_base     = 10000.0
0.00.615.688 I llama_init_from_model: freq_scale    = 1
0.00.615.691 I ggml_metal_init: allocating
0.00.615.762 I ggml_metal_init: found device: Apple M4
0.00.615.774 I ggml_metal_init: picking default device: Apple M4
0.00.617.326 I ggml_metal_init: using embedded metal library
0.00.622.957 I ggml_metal_init: GPU name:   Apple M4
0.00.622.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.964 I ggml_metal_init: simdgroup reduction   = true
0.00.622.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.965 I ggml_metal_init: has residency sets    = true
0.00.622.966 I ggml_metal_init: has bfloat            = true
0.00.622.966 I ggml_metal_init: use bfloat            = true
0.00.622.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.969 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.216 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.702.370 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.394 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.580 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.707.582 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.707.582 I llama_init_from_model: graph nodes  = 967
0.00.707.582 I llama_init_from_model: graph splits = 2
0.00.707.588 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.049 I main: llama threadpool init, n_threads = 4
0.00.766.098 I 
0.00.766.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.121 I 
0.00.766.304 I sampler seed: 1234
0.00.766.308 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.324 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.325 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.452.004 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.452.004 I llama_perf_context_print:        load time =     753.52 ms
0.01.452.005 I llama_perf_context_print: prompt eval time =      49.34 ms /     7 tokens (    7.05 ms per token,   141.88 tokens per second)
0.01.452.006 I llama_perf_context_print:        eval time =     633.51 ms /    63 runs   (   10.06 ms per token,    99.45 tokens per second)
0.01.452.007 I llama_perf_context_print:       total time =     686.68 ms /    70 tokens
0.01.452.263 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.112s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.934 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.529 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.530 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.530 I llama_model_loader: - type  f32:  194 tensors
0.00.026.531 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.531 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.532 I print_info: file format = GGUF V3 (latest)
0.00.026.532 I print_info: file type   = Q4_1
0.00.026.537 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.775 I load: special tokens cache size = 25
0.00.041.122 I load: token to piece cache size = 0.2984 MB
0.00.041.136 I print_info: arch             = gptneox
0.00.041.137 I print_info: vocab_only       = 0
0.00.041.137 I print_info: n_ctx_train      = 2048
0.00.041.137 I print_info: n_embd           = 2048
0.00.041.137 I print_info: n_layer          = 24
0.00.041.140 I print_info: n_head           = 16
0.00.041.141 I print_info: n_head_kv        = 16
0.00.041.141 I print_info: n_rot            = 32
0.00.041.141 I print_info: n_swa            = 0
0.00.041.141 I print_info: n_embd_head_k    = 128
0.00.041.142 I print_info: n_embd_head_v    = 128
0.00.041.142 I print_info: n_gqa            = 1
0.00.041.143 I print_info: n_embd_k_gqa     = 2048
0.00.041.144 I print_info: n_embd_v_gqa     = 2048
0.00.041.144 I print_info: f_norm_eps       = 1.0e-05
0.00.041.145 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.147 I print_info: f_logit_scale    = 0.0e+00
0.00.041.148 I print_info: n_ff             = 8192
0.00.041.148 I print_info: n_expert         = 0
0.00.041.148 I print_info: n_expert_used    = 0
0.00.041.148 I print_info: causal attn      = 1
0.00.041.148 I print_info: pooling type     = 0
0.00.041.149 I print_info: rope type        = 2
0.00.041.149 I print_info: rope scaling     = linear
0.00.041.149 I print_info: freq_base_train  = 10000.0
0.00.041.150 I print_info: freq_scale_train = 1
0.00.041.150 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.150 I print_info: rope_finetuned   = unknown
0.00.041.150 I print_info: ssm_d_conv       = 0
0.00.041.150 I print_info: ssm_d_inner      = 0
0.00.041.150 I print_info: ssm_d_state      = 0
0.00.041.150 I print_info: ssm_dt_rank      = 0
0.00.041.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.151 I print_info: model type       = 1.4B
0.00.041.151 I print_info: model params     = 1.41 B
0.00.041.151 I print_info: general.name     = 1.4B
0.00.041.152 I print_info: vocab type       = BPE
0.00.041.152 I print_info: n_vocab          = 50304
0.00.041.152 I print_info: n_merges         = 50009
0.00.041.152 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.153 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.153 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.153 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.153 I print_info: LF token         = 187 'Ċ'
0.00.041.154 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: max token length = 1024
0.00.041.154 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.754 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.758 I load_tensors: offloading output layer to GPU
0.00.642.759 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.778 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.642.780 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.643.658 I llama_init_from_model: n_seq_max     = 1
0.00.643.663 I llama_init_from_model: n_ctx         = 2048
0.00.643.664 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.664 I llama_init_from_model: n_batch       = 2048
0.00.643.664 I llama_init_from_model: n_ubatch      = 512
0.00.643.665 I llama_init_from_model: flash_attn    = 0
0.00.643.666 I llama_init_from_model: freq_base     = 10000.0
0.00.643.667 I llama_init_from_model: freq_scale    = 1
0.00.643.668 I ggml_metal_init: allocating
0.00.643.702 I ggml_metal_init: found device: Apple M4
0.00.643.713 I ggml_metal_init: picking default device: Apple M4
0.00.644.656 I ggml_metal_init: using embedded metal library
0.00.649.011 I ggml_metal_init: GPU name:   Apple M4
0.00.649.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.018 I ggml_metal_init: simdgroup reduction   = true
0.00.649.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.019 I ggml_metal_init: has residency sets    = true
0.00.649.019 I ggml_metal_init: has bfloat            = true
0.00.649.019 I ggml_metal_init: use bfloat            = true
0.00.649.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.644 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.373 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.395 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.320 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.322 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.322 I llama_init_from_model: graph nodes  = 967
0.00.698.323 I llama_init_from_model: graph splits = 2
0.00.698.327 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.787 I main: llama threadpool init, n_threads = 4
0.00.753.831 I 
0.00.753.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.849 I 
0.00.754.000 I sampler seed: 1234
0.00.754.004 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.019 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.019 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.019 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.485.569 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.485.570 I llama_perf_context_print:        load time =     743.11 ms
0.01.485.574 I llama_perf_context_print: prompt eval time =      48.89 ms /     7 tokens (    6.98 ms per token,   143.18 tokens per second)
0.01.485.575 I llama_perf_context_print:        eval time =     680.32 ms /    63 runs   (   10.80 ms per token,    92.60 tokens per second)
0.01.485.575 I llama_perf_context_print:       total time =     732.56 ms /    70 tokens
0.01.485.825 I ggml_metal_free: deallocating

real	0m1.502s
user	0m0.106s
sys	0m0.173s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.160 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.921 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.706 I llama_model_loader: - type  f32:  194 tensors
0.00.025.706 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.708 I print_info: file format = GGUF V3 (latest)
0.00.025.708 I print_info: file type   = Q5_0
0.00.025.709 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.121 I load: special tokens cache size = 25
0.00.040.634 I load: token to piece cache size = 0.2984 MB
0.00.040.652 I print_info: arch             = gptneox
0.00.040.653 I print_info: vocab_only       = 0
0.00.040.653 I print_info: n_ctx_train      = 2048
0.00.040.653 I print_info: n_embd           = 2048
0.00.040.653 I print_info: n_layer          = 24
0.00.040.657 I print_info: n_head           = 16
0.00.040.658 I print_info: n_head_kv        = 16
0.00.040.658 I print_info: n_rot            = 32
0.00.040.658 I print_info: n_swa            = 0
0.00.040.658 I print_info: n_embd_head_k    = 128
0.00.040.658 I print_info: n_embd_head_v    = 128
0.00.040.659 I print_info: n_gqa            = 1
0.00.040.660 I print_info: n_embd_k_gqa     = 2048
0.00.040.660 I print_info: n_embd_v_gqa     = 2048
0.00.040.661 I print_info: f_norm_eps       = 1.0e-05
0.00.040.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.662 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.662 I print_info: f_logit_scale    = 0.0e+00
0.00.040.663 I print_info: n_ff             = 8192
0.00.040.663 I print_info: n_expert         = 0
0.00.040.663 I print_info: n_expert_used    = 0
0.00.040.663 I print_info: causal attn      = 1
0.00.040.663 I print_info: pooling type     = 0
0.00.040.663 I print_info: rope type        = 2
0.00.040.664 I print_info: rope scaling     = linear
0.00.040.664 I print_info: freq_base_train  = 10000.0
0.00.040.664 I print_info: freq_scale_train = 1
0.00.040.664 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.665 I print_info: rope_finetuned   = unknown
0.00.040.667 I print_info: ssm_d_conv       = 0
0.00.040.667 I print_info: ssm_d_inner      = 0
0.00.040.667 I print_info: ssm_d_state      = 0
0.00.040.667 I print_info: ssm_dt_rank      = 0
0.00.040.667 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.667 I print_info: model type       = 1.4B
0.00.040.668 I print_info: model params     = 1.41 B
0.00.040.668 I print_info: general.name     = 1.4B
0.00.040.668 I print_info: vocab type       = BPE
0.00.040.669 I print_info: n_vocab          = 50304
0.00.040.669 I print_info: n_merges         = 50009
0.00.040.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: LF token         = 187 'Ċ'
0.00.040.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: max token length = 1024
0.00.040.670 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.619 I load_tensors: offloading output layer to GPU
0.00.656.620 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.655 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.656.657 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.658.197 I llama_init_from_model: n_seq_max     = 1
0.00.658.200 I llama_init_from_model: n_ctx         = 2048
0.00.658.201 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.201 I llama_init_from_model: n_batch       = 2048
0.00.658.201 I llama_init_from_model: n_ubatch      = 512
0.00.658.202 I llama_init_from_model: flash_attn    = 0
0.00.658.204 I llama_init_from_model: freq_base     = 10000.0
0.00.658.204 I llama_init_from_model: freq_scale    = 1
0.00.658.206 I ggml_metal_init: allocating
0.00.658.339 I ggml_metal_init: found device: Apple M4
0.00.658.354 I ggml_metal_init: picking default device: Apple M4
0.00.659.814 I ggml_metal_init: using embedded metal library
0.00.666.438 I ggml_metal_init: GPU name:   Apple M4
0.00.666.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.444 I ggml_metal_init: simdgroup reduction   = true
0.00.666.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.445 I ggml_metal_init: has residency sets    = true
0.00.666.445 I ggml_metal_init: has bfloat            = true
0.00.666.445 I ggml_metal_init: use bfloat            = true
0.00.666.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.941 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.171 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.178 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.203 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.314 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.739.316 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.739.316 I llama_init_from_model: graph nodes  = 967
0.00.739.316 I llama_init_from_model: graph splits = 2
0.00.739.322 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.739.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.037 I main: llama threadpool init, n_threads = 4
0.00.798.082 I 
0.00.798.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.103 I 
0.00.798.277 I sampler seed: 1234
0.00.798.282 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.296 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.296 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.298 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.594.275 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.594.276 I llama_perf_context_print:        load time =     788.09 ms
0.01.594.276 I llama_perf_context_print: prompt eval time =      52.86 ms /     7 tokens (    7.55 ms per token,   132.42 tokens per second)
0.01.594.279 I llama_perf_context_print:        eval time =     740.37 ms /    63 runs   (   11.75 ms per token,    85.09 tokens per second)
0.01.594.279 I llama_perf_context_print:       total time =     797.02 ms /    70 tokens
0.01.594.552 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.111s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.527 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.456 I llama_model_loader: - type  f32:  194 tensors
0.00.033.457 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.458 I print_info: file format = GGUF V3 (latest)
0.00.033.458 I print_info: file type   = Q5_1
0.00.033.460 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.568 I load: special tokens cache size = 25
0.00.047.971 I load: token to piece cache size = 0.2984 MB
0.00.047.988 I print_info: arch             = gptneox
0.00.047.989 I print_info: vocab_only       = 0
0.00.047.989 I print_info: n_ctx_train      = 2048
0.00.047.989 I print_info: n_embd           = 2048
0.00.047.989 I print_info: n_layer          = 24
0.00.047.993 I print_info: n_head           = 16
0.00.047.994 I print_info: n_head_kv        = 16
0.00.047.994 I print_info: n_rot            = 32
0.00.047.994 I print_info: n_swa            = 0
0.00.047.994 I print_info: n_embd_head_k    = 128
0.00.047.995 I print_info: n_embd_head_v    = 128
0.00.047.995 I print_info: n_gqa            = 1
0.00.047.996 I print_info: n_embd_k_gqa     = 2048
0.00.047.996 I print_info: n_embd_v_gqa     = 2048
0.00.047.997 I print_info: f_norm_eps       = 1.0e-05
0.00.047.997 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.997 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.998 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.998 I print_info: f_logit_scale    = 0.0e+00
0.00.047.998 I print_info: n_ff             = 8192
0.00.047.998 I print_info: n_expert         = 0
0.00.047.999 I print_info: n_expert_used    = 0
0.00.047.999 I print_info: causal attn      = 1
0.00.047.999 I print_info: pooling type     = 0
0.00.048.001 I print_info: rope type        = 2
0.00.048.005 I print_info: rope scaling     = linear
0.00.048.005 I print_info: freq_base_train  = 10000.0
0.00.048.005 I print_info: freq_scale_train = 1
0.00.048.005 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.005 I print_info: rope_finetuned   = unknown
0.00.048.006 I print_info: ssm_d_conv       = 0
0.00.048.006 I print_info: ssm_d_inner      = 0
0.00.048.006 I print_info: ssm_d_state      = 0
0.00.048.006 I print_info: ssm_dt_rank      = 0
0.00.048.006 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.006 I print_info: model type       = 1.4B
0.00.048.007 I print_info: model params     = 1.41 B
0.00.048.007 I print_info: general.name     = 1.4B
0.00.048.007 I print_info: vocab type       = BPE
0.00.048.008 I print_info: n_vocab          = 50304
0.00.048.008 I print_info: n_merges         = 50009
0.00.048.008 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.008 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: LF token         = 187 'Ċ'
0.00.048.009 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.009 I print_info: max token length = 1024
0.00.048.010 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.786.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.786.337 I load_tensors: offloading output layer to GPU
0.00.786.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.786.376 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.786.378 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.787.626 I llama_init_from_model: n_seq_max     = 1
0.00.787.629 I llama_init_from_model: n_ctx         = 2048
0.00.787.630 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.787.630 I llama_init_from_model: n_batch       = 2048
0.00.787.631 I llama_init_from_model: n_ubatch      = 512
0.00.787.631 I llama_init_from_model: flash_attn    = 0
0.00.787.633 I llama_init_from_model: freq_base     = 10000.0
0.00.787.634 I llama_init_from_model: freq_scale    = 1
0.00.787.636 I ggml_metal_init: allocating
0.00.787.719 I ggml_metal_init: found device: Apple M4
0.00.787.733 I ggml_metal_init: picking default device: Apple M4
0.00.789.391 I ggml_metal_init: using embedded metal library
0.00.795.963 I ggml_metal_init: GPU name:   Apple M4
0.00.795.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.795.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.795.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.795.970 I ggml_metal_init: simdgroup reduction   = true
0.00.795.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.795.971 I ggml_metal_init: has residency sets    = true
0.00.795.971 I ggml_metal_init: has bfloat            = true
0.00.795.971 I ggml_metal_init: use bfloat            = true
0.00.795.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.795.976 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.814.030 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.871.020 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.871.028 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.871.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.875.482 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.875.485 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.875.486 I llama_init_from_model: graph nodes  = 967
0.00.875.486 I llama_init_from_model: graph splits = 2
0.00.875.492 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.875.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.875.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.268 I main: llama threadpool init, n_threads = 4
0.00.933.317 I 
0.00.933.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.339 I 
0.00.933.492 I sampler seed: 1234
0.00.933.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.933.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.933.513 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.933.514 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.766.598 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.01.766.599 I llama_perf_context_print:        load time =     918.99 ms
0.01.766.600 I llama_perf_context_print: prompt eval time =      41.91 ms /     7 tokens (    5.99 ms per token,   167.04 tokens per second)
0.01.766.600 I llama_perf_context_print:        eval time =     788.45 ms /    63 runs   (   12.52 ms per token,    79.90 tokens per second)
0.01.766.601 I llama_perf_context_print:       total time =     834.08 ms /    70 tokens
0.01.766.844 I ggml_metal_free: deallocating

real	0m1.788s
user	0m0.113s
sys	0m0.243s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.008.966 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.316 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.931 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.931 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.931 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.932 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.933 I llama_model_loader: - type  f32:  194 tensors
0.00.024.933 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.933 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.933 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.934 I print_info: file format = GGUF V3 (latest)
0.00.024.934 I print_info: file type   = Q2_K - Medium
0.00.024.936 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.721 I load: special tokens cache size = 25
0.00.039.078 I load: token to piece cache size = 0.2984 MB
0.00.039.092 I print_info: arch             = gptneox
0.00.039.093 I print_info: vocab_only       = 0
0.00.039.093 I print_info: n_ctx_train      = 2048
0.00.039.093 I print_info: n_embd           = 2048
0.00.039.094 I print_info: n_layer          = 24
0.00.039.096 I print_info: n_head           = 16
0.00.039.097 I print_info: n_head_kv        = 16
0.00.039.097 I print_info: n_rot            = 32
0.00.039.097 I print_info: n_swa            = 0
0.00.039.097 I print_info: n_embd_head_k    = 128
0.00.039.097 I print_info: n_embd_head_v    = 128
0.00.039.098 I print_info: n_gqa            = 1
0.00.039.099 I print_info: n_embd_k_gqa     = 2048
0.00.039.100 I print_info: n_embd_v_gqa     = 2048
0.00.039.100 I print_info: f_norm_eps       = 1.0e-05
0.00.039.101 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.101 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.103 I print_info: f_logit_scale    = 0.0e+00
0.00.039.103 I print_info: n_ff             = 8192
0.00.039.104 I print_info: n_expert         = 0
0.00.039.104 I print_info: n_expert_used    = 0
0.00.039.104 I print_info: causal attn      = 1
0.00.039.104 I print_info: pooling type     = 0
0.00.039.104 I print_info: rope type        = 2
0.00.039.104 I print_info: rope scaling     = linear
0.00.039.105 I print_info: freq_base_train  = 10000.0
0.00.039.105 I print_info: freq_scale_train = 1
0.00.039.105 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.105 I print_info: rope_finetuned   = unknown
0.00.039.105 I print_info: ssm_d_conv       = 0
0.00.039.105 I print_info: ssm_d_inner      = 0
0.00.039.106 I print_info: ssm_d_state      = 0
0.00.039.106 I print_info: ssm_dt_rank      = 0
0.00.039.106 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.106 I print_info: model type       = 1.4B
0.00.039.110 I print_info: model params     = 1.41 B
0.00.039.110 I print_info: general.name     = 1.4B
0.00.039.110 I print_info: vocab type       = BPE
0.00.039.110 I print_info: n_vocab          = 50304
0.00.039.111 I print_info: n_merges         = 50009
0.00.039.111 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: LF token         = 187 'Ċ'
0.00.039.113 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: max token length = 1024
0.00.039.113 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.409.877 I load_tensors: offloading 24 repeating layers to GPU
0.00.409.894 I load_tensors: offloading output layer to GPU
0.00.409.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.409.928 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.409.930 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.411.649 I llama_init_from_model: n_seq_max     = 1
0.00.411.652 I llama_init_from_model: n_ctx         = 2048
0.00.411.653 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.411.653 I llama_init_from_model: n_batch       = 2048
0.00.411.654 I llama_init_from_model: n_ubatch      = 512
0.00.411.654 I llama_init_from_model: flash_attn    = 0
0.00.411.656 I llama_init_from_model: freq_base     = 10000.0
0.00.411.657 I llama_init_from_model: freq_scale    = 1
0.00.411.659 I ggml_metal_init: allocating
0.00.411.779 I ggml_metal_init: found device: Apple M4
0.00.411.793 I ggml_metal_init: picking default device: Apple M4
0.00.413.417 I ggml_metal_init: using embedded metal library
0.00.419.037 I ggml_metal_init: GPU name:   Apple M4
0.00.419.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.419.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.419.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.419.054 I ggml_metal_init: simdgroup reduction   = true
0.00.419.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.419.055 I ggml_metal_init: has residency sets    = true
0.00.419.055 I ggml_metal_init: has bfloat            = true
0.00.419.055 I ggml_metal_init: use bfloat            = true
0.00.419.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.419.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.440.307 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.499.109 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.499.115 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.499.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.503.466 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.503.468 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.503.468 I llama_init_from_model: graph nodes  = 967
0.00.503.468 I llama_init_from_model: graph splits = 2
0.00.503.474 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.503.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.503.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.245 I main: llama threadpool init, n_threads = 4
0.00.563.298 I 
0.00.563.317 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.318 I 
0.00.563.499 I sampler seed: 1234
0.00.563.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.563.518 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.563.519 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.563.519 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.251.975 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.251.976 I llama_perf_context_print:        load time =     553.56 ms
0.01.251.977 I llama_perf_context_print: prompt eval time =      44.14 ms /     7 tokens (    6.31 ms per token,   158.58 tokens per second)
0.01.251.979 I llama_perf_context_print:        eval time =     641.53 ms /    63 runs   (   10.18 ms per token,    98.20 tokens per second)
0.01.251.979 I llama_perf_context_print:       total time =     689.45 ms /    70 tokens
0.01.252.206 I ggml_metal_free: deallocating

real	0m1.268s
user	0m0.111s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.616 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.628 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.455 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.238 I llama_model_loader: - type  f32:  194 tensors
0.00.025.239 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.239 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.239 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.240 I print_info: file format = GGUF V3 (latest)
0.00.025.240 I print_info: file type   = Q3_K - Medium
0.00.025.241 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.478 I load: special tokens cache size = 25
0.00.039.931 I load: token to piece cache size = 0.2984 MB
0.00.039.945 I print_info: arch             = gptneox
0.00.039.946 I print_info: vocab_only       = 0
0.00.039.946 I print_info: n_ctx_train      = 2048
0.00.039.946 I print_info: n_embd           = 2048
0.00.039.947 I print_info: n_layer          = 24
0.00.039.949 I print_info: n_head           = 16
0.00.039.950 I print_info: n_head_kv        = 16
0.00.039.950 I print_info: n_rot            = 32
0.00.039.951 I print_info: n_swa            = 0
0.00.039.951 I print_info: n_embd_head_k    = 128
0.00.039.951 I print_info: n_embd_head_v    = 128
0.00.039.952 I print_info: n_gqa            = 1
0.00.039.952 I print_info: n_embd_k_gqa     = 2048
0.00.039.953 I print_info: n_embd_v_gqa     = 2048
0.00.039.954 I print_info: f_norm_eps       = 1.0e-05
0.00.039.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.954 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.954 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.955 I print_info: f_logit_scale    = 0.0e+00
0.00.039.955 I print_info: n_ff             = 8192
0.00.039.955 I print_info: n_expert         = 0
0.00.039.956 I print_info: n_expert_used    = 0
0.00.039.958 I print_info: causal attn      = 1
0.00.039.959 I print_info: pooling type     = 0
0.00.039.959 I print_info: rope type        = 2
0.00.039.959 I print_info: rope scaling     = linear
0.00.039.960 I print_info: freq_base_train  = 10000.0
0.00.039.960 I print_info: freq_scale_train = 1
0.00.039.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.960 I print_info: rope_finetuned   = unknown
0.00.039.960 I print_info: ssm_d_conv       = 0
0.00.039.961 I print_info: ssm_d_inner      = 0
0.00.039.961 I print_info: ssm_d_state      = 0
0.00.039.961 I print_info: ssm_dt_rank      = 0
0.00.039.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.961 I print_info: model type       = 1.4B
0.00.039.961 I print_info: model params     = 1.41 B
0.00.039.962 I print_info: general.name     = 1.4B
0.00.039.962 I print_info: vocab type       = BPE
0.00.039.962 I print_info: n_vocab          = 50304
0.00.039.962 I print_info: n_merges         = 50009
0.00.039.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.964 I print_info: LF token         = 187 'Ċ'
0.00.039.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.965 I print_info: max token length = 1024
0.00.039.965 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.483 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.494 I load_tensors: offloading output layer to GPU
0.00.451.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.529 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.533 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.453.364 I llama_init_from_model: n_seq_max     = 1
0.00.453.367 I llama_init_from_model: n_ctx         = 2048
0.00.453.367 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.453.368 I llama_init_from_model: n_batch       = 2048
0.00.453.368 I llama_init_from_model: n_ubatch      = 512
0.00.453.369 I llama_init_from_model: flash_attn    = 0
0.00.453.371 I llama_init_from_model: freq_base     = 10000.0
0.00.453.371 I llama_init_from_model: freq_scale    = 1
0.00.453.376 I ggml_metal_init: allocating
0.00.453.444 I ggml_metal_init: found device: Apple M4
0.00.453.457 I ggml_metal_init: picking default device: Apple M4
0.00.455.050 I ggml_metal_init: using embedded metal library
0.00.461.067 I ggml_metal_init: GPU name:   Apple M4
0.00.461.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.461.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.461.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.461.075 I ggml_metal_init: simdgroup reduction   = true
0.00.461.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.461.075 I ggml_metal_init: has residency sets    = true
0.00.461.076 I ggml_metal_init: has bfloat            = true
0.00.461.076 I ggml_metal_init: use bfloat            = true
0.00.461.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.461.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.914 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.594 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.538.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.538.626 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.555.686 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.555.688 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.555.688 I llama_init_from_model: graph nodes  = 967
0.00.555.689 I llama_init_from_model: graph splits = 2
0.00.555.694 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.555.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.555.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.297 I main: llama threadpool init, n_threads = 4
0.00.611.357 I 
0.00.611.381 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.381 I 
0.00.611.544 I sampler seed: 1234
0.00.611.549 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.595 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.598 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.361.417 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.361.418 I llama_perf_context_print:        load time =     601.76 ms
0.01.361.418 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.97 tokens per second)
0.01.361.419 I llama_perf_context_print:        eval time =     696.88 ms /    63 runs   (   11.06 ms per token,    90.40 tokens per second)
0.01.361.420 I llama_perf_context_print:       total time =     750.86 ms /    70 tokens
0.01.361.698 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.110s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.373 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.497 I llama_model_loader: - type  f32:  194 tensors
0.00.026.497 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.497 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.497 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.498 I print_info: file format = GGUF V3 (latest)
0.00.026.498 I print_info: file type   = Q4_K - Medium
0.00.026.499 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.695 I load: special tokens cache size = 25
0.00.041.005 I load: token to piece cache size = 0.2984 MB
0.00.041.020 I print_info: arch             = gptneox
0.00.041.021 I print_info: vocab_only       = 0
0.00.041.021 I print_info: n_ctx_train      = 2048
0.00.041.021 I print_info: n_embd           = 2048
0.00.041.022 I print_info: n_layer          = 24
0.00.041.024 I print_info: n_head           = 16
0.00.041.025 I print_info: n_head_kv        = 16
0.00.041.025 I print_info: n_rot            = 32
0.00.041.025 I print_info: n_swa            = 0
0.00.041.025 I print_info: n_embd_head_k    = 128
0.00.041.025 I print_info: n_embd_head_v    = 128
0.00.041.026 I print_info: n_gqa            = 1
0.00.041.027 I print_info: n_embd_k_gqa     = 2048
0.00.041.028 I print_info: n_embd_v_gqa     = 2048
0.00.041.028 I print_info: f_norm_eps       = 1.0e-05
0.00.041.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.029 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.029 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.029 I print_info: f_logit_scale    = 0.0e+00
0.00.041.030 I print_info: n_ff             = 8192
0.00.041.030 I print_info: n_expert         = 0
0.00.041.030 I print_info: n_expert_used    = 0
0.00.041.030 I print_info: causal attn      = 1
0.00.041.032 I print_info: pooling type     = 0
0.00.041.034 I print_info: rope type        = 2
0.00.041.034 I print_info: rope scaling     = linear
0.00.041.034 I print_info: freq_base_train  = 10000.0
0.00.041.035 I print_info: freq_scale_train = 1
0.00.041.036 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.036 I print_info: rope_finetuned   = unknown
0.00.041.036 I print_info: ssm_d_conv       = 0
0.00.041.036 I print_info: ssm_d_inner      = 0
0.00.041.036 I print_info: ssm_d_state      = 0
0.00.041.037 I print_info: ssm_dt_rank      = 0
0.00.041.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.038 I print_info: model type       = 1.4B
0.00.041.038 I print_info: model params     = 1.41 B
0.00.041.038 I print_info: general.name     = 1.4B
0.00.041.039 I print_info: vocab type       = BPE
0.00.041.039 I print_info: n_vocab          = 50304
0.00.041.039 I print_info: n_merges         = 50009
0.00.041.039 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.039 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.039 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.041 I print_info: LF token         = 187 'Ċ'
0.00.041.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.041 I print_info: max token length = 1024
0.00.041.042 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.527.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.407 I load_tensors: offloading output layer to GPU
0.00.527.407 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.441 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.442 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.102 I llama_init_from_model: n_seq_max     = 1
0.00.529.105 I llama_init_from_model: n_ctx         = 2048
0.00.529.105 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.529.106 I llama_init_from_model: n_batch       = 2048
0.00.529.106 I llama_init_from_model: n_ubatch      = 512
0.00.529.107 I llama_init_from_model: flash_attn    = 0
0.00.529.109 I llama_init_from_model: freq_base     = 10000.0
0.00.529.110 I llama_init_from_model: freq_scale    = 1
0.00.529.113 I ggml_metal_init: allocating
0.00.529.191 I ggml_metal_init: found device: Apple M4
0.00.529.205 I ggml_metal_init: picking default device: Apple M4
0.00.530.793 I ggml_metal_init: using embedded metal library
0.00.537.613 I ggml_metal_init: GPU name:   Apple M4
0.00.537.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.619 I ggml_metal_init: simdgroup reduction   = true
0.00.537.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.620 I ggml_metal_init: has residency sets    = true
0.00.537.620 I ggml_metal_init: has bfloat            = true
0.00.537.620 I ggml_metal_init: use bfloat            = true
0.00.537.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.042 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.784 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.825 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.270 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.273 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.273 I llama_init_from_model: graph nodes  = 967
0.00.619.273 I llama_init_from_model: graph splits = 2
0.00.619.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.747 I main: llama threadpool init, n_threads = 4
0.00.677.799 I 
0.00.677.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.818 I 
0.00.677.991 I sampler seed: 1234
0.00.677.995 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.678.039 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.678.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.678.043 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.433.167 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48005.41 tokens per second)
0.01.433.168 I llama_perf_context_print:        load time =     666.65 ms
0.01.433.168 I llama_perf_context_print: prompt eval time =      51.97 ms /     7 tokens (    7.42 ms per token,   134.70 tokens per second)
0.01.433.170 I llama_perf_context_print:        eval time =     700.09 ms /    63 runs   (   11.11 ms per token,    89.99 tokens per second)
0.01.433.170 I llama_perf_context_print:       total time =     756.14 ms /    70 tokens
0.01.433.433 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.967 I llama_model_loader: - type  f32:  194 tensors
0.00.023.968 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.968 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.968 I print_info: file format = GGUF V3 (latest)
0.00.023.969 I print_info: file type   = Q5_K - Medium
0.00.023.974 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.813 I load: special tokens cache size = 25
0.00.038.070 I load: token to piece cache size = 0.2984 MB
0.00.038.083 I print_info: arch             = gptneox
0.00.038.084 I print_info: vocab_only       = 0
0.00.038.085 I print_info: n_ctx_train      = 2048
0.00.038.085 I print_info: n_embd           = 2048
0.00.038.085 I print_info: n_layer          = 24
0.00.038.088 I print_info: n_head           = 16
0.00.038.089 I print_info: n_head_kv        = 16
0.00.038.089 I print_info: n_rot            = 32
0.00.038.089 I print_info: n_swa            = 0
0.00.038.089 I print_info: n_embd_head_k    = 128
0.00.038.089 I print_info: n_embd_head_v    = 128
0.00.038.090 I print_info: n_gqa            = 1
0.00.038.091 I print_info: n_embd_k_gqa     = 2048
0.00.038.092 I print_info: n_embd_v_gqa     = 2048
0.00.038.092 I print_info: f_norm_eps       = 1.0e-05
0.00.038.093 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.093 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.093 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.093 I print_info: f_logit_scale    = 0.0e+00
0.00.038.094 I print_info: n_ff             = 8192
0.00.038.094 I print_info: n_expert         = 0
0.00.038.094 I print_info: n_expert_used    = 0
0.00.038.094 I print_info: causal attn      = 1
0.00.038.094 I print_info: pooling type     = 0
0.00.038.095 I print_info: rope type        = 2
0.00.038.095 I print_info: rope scaling     = linear
0.00.038.095 I print_info: freq_base_train  = 10000.0
0.00.038.096 I print_info: freq_scale_train = 1
0.00.038.096 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.096 I print_info: rope_finetuned   = unknown
0.00.038.098 I print_info: ssm_d_conv       = 0
0.00.038.098 I print_info: ssm_d_inner      = 0
0.00.038.098 I print_info: ssm_d_state      = 0
0.00.038.098 I print_info: ssm_dt_rank      = 0
0.00.038.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.098 I print_info: model type       = 1.4B
0.00.038.099 I print_info: model params     = 1.41 B
0.00.038.099 I print_info: general.name     = 1.4B
0.00.038.099 I print_info: vocab type       = BPE
0.00.038.099 I print_info: n_vocab          = 50304
0.00.038.100 I print_info: n_merges         = 50009
0.00.038.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.100 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.101 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.101 I print_info: LF token         = 187 'Ċ'
0.00.038.102 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.102 I print_info: max token length = 1024
0.00.038.102 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.498 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.506 I load_tensors: offloading output layer to GPU
0.00.626.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.539 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.626.540 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.628.089 I llama_init_from_model: n_seq_max     = 1
0.00.628.092 I llama_init_from_model: n_ctx         = 2048
0.00.628.093 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.093 I llama_init_from_model: n_batch       = 2048
0.00.628.094 I llama_init_from_model: n_ubatch      = 512
0.00.628.094 I llama_init_from_model: flash_attn    = 0
0.00.628.096 I llama_init_from_model: freq_base     = 10000.0
0.00.628.096 I llama_init_from_model: freq_scale    = 1
0.00.628.097 I ggml_metal_init: allocating
0.00.628.132 I ggml_metal_init: found device: Apple M4
0.00.628.141 I ggml_metal_init: picking default device: Apple M4
0.00.629.242 I ggml_metal_init: using embedded metal library
0.00.635.581 I ggml_metal_init: GPU name:   Apple M4
0.00.635.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.587 I ggml_metal_init: simdgroup reduction   = true
0.00.635.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.587 I ggml_metal_init: has residency sets    = true
0.00.635.587 I ggml_metal_init: has bfloat            = true
0.00.635.588 I ggml_metal_init: use bfloat            = true
0.00.635.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.428 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.684 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.719 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.395 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.397 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.398 I llama_init_from_model: graph nodes  = 967
0.00.713.398 I llama_init_from_model: graph splits = 2
0.00.713.404 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.034 I main: llama threadpool init, n_threads = 4
0.00.780.086 I 
0.00.780.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.106 I 
0.00.780.283 I sampler seed: 1234
0.00.780.288 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.312 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.314 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.314 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.632.731 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.632.732 I llama_perf_context_print:        load time =     770.51 ms
0.01.632.733 I llama_perf_context_print: prompt eval time =      63.14 ms /     7 tokens (    9.02 ms per token,   110.87 tokens per second)
0.01.632.734 I llama_perf_context_print:        eval time =     786.52 ms /    63 runs   (   12.48 ms per token,    80.10 tokens per second)
0.01.632.734 I llama_perf_context_print:       total time =     853.43 ms /    70 tokens
0.01.632.962 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.202 I llama_model_loader: - type  f32:  194 tensors
0.00.024.203 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.203 I print_info: file format = GGUF V3 (latest)
0.00.024.204 I print_info: file type   = Q6_K
0.00.024.204 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.424 I load: special tokens cache size = 25
0.00.038.643 I load: token to piece cache size = 0.2984 MB
0.00.038.657 I print_info: arch             = gptneox
0.00.038.658 I print_info: vocab_only       = 0
0.00.038.658 I print_info: n_ctx_train      = 2048
0.00.038.658 I print_info: n_embd           = 2048
0.00.038.658 I print_info: n_layer          = 24
0.00.038.661 I print_info: n_head           = 16
0.00.038.662 I print_info: n_head_kv        = 16
0.00.038.662 I print_info: n_rot            = 32
0.00.038.662 I print_info: n_swa            = 0
0.00.038.663 I print_info: n_embd_head_k    = 128
0.00.038.663 I print_info: n_embd_head_v    = 128
0.00.038.664 I print_info: n_gqa            = 1
0.00.038.664 I print_info: n_embd_k_gqa     = 2048
0.00.038.665 I print_info: n_embd_v_gqa     = 2048
0.00.038.666 I print_info: f_norm_eps       = 1.0e-05
0.00.038.666 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.667 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.671 I print_info: f_logit_scale    = 0.0e+00
0.00.038.671 I print_info: n_ff             = 8192
0.00.038.672 I print_info: n_expert         = 0
0.00.038.672 I print_info: n_expert_used    = 0
0.00.038.672 I print_info: causal attn      = 1
0.00.038.672 I print_info: pooling type     = 0
0.00.038.672 I print_info: rope type        = 2
0.00.038.672 I print_info: rope scaling     = linear
0.00.038.673 I print_info: freq_base_train  = 10000.0
0.00.038.673 I print_info: freq_scale_train = 1
0.00.038.673 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.674 I print_info: rope_finetuned   = unknown
0.00.038.675 I print_info: ssm_d_conv       = 0
0.00.038.675 I print_info: ssm_d_inner      = 0
0.00.038.675 I print_info: ssm_d_state      = 0
0.00.038.675 I print_info: ssm_dt_rank      = 0
0.00.038.675 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.675 I print_info: model type       = 1.4B
0.00.038.676 I print_info: model params     = 1.41 B
0.00.038.676 I print_info: general.name     = 1.4B
0.00.038.676 I print_info: vocab type       = BPE
0.00.038.676 I print_info: n_vocab          = 50304
0.00.038.676 I print_info: n_merges         = 50009
0.00.038.676 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.677 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.679 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.680 I print_info: LF token         = 187 'Ċ'
0.00.038.680 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.680 I print_info: max token length = 1024
0.00.038.680 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.325 I load_tensors: offloading output layer to GPU
0.00.634.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.349 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.354 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.829 I llama_init_from_model: n_seq_max     = 1
0.00.635.830 I llama_init_from_model: n_ctx         = 2048
0.00.635.831 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.831 I llama_init_from_model: n_batch       = 2048
0.00.635.832 I llama_init_from_model: n_ubatch      = 512
0.00.635.832 I llama_init_from_model: flash_attn    = 0
0.00.635.833 I llama_init_from_model: freq_base     = 10000.0
0.00.635.834 I llama_init_from_model: freq_scale    = 1
0.00.635.835 I ggml_metal_init: allocating
0.00.635.883 I ggml_metal_init: found device: Apple M4
0.00.635.894 I ggml_metal_init: picking default device: Apple M4
0.00.637.218 I ggml_metal_init: using embedded metal library
0.00.643.244 I ggml_metal_init: GPU name:   Apple M4
0.00.643.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.249 I ggml_metal_init: simdgroup reduction   = true
0.00.643.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.249 I ggml_metal_init: has residency sets    = true
0.00.643.250 I ggml_metal_init: has bfloat            = true
0.00.643.250 I ggml_metal_init: use bfloat            = true
0.00.643.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.015 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.347 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.353 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.375 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.657 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.660 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.660 I llama_init_from_model: graph nodes  = 967
0.00.713.660 I llama_init_from_model: graph splits = 2
0.00.713.667 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.210 I main: llama threadpool init, n_threads = 4
0.00.776.263 I 
0.00.776.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.285 I 
0.00.776.466 I sampler seed: 1234
0.00.776.470 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.485 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.487 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.487 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.669.928 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.01.669.928 I llama_perf_context_print:        load time =     766.61 ms
0.01.669.929 I llama_perf_context_print: prompt eval time =      57.48 ms /     7 tokens (    8.21 ms per token,   121.78 tokens per second)
0.01.669.930 I llama_perf_context_print:        eval time =     833.34 ms /    63 runs   (   13.23 ms per token,    75.60 tokens per second)
0.01.669.930 I llama_perf_context_print:       total time =     894.44 ms /    70 tokens
0.01.670.211 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.658 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.723 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.765 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.788 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.788 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.791 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.793 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.794 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.017 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.200 I llama_model_loader: - type  f32:  194 tensors
0.00.056.201 I llama_model_loader: - type  f16:   98 tensors
0.00.056.201 I print_info: file format = GGUF V3 (latest)
0.00.056.202 I print_info: file type   = all F32 (guessed)
0.00.056.203 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.134 I load: special tokens cache size = 25
0.00.076.332 I load: token to piece cache size = 0.2984 MB
0.00.076.347 I print_info: arch             = gptneox
0.00.076.348 I print_info: vocab_only       = 0
0.00.076.348 I print_info: n_ctx_train      = 2048
0.00.076.348 I print_info: n_embd           = 2048
0.00.076.348 I print_info: n_layer          = 24
0.00.076.351 I print_info: n_head           = 16
0.00.076.352 I print_info: n_head_kv        = 16
0.00.076.352 I print_info: n_rot            = 32
0.00.076.352 I print_info: n_swa            = 0
0.00.076.353 I print_info: n_embd_head_k    = 128
0.00.076.353 I print_info: n_embd_head_v    = 128
0.00.076.354 I print_info: n_gqa            = 1
0.00.076.354 I print_info: n_embd_k_gqa     = 2048
0.00.076.355 I print_info: n_embd_v_gqa     = 2048
0.00.076.356 I print_info: f_norm_eps       = 1.0e-05
0.00.076.356 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.356 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.356 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.357 I print_info: f_logit_scale    = 0.0e+00
0.00.076.357 I print_info: n_ff             = 8192
0.00.076.357 I print_info: n_expert         = 0
0.00.076.358 I print_info: n_expert_used    = 0
0.00.076.358 I print_info: causal attn      = 1
0.00.076.358 I print_info: pooling type     = 0
0.00.076.358 I print_info: rope type        = 2
0.00.076.358 I print_info: rope scaling     = linear
0.00.076.359 I print_info: freq_base_train  = 10000.0
0.00.076.359 I print_info: freq_scale_train = 1
0.00.076.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.360 I print_info: rope_finetuned   = unknown
0.00.076.360 I print_info: ssm_d_conv       = 0
0.00.076.360 I print_info: ssm_d_inner      = 0
0.00.076.364 I print_info: ssm_d_state      = 0
0.00.076.364 I print_info: ssm_dt_rank      = 0
0.00.076.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.365 I print_info: model type       = 1.4B
0.00.076.365 I print_info: model params     = 1.41 B
0.00.076.365 I print_info: general.name     = 1.4B
0.00.076.366 I print_info: vocab type       = BPE
0.00.076.366 I print_info: n_vocab          = 50304
0.00.076.366 I print_info: n_merges         = 50009
0.00.076.366 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.367 I print_info: LF token         = 187 'Ċ'
0.00.076.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.367 I print_info: max token length = 1024
0.00.076.368 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.419.379 I load_tensors: offloading 24 repeating layers to GPU
0.01.419.384 I load_tensors: offloading output layer to GPU
0.01.419.386 I load_tensors: offloaded 25/25 layers to GPU
0.01.419.407 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.419.408 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.420.503 I llama_init_from_model: n_seq_max     = 1
0.01.420.505 I llama_init_from_model: n_ctx         = 128
0.01.420.505 I llama_init_from_model: n_ctx_per_seq = 128
0.01.420.505 I llama_init_from_model: n_batch       = 128
0.01.420.506 I llama_init_from_model: n_ubatch      = 128
0.01.420.506 I llama_init_from_model: flash_attn    = 0
0.01.420.507 I llama_init_from_model: freq_base     = 10000.0
0.01.420.507 I llama_init_from_model: freq_scale    = 1
0.01.420.507 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.420.509 I ggml_metal_init: allocating
0.01.420.526 I ggml_metal_init: found device: Apple M4
0.01.420.532 I ggml_metal_init: picking default device: Apple M4
0.01.421.422 I ggml_metal_init: using embedded metal library
0.01.425.828 I ggml_metal_init: GPU name:   Apple M4
0.01.425.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.425.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.425.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.425.832 I ggml_metal_init: simdgroup reduction   = true
0.01.425.832 I ggml_metal_init: simdgroup matrix mul. = true
0.01.425.832 I ggml_metal_init: has residency sets    = true
0.01.425.832 I ggml_metal_init: has bfloat            = true
0.01.425.833 I ggml_metal_init: use bfloat            = true
0.01.425.833 I ggml_metal_init: hasUnifiedMemory      = true
0.01.425.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.437.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.439.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.439.383 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.439.398 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.441.127 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.441.128 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.441.128 I llama_init_from_model: graph nodes  = 967
0.01.441.129 I llama_init_from_model: graph splits = 2
0.01.441.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.441.130 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.476.638 I 
0.01.476.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.681 I perplexity: tokenizing the input ..
0.01.482.014 I perplexity: tokenization took 5.331 ms
0.01.482.023 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.600.799 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.602.135 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.602.168 I llama_perf_context_print:        load time =    1451.91 ms
0.01.602.169 I llama_perf_context_print: prompt eval time =     118.46 ms /   128 tokens (    0.93 ms per token,  1080.52 tokens per second)
0.01.602.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.602.170 I llama_perf_context_print:       total time =     125.53 ms /   129 tokens
0.01.602.534 I ggml_metal_free: deallocating

real	0m1.815s
user	0m0.099s
sys	0m0.266s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.284 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.284 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.284 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.285 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.067 I llama_model_loader: - type  f32:  194 tensors
0.00.026.068 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.068 I print_info: file format = GGUF V3 (latest)
0.00.026.069 I print_info: file type   = Q8_0
0.00.026.070 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.747 I load: special tokens cache size = 25
0.00.041.116 I load: token to piece cache size = 0.2984 MB
0.00.041.134 I print_info: arch             = gptneox
0.00.041.135 I print_info: vocab_only       = 0
0.00.041.135 I print_info: n_ctx_train      = 2048
0.00.041.135 I print_info: n_embd           = 2048
0.00.041.135 I print_info: n_layer          = 24
0.00.041.139 I print_info: n_head           = 16
0.00.041.140 I print_info: n_head_kv        = 16
0.00.041.140 I print_info: n_rot            = 32
0.00.041.140 I print_info: n_swa            = 0
0.00.041.140 I print_info: n_embd_head_k    = 128
0.00.041.140 I print_info: n_embd_head_v    = 128
0.00.041.141 I print_info: n_gqa            = 1
0.00.041.141 I print_info: n_embd_k_gqa     = 2048
0.00.041.142 I print_info: n_embd_v_gqa     = 2048
0.00.041.142 I print_info: f_norm_eps       = 1.0e-05
0.00.041.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.143 I print_info: f_logit_scale    = 0.0e+00
0.00.041.144 I print_info: n_ff             = 8192
0.00.041.144 I print_info: n_expert         = 0
0.00.041.144 I print_info: n_expert_used    = 0
0.00.041.148 I print_info: causal attn      = 1
0.00.041.148 I print_info: pooling type     = 0
0.00.041.149 I print_info: rope type        = 2
0.00.041.149 I print_info: rope scaling     = linear
0.00.041.149 I print_info: freq_base_train  = 10000.0
0.00.041.149 I print_info: freq_scale_train = 1
0.00.041.150 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.150 I print_info: rope_finetuned   = unknown
0.00.041.150 I print_info: ssm_d_conv       = 0
0.00.041.150 I print_info: ssm_d_inner      = 0
0.00.041.151 I print_info: ssm_d_state      = 0
0.00.041.151 I print_info: ssm_dt_rank      = 0
0.00.041.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.151 I print_info: model type       = 1.4B
0.00.041.152 I print_info: model params     = 1.41 B
0.00.041.152 I print_info: general.name     = 1.4B
0.00.041.152 I print_info: vocab type       = BPE
0.00.041.153 I print_info: n_vocab          = 50304
0.00.041.153 I print_info: n_merges         = 50009
0.00.041.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.155 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.155 I print_info: LF token         = 187 'Ċ'
0.00.041.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.155 I print_info: max token length = 1024
0.00.041.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.787.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.787.590 I load_tensors: offloading output layer to GPU
0.00.787.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.787.620 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.787.622 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.788.973 I llama_init_from_model: n_seq_max     = 1
0.00.788.975 I llama_init_from_model: n_ctx         = 128
0.00.788.975 I llama_init_from_model: n_ctx_per_seq = 128
0.00.788.975 I llama_init_from_model: n_batch       = 128
0.00.788.976 I llama_init_from_model: n_ubatch      = 128
0.00.788.976 I llama_init_from_model: flash_attn    = 0
0.00.788.977 I llama_init_from_model: freq_base     = 10000.0
0.00.788.977 I llama_init_from_model: freq_scale    = 1
0.00.788.978 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.788.979 I ggml_metal_init: allocating
0.00.789.028 I ggml_metal_init: found device: Apple M4
0.00.789.037 I ggml_metal_init: picking default device: Apple M4
0.00.790.140 I ggml_metal_init: using embedded metal library
0.00.795.570 I ggml_metal_init: GPU name:   Apple M4
0.00.795.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.795.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.795.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.795.575 I ggml_metal_init: simdgroup reduction   = true
0.00.795.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.795.575 I ggml_metal_init: has residency sets    = true
0.00.795.575 I ggml_metal_init: has bfloat            = true
0.00.795.576 I ggml_metal_init: use bfloat            = true
0.00.795.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.795.578 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.811.149 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.814.504 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.814.508 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.814.542 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.600 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.817.601 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.817.602 I llama_init_from_model: graph nodes  = 967
0.00.817.602 I llama_init_from_model: graph splits = 2
0.00.817.605 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.817.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.595 I 
0.00.843.688 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.712 I perplexity: tokenizing the input ..
0.00.850.523 I perplexity: tokenization took 6.809 ms
0.00.850.529 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.988.512 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.989.859 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.989.885 I llama_perf_context_print:        load time =     833.88 ms
0.00.989.886 I llama_perf_context_print: prompt eval time =     137.11 ms /   128 tokens (    1.07 ms per token,   933.56 tokens per second)
0.00.989.887 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.989.887 I llama_perf_context_print:       total time =     146.29 ms /   129 tokens
0.00.990.288 I ggml_metal_free: deallocating

real	0m1.004s
user	0m0.077s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.641 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.258 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.033 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.034 I llama_model_loader: - type  f32:  194 tensors
0.00.026.034 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.034 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.035 I print_info: file format = GGUF V3 (latest)
0.00.026.035 I print_info: file type   = Q4_0
0.00.026.037 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.441 I load: special tokens cache size = 25
0.00.040.829 I load: token to piece cache size = 0.2984 MB
0.00.040.847 I print_info: arch             = gptneox
0.00.040.847 I print_info: vocab_only       = 0
0.00.040.848 I print_info: n_ctx_train      = 2048
0.00.040.848 I print_info: n_embd           = 2048
0.00.040.848 I print_info: n_layer          = 24
0.00.040.852 I print_info: n_head           = 16
0.00.040.853 I print_info: n_head_kv        = 16
0.00.040.853 I print_info: n_rot            = 32
0.00.040.857 I print_info: n_swa            = 0
0.00.040.857 I print_info: n_embd_head_k    = 128
0.00.040.857 I print_info: n_embd_head_v    = 128
0.00.040.857 I print_info: n_gqa            = 1
0.00.040.858 I print_info: n_embd_k_gqa     = 2048
0.00.040.859 I print_info: n_embd_v_gqa     = 2048
0.00.040.859 I print_info: f_norm_eps       = 1.0e-05
0.00.040.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.860 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.860 I print_info: f_logit_scale    = 0.0e+00
0.00.040.861 I print_info: n_ff             = 8192
0.00.040.861 I print_info: n_expert         = 0
0.00.040.861 I print_info: n_expert_used    = 0
0.00.040.861 I print_info: causal attn      = 1
0.00.040.862 I print_info: pooling type     = 0
0.00.040.862 I print_info: rope type        = 2
0.00.040.862 I print_info: rope scaling     = linear
0.00.040.862 I print_info: freq_base_train  = 10000.0
0.00.040.862 I print_info: freq_scale_train = 1
0.00.040.863 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.863 I print_info: rope_finetuned   = unknown
0.00.040.863 I print_info: ssm_d_conv       = 0
0.00.040.863 I print_info: ssm_d_inner      = 0
0.00.040.863 I print_info: ssm_d_state      = 0
0.00.040.863 I print_info: ssm_dt_rank      = 0
0.00.040.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.865 I print_info: model type       = 1.4B
0.00.040.865 I print_info: model params     = 1.41 B
0.00.040.865 I print_info: general.name     = 1.4B
0.00.040.866 I print_info: vocab type       = BPE
0.00.040.866 I print_info: n_vocab          = 50304
0.00.040.866 I print_info: n_merges         = 50009
0.00.040.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: LF token         = 187 'Ċ'
0.00.040.867 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: max token length = 1024
0.00.040.868 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.254 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.273 I load_tensors: offloading output layer to GPU
0.00.589.273 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.307 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.589.308 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.591.064 I llama_init_from_model: n_seq_max     = 1
0.00.591.070 I llama_init_from_model: n_ctx         = 128
0.00.591.071 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.072 I llama_init_from_model: n_batch       = 128
0.00.591.072 I llama_init_from_model: n_ubatch      = 128
0.00.591.072 I llama_init_from_model: flash_attn    = 0
0.00.591.074 I llama_init_from_model: freq_base     = 10000.0
0.00.591.074 I llama_init_from_model: freq_scale    = 1
0.00.591.075 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.077 I ggml_metal_init: allocating
0.00.591.182 I ggml_metal_init: found device: Apple M4
0.00.591.197 I ggml_metal_init: picking default device: Apple M4
0.00.592.740 I ggml_metal_init: using embedded metal library
0.00.598.204 I ggml_metal_init: GPU name:   Apple M4
0.00.598.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.224 I ggml_metal_init: simdgroup reduction   = true
0.00.598.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.225 I ggml_metal_init: has residency sets    = true
0.00.598.225 I ggml_metal_init: has bfloat            = true
0.00.598.226 I ggml_metal_init: use bfloat            = true
0.00.598.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.617 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.633 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.686 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.974 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.976 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.976 I llama_init_from_model: graph nodes  = 967
0.00.625.977 I llama_init_from_model: graph splits = 2
0.00.625.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.027 I 
0.00.652.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.146 I perplexity: tokenizing the input ..
0.00.659.100 I perplexity: tokenization took 6.95 ms
0.00.659.107 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.123 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.791.450 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.791.474 I llama_perf_context_print:        load time =     642.38 ms
0.00.791.475 I llama_perf_context_print: prompt eval time =     130.16 ms /   128 tokens (    1.02 ms per token,   983.42 tokens per second)
0.00.791.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.476 I llama_perf_context_print:       total time =     139.45 ms /   129 tokens
0.00.791.853 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.081s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.076.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.076.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.076.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.076.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.076.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.076.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.076.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.076.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.076.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.076.934 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.076.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.076.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.076.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.076.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.076.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.076.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.076.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.454 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.085.455 I llama_model_loader: - type  f32:  194 tensors
0.00.085.456 I llama_model_loader: - type q4_1:   97 tensors
0.00.085.456 I llama_model_loader: - type q6_K:    1 tensors
0.00.085.457 I print_info: file format = GGUF V3 (latest)
0.00.085.457 I print_info: file type   = Q4_1
0.00.085.458 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.100.064 I load: special tokens cache size = 25
0.00.112.792 I load: token to piece cache size = 0.2984 MB
0.00.112.813 I print_info: arch             = gptneox
0.00.112.816 I print_info: vocab_only       = 0
0.00.112.816 I print_info: n_ctx_train      = 2048
0.00.112.816 I print_info: n_embd           = 2048
0.00.112.817 I print_info: n_layer          = 24
0.00.112.822 I print_info: n_head           = 16
0.00.112.823 I print_info: n_head_kv        = 16
0.00.112.823 I print_info: n_rot            = 32
0.00.112.823 I print_info: n_swa            = 0
0.00.112.823 I print_info: n_embd_head_k    = 128
0.00.112.824 I print_info: n_embd_head_v    = 128
0.00.112.825 I print_info: n_gqa            = 1
0.00.112.826 I print_info: n_embd_k_gqa     = 2048
0.00.112.827 I print_info: n_embd_v_gqa     = 2048
0.00.112.828 I print_info: f_norm_eps       = 1.0e-05
0.00.112.828 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.112.829 I print_info: f_clamp_kqv      = 0.0e+00
0.00.112.829 I print_info: f_max_alibi_bias = 0.0e+00
0.00.112.829 I print_info: f_logit_scale    = 0.0e+00
0.00.112.831 I print_info: n_ff             = 8192
0.00.112.832 I print_info: n_expert         = 0
0.00.112.832 I print_info: n_expert_used    = 0
0.00.112.832 I print_info: causal attn      = 1
0.00.112.832 I print_info: pooling type     = 0
0.00.112.833 I print_info: rope type        = 2
0.00.112.833 I print_info: rope scaling     = linear
0.00.112.834 I print_info: freq_base_train  = 10000.0
0.00.112.834 I print_info: freq_scale_train = 1
0.00.112.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.112.835 I print_info: rope_finetuned   = unknown
0.00.112.835 I print_info: ssm_d_conv       = 0
0.00.112.835 I print_info: ssm_d_inner      = 0
0.00.112.835 I print_info: ssm_d_state      = 0
0.00.112.835 I print_info: ssm_dt_rank      = 0
0.00.112.836 I print_info: ssm_dt_b_c_rms   = 0
0.00.112.836 I print_info: model type       = 1.4B
0.00.112.836 I print_info: model params     = 1.41 B
0.00.112.837 I print_info: general.name     = 1.4B
0.00.112.837 I print_info: vocab type       = BPE
0.00.112.838 I print_info: n_vocab          = 50304
0.00.112.838 I print_info: n_merges         = 50009
0.00.112.838 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.112.838 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.112.839 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.112.839 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.112.839 I print_info: LF token         = 187 'Ċ'
0.00.112.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.112.840 I print_info: max token length = 1024
0.00.112.841 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.697.033 I load_tensors: offloading 24 repeating layers to GPU
0.00.697.048 I load_tensors: offloading output layer to GPU
0.00.697.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.697.094 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.697.097 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.698.701 I llama_init_from_model: n_seq_max     = 1
0.00.698.704 I llama_init_from_model: n_ctx         = 128
0.00.698.705 I llama_init_from_model: n_ctx_per_seq = 128
0.00.698.706 I llama_init_from_model: n_batch       = 128
0.00.698.706 I llama_init_from_model: n_ubatch      = 128
0.00.698.706 I llama_init_from_model: flash_attn    = 0
0.00.698.709 I llama_init_from_model: freq_base     = 10000.0
0.00.698.709 I llama_init_from_model: freq_scale    = 1
0.00.698.710 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.698.712 I ggml_metal_init: allocating
0.00.698.815 I ggml_metal_init: found device: Apple M4
0.00.698.828 I ggml_metal_init: picking default device: Apple M4
0.00.700.412 I ggml_metal_init: using embedded metal library
0.00.707.178 I ggml_metal_init: GPU name:   Apple M4
0.00.707.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.184 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.185 I ggml_metal_init: simdgroup reduction   = true
0.00.707.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.185 I ggml_metal_init: has residency sets    = true
0.00.707.186 I ggml_metal_init: has bfloat            = true
0.00.707.186 I ggml_metal_init: use bfloat            = true
0.00.707.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.728.556 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.728.560 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.728.587 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.838 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.731.840 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.731.840 I llama_init_from_model: graph nodes  = 967
0.00.731.840 I llama_init_from_model: graph splits = 2
0.00.731.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.731.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.771 I 
0.00.760.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.869 I perplexity: tokenizing the input ..
0.00.768.393 I perplexity: tokenization took 7.52 ms
0.00.768.407 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.335 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.906.676 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.906.699 I llama_perf_context_print:        load time =     752.14 ms
0.00.906.699 I llama_perf_context_print: prompt eval time =     136.05 ms /   128 tokens (    1.06 ms per token,   940.86 tokens per second)
0.00.906.700 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.701 I llama_perf_context_print:       total time =     145.93 ms /   129 tokens
0.00.907.059 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.094s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.054 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.590 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.596 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.183 I llama_model_loader: - type  f32:  194 tensors
0.00.025.184 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.184 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.185 I print_info: file format = GGUF V3 (latest)
0.00.025.185 I print_info: file type   = Q5_0
0.00.025.187 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.374 I load: special tokens cache size = 25
0.00.039.813 I load: token to piece cache size = 0.2984 MB
0.00.039.831 I print_info: arch             = gptneox
0.00.039.832 I print_info: vocab_only       = 0
0.00.039.832 I print_info: n_ctx_train      = 2048
0.00.039.832 I print_info: n_embd           = 2048
0.00.039.833 I print_info: n_layer          = 24
0.00.039.837 I print_info: n_head           = 16
0.00.039.837 I print_info: n_head_kv        = 16
0.00.039.839 I print_info: n_rot            = 32
0.00.039.839 I print_info: n_swa            = 0
0.00.039.839 I print_info: n_embd_head_k    = 128
0.00.039.839 I print_info: n_embd_head_v    = 128
0.00.039.840 I print_info: n_gqa            = 1
0.00.039.840 I print_info: n_embd_k_gqa     = 2048
0.00.039.845 I print_info: n_embd_v_gqa     = 2048
0.00.039.846 I print_info: f_norm_eps       = 1.0e-05
0.00.039.846 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.846 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.846 I print_info: f_logit_scale    = 0.0e+00
0.00.039.847 I print_info: n_ff             = 8192
0.00.039.847 I print_info: n_expert         = 0
0.00.039.847 I print_info: n_expert_used    = 0
0.00.039.847 I print_info: causal attn      = 1
0.00.039.847 I print_info: pooling type     = 0
0.00.039.847 I print_info: rope type        = 2
0.00.039.848 I print_info: rope scaling     = linear
0.00.039.848 I print_info: freq_base_train  = 10000.0
0.00.039.848 I print_info: freq_scale_train = 1
0.00.039.848 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.849 I print_info: rope_finetuned   = unknown
0.00.039.849 I print_info: ssm_d_conv       = 0
0.00.039.849 I print_info: ssm_d_inner      = 0
0.00.039.849 I print_info: ssm_d_state      = 0
0.00.039.849 I print_info: ssm_dt_rank      = 0
0.00.039.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.849 I print_info: model type       = 1.4B
0.00.039.850 I print_info: model params     = 1.41 B
0.00.039.850 I print_info: general.name     = 1.4B
0.00.039.850 I print_info: vocab type       = BPE
0.00.039.851 I print_info: n_vocab          = 50304
0.00.039.851 I print_info: n_merges         = 50009
0.00.039.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: LF token         = 187 'Ċ'
0.00.039.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.852 I print_info: max token length = 1024
0.00.039.853 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.383 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.401 I load_tensors: offloading output layer to GPU
0.00.686.401 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.439 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.686.441 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.688.137 I llama_init_from_model: n_seq_max     = 1
0.00.688.140 I llama_init_from_model: n_ctx         = 128
0.00.688.141 I llama_init_from_model: n_ctx_per_seq = 128
0.00.688.141 I llama_init_from_model: n_batch       = 128
0.00.688.142 I llama_init_from_model: n_ubatch      = 128
0.00.688.142 I llama_init_from_model: flash_attn    = 0
0.00.688.144 I llama_init_from_model: freq_base     = 10000.0
0.00.688.145 I llama_init_from_model: freq_scale    = 1
0.00.688.145 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.688.148 I ggml_metal_init: allocating
0.00.688.275 I ggml_metal_init: found device: Apple M4
0.00.688.288 I ggml_metal_init: picking default device: Apple M4
0.00.689.912 I ggml_metal_init: using embedded metal library
0.00.696.622 I ggml_metal_init: GPU name:   Apple M4
0.00.696.628 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.630 I ggml_metal_init: simdgroup reduction   = true
0.00.696.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.631 I ggml_metal_init: has residency sets    = true
0.00.696.631 I ggml_metal_init: has bfloat            = true
0.00.696.631 I ggml_metal_init: use bfloat            = true
0.00.696.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.301 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.859 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.717.864 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.717.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.012 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.014 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.015 I llama_init_from_model: graph nodes  = 967
0.00.721.015 I llama_init_from_model: graph splits = 2
0.00.721.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.523 I 
0.00.753.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.630 I perplexity: tokenizing the input ..
0.00.760.798 I perplexity: tokenization took 7.164 ms
0.00.760.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.627 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.911.973 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.911.999 I llama_perf_context_print:        load time =     744.46 ms
0.00.912.000 I llama_perf_context_print: prompt eval time =     148.89 ms /   128 tokens (    1.16 ms per token,   859.67 tokens per second)
0.00.912.000 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.001 I llama_perf_context_print:       total time =     158.48 ms /   129 tokens
0.00.912.381 I ggml_metal_free: deallocating

real	0m0.926s
user	0m0.080s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.417 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.239 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.056 I llama_model_loader: - type  f32:  194 tensors
0.00.026.056 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.056 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.057 I print_info: file format = GGUF V3 (latest)
0.00.026.058 I print_info: file type   = Q5_1
0.00.026.063 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.742 I load: special tokens cache size = 25
0.00.041.240 I load: token to piece cache size = 0.2984 MB
0.00.041.258 I print_info: arch             = gptneox
0.00.041.259 I print_info: vocab_only       = 0
0.00.041.259 I print_info: n_ctx_train      = 2048
0.00.041.260 I print_info: n_embd           = 2048
0.00.041.260 I print_info: n_layer          = 24
0.00.041.264 I print_info: n_head           = 16
0.00.041.264 I print_info: n_head_kv        = 16
0.00.041.265 I print_info: n_rot            = 32
0.00.041.265 I print_info: n_swa            = 0
0.00.041.265 I print_info: n_embd_head_k    = 128
0.00.041.265 I print_info: n_embd_head_v    = 128
0.00.041.266 I print_info: n_gqa            = 1
0.00.041.266 I print_info: n_embd_k_gqa     = 2048
0.00.041.267 I print_info: n_embd_v_gqa     = 2048
0.00.041.267 I print_info: f_norm_eps       = 1.0e-05
0.00.041.268 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.268 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.268 I print_info: f_logit_scale    = 0.0e+00
0.00.041.271 I print_info: n_ff             = 8192
0.00.041.272 I print_info: n_expert         = 0
0.00.041.272 I print_info: n_expert_used    = 0
0.00.041.272 I print_info: causal attn      = 1
0.00.041.272 I print_info: pooling type     = 0
0.00.041.272 I print_info: rope type        = 2
0.00.041.272 I print_info: rope scaling     = linear
0.00.041.273 I print_info: freq_base_train  = 10000.0
0.00.041.273 I print_info: freq_scale_train = 1
0.00.041.273 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.274 I print_info: rope_finetuned   = unknown
0.00.041.274 I print_info: ssm_d_conv       = 0
0.00.041.274 I print_info: ssm_d_inner      = 0
0.00.041.274 I print_info: ssm_d_state      = 0
0.00.041.274 I print_info: ssm_dt_rank      = 0
0.00.041.274 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.274 I print_info: model type       = 1.4B
0.00.041.275 I print_info: model params     = 1.41 B
0.00.041.275 I print_info: general.name     = 1.4B
0.00.041.275 I print_info: vocab type       = BPE
0.00.041.276 I print_info: n_vocab          = 50304
0.00.041.276 I print_info: n_merges         = 50009
0.00.041.276 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.277 I print_info: LF token         = 187 'Ċ'
0.00.041.277 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.277 I print_info: max token length = 1024
0.00.041.277 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.661.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.760 I load_tensors: offloading output layer to GPU
0.00.661.760 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.830 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.661.833 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.663.485 I llama_init_from_model: n_seq_max     = 1
0.00.663.487 I llama_init_from_model: n_ctx         = 128
0.00.663.487 I llama_init_from_model: n_ctx_per_seq = 128
0.00.663.488 I llama_init_from_model: n_batch       = 128
0.00.663.488 I llama_init_from_model: n_ubatch      = 128
0.00.663.489 I llama_init_from_model: flash_attn    = 0
0.00.663.490 I llama_init_from_model: freq_base     = 10000.0
0.00.663.491 I llama_init_from_model: freq_scale    = 1
0.00.663.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.663.493 I ggml_metal_init: allocating
0.00.663.548 I ggml_metal_init: found device: Apple M4
0.00.663.561 I ggml_metal_init: picking default device: Apple M4
0.00.664.866 I ggml_metal_init: using embedded metal library
0.00.671.097 I ggml_metal_init: GPU name:   Apple M4
0.00.671.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.104 I ggml_metal_init: simdgroup reduction   = true
0.00.671.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.104 I ggml_metal_init: has residency sets    = true
0.00.671.104 I ggml_metal_init: has bfloat            = true
0.00.671.105 I ggml_metal_init: use bfloat            = true
0.00.671.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.028 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.691.497 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.691.560 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.650 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.694.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.694.652 I llama_init_from_model: graph nodes  = 967
0.00.694.653 I llama_init_from_model: graph splits = 2
0.00.694.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.694.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.274 I 
0.00.726.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.382 I perplexity: tokenizing the input ..
0.00.733.143 I perplexity: tokenization took 6.76 ms
0.00.733.150 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.875.083 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.876.499 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.876.525 I llama_perf_context_print:        load time =     716.24 ms
0.00.876.526 I llama_perf_context_print: prompt eval time =     141.38 ms /   128 tokens (    1.10 ms per token,   905.34 tokens per second)
0.00.876.527 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.527 I llama_perf_context_print:       total time =     150.25 ms /   129 tokens
0.00.876.882 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.078s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.134 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.291 I llama_model_loader: - type  f32:  194 tensors
0.00.025.291 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.292 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.292 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.293 I print_info: file format = GGUF V3 (latest)
0.00.025.296 I print_info: file type   = Q2_K - Medium
0.00.025.297 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.979 I load: special tokens cache size = 25
0.00.040.355 I load: token to piece cache size = 0.2984 MB
0.00.040.367 I print_info: arch             = gptneox
0.00.040.368 I print_info: vocab_only       = 0
0.00.040.368 I print_info: n_ctx_train      = 2048
0.00.040.369 I print_info: n_embd           = 2048
0.00.040.369 I print_info: n_layer          = 24
0.00.040.373 I print_info: n_head           = 16
0.00.040.374 I print_info: n_head_kv        = 16
0.00.040.374 I print_info: n_rot            = 32
0.00.040.374 I print_info: n_swa            = 0
0.00.040.374 I print_info: n_embd_head_k    = 128
0.00.040.374 I print_info: n_embd_head_v    = 128
0.00.040.375 I print_info: n_gqa            = 1
0.00.040.376 I print_info: n_embd_k_gqa     = 2048
0.00.040.376 I print_info: n_embd_v_gqa     = 2048
0.00.040.377 I print_info: f_norm_eps       = 1.0e-05
0.00.040.377 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.377 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.377 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.379 I print_info: f_logit_scale    = 0.0e+00
0.00.040.379 I print_info: n_ff             = 8192
0.00.040.380 I print_info: n_expert         = 0
0.00.040.380 I print_info: n_expert_used    = 0
0.00.040.380 I print_info: causal attn      = 1
0.00.040.380 I print_info: pooling type     = 0
0.00.040.380 I print_info: rope type        = 2
0.00.040.380 I print_info: rope scaling     = linear
0.00.040.381 I print_info: freq_base_train  = 10000.0
0.00.040.381 I print_info: freq_scale_train = 1
0.00.040.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.381 I print_info: rope_finetuned   = unknown
0.00.040.381 I print_info: ssm_d_conv       = 0
0.00.040.381 I print_info: ssm_d_inner      = 0
0.00.040.381 I print_info: ssm_d_state      = 0
0.00.040.382 I print_info: ssm_dt_rank      = 0
0.00.040.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.382 I print_info: model type       = 1.4B
0.00.040.382 I print_info: model params     = 1.41 B
0.00.040.382 I print_info: general.name     = 1.4B
0.00.040.383 I print_info: vocab type       = BPE
0.00.040.383 I print_info: n_vocab          = 50304
0.00.040.383 I print_info: n_merges         = 50009
0.00.040.384 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.384 I print_info: LF token         = 187 'Ċ'
0.00.040.385 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.385 I print_info: max token length = 1024
0.00.040.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.422.503 I load_tensors: offloading 24 repeating layers to GPU
0.00.422.512 I load_tensors: offloading output layer to GPU
0.00.422.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.422.545 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.422.548 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.424.101 I llama_init_from_model: n_seq_max     = 1
0.00.424.104 I llama_init_from_model: n_ctx         = 128
0.00.424.105 I llama_init_from_model: n_ctx_per_seq = 128
0.00.424.105 I llama_init_from_model: n_batch       = 128
0.00.424.105 I llama_init_from_model: n_ubatch      = 128
0.00.424.105 I llama_init_from_model: flash_attn    = 0
0.00.424.107 I llama_init_from_model: freq_base     = 10000.0
0.00.424.108 I llama_init_from_model: freq_scale    = 1
0.00.424.108 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.424.110 I ggml_metal_init: allocating
0.00.424.179 I ggml_metal_init: found device: Apple M4
0.00.424.192 I ggml_metal_init: picking default device: Apple M4
0.00.425.972 I ggml_metal_init: using embedded metal library
0.00.431.706 I ggml_metal_init: GPU name:   Apple M4
0.00.431.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.431.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.431.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.431.720 I ggml_metal_init: simdgroup reduction   = true
0.00.431.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.431.721 I ggml_metal_init: has residency sets    = true
0.00.431.721 I ggml_metal_init: has bfloat            = true
0.00.431.721 I ggml_metal_init: use bfloat            = true
0.00.431.723 I ggml_metal_init: hasUnifiedMemory      = true
0.00.431.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.454.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.458.218 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.458.226 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.458.264 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.461.607 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.461.610 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.461.610 I llama_init_from_model: graph nodes  = 967
0.00.461.611 I llama_init_from_model: graph splits = 2
0.00.461.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.461.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.896 I 
0.00.491.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.994 I perplexity: tokenizing the input ..
0.00.498.381 I perplexity: tokenization took 6.385 ms
0.00.498.386 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.724 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.632.051 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.632.075 I llama_perf_context_print:        load time =     482.75 ms
0.00.632.077 I llama_perf_context_print: prompt eval time =     131.47 ms /   128 tokens (    1.03 ms per token,   973.57 tokens per second)
0.00.632.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.078 I llama_perf_context_print:       total time =     140.18 ms /   129 tokens
0.00.632.451 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.082s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.130 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.954 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.967 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.968 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.971 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.602 I llama_model_loader: - type  f32:  194 tensors
0.00.024.603 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.603 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.603 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.603 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.604 I print_info: file format = GGUF V3 (latest)
0.00.024.604 I print_info: file type   = Q3_K - Medium
0.00.024.606 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.788 I load: special tokens cache size = 25
0.00.039.225 I load: token to piece cache size = 0.2984 MB
0.00.039.243 I print_info: arch             = gptneox
0.00.039.244 I print_info: vocab_only       = 0
0.00.039.244 I print_info: n_ctx_train      = 2048
0.00.039.244 I print_info: n_embd           = 2048
0.00.039.244 I print_info: n_layer          = 24
0.00.039.248 I print_info: n_head           = 16
0.00.039.249 I print_info: n_head_kv        = 16
0.00.039.249 I print_info: n_rot            = 32
0.00.039.249 I print_info: n_swa            = 0
0.00.039.252 I print_info: n_embd_head_k    = 128
0.00.039.252 I print_info: n_embd_head_v    = 128
0.00.039.253 I print_info: n_gqa            = 1
0.00.039.253 I print_info: n_embd_k_gqa     = 2048
0.00.039.254 I print_info: n_embd_v_gqa     = 2048
0.00.039.255 I print_info: f_norm_eps       = 1.0e-05
0.00.039.255 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.255 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.256 I print_info: f_logit_scale    = 0.0e+00
0.00.039.256 I print_info: n_ff             = 8192
0.00.039.256 I print_info: n_expert         = 0
0.00.039.256 I print_info: n_expert_used    = 0
0.00.039.256 I print_info: causal attn      = 1
0.00.039.259 I print_info: pooling type     = 0
0.00.039.259 I print_info: rope type        = 2
0.00.039.259 I print_info: rope scaling     = linear
0.00.039.259 I print_info: freq_base_train  = 10000.0
0.00.039.260 I print_info: freq_scale_train = 1
0.00.039.260 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.260 I print_info: rope_finetuned   = unknown
0.00.039.260 I print_info: ssm_d_conv       = 0
0.00.039.260 I print_info: ssm_d_inner      = 0
0.00.039.260 I print_info: ssm_d_state      = 0
0.00.039.260 I print_info: ssm_dt_rank      = 0
0.00.039.261 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.261 I print_info: model type       = 1.4B
0.00.039.261 I print_info: model params     = 1.41 B
0.00.039.265 I print_info: general.name     = 1.4B
0.00.039.266 I print_info: vocab type       = BPE
0.00.039.266 I print_info: n_vocab          = 50304
0.00.039.266 I print_info: n_merges         = 50009
0.00.039.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: LF token         = 187 'Ċ'
0.00.039.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: max token length = 1024
0.00.039.272 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.031 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.050 I load_tensors: offloading output layer to GPU
0.00.444.051 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.087 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.088 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.736 I llama_init_from_model: n_seq_max     = 1
0.00.445.739 I llama_init_from_model: n_ctx         = 128
0.00.445.739 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.740 I llama_init_from_model: n_batch       = 128
0.00.445.740 I llama_init_from_model: n_ubatch      = 128
0.00.445.741 I llama_init_from_model: flash_attn    = 0
0.00.445.743 I llama_init_from_model: freq_base     = 10000.0
0.00.445.744 I llama_init_from_model: freq_scale    = 1
0.00.445.745 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.747 I ggml_metal_init: allocating
0.00.445.828 I ggml_metal_init: found device: Apple M4
0.00.445.842 I ggml_metal_init: picking default device: Apple M4
0.00.447.384 I ggml_metal_init: using embedded metal library
0.00.453.024 I ggml_metal_init: GPU name:   Apple M4
0.00.453.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.034 I ggml_metal_init: simdgroup reduction   = true
0.00.453.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.034 I ggml_metal_init: has residency sets    = true
0.00.453.035 I ggml_metal_init: has bfloat            = true
0.00.453.035 I ggml_metal_init: use bfloat            = true
0.00.453.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.040 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.556 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.603 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.479.965 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.479.967 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.479.967 I llama_init_from_model: graph nodes  = 967
0.00.479.967 I llama_init_from_model: graph splits = 2
0.00.479.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.092 I 
0.00.508.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.187 I perplexity: tokenizing the input ..
0.00.515.577 I perplexity: tokenization took 7.388 ms
0.00.515.591 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.660.612 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.661.952 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.661.977 I llama_perf_context_print:        load time =     498.95 ms
0.00.661.978 I llama_perf_context_print: prompt eval time =     144.16 ms /   128 tokens (    1.13 ms per token,   887.89 tokens per second)
0.00.661.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.661.979 I llama_perf_context_print:       total time =     153.89 ms /   129 tokens
0.00.662.347 I ggml_metal_free: deallocating

real	0m0.676s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.960 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.810 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.688 I llama_model_loader: - type  f32:  194 tensors
0.00.024.688 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.688 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.688 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.689 I print_info: file format = GGUF V3 (latest)
0.00.024.690 I print_info: file type   = Q4_K - Medium
0.00.024.691 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.128 I load: special tokens cache size = 25
0.00.039.446 I load: token to piece cache size = 0.2984 MB
0.00.039.464 I print_info: arch             = gptneox
0.00.039.465 I print_info: vocab_only       = 0
0.00.039.465 I print_info: n_ctx_train      = 2048
0.00.039.465 I print_info: n_embd           = 2048
0.00.039.465 I print_info: n_layer          = 24
0.00.039.469 I print_info: n_head           = 16
0.00.039.470 I print_info: n_head_kv        = 16
0.00.039.470 I print_info: n_rot            = 32
0.00.039.470 I print_info: n_swa            = 0
0.00.039.470 I print_info: n_embd_head_k    = 128
0.00.039.470 I print_info: n_embd_head_v    = 128
0.00.039.471 I print_info: n_gqa            = 1
0.00.039.471 I print_info: n_embd_k_gqa     = 2048
0.00.039.472 I print_info: n_embd_v_gqa     = 2048
0.00.039.473 I print_info: f_norm_eps       = 1.0e-05
0.00.039.473 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.473 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.473 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.473 I print_info: f_logit_scale    = 0.0e+00
0.00.039.474 I print_info: n_ff             = 8192
0.00.039.474 I print_info: n_expert         = 0
0.00.039.474 I print_info: n_expert_used    = 0
0.00.039.474 I print_info: causal attn      = 1
0.00.039.474 I print_info: pooling type     = 0
0.00.039.476 I print_info: rope type        = 2
0.00.039.476 I print_info: rope scaling     = linear
0.00.039.477 I print_info: freq_base_train  = 10000.0
0.00.039.477 I print_info: freq_scale_train = 1
0.00.039.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.482 I print_info: ssm_d_conv       = 0
0.00.039.482 I print_info: ssm_d_inner      = 0
0.00.039.482 I print_info: ssm_d_state      = 0
0.00.039.482 I print_info: ssm_dt_rank      = 0
0.00.039.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.483 I print_info: model params     = 1.41 B
0.00.039.483 I print_info: general.name     = 1.4B
0.00.039.484 I print_info: vocab type       = BPE
0.00.039.484 I print_info: n_vocab          = 50304
0.00.039.484 I print_info: n_merges         = 50009
0.00.039.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: LF token         = 187 'Ċ'
0.00.039.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: max token length = 1024
0.00.039.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.514.007 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.023 I load_tensors: offloading output layer to GPU
0.00.514.023 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.063 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.065 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.185 I llama_init_from_model: n_seq_max     = 1
0.00.515.188 I llama_init_from_model: n_ctx         = 128
0.00.515.189 I llama_init_from_model: n_ctx_per_seq = 128
0.00.515.189 I llama_init_from_model: n_batch       = 128
0.00.515.189 I llama_init_from_model: n_ubatch      = 128
0.00.515.190 I llama_init_from_model: flash_attn    = 0
0.00.515.192 I llama_init_from_model: freq_base     = 10000.0
0.00.515.192 I llama_init_from_model: freq_scale    = 1
0.00.515.193 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.515.195 I ggml_metal_init: allocating
0.00.515.290 I ggml_metal_init: found device: Apple M4
0.00.515.304 I ggml_metal_init: picking default device: Apple M4
0.00.516.882 I ggml_metal_init: using embedded metal library
0.00.523.773 I ggml_metal_init: GPU name:   Apple M4
0.00.523.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.783 I ggml_metal_init: simdgroup reduction   = true
0.00.523.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.783 I ggml_metal_init: has residency sets    = true
0.00.523.784 I ggml_metal_init: has bfloat            = true
0.00.523.784 I ggml_metal_init: use bfloat            = true
0.00.523.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.606 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.546.142 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.546.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.531 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.533 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.534 I llama_init_from_model: graph nodes  = 967
0.00.549.534 I llama_init_from_model: graph splits = 2
0.00.549.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.877 I 
0.00.578.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.973 I perplexity: tokenizing the input ..
0.00.586.315 I perplexity: tokenization took 7.339 ms
0.00.586.323 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.111 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.729.436 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.729.460 I llama_perf_context_print:        load time =     569.96 ms
0.00.729.461 I llama_perf_context_print: prompt eval time =     140.92 ms /   128 tokens (    1.10 ms per token,   908.29 tokens per second)
0.00.729.461 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.462 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.729.843 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.081s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.034 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.611 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.611 I llama_model_loader: - type  f32:  194 tensors
0.00.025.612 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.612 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.613 I print_info: file format = GGUF V3 (latest)
0.00.025.613 I print_info: file type   = Q5_K - Medium
0.00.025.614 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.240 I load: special tokens cache size = 25
0.00.040.826 I load: token to piece cache size = 0.2984 MB
0.00.040.843 I print_info: arch             = gptneox
0.00.040.844 I print_info: vocab_only       = 0
0.00.040.844 I print_info: n_ctx_train      = 2048
0.00.040.845 I print_info: n_embd           = 2048
0.00.040.845 I print_info: n_layer          = 24
0.00.040.849 I print_info: n_head           = 16
0.00.040.855 I print_info: n_head_kv        = 16
0.00.040.855 I print_info: n_rot            = 32
0.00.040.855 I print_info: n_swa            = 0
0.00.040.855 I print_info: n_embd_head_k    = 128
0.00.040.855 I print_info: n_embd_head_v    = 128
0.00.040.856 I print_info: n_gqa            = 1
0.00.040.857 I print_info: n_embd_k_gqa     = 2048
0.00.040.857 I print_info: n_embd_v_gqa     = 2048
0.00.040.858 I print_info: f_norm_eps       = 1.0e-05
0.00.040.858 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.859 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.859 I print_info: f_logit_scale    = 0.0e+00
0.00.040.859 I print_info: n_ff             = 8192
0.00.040.859 I print_info: n_expert         = 0
0.00.040.860 I print_info: n_expert_used    = 0
0.00.040.860 I print_info: causal attn      = 1
0.00.040.860 I print_info: pooling type     = 0
0.00.040.860 I print_info: rope type        = 2
0.00.040.860 I print_info: rope scaling     = linear
0.00.040.861 I print_info: freq_base_train  = 10000.0
0.00.040.861 I print_info: freq_scale_train = 1
0.00.040.861 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.861 I print_info: rope_finetuned   = unknown
0.00.040.861 I print_info: ssm_d_conv       = 0
0.00.040.861 I print_info: ssm_d_inner      = 0
0.00.040.862 I print_info: ssm_d_state      = 0
0.00.040.862 I print_info: ssm_dt_rank      = 0
0.00.040.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.863 I print_info: model type       = 1.4B
0.00.040.864 I print_info: model params     = 1.41 B
0.00.040.864 I print_info: general.name     = 1.4B
0.00.040.864 I print_info: vocab type       = BPE
0.00.040.864 I print_info: n_vocab          = 50304
0.00.040.864 I print_info: n_merges         = 50009
0.00.040.865 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.865 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.865 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.865 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.865 I print_info: LF token         = 187 'Ċ'
0.00.040.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.866 I print_info: max token length = 1024
0.00.040.866 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.618.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.005 I load_tensors: offloading output layer to GPU
0.00.619.006 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.032 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.619.036 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.620.620 I llama_init_from_model: n_seq_max     = 1
0.00.620.622 I llama_init_from_model: n_ctx         = 128
0.00.620.622 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.623 I llama_init_from_model: n_batch       = 128
0.00.620.623 I llama_init_from_model: n_ubatch      = 128
0.00.620.624 I llama_init_from_model: flash_attn    = 0
0.00.620.625 I llama_init_from_model: freq_base     = 10000.0
0.00.620.626 I llama_init_from_model: freq_scale    = 1
0.00.620.626 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.628 I ggml_metal_init: allocating
0.00.620.683 I ggml_metal_init: found device: Apple M4
0.00.620.694 I ggml_metal_init: picking default device: Apple M4
0.00.621.987 I ggml_metal_init: using embedded metal library
0.00.628.163 I ggml_metal_init: GPU name:   Apple M4
0.00.628.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.169 I ggml_metal_init: simdgroup reduction   = true
0.00.628.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.169 I ggml_metal_init: has residency sets    = true
0.00.628.169 I ggml_metal_init: has bfloat            = true
0.00.628.170 I ggml_metal_init: use bfloat            = true
0.00.628.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.726 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.285 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.290 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.438 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.652.597 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.652.599 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.652.600 I llama_init_from_model: graph nodes  = 967
0.00.652.600 I llama_init_from_model: graph splits = 2
0.00.652.603 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.652.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.474 I 
0.00.688.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.577 I perplexity: tokenizing the input ..
0.00.694.741 I perplexity: tokenization took 6.163 ms
0.00.694.747 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.725 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.832.069 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.832.094 I llama_perf_context_print:        load time =     678.43 ms
0.00.832.096 I llama_perf_context_print: prompt eval time =     135.75 ms /   128 tokens (    1.06 ms per token,   942.91 tokens per second)
0.00.832.097 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.097 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.832.463 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.078s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.566 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.568 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.573 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.573 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.413 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.320 I llama_model_loader: - type  f32:  194 tensors
0.00.024.320 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.321 I print_info: file format = GGUF V3 (latest)
0.00.024.322 I print_info: file type   = Q6_K
0.00.024.323 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.713 I load: special tokens cache size = 25
0.00.039.141 I load: token to piece cache size = 0.2984 MB
0.00.039.159 I print_info: arch             = gptneox
0.00.039.160 I print_info: vocab_only       = 0
0.00.039.160 I print_info: n_ctx_train      = 2048
0.00.039.160 I print_info: n_embd           = 2048
0.00.039.160 I print_info: n_layer          = 24
0.00.039.164 I print_info: n_head           = 16
0.00.039.165 I print_info: n_head_kv        = 16
0.00.039.165 I print_info: n_rot            = 32
0.00.039.165 I print_info: n_swa            = 0
0.00.039.169 I print_info: n_embd_head_k    = 128
0.00.039.169 I print_info: n_embd_head_v    = 128
0.00.039.170 I print_info: n_gqa            = 1
0.00.039.170 I print_info: n_embd_k_gqa     = 2048
0.00.039.171 I print_info: n_embd_v_gqa     = 2048
0.00.039.171 I print_info: f_norm_eps       = 1.0e-05
0.00.039.171 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.173 I print_info: f_logit_scale    = 0.0e+00
0.00.039.174 I print_info: n_ff             = 8192
0.00.039.174 I print_info: n_expert         = 0
0.00.039.174 I print_info: n_expert_used    = 0
0.00.039.174 I print_info: causal attn      = 1
0.00.039.174 I print_info: pooling type     = 0
0.00.039.175 I print_info: rope type        = 2
0.00.039.175 I print_info: rope scaling     = linear
0.00.039.176 I print_info: freq_base_train  = 10000.0
0.00.039.177 I print_info: freq_scale_train = 1
0.00.039.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.177 I print_info: rope_finetuned   = unknown
0.00.039.177 I print_info: ssm_d_conv       = 0
0.00.039.177 I print_info: ssm_d_inner      = 0
0.00.039.178 I print_info: ssm_d_state      = 0
0.00.039.178 I print_info: ssm_dt_rank      = 0
0.00.039.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.178 I print_info: model type       = 1.4B
0.00.039.178 I print_info: model params     = 1.41 B
0.00.039.178 I print_info: general.name     = 1.4B
0.00.039.179 I print_info: vocab type       = BPE
0.00.039.179 I print_info: n_vocab          = 50304
0.00.039.179 I print_info: n_merges         = 50009
0.00.039.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.181 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: LF token         = 187 'Ċ'
0.00.039.182 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: max token length = 1024
0.00.039.183 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.115 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.120 I load_tensors: offloading output layer to GPU
0.00.591.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.151 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.591.155 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.592.429 I llama_init_from_model: n_seq_max     = 1
0.00.592.431 I llama_init_from_model: n_ctx         = 128
0.00.592.432 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.432 I llama_init_from_model: n_batch       = 128
0.00.592.432 I llama_init_from_model: n_ubatch      = 128
0.00.592.433 I llama_init_from_model: flash_attn    = 0
0.00.592.434 I llama_init_from_model: freq_base     = 10000.0
0.00.592.435 I llama_init_from_model: freq_scale    = 1
0.00.592.435 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.437 I ggml_metal_init: allocating
0.00.592.508 I ggml_metal_init: found device: Apple M4
0.00.592.519 I ggml_metal_init: picking default device: Apple M4
0.00.593.779 I ggml_metal_init: using embedded metal library
0.00.599.862 I ggml_metal_init: GPU name:   Apple M4
0.00.599.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.868 I ggml_metal_init: simdgroup reduction   = true
0.00.599.868 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.868 I ggml_metal_init: has residency sets    = true
0.00.599.869 I ggml_metal_init: has bfloat            = true
0.00.599.869 I ggml_metal_init: use bfloat            = true
0.00.599.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.351 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.355 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.775 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.777 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.777 I llama_init_from_model: graph nodes  = 967
0.00.623.777 I llama_init_from_model: graph splits = 2
0.00.623.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.171 I 
0.00.659.258 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.265 I perplexity: tokenizing the input ..
0.00.666.385 I perplexity: tokenization took 7.116 ms
0.00.666.392 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.027 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.800.358 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.800.381 I llama_perf_context_print:        load time =     650.22 ms
0.00.800.383 I llama_perf_context_print: prompt eval time =     131.69 ms /   128 tokens (    1.03 ms per token,   972.02 tokens per second)
0.00.800.384 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.384 I llama_perf_context_print:       total time =     141.22 ms /   129 tokens
0.00.800.755 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.281 I build: 4866 (ab1c0b34) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.329 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.011 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.020 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.022 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.022 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.024 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.310 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.311 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.311 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.312 I llama_model_loader: - type  f32:  194 tensors
0.00.054.313 I llama_model_loader: - type  f16:   98 tensors
0.00.054.313 I print_info: file format = GGUF V3 (latest)
0.00.054.314 I print_info: file type   = all F32 (guessed)
0.00.054.315 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.961 I load: special tokens cache size = 25
0.00.073.978 I load: token to piece cache size = 0.2984 MB
0.00.073.993 I print_info: arch             = gptneox
0.00.073.994 I print_info: vocab_only       = 0
0.00.073.995 I print_info: n_ctx_train      = 2048
0.00.073.995 I print_info: n_embd           = 2048
0.00.073.995 I print_info: n_layer          = 24
0.00.073.998 I print_info: n_head           = 16
0.00.073.999 I print_info: n_head_kv        = 16
0.00.073.999 I print_info: n_rot            = 32
0.00.073.999 I print_info: n_swa            = 0
0.00.074.000 I print_info: n_embd_head_k    = 128
0.00.074.001 I print_info: n_embd_head_v    = 128
0.00.074.001 I print_info: n_gqa            = 1
0.00.074.002 I print_info: n_embd_k_gqa     = 2048
0.00.074.003 I print_info: n_embd_v_gqa     = 2048
0.00.074.003 I print_info: f_norm_eps       = 1.0e-05
0.00.074.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.012 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.013 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.013 I print_info: f_logit_scale    = 0.0e+00
0.00.074.021 I print_info: n_ff             = 8192
0.00.074.021 I print_info: n_expert         = 0
0.00.074.021 I print_info: n_expert_used    = 0
0.00.074.021 I print_info: causal attn      = 1
0.00.074.021 I print_info: pooling type     = 0
0.00.074.023 I print_info: rope type        = 2
0.00.074.023 I print_info: rope scaling     = linear
0.00.074.023 I print_info: freq_base_train  = 10000.0
0.00.074.024 I print_info: freq_scale_train = 1
0.00.074.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.026 I print_info: rope_finetuned   = unknown
0.00.074.026 I print_info: ssm_d_conv       = 0
0.00.074.026 I print_info: ssm_d_inner      = 0
0.00.074.026 I print_info: ssm_d_state      = 0
0.00.074.026 I print_info: ssm_dt_rank      = 0
0.00.074.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.028 I print_info: model type       = 1.4B
0.00.074.028 I print_info: model params     = 1.41 B
0.00.074.029 I print_info: general.name     = 1.4B
0.00.074.031 I print_info: vocab type       = BPE
0.00.074.031 I print_info: n_vocab          = 50304
0.00.074.031 I print_info: n_merges         = 50009
0.00.074.032 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.032 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.033 I print_info: LF token         = 187 'Ċ'
0.00.074.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.033 I print_info: max token length = 1024
0.00.074.034 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.327.562 I load_tensors: offloading 24 repeating layers to GPU
0.01.327.565 I load_tensors: offloading output layer to GPU
0.01.327.566 I load_tensors: offloaded 25/25 layers to GPU
0.01.327.591 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.327.592 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.328.642 I llama_init_from_model: n_seq_max     = 1
0.01.328.644 I llama_init_from_model: n_ctx         = 128
0.01.328.644 I llama_init_from_model: n_ctx_per_seq = 128
0.01.328.645 I llama_init_from_model: n_batch       = 128
0.01.328.645 I llama_init_from_model: n_ubatch      = 128
0.01.328.645 I llama_init_from_model: flash_attn    = 0
0.01.328.646 I llama_init_from_model: freq_base     = 10000.0
0.01.328.646 I llama_init_from_model: freq_scale    = 1
0.01.328.647 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.328.650 I ggml_metal_init: allocating
0.01.328.731 I ggml_metal_init: found device: Apple M4
0.01.328.743 I ggml_metal_init: picking default device: Apple M4
0.01.329.566 I ggml_metal_init: using embedded metal library
0.01.333.953 I ggml_metal_init: GPU name:   Apple M4
0.01.333.956 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.333.957 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.333.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.333.958 I ggml_metal_init: simdgroup reduction   = true
0.01.333.958 I ggml_metal_init: simdgroup matrix mul. = true
0.01.333.959 I ggml_metal_init: has residency sets    = true
0.01.333.959 I ggml_metal_init: has bfloat            = true
0.01.333.959 I ggml_metal_init: use bfloat            = true
0.01.333.960 I ggml_metal_init: hasUnifiedMemory      = true
0.01.333.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.346.543 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.348.306 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.348.309 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.348.326 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.350.029 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.350.031 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.350.031 I llama_init_from_model: graph nodes  = 967
0.01.350.031 I llama_init_from_model: graph splits = 2
0.01.350.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.350.033 I 
0.01.350.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.350.065 I compute_imatrix: tokenizing the input ..
0.01.354.491 I compute_imatrix: tokenization took 4.425 ms
0.01.354.493 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.622.678 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.625.062 I llama_perf_context_print:        load time =    1599.35 ms
0.01.625.063 I llama_perf_context_print: prompt eval time =     266.37 ms /   128 tokens (    2.08 ms per token,   480.54 tokens per second)
0.01.625.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.625.064 I llama_perf_context_print:       total time =    1601.73 ms /   129 tokens
0.01.625.536 I ggml_metal_free: deallocating

real	0m1.841s
user	0m0.130s
sys	0m0.256s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4866 (ab1c0b34)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15ca059a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15ca06010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15ca06480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15ca09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15ca095e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15ca09a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15ca0a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15ca0a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15ca0ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15ca0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15ca0b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15ca0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15ca0c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15ca0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15ca0d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15ca0dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15ca0e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15ca0eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15ca0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15ca0f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15ca100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15ca107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15ca10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15ca11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15ca11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15ca12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15ca127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15ca12e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15ca13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15ca137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15ca13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15ca14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15ca14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15ca148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15ca14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15ca15220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15ca156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15ca15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15ca16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15ca164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15ca16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15ca16de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15ca17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15ca17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15ca179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15ca17ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15ca18400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15ca18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15ca192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15ca19740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15ca19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15ca1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15ca1a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15ca1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15ca1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15ca1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15ca1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15ca1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15ca1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15ca1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15ca1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15ca1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15ca1d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15ca1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15ca1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15ca1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15ca1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15ca1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15ca1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15ca1f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15ca1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15ca1fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15ca20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15ca205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15ca20b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15ca21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15ca215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15ca21b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15ca22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15ca225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15ca22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15ca23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15ca23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15ca23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15ca24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15ca24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15ca24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15ca25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15ca25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15ca25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15ca26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15ca26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15ca26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15ca27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15ca27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15ca27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15ca27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15ca18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15ca28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15ca28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15ca29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15ca296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15ca29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15ca2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15ca2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15ca2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15ca2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15ca2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15ca2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15ca2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15ca2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15ca2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15ca2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15ca2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15ca2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15ca2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15ca2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15ca2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15ca2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15ca2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15ca2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15ca2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15ca2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15ca30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15ca30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15ca30e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15ca31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15ca31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15ca31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15ca32230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15ca32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15ca32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15ca33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15ca33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15ca33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15ca34030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15ca34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15ca34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15ca34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15ca35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15ca35930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15ca35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15ca36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15ca36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15ca36d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15ca37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15ca37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15ca37c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15ca38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15ca38630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15ca38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15ca39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15ca39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15ca39a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15ca39f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15ca3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15ca3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15ca3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15ca3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15ca3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15ca3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15ca3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15ca3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15ca3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15ca3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15ca3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15ca3db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15ca3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15ca3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15ca3ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15ca3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15ca3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15ca3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15ca3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15ca40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15ca40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15ca40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15ca41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15ca41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15ca41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15ca42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15ca42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15ca42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15ca43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15ca43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15ca43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15ca43f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15ca44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15ca44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15ca44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15ca45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15ca45830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15ca45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15ca46230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15ca467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15ca46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15ca47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15ca478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15ca47df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15ca482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15ca487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15ca48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15ca49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15ca49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15ca49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15ca4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15ca4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15ca4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15ca4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15ca4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15ca4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15ca4c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15ca4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15ca4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15ca4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15ca4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15ca4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15ca4e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15ca4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15ca4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15ca4f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15ca4f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15ca4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15ca504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15ca50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15ca51050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15ca51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15ca51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15ca52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15ca52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15ca52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15ca53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15ca53820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15ca53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15ca54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15ca54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15ca54ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15ca55490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15ca55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15ca55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15ca565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15ca56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15ca57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15ca576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15ca57c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15ca58210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15ca587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15ca58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15ca59320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15ca598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15ca59e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15ca5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15ca5a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15ca5af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15ca5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15ca5baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15ca5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15ca5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15ca5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15ca5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15ca5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15ca5dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15ca5e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15ca5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15ca5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15ca5f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15ca5f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15ca5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15ca60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15ca60930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15ca60e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15ca61330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15ca61830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15ca61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15ca62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15ca62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15ca62c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15ca63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15ca63630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15ca63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15ca64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15ca64530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15ca64a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15ca64f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15ca65430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15ca65930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15ca65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15ca66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15ca66830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15ca66d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15ca67230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15ca67730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15ca67c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15ca68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15ca68d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15ca69480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15ca69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15ca69e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15ca6a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15ca6aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15ca6af30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.723.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15cc04e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15cc052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15cc05730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15cc05ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15cc06010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15cc06480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15cc068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15cc06d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15cc071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15cc07640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15cc07ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15cc081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15cc08cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15cc09470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15cc09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15cc0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15cc0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15cc0b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15cc0b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15cc0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15cc0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15cc0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15cc0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15cc0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15cc0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15cc0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15cc0e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15cc0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15cc0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15cc0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15cc0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15cc10170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15cc10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15cc10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15cc11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15cc11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15cc11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15cc11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15cc12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15cc128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15cc12d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15cc131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15cc13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15cc13b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15cc13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15cc14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15cc14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15cc14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15cc15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15cc156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15cc15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15cc16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15cc164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15cc16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15cc16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15cc172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15cc17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15cc17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15cc17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15cc18140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15cc185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15cc18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15cc18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15cc19300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15cc19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15cc19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15cc1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15cc1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15cc1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15cc1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15cc1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15cc1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15cc1baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15cc1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15cc1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15cc1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15cc1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15cc1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15cc1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15cc1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15cc1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15cc1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15cc1e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15cc1ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15cc1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15cc1f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15cc1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15cc1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15cc201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15cc20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15cc20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15cc20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15cc213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15cc21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15cc21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15cc22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15cc22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15cc229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15cc22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15cc232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15cc23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15cc23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15cc24010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15cc24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15cc248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15cc24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15cc251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15cc25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15cc25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15cc25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15cc26390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15cc26800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15cc26c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15cc270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15cc27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15cc279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15cc27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15cc282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15cc28710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15cc28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15cc28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15cc29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15cc298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15cc29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15cc2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15cc2a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15cc2aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15cc2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15cc2b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15cc2b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15cc2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15cc2c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15cc2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15cc2c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15cc2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15cc2d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15cc2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15cc2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15cc2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15cc2e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15cc2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15cc2ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15cc2f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15cc2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15cc2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15cc2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15cc30350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15cc307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15cc30c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15cc310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15cc31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15cc31980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15cc31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15cc32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15cc326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15cc32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15cc32fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15cc33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15cc33890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15cc33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15cc34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15cc345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15cc34a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15cc34ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15cc35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15cc357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15cc35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15cc361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15cc36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15cc36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15cc36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15cc373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15cc37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15cc37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15cc380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15cc38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15cc389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15cc38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15cc392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15cc39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15cc39b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15cc3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15cc3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15cc3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15cc3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15cc3b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15cc3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15cc3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15cc3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15cc3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15cc3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15cc3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15cc3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15cc3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15cc3d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15cc3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15cc3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15cc3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15cc3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15cc3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15cc3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15cc3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15cc40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15cc40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15cc40900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15cc40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15cc414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15cc41980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15cc41e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15cc422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15cc42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15cc42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15cc43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15cc43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15cc43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15cc44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15cc44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15cc44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15cc455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15cc45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15cc46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15cc466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15cc46c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15cc47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15cc477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15cc47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15cc48320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15cc488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15cc48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15cc49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15cc499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15cc49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15cc4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15cc4aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15cc4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15cc4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15cc4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15cc4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15cc4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15cc4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15cc4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15cc4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15cc4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15cc4e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15cc4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15cc4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15cc4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15cc4fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15cc50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15cc505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15cc50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15cc51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15cc51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15cc51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15cc52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15cc52810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15cc52dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15cc53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15cc53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15cc53ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15cc54480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15cc54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15cc54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15cc55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15cc55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15cc560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15cc566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15cc56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15cc57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15cc57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15cc57b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15cc58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15cc58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15cc58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15cc58f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15cc59450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15cc59950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15cc59e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15cc5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15cc5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15cc5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15cc5b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15cc5b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15cc5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15cc5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15cc5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15cc5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15cc5d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15cc5d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15cc5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15cc5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15cc5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15cc5e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15cc5f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15cc5fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15cc601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15cc608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15cc60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15cc61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15cc617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15cc61c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15cb05610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15cb05a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15cb05ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15cb06360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15cb067d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15cb06c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15cb070b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15cb07520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15cb07990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15cb07e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15cb08270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15cb08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15cb09470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15cb09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15cb0a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15cb0ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15cb0b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15cb0b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15cb0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15cb0c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15cb0cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15cb0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15cb0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15cb0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15cb0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15cb0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15cb0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15cb0f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15cb0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15cb0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15cb104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15cb109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15cb110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15cb11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15cb119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15cb11e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15cb12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15cb127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15cb12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15cb13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15cb135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15cb13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15cb13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15cb14380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15cb14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15cb14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15cb15160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15cb15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15cb15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15cb15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15cb163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15cb16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15cb16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15cb171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15cb17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15cb17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15cb17fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15cb18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15cb18520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15cb18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15cb18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15cb19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15cb196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15cb19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15cb19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15cb1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15cb1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15cb1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15cb1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15cb1b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15cb1ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15cb1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15cb1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15cb1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15cb1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15cb1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15cb1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15cb1d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15cb1dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15cb1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15cb1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15cb1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15cb1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15cb1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15cb1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15cb1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15cb20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15cb205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15cb20a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15cb20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15cb21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15cb21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15cb21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15cb22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15cb224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15cb22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15cb22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15cb23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15cb23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15cb23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15cb243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15cb24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15cb24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15cb254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15cb25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15cb26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15cb26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15cb26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15cb27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15cb27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15cb27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15cb28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15cb28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15cb28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15cb292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15cb297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15cb29cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15cb2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15cb2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15cb2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15cb2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15cb2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15cb2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15cb2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15cb2c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15cb2c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15cb2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15cb2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15cb2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15cb2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15cb2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15cb2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15cb2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15cb2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15cb2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15cb2fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15cb300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15cb305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15cb30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15cb30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15cb314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15cb319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15cb31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15cb323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15cb328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15cb32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15cb332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15cb337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15cb33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15cb341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15cb346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15cb34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15cb350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15cb355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15cb35ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15cb35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15cb364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15cb369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15cb36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15cb373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15cb378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15cb37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15cb382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15cb387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15cb38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15cb391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15cb396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15cb39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15cb3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15cb3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15cb3aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15cb3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15cb3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15cb3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15cb3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15cb3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15cb3c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15cb3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15cb3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15cb3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15cb3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15cb3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15cb3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15cb3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15cb3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15cb3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15cb3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15cb3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15cb404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15cb409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15cb40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15cb413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15cb418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15cb41dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15cb42380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15cb42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15cb42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15cb43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15cb43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15cb43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15cb44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15cb44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15cb44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15cb451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15cb45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15cb45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15cb46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15cb46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15cb46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15cb47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15cb47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15cb47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15cb48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15cb487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15cb48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15cb49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15cb498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15cb49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15cb4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15cb4a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15cb4af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15cb4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15cb4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15cb4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15cb4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15cb4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15cb4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15cb4d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15cb4dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15cb4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15cb4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15cb4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15cb4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15cb4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15cb4ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15cb504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15cb50a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15cb51030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15cb515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15cb51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15cb52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15cb526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15cb52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15cb53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15cb53800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15cb53db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15cb54360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15cb54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15cb54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15cb55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15cb55a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15cb55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15cb56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15cb56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15cb570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15cb57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15cb57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15cb581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15cb587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15cb58d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15cb59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15cb598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15cb59e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15cb5a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15cb5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15cb5af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15cb5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15cb5bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15cb5bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15cb5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15cb5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15cb5ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15cb5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15cb5d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15cb5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15cb5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15cb5e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15cb5ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15cb5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15cb5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15cb5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15cb600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15cb605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15cb60ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15cb60fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15cb614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15cb619d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15cb61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15cb623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15cb628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15cb62dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15cb632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15cb637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15cb641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15cb64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15cb65020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15cb65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15cb65a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15cb66190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15cb66630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15cb66ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.805s
user	0m0.276s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4866 (ab1c0b34)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff0d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff0dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff0e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff0f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff10c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe08ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe095e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe09d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe0a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe0add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe0b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe0bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe0cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe0ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe0fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe12200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe14160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe15880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe16b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe18ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe1aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe1b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe21b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe22c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe23930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe46190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe46ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe4e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe4f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe4fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe50300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe52ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe5a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe5ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe5b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe5be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe5c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe5cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe5dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe5e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe5e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe5eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe5f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe5f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13fe60400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13fe60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13fe60e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13fe61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13fe61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13fe61d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13fe62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13fe62700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13fe62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13fe63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe63600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe64010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe64730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe64e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe65830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe65fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe66460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe66900 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.102.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff15170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff17b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff20dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff21700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff26a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff27380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff27820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff2a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff2c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff2d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff2dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff2f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff47e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff48330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff48f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff4c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff4c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff4cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff4da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff55b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff56970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff57480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff57fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff59c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff5ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff5be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff5c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe20cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe55560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe53340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe20710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe27800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe4ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe54450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe51120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe4cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe5a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe42470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe57d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe52d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe26ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe5aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe1fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13fe4e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13fe24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13fe266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13fe50b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13fe54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13fe4c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13fe56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13fe21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13fe1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13fe4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe21270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fe49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fe1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fe599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fe538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fe55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fe58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fe25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fe571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fe4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fe0c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fe0d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fe0d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fe23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fe44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fe66f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe67770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe67a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe67cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe67fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe68f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe694b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe69cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe69fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe6a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe6a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe6a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe6ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe6b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe6b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe6b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe6b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe6bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe6bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe6c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe6c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe6c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe6c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe6cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe6ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe6d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe6d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe6d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe6dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe6def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe6e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe6e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe6e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe6e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe6ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe6ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe6f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe6f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe6f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe6fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe6fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe6fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe702b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe70570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe70830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe70af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe70db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe71070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe71330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe718b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe71b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe71e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe720f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe723b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe72670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe72930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe72bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe72eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe73170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe73430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe736f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe739b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe73c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe73f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe741f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe744b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe74770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe74a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe74cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe74fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe75270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe75530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe757f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe75ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe75d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe76030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe762f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe765b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe76870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe76b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe76df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe770b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe77370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe77630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe778f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe77bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe77e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe78130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe783f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe786b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe78970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe78c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe78ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe791b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe79470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe79730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe799f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe79cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe79f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe7a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe7a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe7a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe7aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe7ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe7aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe7b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe7b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe7b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe7baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe7bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe7c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe7c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe7c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe7c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe7cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe7ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe7d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe7d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe7d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe7d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe7dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe7deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe7e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe7e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe7e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe7e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe7ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe7ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe7f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe7f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe7f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe7fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe7fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe7ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe80270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe80530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe807f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe80ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe80d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe81030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe81430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe816f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe819b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe81e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe82290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe82700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe82b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe82fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe83450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe83d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe84610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe84a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe84ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe85360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe85c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe86520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe86990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe86e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe87270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe876e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe87b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe87fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe88430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe88d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe89180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe89a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe89ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe8a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe8a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe8ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe8b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe8b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe8b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe8be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe8c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe8c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe8cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe8d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe8d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe8db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe8e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe8ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe8ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe8f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe8fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe90040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe90600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe90bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe91180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe91740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe91d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe922c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe92880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe92e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe93400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe939c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe93f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe94540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe94b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe95680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe95c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe96200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe967c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe96d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe97340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe97900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe97ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe98480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe98a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe99000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe995c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe99b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe9a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe9a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe9acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe9b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe9b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe9be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe9c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe9c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe9cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe9d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe9dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe9e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe9e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe9ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe9f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe9f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe9fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fea0300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fea08c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fea0e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fea1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fea1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fea1fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fea2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fea2b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fea3100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fea3600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fea3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fea4000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fea4500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fea4a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fea4f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fea5400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fea5900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fea5e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fea6300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fea6800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fea6d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fea7200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fea7700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13fea7c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13fea8100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13fea8600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13fea8b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13fea9000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13fea9500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13fea9a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13fea9f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13feaa400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13feaa900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13feaae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13feab810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13feabf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13feac650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13feacd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fead030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fead7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13feada80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13feadf90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.966s
user	0m0.237s
sys	0m0.202s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
