Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.471s
user	0m0.909s
sys	0m1.202s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Built target build_info
[  4%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-sampling
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-cli
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-passkey
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Built target llama-quantize
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-tts
[ 91%] Built target llama-gen-docs
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.189s
user	0m6.899s
sys	0m11.440s

main: quantize time =  3777.47 ms
main:    total time =  3777.47 ms

main: quantize time =  1677.77 ms
main:    total time =  1677.77 ms

main: quantize time =  1891.78 ms
main:    total time =  1891.78 ms

main: quantize time =  2229.67 ms
main:    total time =  2229.67 ms

main: quantize time =  1906.30 ms
main:    total time =  1906.30 ms

main: quantize time =  5394.95 ms
main:    total time =  5394.95 ms

main: quantize time =  5809.38 ms
main:    total time =  5809.38 ms

main: quantize time =  6999.11 ms
main:    total time =  6999.11 ms

main: quantize time =  6129.96 ms
main:    total time =  6129.96 ms

main: quantize time =  4399.42 ms
main:    total time =  4399.42 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.211 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.356 I main: llama backend init
0.00.000.363 I main: load the model and apply lora adapter, if any
0.00.029.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.263 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.379 I llama_model_loader: - type  f32:  194 tensors
0.00.060.379 I llama_model_loader: - type  f16:   98 tensors
0.00.060.380 I print_info: file format = GGUF V3 (latest)
0.00.060.381 I print_info: file type   = all F32 (guessed)
0.00.060.382 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.071.563 I load: special tokens cache size = 25
0.00.078.858 I load: token to piece cache size = 0.2984 MB
0.00.078.861 I print_info: arch             = gptneox
0.00.078.861 I print_info: vocab_only       = 0
0.00.078.861 I print_info: n_ctx_train      = 2048
0.00.078.862 I print_info: n_embd           = 2048
0.00.078.862 I print_info: n_layer          = 24
0.00.078.865 I print_info: n_head           = 16
0.00.078.865 I print_info: n_head_kv        = 16
0.00.078.866 I print_info: n_rot            = 32
0.00.078.866 I print_info: n_swa            = 0
0.00.078.868 I print_info: n_embd_head_k    = 128
0.00.078.868 I print_info: n_embd_head_v    = 128
0.00.078.868 I print_info: n_gqa            = 1
0.00.078.869 I print_info: n_embd_k_gqa     = 2048
0.00.078.870 I print_info: n_embd_v_gqa     = 2048
0.00.078.870 I print_info: f_norm_eps       = 1.0e-05
0.00.078.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.871 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.871 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.871 I print_info: f_logit_scale    = 0.0e+00
0.00.078.872 I print_info: n_ff             = 8192
0.00.078.872 I print_info: n_expert         = 0
0.00.078.872 I print_info: n_expert_used    = 0
0.00.078.872 I print_info: causal attn      = 1
0.00.078.872 I print_info: pooling type     = 0
0.00.078.872 I print_info: rope type        = 2
0.00.078.873 I print_info: rope scaling     = linear
0.00.078.873 I print_info: freq_base_train  = 10000.0
0.00.078.874 I print_info: freq_scale_train = 1
0.00.078.874 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.878 I print_info: rope_finetuned   = unknown
0.00.078.878 I print_info: ssm_d_conv       = 0
0.00.078.878 I print_info: ssm_d_inner      = 0
0.00.078.878 I print_info: ssm_d_state      = 0
0.00.078.878 I print_info: ssm_dt_rank      = 0
0.00.078.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.879 I print_info: model type       = 1.4B
0.00.078.879 I print_info: model params     = 1.41 B
0.00.078.880 I print_info: general.name     = 1.4B
0.00.078.880 I print_info: vocab type       = BPE
0.00.078.880 I print_info: n_vocab          = 50304
0.00.078.880 I print_info: n_merges         = 50009
0.00.078.881 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.881 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.881 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.881 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.881 I print_info: LF token         = 187 'Ċ'
0.00.078.882 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.882 I print_info: max token length = 1024
0.00.078.882 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.111.189 I load_tensors: offloading 24 repeating layers to GPU
0.00.111.192 I load_tensors: offloading output layer to GPU
0.00.111.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.111.214 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.111.216 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.111.581 I llama_init_from_model: n_seq_max     = 1
0.00.111.582 I llama_init_from_model: n_ctx         = 2048
0.00.111.582 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.111.582 I llama_init_from_model: n_batch       = 2048
0.00.111.582 I llama_init_from_model: n_ubatch      = 512
0.00.111.582 I llama_init_from_model: flash_attn    = 0
0.00.111.583 I llama_init_from_model: freq_base     = 10000.0
0.00.111.583 I llama_init_from_model: freq_scale    = 1
0.00.111.584 I ggml_metal_init: allocating
0.00.111.610 I ggml_metal_init: found device: Apple M4
0.00.111.616 I ggml_metal_init: picking default device: Apple M4
0.00.112.203 I ggml_metal_init: using embedded metal library
0.00.121.158 I ggml_metal_init: GPU name:   Apple M4
0.00.121.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.161 I ggml_metal_init: simdgroup reduction   = true
0.00.121.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.161 I ggml_metal_init: has residency sets    = true
0.00.121.161 I ggml_metal_init: has bfloat            = true
0.00.121.161 I ggml_metal_init: use bfloat            = true
0.00.121.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.149.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.179.226 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.179.233 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.179.256 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.182.681 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.182.683 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.182.683 I llama_init_from_model: graph nodes  = 967
0.00.182.683 I llama_init_from_model: graph splits = 2
0.00.182.687 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.182.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.182.816 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.854 I main: llama threadpool init, n_threads = 4
0.00.239.897 I 
0.00.239.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.917 I 
0.00.239.965 I sampler seed: 1234
0.00.239.969 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.995 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.995 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.047.288 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.02.047.288 I llama_perf_context_print:        load time =     209.05 ms
0.02.047.289 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.83 tokens per second)
0.02.047.290 I llama_perf_context_print:        eval time =    1760.92 ms /    63 runs   (   27.95 ms per token,    35.78 tokens per second)
0.02.047.290 I llama_perf_context_print:       total time =    1808.29 ms /    70 tokens
0.02.047.518 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.130s
sys	0m0.120s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.033.894 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.896 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.896 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.897 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.897 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.897 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.898 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.898 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.899 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.899 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.899 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.900 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.900 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.042.773 I llama_model_loader: - type  f32:  194 tensors
0.00.042.773 I llama_model_loader: - type q8_0:   98 tensors
0.00.042.774 I print_info: file format = GGUF V3 (latest)
0.00.042.774 I print_info: file type   = Q8_0
0.00.042.775 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.989 I load: special tokens cache size = 25
0.00.059.275 I load: token to piece cache size = 0.2984 MB
0.00.059.281 I print_info: arch             = gptneox
0.00.059.281 I print_info: vocab_only       = 0
0.00.059.282 I print_info: n_ctx_train      = 2048
0.00.059.282 I print_info: n_embd           = 2048
0.00.059.282 I print_info: n_layer          = 24
0.00.059.289 I print_info: n_head           = 16
0.00.059.294 I print_info: n_head_kv        = 16
0.00.059.294 I print_info: n_rot            = 32
0.00.059.295 I print_info: n_swa            = 0
0.00.059.295 I print_info: n_embd_head_k    = 128
0.00.059.295 I print_info: n_embd_head_v    = 128
0.00.059.296 I print_info: n_gqa            = 1
0.00.059.297 I print_info: n_embd_k_gqa     = 2048
0.00.059.298 I print_info: n_embd_v_gqa     = 2048
0.00.059.298 I print_info: f_norm_eps       = 1.0e-05
0.00.059.299 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.299 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.301 I print_info: f_logit_scale    = 0.0e+00
0.00.059.301 I print_info: n_ff             = 8192
0.00.059.302 I print_info: n_expert         = 0
0.00.059.303 I print_info: n_expert_used    = 0
0.00.059.303 I print_info: causal attn      = 1
0.00.059.303 I print_info: pooling type     = 0
0.00.059.304 I print_info: rope type        = 2
0.00.059.304 I print_info: rope scaling     = linear
0.00.059.304 I print_info: freq_base_train  = 10000.0
0.00.059.305 I print_info: freq_scale_train = 1
0.00.059.305 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.305 I print_info: rope_finetuned   = unknown
0.00.059.305 I print_info: ssm_d_conv       = 0
0.00.059.305 I print_info: ssm_d_inner      = 0
0.00.059.306 I print_info: ssm_d_state      = 0
0.00.059.307 I print_info: ssm_dt_rank      = 0
0.00.059.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.308 I print_info: model type       = 1.4B
0.00.059.308 I print_info: model params     = 1.41 B
0.00.059.308 I print_info: general.name     = 1.4B
0.00.059.309 I print_info: vocab type       = BPE
0.00.059.309 I print_info: n_vocab          = 50304
0.00.059.309 I print_info: n_merges         = 50009
0.00.059.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.310 I print_info: LF token         = 187 'Ċ'
0.00.059.311 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.311 I print_info: max token length = 1024
0.00.059.311 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.221.519 I load_tensors: offloading 24 repeating layers to GPU
0.01.221.524 I load_tensors: offloading output layer to GPU
0.01.221.525 I load_tensors: offloaded 25/25 layers to GPU
0.01.221.545 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.221.546 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.222.309 I llama_init_from_model: n_seq_max     = 1
0.01.222.310 I llama_init_from_model: n_ctx         = 2048
0.01.222.311 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.222.311 I llama_init_from_model: n_batch       = 2048
0.01.222.312 I llama_init_from_model: n_ubatch      = 512
0.01.222.312 I llama_init_from_model: flash_attn    = 0
0.01.222.313 I llama_init_from_model: freq_base     = 10000.0
0.01.222.313 I llama_init_from_model: freq_scale    = 1
0.01.222.314 I ggml_metal_init: allocating
0.01.222.322 I ggml_metal_init: found device: Apple M4
0.01.222.329 I ggml_metal_init: picking default device: Apple M4
0.01.223.579 I ggml_metal_init: using embedded metal library
0.01.228.751 I ggml_metal_init: GPU name:   Apple M4
0.01.228.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.228.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.228.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.228.756 I ggml_metal_init: simdgroup reduction   = true
0.01.228.756 I ggml_metal_init: simdgroup matrix mul. = true
0.01.228.757 I ggml_metal_init: has residency sets    = true
0.01.228.757 I ggml_metal_init: has bfloat            = true
0.01.228.757 I ggml_metal_init: use bfloat            = true
0.01.228.758 I ggml_metal_init: hasUnifiedMemory      = true
0.01.228.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.244.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.295.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.295.650 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.295.673 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.300.649 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.300.651 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.300.651 I llama_init_from_model: graph nodes  = 967
0.01.300.652 I llama_init_from_model: graph splits = 2
0.01.300.656 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.300.783 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.300.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.350.009 I main: llama threadpool init, n_threads = 4
0.01.350.053 I 
0.01.350.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.350.070 I 
0.01.350.196 I sampler seed: 1234
0.01.350.200 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.350.235 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.350.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.350.236 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.450.614 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.02.450.616 I llama_perf_context_print:        load time =    1339.41 ms
0.02.450.617 I llama_perf_context_print: prompt eval time =      50.16 ms /     7 tokens (    7.17 ms per token,   139.56 tokens per second)
0.02.450.617 I llama_perf_context_print:        eval time =    1047.14 ms /    63 runs   (   16.62 ms per token,    60.16 tokens per second)
0.02.450.618 I llama_perf_context_print:       total time =    1101.31 ms /    70 tokens
0.02.450.861 I ggml_metal_free: deallocating

real	0m2.483s
user	0m0.110s
sys	0m0.313s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.096 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.138 I main: llama backend init
0.00.000.141 I main: load the model and apply lora adapter, if any
0.00.018.501 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.525 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.537 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.538 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.539 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.541 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.544 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.544 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.545 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.486 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.487 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.489 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.058.490 I llama_model_loader: - type  f32:  194 tensors
0.00.058.494 I llama_model_loader: - type q4_0:   97 tensors
0.00.058.495 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.497 I print_info: file format = GGUF V3 (latest)
0.00.058.498 I print_info: file type   = Q4_0
0.00.058.500 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.077.360 I load: special tokens cache size = 25
0.00.088.584 I load: token to piece cache size = 0.2984 MB
0.00.088.588 I print_info: arch             = gptneox
0.00.088.589 I print_info: vocab_only       = 0
0.00.088.589 I print_info: n_ctx_train      = 2048
0.00.088.589 I print_info: n_embd           = 2048
0.00.088.590 I print_info: n_layer          = 24
0.00.088.593 I print_info: n_head           = 16
0.00.088.594 I print_info: n_head_kv        = 16
0.00.088.595 I print_info: n_rot            = 32
0.00.088.595 I print_info: n_swa            = 0
0.00.088.595 I print_info: n_embd_head_k    = 128
0.00.088.595 I print_info: n_embd_head_v    = 128
0.00.088.596 I print_info: n_gqa            = 1
0.00.088.597 I print_info: n_embd_k_gqa     = 2048
0.00.088.602 I print_info: n_embd_v_gqa     = 2048
0.00.088.602 I print_info: f_norm_eps       = 1.0e-05
0.00.088.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.603 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.604 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.604 I print_info: f_logit_scale    = 0.0e+00
0.00.088.605 I print_info: n_ff             = 8192
0.00.088.605 I print_info: n_expert         = 0
0.00.088.605 I print_info: n_expert_used    = 0
0.00.088.605 I print_info: causal attn      = 1
0.00.088.606 I print_info: pooling type     = 0
0.00.088.606 I print_info: rope type        = 2
0.00.088.606 I print_info: rope scaling     = linear
0.00.088.607 I print_info: freq_base_train  = 10000.0
0.00.088.607 I print_info: freq_scale_train = 1
0.00.088.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.608 I print_info: rope_finetuned   = unknown
0.00.088.608 I print_info: ssm_d_conv       = 0
0.00.088.608 I print_info: ssm_d_inner      = 0
0.00.088.610 I print_info: ssm_d_state      = 0
0.00.088.610 I print_info: ssm_dt_rank      = 0
0.00.088.611 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.611 I print_info: model type       = 1.4B
0.00.088.611 I print_info: model params     = 1.41 B
0.00.088.612 I print_info: general.name     = 1.4B
0.00.088.612 I print_info: vocab type       = BPE
0.00.088.612 I print_info: n_vocab          = 50304
0.00.088.613 I print_info: n_merges         = 50009
0.00.088.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.613 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.613 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.614 I print_info: LF token         = 187 'Ċ'
0.00.088.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.615 I print_info: max token length = 1024
0.00.088.620 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.736.980 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.988 I load_tensors: offloading output layer to GPU
0.00.736.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.737.019 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.737.021 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.738.310 I llama_init_from_model: n_seq_max     = 1
0.00.738.312 I llama_init_from_model: n_ctx         = 2048
0.00.738.313 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.738.314 I llama_init_from_model: n_batch       = 2048
0.00.738.315 I llama_init_from_model: n_ubatch      = 512
0.00.738.315 I llama_init_from_model: flash_attn    = 0
0.00.738.317 I llama_init_from_model: freq_base     = 10000.0
0.00.738.317 I llama_init_from_model: freq_scale    = 1
0.00.738.319 I ggml_metal_init: allocating
0.00.738.379 I ggml_metal_init: found device: Apple M4
0.00.738.392 I ggml_metal_init: picking default device: Apple M4
0.00.740.235 I ggml_metal_init: using embedded metal library
0.00.746.837 I ggml_metal_init: GPU name:   Apple M4
0.00.746.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.843 I ggml_metal_init: simdgroup reduction   = true
0.00.746.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.844 I ggml_metal_init: has residency sets    = true
0.00.746.844 I ggml_metal_init: has bfloat            = true
0.00.746.844 I ggml_metal_init: use bfloat            = true
0.00.746.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.764.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.812.601 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.812.607 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.812.636 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.359 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.817.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.817.362 I llama_init_from_model: graph nodes  = 967
0.00.817.362 I llama_init_from_model: graph splits = 2
0.00.817.368 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.817.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.817.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.865.939 I main: llama threadpool init, n_threads = 4
0.00.865.984 I 
0.00.866.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.866.003 I 
0.00.866.117 I sampler seed: 1234
0.00.866.121 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.866.131 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.866.131 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.866.131 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.555.902 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.555.903 I llama_perf_context_print:        load time =     846.72 ms
0.01.555.905 I llama_perf_context_print: prompt eval time =      49.74 ms /     7 tokens (    7.11 ms per token,   140.73 tokens per second)
0.01.555.905 I llama_perf_context_print:        eval time =     637.05 ms /    63 runs   (   10.11 ms per token,    98.89 tokens per second)
0.01.555.906 I llama_perf_context_print:       total time =     690.68 ms /    70 tokens
0.01.556.188 I ggml_metal_free: deallocating

real	0m1.589s
user	0m0.141s
sys	0m0.236s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.503 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.505 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.505 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.506 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.508 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.700 I llama_model_loader: - type  f32:  194 tensors
0.00.038.701 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.701 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.702 I print_info: file format = GGUF V3 (latest)
0.00.038.702 I print_info: file type   = Q4_1
0.00.038.703 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.241 I load: special tokens cache size = 25
0.00.057.805 I load: token to piece cache size = 0.2984 MB
0.00.057.809 I print_info: arch             = gptneox
0.00.057.809 I print_info: vocab_only       = 0
0.00.057.809 I print_info: n_ctx_train      = 2048
0.00.057.809 I print_info: n_embd           = 2048
0.00.057.810 I print_info: n_layer          = 24
0.00.057.813 I print_info: n_head           = 16
0.00.057.814 I print_info: n_head_kv        = 16
0.00.057.814 I print_info: n_rot            = 32
0.00.057.814 I print_info: n_swa            = 0
0.00.057.815 I print_info: n_embd_head_k    = 128
0.00.057.815 I print_info: n_embd_head_v    = 128
0.00.057.816 I print_info: n_gqa            = 1
0.00.057.816 I print_info: n_embd_k_gqa     = 2048
0.00.057.817 I print_info: n_embd_v_gqa     = 2048
0.00.057.818 I print_info: f_norm_eps       = 1.0e-05
0.00.057.818 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.818 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.819 I print_info: f_logit_scale    = 0.0e+00
0.00.057.819 I print_info: n_ff             = 8192
0.00.057.820 I print_info: n_expert         = 0
0.00.057.820 I print_info: n_expert_used    = 0
0.00.057.820 I print_info: causal attn      = 1
0.00.057.820 I print_info: pooling type     = 0
0.00.057.821 I print_info: rope type        = 2
0.00.057.823 I print_info: rope scaling     = linear
0.00.057.823 I print_info: freq_base_train  = 10000.0
0.00.057.824 I print_info: freq_scale_train = 1
0.00.057.825 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.825 I print_info: rope_finetuned   = unknown
0.00.057.826 I print_info: ssm_d_conv       = 0
0.00.057.826 I print_info: ssm_d_inner      = 0
0.00.057.826 I print_info: ssm_d_state      = 0
0.00.057.826 I print_info: ssm_dt_rank      = 0
0.00.057.826 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.827 I print_info: model type       = 1.4B
0.00.057.828 I print_info: model params     = 1.41 B
0.00.057.828 I print_info: general.name     = 1.4B
0.00.057.829 I print_info: vocab type       = BPE
0.00.057.829 I print_info: n_vocab          = 50304
0.00.057.829 I print_info: n_merges         = 50009
0.00.057.829 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.830 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.830 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.830 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.830 I print_info: LF token         = 187 'Ċ'
0.00.057.831 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.831 I print_info: max token length = 1024
0.00.057.832 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.870.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.870.366 I load_tensors: offloading output layer to GPU
0.00.870.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.870.420 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.870.423 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.871.661 I llama_init_from_model: n_seq_max     = 1
0.00.871.664 I llama_init_from_model: n_ctx         = 2048
0.00.871.665 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.871.665 I llama_init_from_model: n_batch       = 2048
0.00.871.666 I llama_init_from_model: n_ubatch      = 512
0.00.871.666 I llama_init_from_model: flash_attn    = 0
0.00.871.668 I llama_init_from_model: freq_base     = 10000.0
0.00.871.668 I llama_init_from_model: freq_scale    = 1
0.00.871.671 I ggml_metal_init: allocating
0.00.871.716 I ggml_metal_init: found device: Apple M4
0.00.871.729 I ggml_metal_init: picking default device: Apple M4
0.00.873.315 I ggml_metal_init: using embedded metal library
0.00.879.498 I ggml_metal_init: GPU name:   Apple M4
0.00.879.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.879.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.879.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.879.504 I ggml_metal_init: simdgroup reduction   = true
0.00.879.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.879.505 I ggml_metal_init: has residency sets    = true
0.00.879.505 I ggml_metal_init: has bfloat            = true
0.00.879.505 I ggml_metal_init: use bfloat            = true
0.00.879.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.879.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.897.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.952.833 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.952.840 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.952.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.957.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.957.854 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.957.854 I llama_init_from_model: graph nodes  = 967
0.00.957.854 I llama_init_from_model: graph splits = 2
0.00.957.859 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.957.999 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.958.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.004.144 I main: llama threadpool init, n_threads = 4
0.01.004.183 I 
0.01.004.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.004.202 I 
0.01.004.328 I sampler seed: 1234
0.01.004.332 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.004.350 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.004.350 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.004.350 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.737.069 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.737.070 I llama_perf_context_print:        load time =     990.58 ms
0.01.737.071 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.25 tokens per second)
0.01.737.071 I llama_perf_context_print:        eval time =     680.68 ms /    63 runs   (   10.80 ms per token,    92.55 tokens per second)
0.01.737.073 I llama_perf_context_print:       total time =     733.62 ms /    70 tokens
0.01.737.316 I ggml_metal_free: deallocating

real	0m1.762s
user	0m0.116s
sys	0m0.261s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.167 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.285 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.286 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.287 I print_info: file format = GGUF V3 (latest)
0.00.025.287 I print_info: file type   = Q5_0
0.00.025.288 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.712 I load: special tokens cache size = 25
0.00.039.958 I load: token to piece cache size = 0.2984 MB
0.00.039.960 I print_info: arch             = gptneox
0.00.039.961 I print_info: vocab_only       = 0
0.00.039.961 I print_info: n_ctx_train      = 2048
0.00.039.961 I print_info: n_embd           = 2048
0.00.039.961 I print_info: n_layer          = 24
0.00.039.964 I print_info: n_head           = 16
0.00.039.965 I print_info: n_head_kv        = 16
0.00.039.966 I print_info: n_rot            = 32
0.00.039.966 I print_info: n_swa            = 0
0.00.039.967 I print_info: n_embd_head_k    = 128
0.00.039.967 I print_info: n_embd_head_v    = 128
0.00.039.968 I print_info: n_gqa            = 1
0.00.039.968 I print_info: n_embd_k_gqa     = 2048
0.00.039.969 I print_info: n_embd_v_gqa     = 2048
0.00.039.970 I print_info: f_norm_eps       = 1.0e-05
0.00.039.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.971 I print_info: f_logit_scale    = 0.0e+00
0.00.039.972 I print_info: n_ff             = 8192
0.00.039.972 I print_info: n_expert         = 0
0.00.039.972 I print_info: n_expert_used    = 0
0.00.039.972 I print_info: causal attn      = 1
0.00.039.973 I print_info: pooling type     = 0
0.00.039.973 I print_info: rope type        = 2
0.00.039.973 I print_info: rope scaling     = linear
0.00.039.974 I print_info: freq_base_train  = 10000.0
0.00.039.974 I print_info: freq_scale_train = 1
0.00.039.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.974 I print_info: rope_finetuned   = unknown
0.00.039.974 I print_info: ssm_d_conv       = 0
0.00.039.975 I print_info: ssm_d_inner      = 0
0.00.039.975 I print_info: ssm_d_state      = 0
0.00.039.975 I print_info: ssm_dt_rank      = 0
0.00.039.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.975 I print_info: model type       = 1.4B
0.00.039.976 I print_info: model params     = 1.41 B
0.00.039.976 I print_info: general.name     = 1.4B
0.00.039.976 I print_info: vocab type       = BPE
0.00.039.977 I print_info: n_vocab          = 50304
0.00.039.977 I print_info: n_merges         = 50009
0.00.039.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: LF token         = 187 'Ċ'
0.00.039.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.980 I print_info: max token length = 1024
0.00.039.981 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.724.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.724.807 I load_tensors: offloading output layer to GPU
0.00.724.808 I load_tensors: offloaded 25/25 layers to GPU
0.00.724.826 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.724.828 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.726.072 I llama_init_from_model: n_seq_max     = 1
0.00.726.074 I llama_init_from_model: n_ctx         = 2048
0.00.726.074 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.726.075 I llama_init_from_model: n_batch       = 2048
0.00.726.076 I llama_init_from_model: n_ubatch      = 512
0.00.726.076 I llama_init_from_model: flash_attn    = 0
0.00.726.077 I llama_init_from_model: freq_base     = 10000.0
0.00.726.077 I llama_init_from_model: freq_scale    = 1
0.00.726.079 I ggml_metal_init: allocating
0.00.726.100 I ggml_metal_init: found device: Apple M4
0.00.726.108 I ggml_metal_init: picking default device: Apple M4
0.00.727.571 I ggml_metal_init: using embedded metal library
0.00.733.447 I ggml_metal_init: GPU name:   Apple M4
0.00.733.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.733.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.733.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.733.452 I ggml_metal_init: simdgroup reduction   = true
0.00.733.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.733.453 I ggml_metal_init: has residency sets    = true
0.00.733.453 I ggml_metal_init: has bfloat            = true
0.00.733.453 I ggml_metal_init: use bfloat            = true
0.00.733.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.733.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.750.067 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.806.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.806.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.806.409 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.811.773 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.811.775 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.811.775 I llama_init_from_model: graph nodes  = 967
0.00.811.776 I llama_init_from_model: graph splits = 2
0.00.811.786 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.811.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.811.942 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.689 I main: llama threadpool init, n_threads = 4
0.00.863.731 I 
0.00.863.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.749 I 
0.00.863.879 I sampler seed: 1234
0.00.863.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.893 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.894 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.658.760 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.658.761 I llama_perf_context_print:        load time =     853.81 ms
0.01.658.762 I llama_perf_context_print: prompt eval time =      53.57 ms /     7 tokens (    7.65 ms per token,   130.67 tokens per second)
0.01.658.762 I llama_perf_context_print:        eval time =     738.28 ms /    63 runs   (   11.72 ms per token,    85.33 tokens per second)
0.01.658.767 I llama_perf_context_print:       total time =     795.78 ms /    70 tokens
0.01.659.000 I ggml_metal_free: deallocating

real	0m1.678s
user	0m0.108s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.636 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.641 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.642 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.467 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.468 I llama_model_loader: - type  f32:  194 tensors
0.00.026.468 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.468 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.469 I print_info: file format = GGUF V3 (latest)
0.00.026.469 I print_info: file type   = Q5_1
0.00.026.470 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.586 I load: special tokens cache size = 25
0.00.040.674 I load: token to piece cache size = 0.2984 MB
0.00.040.676 I print_info: arch             = gptneox
0.00.040.676 I print_info: vocab_only       = 0
0.00.040.677 I print_info: n_ctx_train      = 2048
0.00.040.677 I print_info: n_embd           = 2048
0.00.040.677 I print_info: n_layer          = 24
0.00.040.680 I print_info: n_head           = 16
0.00.040.680 I print_info: n_head_kv        = 16
0.00.040.681 I print_info: n_rot            = 32
0.00.040.681 I print_info: n_swa            = 0
0.00.040.681 I print_info: n_embd_head_k    = 128
0.00.040.681 I print_info: n_embd_head_v    = 128
0.00.040.682 I print_info: n_gqa            = 1
0.00.040.683 I print_info: n_embd_k_gqa     = 2048
0.00.040.683 I print_info: n_embd_v_gqa     = 2048
0.00.040.686 I print_info: f_norm_eps       = 1.0e-05
0.00.040.686 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.686 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.687 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.687 I print_info: f_logit_scale    = 0.0e+00
0.00.040.687 I print_info: n_ff             = 8192
0.00.040.688 I print_info: n_expert         = 0
0.00.040.688 I print_info: n_expert_used    = 0
0.00.040.688 I print_info: causal attn      = 1
0.00.040.688 I print_info: pooling type     = 0
0.00.040.689 I print_info: rope type        = 2
0.00.040.689 I print_info: rope scaling     = linear
0.00.040.690 I print_info: freq_base_train  = 10000.0
0.00.040.690 I print_info: freq_scale_train = 1
0.00.040.690 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.690 I print_info: rope_finetuned   = unknown
0.00.040.691 I print_info: ssm_d_conv       = 0
0.00.040.691 I print_info: ssm_d_inner      = 0
0.00.040.691 I print_info: ssm_d_state      = 0
0.00.040.691 I print_info: ssm_dt_rank      = 0
0.00.040.691 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.696 I print_info: model type       = 1.4B
0.00.040.696 I print_info: model params     = 1.41 B
0.00.040.697 I print_info: general.name     = 1.4B
0.00.040.697 I print_info: vocab type       = BPE
0.00.040.697 I print_info: n_vocab          = 50304
0.00.040.697 I print_info: n_merges         = 50009
0.00.040.698 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.698 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.699 I print_info: LF token         = 187 'Ċ'
0.00.040.699 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: max token length = 1024
0.00.040.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.218 I load_tensors: offloading output layer to GPU
0.00.685.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.242 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.685.242 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.686.211 I llama_init_from_model: n_seq_max     = 1
0.00.686.212 I llama_init_from_model: n_ctx         = 2048
0.00.686.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.686.213 I llama_init_from_model: n_batch       = 2048
0.00.686.213 I llama_init_from_model: n_ubatch      = 512
0.00.686.214 I llama_init_from_model: flash_attn    = 0
0.00.686.214 I llama_init_from_model: freq_base     = 10000.0
0.00.686.215 I llama_init_from_model: freq_scale    = 1
0.00.686.216 I ggml_metal_init: allocating
0.00.686.227 I ggml_metal_init: found device: Apple M4
0.00.686.235 I ggml_metal_init: picking default device: Apple M4
0.00.687.547 I ggml_metal_init: using embedded metal library
0.00.693.285 I ggml_metal_init: GPU name:   Apple M4
0.00.693.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.290 I ggml_metal_init: simdgroup reduction   = true
0.00.693.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.290 I ggml_metal_init: has residency sets    = true
0.00.693.291 I ggml_metal_init: has bfloat            = true
0.00.693.291 I ggml_metal_init: use bfloat            = true
0.00.693.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.572 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.761.021 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.761.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.079 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.767.082 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.767.082 I llama_init_from_model: graph nodes  = 967
0.00.767.082 I llama_init_from_model: graph splits = 2
0.00.767.089 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.767.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.767.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.260 I main: llama threadpool init, n_threads = 4
0.00.818.307 I 
0.00.818.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.326 I 
0.00.818.439 I sampler seed: 1234
0.00.818.443 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.453 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.454 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.662.869 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.662.870 I llama_perf_context_print:        load time =     807.50 ms
0.01.662.870 I llama_perf_context_print: prompt eval time =      52.79 ms /     7 tokens (    7.54 ms per token,   132.60 tokens per second)
0.01.662.871 I llama_perf_context_print:        eval time =     788.62 ms /    63 runs   (   12.52 ms per token,    79.89 tokens per second)
0.01.662.872 I llama_perf_context_print:       total time =     845.35 ms /    70 tokens
0.01.663.124 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.107s
sys	0m0.258s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.435 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.436 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.128 I llama_model_loader: - type  f32:  194 tensors
0.00.024.128 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.128 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.129 I print_info: file format = GGUF V3 (latest)
0.00.024.129 I print_info: file type   = Q2_K - Medium
0.00.024.130 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.880 I load: special tokens cache size = 25
0.00.038.148 I load: token to piece cache size = 0.2984 MB
0.00.038.153 I print_info: arch             = gptneox
0.00.038.153 I print_info: vocab_only       = 0
0.00.038.153 I print_info: n_ctx_train      = 2048
0.00.038.154 I print_info: n_embd           = 2048
0.00.038.154 I print_info: n_layer          = 24
0.00.038.157 I print_info: n_head           = 16
0.00.038.158 I print_info: n_head_kv        = 16
0.00.038.158 I print_info: n_rot            = 32
0.00.038.158 I print_info: n_swa            = 0
0.00.038.158 I print_info: n_embd_head_k    = 128
0.00.038.158 I print_info: n_embd_head_v    = 128
0.00.038.162 I print_info: n_gqa            = 1
0.00.038.163 I print_info: n_embd_k_gqa     = 2048
0.00.038.163 I print_info: n_embd_v_gqa     = 2048
0.00.038.164 I print_info: f_norm_eps       = 1.0e-05
0.00.038.165 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.165 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.165 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.165 I print_info: f_logit_scale    = 0.0e+00
0.00.038.166 I print_info: n_ff             = 8192
0.00.038.166 I print_info: n_expert         = 0
0.00.038.166 I print_info: n_expert_used    = 0
0.00.038.166 I print_info: causal attn      = 1
0.00.038.166 I print_info: pooling type     = 0
0.00.038.167 I print_info: rope type        = 2
0.00.038.167 I print_info: rope scaling     = linear
0.00.038.167 I print_info: freq_base_train  = 10000.0
0.00.038.168 I print_info: freq_scale_train = 1
0.00.038.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.168 I print_info: rope_finetuned   = unknown
0.00.038.170 I print_info: ssm_d_conv       = 0
0.00.038.170 I print_info: ssm_d_inner      = 0
0.00.038.170 I print_info: ssm_d_state      = 0
0.00.038.170 I print_info: ssm_dt_rank      = 0
0.00.038.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.170 I print_info: model type       = 1.4B
0.00.038.171 I print_info: model params     = 1.41 B
0.00.038.171 I print_info: general.name     = 1.4B
0.00.038.171 I print_info: vocab type       = BPE
0.00.038.172 I print_info: n_vocab          = 50304
0.00.038.172 I print_info: n_merges         = 50009
0.00.038.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.173 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.173 I print_info: LF token         = 187 'Ċ'
0.00.038.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.173 I print_info: max token length = 1024
0.00.038.174 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.373.016 I load_tensors: offloading 24 repeating layers to GPU
0.00.373.027 I load_tensors: offloading output layer to GPU
0.00.373.028 I load_tensors: offloaded 25/25 layers to GPU
0.00.373.058 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.373.059 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.374.629 I llama_init_from_model: n_seq_max     = 1
0.00.374.632 I llama_init_from_model: n_ctx         = 2048
0.00.374.633 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.374.633 I llama_init_from_model: n_batch       = 2048
0.00.374.634 I llama_init_from_model: n_ubatch      = 512
0.00.374.634 I llama_init_from_model: flash_attn    = 0
0.00.374.636 I llama_init_from_model: freq_base     = 10000.0
0.00.374.637 I llama_init_from_model: freq_scale    = 1
0.00.374.639 I ggml_metal_init: allocating
0.00.374.691 I ggml_metal_init: found device: Apple M4
0.00.374.703 I ggml_metal_init: picking default device: Apple M4
0.00.376.568 I ggml_metal_init: using embedded metal library
0.00.383.280 I ggml_metal_init: GPU name:   Apple M4
0.00.383.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.286 I ggml_metal_init: simdgroup reduction   = true
0.00.383.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.287 I ggml_metal_init: has residency sets    = true
0.00.383.287 I ggml_metal_init: has bfloat            = true
0.00.383.288 I ggml_metal_init: use bfloat            = true
0.00.383.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.402.489 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.459.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.459.581 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.459.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.464.967 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.464.969 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.464.969 I llama_init_from_model: graph nodes  = 967
0.00.464.969 I llama_init_from_model: graph splits = 2
0.00.464.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.465.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.465.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.027 I main: llama threadpool init, n_threads = 4
0.00.519.076 I 
0.00.519.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.519.094 I 
0.00.519.240 I sampler seed: 1234
0.00.519.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.277 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.278 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.210.453 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.210.454 I llama_perf_context_print:        load time =     509.11 ms
0.01.210.454 I llama_perf_context_print: prompt eval time =      44.54 ms /     7 tokens (    6.36 ms per token,   157.17 tokens per second)
0.01.210.455 I llama_perf_context_print:        eval time =     643.66 ms /    63 runs   (   10.22 ms per token,    97.88 tokens per second)
0.01.210.455 I llama_perf_context_print:       total time =     692.43 ms /    70 tokens
0.01.210.724 I ggml_metal_free: deallocating

real	0m1.230s
user	0m0.111s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.672 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.785 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.216 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.216 I llama_model_loader: - type  f32:  194 tensors
0.00.025.216 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.217 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.217 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.217 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.217 I print_info: file format = GGUF V3 (latest)
0.00.025.218 I print_info: file type   = Q3_K - Medium
0.00.025.218 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.005 I load: special tokens cache size = 25
0.00.039.182 I load: token to piece cache size = 0.2984 MB
0.00.039.185 I print_info: arch             = gptneox
0.00.039.185 I print_info: vocab_only       = 0
0.00.039.185 I print_info: n_ctx_train      = 2048
0.00.039.185 I print_info: n_embd           = 2048
0.00.039.186 I print_info: n_layer          = 24
0.00.039.189 I print_info: n_head           = 16
0.00.039.190 I print_info: n_head_kv        = 16
0.00.039.190 I print_info: n_rot            = 32
0.00.039.191 I print_info: n_swa            = 0
0.00.039.191 I print_info: n_embd_head_k    = 128
0.00.039.191 I print_info: n_embd_head_v    = 128
0.00.039.192 I print_info: n_gqa            = 1
0.00.039.192 I print_info: n_embd_k_gqa     = 2048
0.00.039.193 I print_info: n_embd_v_gqa     = 2048
0.00.039.194 I print_info: f_norm_eps       = 1.0e-05
0.00.039.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.194 I print_info: f_logit_scale    = 0.0e+00
0.00.039.197 I print_info: n_ff             = 8192
0.00.039.197 I print_info: n_expert         = 0
0.00.039.197 I print_info: n_expert_used    = 0
0.00.039.198 I print_info: causal attn      = 1
0.00.039.200 I print_info: pooling type     = 0
0.00.039.200 I print_info: rope type        = 2
0.00.039.200 I print_info: rope scaling     = linear
0.00.039.201 I print_info: freq_base_train  = 10000.0
0.00.039.205 I print_info: freq_scale_train = 1
0.00.039.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.205 I print_info: rope_finetuned   = unknown
0.00.039.205 I print_info: ssm_d_conv       = 0
0.00.039.206 I print_info: ssm_d_inner      = 0
0.00.039.206 I print_info: ssm_d_state      = 0
0.00.039.206 I print_info: ssm_dt_rank      = 0
0.00.039.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.206 I print_info: model type       = 1.4B
0.00.039.206 I print_info: model params     = 1.41 B
0.00.039.207 I print_info: general.name     = 1.4B
0.00.039.208 I print_info: vocab type       = BPE
0.00.039.208 I print_info: n_vocab          = 50304
0.00.039.209 I print_info: n_merges         = 50009
0.00.039.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.209 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.209 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.209 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.209 I print_info: LF token         = 187 'Ċ'
0.00.039.210 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.210 I print_info: max token length = 1024
0.00.039.210 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.478.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.478.815 I load_tensors: offloading output layer to GPU
0.00.478.816 I load_tensors: offloaded 25/25 layers to GPU
0.00.478.844 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.478.845 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.480.211 I llama_init_from_model: n_seq_max     = 1
0.00.480.213 I llama_init_from_model: n_ctx         = 2048
0.00.480.215 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.480.215 I llama_init_from_model: n_batch       = 2048
0.00.480.215 I llama_init_from_model: n_ubatch      = 512
0.00.480.216 I llama_init_from_model: flash_attn    = 0
0.00.480.217 I llama_init_from_model: freq_base     = 10000.0
0.00.480.217 I llama_init_from_model: freq_scale    = 1
0.00.480.220 I ggml_metal_init: allocating
0.00.480.285 I ggml_metal_init: found device: Apple M4
0.00.480.300 I ggml_metal_init: picking default device: Apple M4
0.00.482.207 I ggml_metal_init: using embedded metal library
0.00.488.982 I ggml_metal_init: GPU name:   Apple M4
0.00.488.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.488.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.488.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.488.988 I ggml_metal_init: simdgroup reduction   = true
0.00.488.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.488.989 I ggml_metal_init: has residency sets    = true
0.00.488.989 I ggml_metal_init: has bfloat            = true
0.00.488.989 I ggml_metal_init: use bfloat            = true
0.00.488.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.488.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.507.107 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.581 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.558.587 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.558.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.922 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.563.924 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.563.925 I llama_init_from_model: graph nodes  = 967
0.00.563.925 I llama_init_from_model: graph splits = 2
0.00.563.930 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.564.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.564.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.438 I main: llama threadpool init, n_threads = 4
0.00.612.486 I 
0.00.612.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.503 I 
0.00.612.618 I sampler seed: 1234
0.00.612.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.632 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.634 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.356.949 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50318.92 tokens per second)
0.01.356.950 I llama_perf_context_print:        load time =     603.06 ms
0.01.356.951 I llama_perf_context_print: prompt eval time =      43.95 ms /     7 tokens (    6.28 ms per token,   159.29 tokens per second)
0.01.356.952 I llama_perf_context_print:        eval time =     697.32 ms /    63 runs   (   11.07 ms per token,    90.35 tokens per second)
0.01.356.952 I llama_perf_context_print:       total time =     745.22 ms /    70 tokens
0.01.357.212 I ggml_metal_free: deallocating

real	0m1.371s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.433 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.349 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.152 I llama_model_loader: - type  f32:  194 tensors
0.00.026.153 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.153 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.153 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.154 I print_info: file format = GGUF V3 (latest)
0.00.026.154 I print_info: file type   = Q4_K - Medium
0.00.026.159 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.319 I load: special tokens cache size = 25
0.00.040.520 I load: token to piece cache size = 0.2984 MB
0.00.040.523 I print_info: arch             = gptneox
0.00.040.523 I print_info: vocab_only       = 0
0.00.040.524 I print_info: n_ctx_train      = 2048
0.00.040.524 I print_info: n_embd           = 2048
0.00.040.524 I print_info: n_layer          = 24
0.00.040.527 I print_info: n_head           = 16
0.00.040.528 I print_info: n_head_kv        = 16
0.00.040.528 I print_info: n_rot            = 32
0.00.040.528 I print_info: n_swa            = 0
0.00.040.529 I print_info: n_embd_head_k    = 128
0.00.040.529 I print_info: n_embd_head_v    = 128
0.00.040.530 I print_info: n_gqa            = 1
0.00.040.530 I print_info: n_embd_k_gqa     = 2048
0.00.040.531 I print_info: n_embd_v_gqa     = 2048
0.00.040.532 I print_info: f_norm_eps       = 1.0e-05
0.00.040.532 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.532 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.533 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.533 I print_info: f_logit_scale    = 0.0e+00
0.00.040.533 I print_info: n_ff             = 8192
0.00.040.534 I print_info: n_expert         = 0
0.00.040.534 I print_info: n_expert_used    = 0
0.00.040.534 I print_info: causal attn      = 1
0.00.040.534 I print_info: pooling type     = 0
0.00.040.534 I print_info: rope type        = 2
0.00.040.536 I print_info: rope scaling     = linear
0.00.040.537 I print_info: freq_base_train  = 10000.0
0.00.040.537 I print_info: freq_scale_train = 1
0.00.040.537 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.537 I print_info: rope_finetuned   = unknown
0.00.040.538 I print_info: ssm_d_conv       = 0
0.00.040.538 I print_info: ssm_d_inner      = 0
0.00.040.538 I print_info: ssm_d_state      = 0
0.00.040.538 I print_info: ssm_dt_rank      = 0
0.00.040.538 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.539 I print_info: model type       = 1.4B
0.00.040.539 I print_info: model params     = 1.41 B
0.00.040.539 I print_info: general.name     = 1.4B
0.00.040.540 I print_info: vocab type       = BPE
0.00.040.540 I print_info: n_vocab          = 50304
0.00.040.541 I print_info: n_merges         = 50009
0.00.040.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.543 I print_info: LF token         = 187 'Ċ'
0.00.040.543 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.544 I print_info: max token length = 1024
0.00.040.544 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.424 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.429 I load_tensors: offloading output layer to GPU
0.00.592.430 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.453 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.592.454 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.593.570 I llama_init_from_model: n_seq_max     = 1
0.00.593.572 I llama_init_from_model: n_ctx         = 2048
0.00.593.573 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.573 I llama_init_from_model: n_batch       = 2048
0.00.593.573 I llama_init_from_model: n_ubatch      = 512
0.00.593.574 I llama_init_from_model: flash_attn    = 0
0.00.593.575 I llama_init_from_model: freq_base     = 10000.0
0.00.593.575 I llama_init_from_model: freq_scale    = 1
0.00.593.576 I ggml_metal_init: allocating
0.00.593.592 I ggml_metal_init: found device: Apple M4
0.00.593.602 I ggml_metal_init: picking default device: Apple M4
0.00.595.068 I ggml_metal_init: using embedded metal library
0.00.601.316 I ggml_metal_init: GPU name:   Apple M4
0.00.601.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.322 I ggml_metal_init: simdgroup reduction   = true
0.00.601.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.323 I ggml_metal_init: has residency sets    = true
0.00.601.323 I ggml_metal_init: has bfloat            = true
0.00.601.323 I ggml_metal_init: use bfloat            = true
0.00.601.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.701 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.403 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.427 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.238 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.240 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.240 I llama_init_from_model: graph nodes  = 967
0.00.676.240 I llama_init_from_model: graph splits = 2
0.00.676.247 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.709 I main: llama threadpool init, n_threads = 4
0.00.723.758 I 
0.00.723.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.774 I 
0.00.723.892 I sampler seed: 1234
0.00.723.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.931 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.934 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.934 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.477.655 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49894.59 tokens per second)
0.01.477.655 I llama_perf_context_print:        load time =     713.58 ms
0.01.477.656 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.56 tokens per second)
0.01.477.657 I llama_perf_context_print:        eval time =     703.47 ms /    63 runs   (   11.17 ms per token,    89.56 tokens per second)
0.01.477.658 I llama_perf_context_print:       total time =     754.64 ms /    70 tokens
0.01.477.903 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.109s
sys	0m0.243s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.681 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.185 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.818 I llama_model_loader: - type  f32:  194 tensors
0.00.024.818 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.819 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.819 I print_info: file format = GGUF V3 (latest)
0.00.024.820 I print_info: file type   = Q5_K - Medium
0.00.024.820 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.756 I load: special tokens cache size = 25
0.00.038.600 I load: token to piece cache size = 0.2984 MB
0.00.038.603 I print_info: arch             = gptneox
0.00.038.603 I print_info: vocab_only       = 0
0.00.038.603 I print_info: n_ctx_train      = 2048
0.00.038.603 I print_info: n_embd           = 2048
0.00.038.603 I print_info: n_layer          = 24
0.00.038.606 I print_info: n_head           = 16
0.00.038.606 I print_info: n_head_kv        = 16
0.00.038.606 I print_info: n_rot            = 32
0.00.038.607 I print_info: n_swa            = 0
0.00.038.607 I print_info: n_embd_head_k    = 128
0.00.038.607 I print_info: n_embd_head_v    = 128
0.00.038.608 I print_info: n_gqa            = 1
0.00.038.608 I print_info: n_embd_k_gqa     = 2048
0.00.038.609 I print_info: n_embd_v_gqa     = 2048
0.00.038.610 I print_info: f_norm_eps       = 1.0e-05
0.00.038.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.610 I print_info: f_logit_scale    = 0.0e+00
0.00.038.613 I print_info: n_ff             = 8192
0.00.038.613 I print_info: n_expert         = 0
0.00.038.613 I print_info: n_expert_used    = 0
0.00.038.613 I print_info: causal attn      = 1
0.00.038.613 I print_info: pooling type     = 0
0.00.038.613 I print_info: rope type        = 2
0.00.038.614 I print_info: rope scaling     = linear
0.00.038.614 I print_info: freq_base_train  = 10000.0
0.00.038.614 I print_info: freq_scale_train = 1
0.00.038.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.615 I print_info: rope_finetuned   = unknown
0.00.038.615 I print_info: ssm_d_conv       = 0
0.00.038.615 I print_info: ssm_d_inner      = 0
0.00.038.615 I print_info: ssm_d_state      = 0
0.00.038.615 I print_info: ssm_dt_rank      = 0
0.00.038.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.616 I print_info: model type       = 1.4B
0.00.038.616 I print_info: model params     = 1.41 B
0.00.038.616 I print_info: general.name     = 1.4B
0.00.038.616 I print_info: vocab type       = BPE
0.00.038.623 I print_info: n_vocab          = 50304
0.00.038.624 I print_info: n_merges         = 50009
0.00.038.624 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: LF token         = 187 'Ċ'
0.00.038.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: max token length = 1024
0.00.038.628 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.667.198 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.202 I load_tensors: offloading output layer to GPU
0.00.667.203 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.222 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.667.224 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.668.427 I llama_init_from_model: n_seq_max     = 1
0.00.668.429 I llama_init_from_model: n_ctx         = 2048
0.00.668.430 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.430 I llama_init_from_model: n_batch       = 2048
0.00.668.430 I llama_init_from_model: n_ubatch      = 512
0.00.668.431 I llama_init_from_model: flash_attn    = 0
0.00.668.431 I llama_init_from_model: freq_base     = 10000.0
0.00.668.432 I llama_init_from_model: freq_scale    = 1
0.00.668.433 I ggml_metal_init: allocating
0.00.668.452 I ggml_metal_init: found device: Apple M4
0.00.668.461 I ggml_metal_init: picking default device: Apple M4
0.00.669.847 I ggml_metal_init: using embedded metal library
0.00.675.677 I ggml_metal_init: GPU name:   Apple M4
0.00.675.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.682 I ggml_metal_init: simdgroup reduction   = true
0.00.675.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.683 I ggml_metal_init: has residency sets    = true
0.00.675.683 I ggml_metal_init: has bfloat            = true
0.00.675.683 I ggml_metal_init: use bfloat            = true
0.00.675.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.755 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.525 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.531 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.553 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.900 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.902 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.903 I llama_init_from_model: graph nodes  = 967
0.00.748.903 I llama_init_from_model: graph splits = 2
0.00.748.910 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.799 I main: llama threadpool init, n_threads = 4
0.00.801.840 I 
0.00.801.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.860 I 
0.00.801.993 I sampler seed: 1234
0.00.801.998 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.008 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.010 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.010 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.644.398 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.644.399 I llama_perf_context_print:        load time =     792.39 ms
0.01.644.400 I llama_perf_context_print: prompt eval time =      51.16 ms /     7 tokens (    7.31 ms per token,   136.83 tokens per second)
0.01.644.402 I llama_perf_context_print:        eval time =     788.35 ms /    63 runs   (   12.51 ms per token,    79.91 tokens per second)
0.01.644.403 I llama_perf_context_print:       total time =     843.33 ms /    70 tokens
0.01.644.653 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.106s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.663 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.375 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.387 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.392 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.406 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.171 I llama_model_loader: - type  f32:  194 tensors
0.00.026.172 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.172 I print_info: file format = GGUF V3 (latest)
0.00.026.172 I print_info: file type   = Q6_K
0.00.026.173 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.204 I load: special tokens cache size = 25
0.00.040.437 I load: token to piece cache size = 0.2984 MB
0.00.040.440 I print_info: arch             = gptneox
0.00.040.440 I print_info: vocab_only       = 0
0.00.040.440 I print_info: n_ctx_train      = 2048
0.00.040.441 I print_info: n_embd           = 2048
0.00.040.441 I print_info: n_layer          = 24
0.00.040.443 I print_info: n_head           = 16
0.00.040.444 I print_info: n_head_kv        = 16
0.00.040.444 I print_info: n_rot            = 32
0.00.040.449 I print_info: n_swa            = 0
0.00.040.449 I print_info: n_embd_head_k    = 128
0.00.040.449 I print_info: n_embd_head_v    = 128
0.00.040.450 I print_info: n_gqa            = 1
0.00.040.451 I print_info: n_embd_k_gqa     = 2048
0.00.040.453 I print_info: n_embd_v_gqa     = 2048
0.00.040.454 I print_info: f_norm_eps       = 1.0e-05
0.00.040.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.455 I print_info: f_logit_scale    = 0.0e+00
0.00.040.455 I print_info: n_ff             = 8192
0.00.040.463 I print_info: n_expert         = 0
0.00.040.465 I print_info: n_expert_used    = 0
0.00.040.465 I print_info: causal attn      = 1
0.00.040.465 I print_info: pooling type     = 0
0.00.040.465 I print_info: rope type        = 2
0.00.040.466 I print_info: rope scaling     = linear
0.00.040.467 I print_info: freq_base_train  = 10000.0
0.00.040.467 I print_info: freq_scale_train = 1
0.00.040.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.467 I print_info: rope_finetuned   = unknown
0.00.040.468 I print_info: ssm_d_conv       = 0
0.00.040.468 I print_info: ssm_d_inner      = 0
0.00.040.468 I print_info: ssm_d_state      = 0
0.00.040.468 I print_info: ssm_dt_rank      = 0
0.00.040.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.468 I print_info: model type       = 1.4B
0.00.040.469 I print_info: model params     = 1.41 B
0.00.040.469 I print_info: general.name     = 1.4B
0.00.040.469 I print_info: vocab type       = BPE
0.00.040.470 I print_info: n_vocab          = 50304
0.00.040.470 I print_info: n_merges         = 50009
0.00.040.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.472 I print_info: LF token         = 187 'Ċ'
0.00.040.472 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.472 I print_info: max token length = 1024
0.00.040.473 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.729.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.729.906 I load_tensors: offloading output layer to GPU
0.00.729.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.729.929 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.729.931 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.730.898 I llama_init_from_model: n_seq_max     = 1
0.00.730.899 I llama_init_from_model: n_ctx         = 2048
0.00.730.900 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.730.900 I llama_init_from_model: n_batch       = 2048
0.00.730.901 I llama_init_from_model: n_ubatch      = 512
0.00.730.901 I llama_init_from_model: flash_attn    = 0
0.00.730.902 I llama_init_from_model: freq_base     = 10000.0
0.00.730.902 I llama_init_from_model: freq_scale    = 1
0.00.730.903 I ggml_metal_init: allocating
0.00.730.916 I ggml_metal_init: found device: Apple M4
0.00.730.924 I ggml_metal_init: picking default device: Apple M4
0.00.732.170 I ggml_metal_init: using embedded metal library
0.00.737.706 I ggml_metal_init: GPU name:   Apple M4
0.00.737.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.714 I ggml_metal_init: simdgroup reduction   = true
0.00.737.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.714 I ggml_metal_init: has residency sets    = true
0.00.737.715 I ggml_metal_init: has bfloat            = true
0.00.737.715 I ggml_metal_init: use bfloat            = true
0.00.737.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.753.062 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.805.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.805.816 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.805.837 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.810.914 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.810.916 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.810.916 I llama_init_from_model: graph nodes  = 967
0.00.810.917 I llama_init_from_model: graph splits = 2
0.00.810.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.811.050 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.811.051 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.869.837 I main: llama threadpool init, n_threads = 4
0.00.869.894 I 
0.00.869.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.869.912 I 
0.00.870.043 I sampler seed: 1234
0.00.870.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.870.079 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.870.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.870.081 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.736.646 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.736.646 I llama_perf_context_print:        load time =     859.46 ms
0.01.736.647 I llama_perf_context_print: prompt eval time =      54.03 ms /     7 tokens (    7.72 ms per token,   129.55 tokens per second)
0.01.736.648 I llama_perf_context_print:        eval time =     809.60 ms /    63 runs   (   12.85 ms per token,    77.82 tokens per second)
0.01.736.648 I llama_perf_context_print:       total time =     867.52 ms /    70 tokens
0.01.736.908 I ggml_metal_free: deallocating

real	0m1.755s
user	0m0.105s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.641 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.584 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.599 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.563 I llama_model_loader: - type  f32:  194 tensors
0.00.056.563 I llama_model_loader: - type  f16:   98 tensors
0.00.056.564 I print_info: file format = GGUF V3 (latest)
0.00.056.565 I print_info: file type   = all F32 (guessed)
0.00.056.566 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.317 I load: special tokens cache size = 25
0.00.077.217 I load: token to piece cache size = 0.2984 MB
0.00.077.220 I print_info: arch             = gptneox
0.00.077.220 I print_info: vocab_only       = 0
0.00.077.221 I print_info: n_ctx_train      = 2048
0.00.077.221 I print_info: n_embd           = 2048
0.00.077.221 I print_info: n_layer          = 24
0.00.077.223 I print_info: n_head           = 16
0.00.077.224 I print_info: n_head_kv        = 16
0.00.077.224 I print_info: n_rot            = 32
0.00.077.225 I print_info: n_swa            = 0
0.00.077.225 I print_info: n_embd_head_k    = 128
0.00.077.225 I print_info: n_embd_head_v    = 128
0.00.077.226 I print_info: n_gqa            = 1
0.00.077.227 I print_info: n_embd_k_gqa     = 2048
0.00.077.227 I print_info: n_embd_v_gqa     = 2048
0.00.077.228 I print_info: f_norm_eps       = 1.0e-05
0.00.077.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.229 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.229 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.229 I print_info: f_logit_scale    = 0.0e+00
0.00.077.230 I print_info: n_ff             = 8192
0.00.077.230 I print_info: n_expert         = 0
0.00.077.230 I print_info: n_expert_used    = 0
0.00.077.230 I print_info: causal attn      = 1
0.00.077.230 I print_info: pooling type     = 0
0.00.077.230 I print_info: rope type        = 2
0.00.077.231 I print_info: rope scaling     = linear
0.00.077.232 I print_info: freq_base_train  = 10000.0
0.00.077.232 I print_info: freq_scale_train = 1
0.00.077.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.233 I print_info: rope_finetuned   = unknown
0.00.077.233 I print_info: ssm_d_conv       = 0
0.00.077.233 I print_info: ssm_d_inner      = 0
0.00.077.233 I print_info: ssm_d_state      = 0
0.00.077.235 I print_info: ssm_dt_rank      = 0
0.00.077.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.235 I print_info: model type       = 1.4B
0.00.077.235 I print_info: model params     = 1.41 B
0.00.077.235 I print_info: general.name     = 1.4B
0.00.077.236 I print_info: vocab type       = BPE
0.00.077.236 I print_info: n_vocab          = 50304
0.00.077.236 I print_info: n_merges         = 50009
0.00.077.237 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.237 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.237 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.237 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.237 I print_info: LF token         = 187 'Ċ'
0.00.077.241 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.241 I print_info: max token length = 1024
0.00.077.242 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.061.464 I load_tensors: offloading 24 repeating layers to GPU
0.01.061.468 I load_tensors: offloading output layer to GPU
0.01.061.468 I load_tensors: offloaded 25/25 layers to GPU
0.01.061.493 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.061.494 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.062.346 I llama_init_from_model: n_seq_max     = 1
0.01.062.347 I llama_init_from_model: n_ctx         = 128
0.01.062.347 I llama_init_from_model: n_ctx_per_seq = 128
0.01.062.347 I llama_init_from_model: n_batch       = 128
0.01.062.348 I llama_init_from_model: n_ubatch      = 128
0.01.062.348 I llama_init_from_model: flash_attn    = 0
0.01.062.348 I llama_init_from_model: freq_base     = 10000.0
0.01.062.349 I llama_init_from_model: freq_scale    = 1
0.01.062.349 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.062.350 I ggml_metal_init: allocating
0.01.062.403 I ggml_metal_init: found device: Apple M4
0.01.062.409 I ggml_metal_init: picking default device: Apple M4
0.01.063.404 I ggml_metal_init: using embedded metal library
0.01.066.916 I ggml_metal_init: GPU name:   Apple M4
0.01.066.918 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.066.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.066.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.066.919 I ggml_metal_init: simdgroup reduction   = true
0.01.066.919 I ggml_metal_init: simdgroup matrix mul. = true
0.01.066.920 I ggml_metal_init: has residency sets    = true
0.01.066.920 I ggml_metal_init: has bfloat            = true
0.01.066.920 I ggml_metal_init: use bfloat            = true
0.01.066.920 I ggml_metal_init: hasUnifiedMemory      = true
0.01.066.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.076.402 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.078.010 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.078.014 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.078.031 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.079.531 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.079.532 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.079.532 I llama_init_from_model: graph nodes  = 967
0.01.079.532 I llama_init_from_model: graph splits = 2
0.01.079.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.079.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.113.383 I 
0.01.113.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.113.432 I perplexity: tokenizing the input ..
0.01.117.930 I perplexity: tokenization took 4.497 ms
0.01.117.949 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.249.483 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.250.749 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.250.761 I llama_perf_context_print:        load time =    1090.61 ms
0.01.250.762 I llama_perf_context_print: prompt eval time =     131.28 ms /   128 tokens (    1.03 ms per token,   975.02 tokens per second)
0.01.250.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.250.762 I llama_perf_context_print:       total time =     137.38 ms /   129 tokens
0.01.251.074 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.101s
sys	0m0.281s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.646 I llama_model_loader: - type  f32:  194 tensors
0.00.026.647 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.647 I print_info: file format = GGUF V3 (latest)
0.00.026.648 I print_info: file type   = Q8_0
0.00.026.649 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.026 I load: special tokens cache size = 25
0.00.041.318 I load: token to piece cache size = 0.2984 MB
0.00.041.323 I print_info: arch             = gptneox
0.00.041.323 I print_info: vocab_only       = 0
0.00.041.323 I print_info: n_ctx_train      = 2048
0.00.041.324 I print_info: n_embd           = 2048
0.00.041.324 I print_info: n_layer          = 24
0.00.041.328 I print_info: n_head           = 16
0.00.041.329 I print_info: n_head_kv        = 16
0.00.041.329 I print_info: n_rot            = 32
0.00.041.329 I print_info: n_swa            = 0
0.00.041.330 I print_info: n_embd_head_k    = 128
0.00.041.330 I print_info: n_embd_head_v    = 128
0.00.041.330 I print_info: n_gqa            = 1
0.00.041.331 I print_info: n_embd_k_gqa     = 2048
0.00.041.332 I print_info: n_embd_v_gqa     = 2048
0.00.041.333 I print_info: f_norm_eps       = 1.0e-05
0.00.041.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.335 I print_info: f_logit_scale    = 0.0e+00
0.00.041.336 I print_info: n_ff             = 8192
0.00.041.336 I print_info: n_expert         = 0
0.00.041.336 I print_info: n_expert_used    = 0
0.00.041.336 I print_info: causal attn      = 1
0.00.041.336 I print_info: pooling type     = 0
0.00.041.336 I print_info: rope type        = 2
0.00.041.337 I print_info: rope scaling     = linear
0.00.041.337 I print_info: freq_base_train  = 10000.0
0.00.041.337 I print_info: freq_scale_train = 1
0.00.041.338 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.338 I print_info: rope_finetuned   = unknown
0.00.041.338 I print_info: ssm_d_conv       = 0
0.00.041.338 I print_info: ssm_d_inner      = 0
0.00.041.338 I print_info: ssm_d_state      = 0
0.00.041.338 I print_info: ssm_dt_rank      = 0
0.00.041.338 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.339 I print_info: model type       = 1.4B
0.00.041.339 I print_info: model params     = 1.41 B
0.00.041.340 I print_info: general.name     = 1.4B
0.00.041.340 I print_info: vocab type       = BPE
0.00.041.342 I print_info: n_vocab          = 50304
0.00.041.342 I print_info: n_merges         = 50009
0.00.041.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.343 I print_info: LF token         = 187 'Ċ'
0.00.041.343 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.343 I print_info: max token length = 1024
0.00.041.344 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.017.152 I load_tensors: offloading 24 repeating layers to GPU
0.01.017.159 I load_tensors: offloading output layer to GPU
0.01.017.160 I load_tensors: offloaded 25/25 layers to GPU
0.01.017.184 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.017.187 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.018.121 I llama_init_from_model: n_seq_max     = 1
0.01.018.122 I llama_init_from_model: n_ctx         = 128
0.01.018.123 I llama_init_from_model: n_ctx_per_seq = 128
0.01.018.123 I llama_init_from_model: n_batch       = 128
0.01.018.123 I llama_init_from_model: n_ubatch      = 128
0.01.018.124 I llama_init_from_model: flash_attn    = 0
0.01.018.124 I llama_init_from_model: freq_base     = 10000.0
0.01.018.125 I llama_init_from_model: freq_scale    = 1
0.01.018.125 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.018.126 I ggml_metal_init: allocating
0.01.018.165 I ggml_metal_init: found device: Apple M4
0.01.018.174 I ggml_metal_init: picking default device: Apple M4
0.01.019.252 I ggml_metal_init: using embedded metal library
0.01.023.707 I ggml_metal_init: GPU name:   Apple M4
0.01.023.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.023.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.023.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.023.711 I ggml_metal_init: simdgroup reduction   = true
0.01.023.712 I ggml_metal_init: simdgroup matrix mul. = true
0.01.023.712 I ggml_metal_init: has residency sets    = true
0.01.023.712 I ggml_metal_init: has bfloat            = true
0.01.023.712 I ggml_metal_init: use bfloat            = true
0.01.023.713 I ggml_metal_init: hasUnifiedMemory      = true
0.01.023.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.036.877 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.038.785 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.038.787 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.038.803 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.040.644 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.040.646 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.040.646 I llama_init_from_model: graph nodes  = 967
0.01.040.647 I llama_init_from_model: graph splits = 2
0.01.040.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.040.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.063.583 I 
0.01.063.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.063.617 I perplexity: tokenizing the input ..
0.01.068.711 I perplexity: tokenization took 5.092 ms
0.01.068.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.205.848 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.207.102 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.207.117 I llama_perf_context_print:        load time =    1053.27 ms
0.01.207.118 I llama_perf_context_print: prompt eval time =     136.90 ms /   128 tokens (    1.07 ms per token,   935.00 tokens per second)
0.01.207.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.207.119 I llama_perf_context_print:       total time =     143.53 ms /   129 tokens
0.01.207.494 I ggml_metal_free: deallocating

real	0m1.221s
user	0m0.072s
sys	0m0.244s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.832 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.835 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.835 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.836 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.837 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.839 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.725 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.726 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.727 I llama_model_loader: - type  f32:  194 tensors
0.00.025.727 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.728 I print_info: file format = GGUF V3 (latest)
0.00.025.729 I print_info: file type   = Q4_0
0.00.025.730 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.293 I load: special tokens cache size = 25
0.00.040.529 I load: token to piece cache size = 0.2984 MB
0.00.040.533 I print_info: arch             = gptneox
0.00.040.533 I print_info: vocab_only       = 0
0.00.040.533 I print_info: n_ctx_train      = 2048
0.00.040.534 I print_info: n_embd           = 2048
0.00.040.534 I print_info: n_layer          = 24
0.00.040.538 I print_info: n_head           = 16
0.00.040.539 I print_info: n_head_kv        = 16
0.00.040.539 I print_info: n_rot            = 32
0.00.040.539 I print_info: n_swa            = 0
0.00.040.540 I print_info: n_embd_head_k    = 128
0.00.040.543 I print_info: n_embd_head_v    = 128
0.00.040.543 I print_info: n_gqa            = 1
0.00.040.544 I print_info: n_embd_k_gqa     = 2048
0.00.040.544 I print_info: n_embd_v_gqa     = 2048
0.00.040.545 I print_info: f_norm_eps       = 1.0e-05
0.00.040.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.549 I print_info: f_logit_scale    = 0.0e+00
0.00.040.550 I print_info: n_ff             = 8192
0.00.040.550 I print_info: n_expert         = 0
0.00.040.551 I print_info: n_expert_used    = 0
0.00.040.551 I print_info: causal attn      = 1
0.00.040.551 I print_info: pooling type     = 0
0.00.040.551 I print_info: rope type        = 2
0.00.040.551 I print_info: rope scaling     = linear
0.00.040.551 I print_info: freq_base_train  = 10000.0
0.00.040.552 I print_info: freq_scale_train = 1
0.00.040.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.552 I print_info: rope_finetuned   = unknown
0.00.040.552 I print_info: ssm_d_conv       = 0
0.00.040.554 I print_info: ssm_d_inner      = 0
0.00.040.554 I print_info: ssm_d_state      = 0
0.00.040.554 I print_info: ssm_dt_rank      = 0
0.00.040.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.554 I print_info: model type       = 1.4B
0.00.040.554 I print_info: model params     = 1.41 B
0.00.040.555 I print_info: general.name     = 1.4B
0.00.040.555 I print_info: vocab type       = BPE
0.00.040.555 I print_info: n_vocab          = 50304
0.00.040.555 I print_info: n_merges         = 50009
0.00.040.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: LF token         = 187 'Ċ'
0.00.040.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.558 I print_info: max token length = 1024
0.00.040.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.756.093 I load_tensors: offloading 24 repeating layers to GPU
0.00.756.097 I load_tensors: offloading output layer to GPU
0.00.756.097 I load_tensors: offloaded 25/25 layers to GPU
0.00.756.121 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.756.124 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.757.413 I llama_init_from_model: n_seq_max     = 1
0.00.757.415 I llama_init_from_model: n_ctx         = 128
0.00.757.416 I llama_init_from_model: n_ctx_per_seq = 128
0.00.757.416 I llama_init_from_model: n_batch       = 128
0.00.757.417 I llama_init_from_model: n_ubatch      = 128
0.00.757.417 I llama_init_from_model: flash_attn    = 0
0.00.757.418 I llama_init_from_model: freq_base     = 10000.0
0.00.757.419 I llama_init_from_model: freq_scale    = 1
0.00.757.419 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.757.421 I ggml_metal_init: allocating
0.00.757.460 I ggml_metal_init: found device: Apple M4
0.00.757.470 I ggml_metal_init: picking default device: Apple M4
0.00.758.955 I ggml_metal_init: using embedded metal library
0.00.764.927 I ggml_metal_init: GPU name:   Apple M4
0.00.764.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.764.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.764.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.764.933 I ggml_metal_init: simdgroup reduction   = true
0.00.764.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.764.934 I ggml_metal_init: has residency sets    = true
0.00.764.934 I ggml_metal_init: has bfloat            = true
0.00.764.934 I ggml_metal_init: use bfloat            = true
0.00.764.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.764.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.781.958 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.242 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.785.249 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.785.281 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.168 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.788.170 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.788.171 I llama_init_from_model: graph nodes  = 967
0.00.788.171 I llama_init_from_model: graph splits = 2
0.00.788.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.788.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.739 I 
0.00.816.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.828 I perplexity: tokenizing the input ..
0.00.823.745 I perplexity: tokenization took 6.915 ms
0.00.823.764 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.961.939 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.963.231 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.963.244 I llama_perf_context_print:        load time =     807.10 ms
0.00.963.245 I llama_perf_context_print: prompt eval time =     137.34 ms /   128 tokens (    1.07 ms per token,   931.99 tokens per second)
0.00.963.245 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.963.246 I llama_perf_context_print:       total time =     146.51 ms /   129 tokens
0.00.963.634 I ggml_metal_free: deallocating

real	0m0.979s
user	0m0.080s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.515 I llama_model_loader: - type  f32:  194 tensors
0.00.025.515 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.516 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.516 I print_info: file format = GGUF V3 (latest)
0.00.025.517 I print_info: file type   = Q4_1
0.00.025.518 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.254 I load: special tokens cache size = 25
0.00.040.935 I load: token to piece cache size = 0.2984 MB
0.00.040.939 I print_info: arch             = gptneox
0.00.040.940 I print_info: vocab_only       = 0
0.00.040.940 I print_info: n_ctx_train      = 2048
0.00.040.940 I print_info: n_embd           = 2048
0.00.040.940 I print_info: n_layer          = 24
0.00.040.944 I print_info: n_head           = 16
0.00.040.945 I print_info: n_head_kv        = 16
0.00.040.945 I print_info: n_rot            = 32
0.00.040.945 I print_info: n_swa            = 0
0.00.040.946 I print_info: n_embd_head_k    = 128
0.00.040.946 I print_info: n_embd_head_v    = 128
0.00.040.946 I print_info: n_gqa            = 1
0.00.040.947 I print_info: n_embd_k_gqa     = 2048
0.00.040.948 I print_info: n_embd_v_gqa     = 2048
0.00.040.949 I print_info: f_norm_eps       = 1.0e-05
0.00.040.949 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.952 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.952 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.952 I print_info: f_logit_scale    = 0.0e+00
0.00.040.953 I print_info: n_ff             = 8192
0.00.040.953 I print_info: n_expert         = 0
0.00.040.953 I print_info: n_expert_used    = 0
0.00.040.954 I print_info: causal attn      = 1
0.00.040.954 I print_info: pooling type     = 0
0.00.040.954 I print_info: rope type        = 2
0.00.040.954 I print_info: rope scaling     = linear
0.00.040.955 I print_info: freq_base_train  = 10000.0
0.00.040.956 I print_info: freq_scale_train = 1
0.00.040.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.957 I print_info: rope_finetuned   = unknown
0.00.040.957 I print_info: ssm_d_conv       = 0
0.00.040.957 I print_info: ssm_d_inner      = 0
0.00.040.957 I print_info: ssm_d_state      = 0
0.00.040.957 I print_info: ssm_dt_rank      = 0
0.00.040.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.957 I print_info: model type       = 1.4B
0.00.040.958 I print_info: model params     = 1.41 B
0.00.040.958 I print_info: general.name     = 1.4B
0.00.040.958 I print_info: vocab type       = BPE
0.00.040.959 I print_info: n_vocab          = 50304
0.00.040.959 I print_info: n_merges         = 50009
0.00.040.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.964 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.964 I print_info: LF token         = 187 'Ċ'
0.00.040.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.965 I print_info: max token length = 1024
0.00.040.965 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.710.457 I load_tensors: offloading 24 repeating layers to GPU
0.00.710.462 I load_tensors: offloading output layer to GPU
0.00.710.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.710.483 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.710.485 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.711.563 I llama_init_from_model: n_seq_max     = 1
0.00.711.565 I llama_init_from_model: n_ctx         = 128
0.00.711.565 I llama_init_from_model: n_ctx_per_seq = 128
0.00.711.565 I llama_init_from_model: n_batch       = 128
0.00.711.566 I llama_init_from_model: n_ubatch      = 128
0.00.711.566 I llama_init_from_model: flash_attn    = 0
0.00.711.567 I llama_init_from_model: freq_base     = 10000.0
0.00.711.567 I llama_init_from_model: freq_scale    = 1
0.00.711.568 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.711.569 I ggml_metal_init: allocating
0.00.711.584 I ggml_metal_init: found device: Apple M4
0.00.711.591 I ggml_metal_init: picking default device: Apple M4
0.00.712.872 I ggml_metal_init: using embedded metal library
0.00.718.883 I ggml_metal_init: GPU name:   Apple M4
0.00.718.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.718.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.718.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.718.890 I ggml_metal_init: simdgroup reduction   = true
0.00.718.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.718.890 I ggml_metal_init: has residency sets    = true
0.00.718.890 I ggml_metal_init: has bfloat            = true
0.00.718.891 I ggml_metal_init: use bfloat            = true
0.00.718.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.718.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.232 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.567 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.738.571 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.738.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.583 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.741.584 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.741.585 I llama_init_from_model: graph nodes  = 967
0.00.741.585 I llama_init_from_model: graph splits = 2
0.00.741.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.741.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.028 I 
0.00.765.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.115 I perplexity: tokenizing the input ..
0.00.772.203 I perplexity: tokenization took 7.085 ms
0.00.772.221 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.908.987 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.910.229 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.910.266 I llama_perf_context_print:        load time =     755.41 ms
0.00.910.267 I llama_perf_context_print: prompt eval time =     135.84 ms /   128 tokens (    1.06 ms per token,   942.28 tokens per second)
0.00.910.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.910.268 I llama_perf_context_print:       total time =     145.24 ms /   129 tokens
0.00.910.665 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.080s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.820 I llama_model_loader: - type  f32:  194 tensors
0.00.026.821 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.822 I print_info: file format = GGUF V3 (latest)
0.00.026.822 I print_info: file type   = Q5_0
0.00.026.823 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.016 I load: special tokens cache size = 25
0.00.041.471 I load: token to piece cache size = 0.2984 MB
0.00.041.475 I print_info: arch             = gptneox
0.00.041.475 I print_info: vocab_only       = 0
0.00.041.475 I print_info: n_ctx_train      = 2048
0.00.041.475 I print_info: n_embd           = 2048
0.00.041.476 I print_info: n_layer          = 24
0.00.041.479 I print_info: n_head           = 16
0.00.041.480 I print_info: n_head_kv        = 16
0.00.041.480 I print_info: n_rot            = 32
0.00.041.480 I print_info: n_swa            = 0
0.00.041.481 I print_info: n_embd_head_k    = 128
0.00.041.481 I print_info: n_embd_head_v    = 128
0.00.041.481 I print_info: n_gqa            = 1
0.00.041.482 I print_info: n_embd_k_gqa     = 2048
0.00.041.483 I print_info: n_embd_v_gqa     = 2048
0.00.041.484 I print_info: f_norm_eps       = 1.0e-05
0.00.041.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.484 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.484 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.485 I print_info: f_logit_scale    = 0.0e+00
0.00.041.485 I print_info: n_ff             = 8192
0.00.041.485 I print_info: n_expert         = 0
0.00.041.486 I print_info: n_expert_used    = 0
0.00.041.486 I print_info: causal attn      = 1
0.00.041.486 I print_info: pooling type     = 0
0.00.041.486 I print_info: rope type        = 2
0.00.041.486 I print_info: rope scaling     = linear
0.00.041.487 I print_info: freq_base_train  = 10000.0
0.00.041.487 I print_info: freq_scale_train = 1
0.00.041.487 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.488 I print_info: rope_finetuned   = unknown
0.00.041.488 I print_info: ssm_d_conv       = 0
0.00.041.489 I print_info: ssm_d_inner      = 0
0.00.041.490 I print_info: ssm_d_state      = 0
0.00.041.492 I print_info: ssm_dt_rank      = 0
0.00.041.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.492 I print_info: model type       = 1.4B
0.00.041.492 I print_info: model params     = 1.41 B
0.00.041.493 I print_info: general.name     = 1.4B
0.00.041.493 I print_info: vocab type       = BPE
0.00.041.493 I print_info: n_vocab          = 50304
0.00.041.494 I print_info: n_merges         = 50009
0.00.041.494 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.495 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.495 I print_info: LF token         = 187 'Ċ'
0.00.041.495 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.495 I print_info: max token length = 1024
0.00.041.496 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.733.700 I load_tensors: offloading 24 repeating layers to GPU
0.00.733.704 I load_tensors: offloading output layer to GPU
0.00.733.705 I load_tensors: offloaded 25/25 layers to GPU
0.00.733.728 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.733.730 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.734.857 I llama_init_from_model: n_seq_max     = 1
0.00.734.859 I llama_init_from_model: n_ctx         = 128
0.00.734.859 I llama_init_from_model: n_ctx_per_seq = 128
0.00.734.859 I llama_init_from_model: n_batch       = 128
0.00.734.860 I llama_init_from_model: n_ubatch      = 128
0.00.734.860 I llama_init_from_model: flash_attn    = 0
0.00.734.861 I llama_init_from_model: freq_base     = 10000.0
0.00.734.862 I llama_init_from_model: freq_scale    = 1
0.00.734.862 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.734.871 I ggml_metal_init: allocating
0.00.734.908 I ggml_metal_init: found device: Apple M4
0.00.734.919 I ggml_metal_init: picking default device: Apple M4
0.00.736.256 I ggml_metal_init: using embedded metal library
0.00.741.806 I ggml_metal_init: GPU name:   Apple M4
0.00.741.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.741.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.741.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.741.811 I ggml_metal_init: simdgroup reduction   = true
0.00.741.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.741.811 I ggml_metal_init: has residency sets    = true
0.00.741.811 I ggml_metal_init: has bfloat            = true
0.00.741.812 I ggml_metal_init: use bfloat            = true
0.00.741.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.741.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.757.900 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.224 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.761.228 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.761.253 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.764.201 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.764.203 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.764.203 I llama_init_from_model: graph nodes  = 967
0.00.764.204 I llama_init_from_model: graph splits = 2
0.00.764.206 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.764.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.179 I 
0.00.790.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.257 I perplexity: tokenizing the input ..
0.00.796.724 I perplexity: tokenization took 6.464 ms
0.00.796.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.931.756 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.933.020 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.933.031 I llama_perf_context_print:        load time =     779.42 ms
0.00.933.032 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.57 tokens per second)
0.00.933.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.933.033 I llama_perf_context_print:       total time =     142.86 ms /   129 tokens
0.00.933.442 I ggml_metal_free: deallocating

real	0m0.950s
user	0m0.077s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.153 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.944 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.739 I llama_model_loader: - type  f32:  194 tensors
0.00.023.740 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.741 I print_info: file format = GGUF V3 (latest)
0.00.023.741 I print_info: file type   = Q5_1
0.00.023.742 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.031.726 I load: special tokens cache size = 25
0.00.037.959 I load: token to piece cache size = 0.2984 MB
0.00.037.963 I print_info: arch             = gptneox
0.00.037.963 I print_info: vocab_only       = 0
0.00.037.964 I print_info: n_ctx_train      = 2048
0.00.037.964 I print_info: n_embd           = 2048
0.00.037.964 I print_info: n_layer          = 24
0.00.037.968 I print_info: n_head           = 16
0.00.037.969 I print_info: n_head_kv        = 16
0.00.037.969 I print_info: n_rot            = 32
0.00.037.969 I print_info: n_swa            = 0
0.00.037.970 I print_info: n_embd_head_k    = 128
0.00.037.970 I print_info: n_embd_head_v    = 128
0.00.037.970 I print_info: n_gqa            = 1
0.00.037.971 I print_info: n_embd_k_gqa     = 2048
0.00.037.972 I print_info: n_embd_v_gqa     = 2048
0.00.037.972 I print_info: f_norm_eps       = 1.0e-05
0.00.037.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.975 I print_info: f_logit_scale    = 0.0e+00
0.00.037.976 I print_info: n_ff             = 8192
0.00.037.976 I print_info: n_expert         = 0
0.00.037.976 I print_info: n_expert_used    = 0
0.00.037.976 I print_info: causal attn      = 1
0.00.037.976 I print_info: pooling type     = 0
0.00.037.976 I print_info: rope type        = 2
0.00.037.976 I print_info: rope scaling     = linear
0.00.037.977 I print_info: freq_base_train  = 10000.0
0.00.037.978 I print_info: freq_scale_train = 1
0.00.037.978 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.978 I print_info: rope_finetuned   = unknown
0.00.037.978 I print_info: ssm_d_conv       = 0
0.00.037.978 I print_info: ssm_d_inner      = 0
0.00.037.978 I print_info: ssm_d_state      = 0
0.00.037.979 I print_info: ssm_dt_rank      = 0
0.00.037.979 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.981 I print_info: model type       = 1.4B
0.00.037.982 I print_info: model params     = 1.41 B
0.00.037.982 I print_info: general.name     = 1.4B
0.00.037.982 I print_info: vocab type       = BPE
0.00.037.982 I print_info: n_vocab          = 50304
0.00.037.982 I print_info: n_merges         = 50009
0.00.037.983 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.983 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.983 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.983 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.983 I print_info: LF token         = 187 'Ċ'
0.00.037.984 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.984 I print_info: max token length = 1024
0.00.037.984 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.702.067 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.072 I load_tensors: offloading output layer to GPU
0.00.702.073 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.094 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.702.096 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.703.024 I llama_init_from_model: n_seq_max     = 1
0.00.703.025 I llama_init_from_model: n_ctx         = 128
0.00.703.025 I llama_init_from_model: n_ctx_per_seq = 128
0.00.703.026 I llama_init_from_model: n_batch       = 128
0.00.703.026 I llama_init_from_model: n_ubatch      = 128
0.00.703.027 I llama_init_from_model: flash_attn    = 0
0.00.703.027 I llama_init_from_model: freq_base     = 10000.0
0.00.703.028 I llama_init_from_model: freq_scale    = 1
0.00.703.028 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.703.030 I ggml_metal_init: allocating
0.00.703.067 I ggml_metal_init: found device: Apple M4
0.00.703.078 I ggml_metal_init: picking default device: Apple M4
0.00.704.275 I ggml_metal_init: using embedded metal library
0.00.709.522 I ggml_metal_init: GPU name:   Apple M4
0.00.709.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.709.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.709.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.709.528 I ggml_metal_init: simdgroup reduction   = true
0.00.709.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.709.528 I ggml_metal_init: has residency sets    = true
0.00.709.528 I ggml_metal_init: has bfloat            = true
0.00.709.528 I ggml_metal_init: use bfloat            = true
0.00.709.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.709.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.723.999 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.727.319 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.727.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.168 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.730.169 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.730.170 I llama_init_from_model: graph nodes  = 967
0.00.730.170 I llama_init_from_model: graph splits = 2
0.00.730.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.730.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.039 I 
0.00.757.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.126 I perplexity: tokenizing the input ..
0.00.763.694 I perplexity: tokenization took 6.565 ms
0.00.763.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.898.451 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.899.697 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.899.735 I llama_perf_context_print:        load time =     748.67 ms
0.00.899.736 I llama_perf_context_print: prompt eval time =     133.86 ms /   128 tokens (    1.05 ms per token,   956.20 tokens per second)
0.00.899.737 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.739 I llama_perf_context_print:       total time =     142.70 ms /   129 tokens
0.00.900.191 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.075s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.701 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.690 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.693 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.694 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.694 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.694 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.696 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.697 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.423 I llama_model_loader: - type  f32:  194 tensors
0.00.026.423 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.424 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.425 I print_info: file format = GGUF V3 (latest)
0.00.026.425 I print_info: file type   = Q2_K - Medium
0.00.026.431 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.949 I load: special tokens cache size = 25
0.00.041.267 I load: token to piece cache size = 0.2984 MB
0.00.041.271 I print_info: arch             = gptneox
0.00.041.272 I print_info: vocab_only       = 0
0.00.041.272 I print_info: n_ctx_train      = 2048
0.00.041.272 I print_info: n_embd           = 2048
0.00.041.272 I print_info: n_layer          = 24
0.00.041.277 I print_info: n_head           = 16
0.00.041.278 I print_info: n_head_kv        = 16
0.00.041.278 I print_info: n_rot            = 32
0.00.041.278 I print_info: n_swa            = 0
0.00.041.278 I print_info: n_embd_head_k    = 128
0.00.041.279 I print_info: n_embd_head_v    = 128
0.00.041.279 I print_info: n_gqa            = 1
0.00.041.280 I print_info: n_embd_k_gqa     = 2048
0.00.041.281 I print_info: n_embd_v_gqa     = 2048
0.00.041.284 I print_info: f_norm_eps       = 1.0e-05
0.00.041.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.284 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.284 I print_info: f_logit_scale    = 0.0e+00
0.00.041.285 I print_info: n_ff             = 8192
0.00.041.285 I print_info: n_expert         = 0
0.00.041.285 I print_info: n_expert_used    = 0
0.00.041.285 I print_info: causal attn      = 1
0.00.041.285 I print_info: pooling type     = 0
0.00.041.286 I print_info: rope type        = 2
0.00.041.286 I print_info: rope scaling     = linear
0.00.041.286 I print_info: freq_base_train  = 10000.0
0.00.041.286 I print_info: freq_scale_train = 1
0.00.041.286 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.287 I print_info: rope_finetuned   = unknown
0.00.041.287 I print_info: ssm_d_conv       = 0
0.00.041.287 I print_info: ssm_d_inner      = 0
0.00.041.287 I print_info: ssm_d_state      = 0
0.00.041.287 I print_info: ssm_dt_rank      = 0
0.00.041.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.288 I print_info: model type       = 1.4B
0.00.041.288 I print_info: model params     = 1.41 B
0.00.041.288 I print_info: general.name     = 1.4B
0.00.041.289 I print_info: vocab type       = BPE
0.00.041.289 I print_info: n_vocab          = 50304
0.00.041.289 I print_info: n_merges         = 50009
0.00.041.289 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.289 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.290 I print_info: LF token         = 187 'Ċ'
0.00.041.290 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.290 I print_info: max token length = 1024
0.00.041.291 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.377.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.377.286 I load_tensors: offloading output layer to GPU
0.00.377.287 I load_tensors: offloaded 25/25 layers to GPU
0.00.377.316 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.377.317 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.378.595 I llama_init_from_model: n_seq_max     = 1
0.00.378.598 I llama_init_from_model: n_ctx         = 128
0.00.378.599 I llama_init_from_model: n_ctx_per_seq = 128
0.00.378.599 I llama_init_from_model: n_batch       = 128
0.00.378.599 I llama_init_from_model: n_ubatch      = 128
0.00.378.600 I llama_init_from_model: flash_attn    = 0
0.00.378.601 I llama_init_from_model: freq_base     = 10000.0
0.00.378.602 I llama_init_from_model: freq_scale    = 1
0.00.378.602 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.378.605 I ggml_metal_init: allocating
0.00.378.672 I ggml_metal_init: found device: Apple M4
0.00.378.685 I ggml_metal_init: picking default device: Apple M4
0.00.380.411 I ggml_metal_init: using embedded metal library
0.00.387.155 I ggml_metal_init: GPU name:   Apple M4
0.00.387.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.387.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.387.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.387.162 I ggml_metal_init: simdgroup reduction   = true
0.00.387.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.387.162 I ggml_metal_init: has residency sets    = true
0.00.387.163 I ggml_metal_init: has bfloat            = true
0.00.387.163 I ggml_metal_init: use bfloat            = true
0.00.387.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.387.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.144 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.408.658 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.408.662 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.408.687 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.411.854 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.411.855 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.411.856 I llama_init_from_model: graph nodes  = 967
0.00.411.856 I llama_init_from_model: graph splits = 2
0.00.411.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.411.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.388 I 
0.00.439.432 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.458 I perplexity: tokenizing the input ..
0.00.445.571 I perplexity: tokenization took 6.109 ms
0.00.445.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.788 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.035 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.049 I llama_perf_context_print:        load time =     428.68 ms
0.00.579.050 I llama_perf_context_print: prompt eval time =     131.26 ms /   128 tokens (    1.03 ms per token,   975.14 tokens per second)
0.00.579.051 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.051 I llama_perf_context_print:       total time =     139.66 ms /   129 tokens
0.00.579.453 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.080s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.073 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.074 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.074 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.075 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.076 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.077 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.587 I llama_model_loader: - type  f32:  194 tensors
0.00.025.587 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.587 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.588 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.588 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.588 I print_info: file format = GGUF V3 (latest)
0.00.025.595 I print_info: file type   = Q3_K - Medium
0.00.025.596 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.690 I load: special tokens cache size = 25
0.00.039.681 I load: token to piece cache size = 0.2984 MB
0.00.039.685 I print_info: arch             = gptneox
0.00.039.686 I print_info: vocab_only       = 0
0.00.039.686 I print_info: n_ctx_train      = 2048
0.00.039.686 I print_info: n_embd           = 2048
0.00.039.686 I print_info: n_layer          = 24
0.00.039.691 I print_info: n_head           = 16
0.00.039.691 I print_info: n_head_kv        = 16
0.00.039.692 I print_info: n_rot            = 32
0.00.039.692 I print_info: n_swa            = 0
0.00.039.692 I print_info: n_embd_head_k    = 128
0.00.039.695 I print_info: n_embd_head_v    = 128
0.00.039.696 I print_info: n_gqa            = 1
0.00.039.696 I print_info: n_embd_k_gqa     = 2048
0.00.039.697 I print_info: n_embd_v_gqa     = 2048
0.00.039.697 I print_info: f_norm_eps       = 1.0e-05
0.00.039.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.700 I print_info: f_logit_scale    = 0.0e+00
0.00.039.700 I print_info: n_ff             = 8192
0.00.039.700 I print_info: n_expert         = 0
0.00.039.700 I print_info: n_expert_used    = 0
0.00.039.701 I print_info: causal attn      = 1
0.00.039.701 I print_info: pooling type     = 0
0.00.039.701 I print_info: rope type        = 2
0.00.039.736 I print_info: rope scaling     = linear
0.00.039.738 I print_info: freq_base_train  = 10000.0
0.00.039.739 I print_info: freq_scale_train = 1
0.00.039.739 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.739 I print_info: rope_finetuned   = unknown
0.00.039.739 I print_info: ssm_d_conv       = 0
0.00.039.739 I print_info: ssm_d_inner      = 0
0.00.039.739 I print_info: ssm_d_state      = 0
0.00.039.739 I print_info: ssm_dt_rank      = 0
0.00.039.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.740 I print_info: model type       = 1.4B
0.00.039.741 I print_info: model params     = 1.41 B
0.00.039.741 I print_info: general.name     = 1.4B
0.00.039.741 I print_info: vocab type       = BPE
0.00.039.742 I print_info: n_vocab          = 50304
0.00.039.742 I print_info: n_merges         = 50009
0.00.039.742 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.742 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.742 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.742 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.743 I print_info: LF token         = 187 'Ċ'
0.00.039.743 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.743 I print_info: max token length = 1024
0.00.039.744 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.478.403 I load_tensors: offloading 24 repeating layers to GPU
0.00.478.408 I load_tensors: offloading output layer to GPU
0.00.478.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.478.430 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.478.431 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.479.594 I llama_init_from_model: n_seq_max     = 1
0.00.479.596 I llama_init_from_model: n_ctx         = 128
0.00.479.596 I llama_init_from_model: n_ctx_per_seq = 128
0.00.479.596 I llama_init_from_model: n_batch       = 128
0.00.479.597 I llama_init_from_model: n_ubatch      = 128
0.00.479.597 I llama_init_from_model: flash_attn    = 0
0.00.479.598 I llama_init_from_model: freq_base     = 10000.0
0.00.479.599 I llama_init_from_model: freq_scale    = 1
0.00.479.600 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.479.601 I ggml_metal_init: allocating
0.00.479.626 I ggml_metal_init: found device: Apple M4
0.00.479.635 I ggml_metal_init: picking default device: Apple M4
0.00.481.199 I ggml_metal_init: using embedded metal library
0.00.487.151 I ggml_metal_init: GPU name:   Apple M4
0.00.487.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.487.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.487.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.487.158 I ggml_metal_init: simdgroup reduction   = true
0.00.487.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.487.159 I ggml_metal_init: has residency sets    = true
0.00.487.159 I ggml_metal_init: has bfloat            = true
0.00.487.159 I ggml_metal_init: use bfloat            = true
0.00.487.160 I ggml_metal_init: hasUnifiedMemory      = true
0.00.487.162 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.503.742 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.507.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.507.047 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.507.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.510.053 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.510.055 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.510.055 I llama_init_from_model: graph nodes  = 967
0.00.510.055 I llama_init_from_model: graph splits = 2
0.00.510.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.510.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.019 I 
0.00.539.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.102 I perplexity: tokenizing the input ..
0.00.544.328 I perplexity: tokenization took 5.224 ms
0.00.544.341 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.488 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.783 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.797 I llama_perf_context_print:        load time =     529.16 ms
0.00.679.798 I llama_perf_context_print: prompt eval time =     133.92 ms /   128 tokens (    1.05 ms per token,   955.77 tokens per second)
0.00.679.798 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.799 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.680.160 I ggml_metal_free: deallocating

real	0m0.694s
user	0m0.075s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.534 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.214 I llama_model_loader: - type  f32:  194 tensors
0.00.025.215 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.215 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.215 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.216 I print_info: file format = GGUF V3 (latest)
0.00.025.216 I print_info: file type   = Q4_K - Medium
0.00.025.218 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.278 I load: special tokens cache size = 25
0.00.039.140 I load: token to piece cache size = 0.2984 MB
0.00.039.145 I print_info: arch             = gptneox
0.00.039.145 I print_info: vocab_only       = 0
0.00.039.145 I print_info: n_ctx_train      = 2048
0.00.039.146 I print_info: n_embd           = 2048
0.00.039.146 I print_info: n_layer          = 24
0.00.039.150 I print_info: n_head           = 16
0.00.039.151 I print_info: n_head_kv        = 16
0.00.039.151 I print_info: n_rot            = 32
0.00.039.151 I print_info: n_swa            = 0
0.00.039.151 I print_info: n_embd_head_k    = 128
0.00.039.151 I print_info: n_embd_head_v    = 128
0.00.039.152 I print_info: n_gqa            = 1
0.00.039.153 I print_info: n_embd_k_gqa     = 2048
0.00.039.153 I print_info: n_embd_v_gqa     = 2048
0.00.039.154 I print_info: f_norm_eps       = 1.0e-05
0.00.039.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.155 I print_info: f_logit_scale    = 0.0e+00
0.00.039.156 I print_info: n_ff             = 8192
0.00.039.156 I print_info: n_expert         = 0
0.00.039.156 I print_info: n_expert_used    = 0
0.00.039.156 I print_info: causal attn      = 1
0.00.039.156 I print_info: pooling type     = 0
0.00.039.157 I print_info: rope type        = 2
0.00.039.157 I print_info: rope scaling     = linear
0.00.039.157 I print_info: freq_base_train  = 10000.0
0.00.039.157 I print_info: freq_scale_train = 1
0.00.039.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.158 I print_info: rope_finetuned   = unknown
0.00.039.158 I print_info: ssm_d_conv       = 0
0.00.039.158 I print_info: ssm_d_inner      = 0
0.00.039.158 I print_info: ssm_d_state      = 0
0.00.039.158 I print_info: ssm_dt_rank      = 0
0.00.039.159 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.159 I print_info: model type       = 1.4B
0.00.039.159 I print_info: model params     = 1.41 B
0.00.039.162 I print_info: general.name     = 1.4B
0.00.039.163 I print_info: vocab type       = BPE
0.00.039.163 I print_info: n_vocab          = 50304
0.00.039.163 I print_info: n_merges         = 50009
0.00.039.163 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.163 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.164 I print_info: LF token         = 187 'Ċ'
0.00.039.164 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: max token length = 1024
0.00.039.165 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.582.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.582.606 I load_tensors: offloading output layer to GPU
0.00.582.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.582.628 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.582.630 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.583.709 I llama_init_from_model: n_seq_max     = 1
0.00.583.711 I llama_init_from_model: n_ctx         = 128
0.00.583.711 I llama_init_from_model: n_ctx_per_seq = 128
0.00.583.711 I llama_init_from_model: n_batch       = 128
0.00.583.711 I llama_init_from_model: n_ubatch      = 128
0.00.583.712 I llama_init_from_model: flash_attn    = 0
0.00.583.713 I llama_init_from_model: freq_base     = 10000.0
0.00.583.713 I llama_init_from_model: freq_scale    = 1
0.00.583.714 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.583.715 I ggml_metal_init: allocating
0.00.583.731 I ggml_metal_init: found device: Apple M4
0.00.583.738 I ggml_metal_init: picking default device: Apple M4
0.00.585.023 I ggml_metal_init: using embedded metal library
0.00.590.744 I ggml_metal_init: GPU name:   Apple M4
0.00.590.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.749 I ggml_metal_init: simdgroup reduction   = true
0.00.590.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.750 I ggml_metal_init: has residency sets    = true
0.00.590.750 I ggml_metal_init: has bfloat            = true
0.00.590.750 I ggml_metal_init: use bfloat            = true
0.00.590.751 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.752 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.606.635 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.965 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.969 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.993 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.098 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.613.099 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.613.100 I llama_init_from_model: graph nodes  = 967
0.00.613.101 I llama_init_from_model: graph splits = 2
0.00.613.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.613.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.258 I 
0.00.642.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.337 I perplexity: tokenizing the input ..
0.00.648.822 I perplexity: tokenization took 6.483 ms
0.00.648.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.721 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.796.019 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.796.040 I llama_perf_context_print:        load time =     632.72 ms
0.00.796.041 I llama_perf_context_print: prompt eval time =     144.98 ms /   128 tokens (    1.13 ms per token,   882.88 tokens per second)
0.00.796.041 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.042 I llama_perf_context_print:       total time =     153.78 ms /   129 tokens
0.00.796.485 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.572 I llama_model_loader: - type  f32:  194 tensors
0.00.025.573 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.573 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.574 I print_info: file format = GGUF V3 (latest)
0.00.025.574 I print_info: file type   = Q5_K - Medium
0.00.025.576 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.877 I load: special tokens cache size = 25
0.00.040.243 I load: token to piece cache size = 0.2984 MB
0.00.040.246 I print_info: arch             = gptneox
0.00.040.247 I print_info: vocab_only       = 0
0.00.040.247 I print_info: n_ctx_train      = 2048
0.00.040.247 I print_info: n_embd           = 2048
0.00.040.247 I print_info: n_layer          = 24
0.00.040.251 I print_info: n_head           = 16
0.00.040.252 I print_info: n_head_kv        = 16
0.00.040.252 I print_info: n_rot            = 32
0.00.040.252 I print_info: n_swa            = 0
0.00.040.252 I print_info: n_embd_head_k    = 128
0.00.040.252 I print_info: n_embd_head_v    = 128
0.00.040.253 I print_info: n_gqa            = 1
0.00.040.254 I print_info: n_embd_k_gqa     = 2048
0.00.040.255 I print_info: n_embd_v_gqa     = 2048
0.00.040.255 I print_info: f_norm_eps       = 1.0e-05
0.00.040.256 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.256 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.256 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.256 I print_info: f_logit_scale    = 0.0e+00
0.00.040.257 I print_info: n_ff             = 8192
0.00.040.257 I print_info: n_expert         = 0
0.00.040.257 I print_info: n_expert_used    = 0
0.00.040.257 I print_info: causal attn      = 1
0.00.040.257 I print_info: pooling type     = 0
0.00.040.258 I print_info: rope type        = 2
0.00.040.258 I print_info: rope scaling     = linear
0.00.040.258 I print_info: freq_base_train  = 10000.0
0.00.040.259 I print_info: freq_scale_train = 1
0.00.040.259 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.259 I print_info: rope_finetuned   = unknown
0.00.040.259 I print_info: ssm_d_conv       = 0
0.00.040.259 I print_info: ssm_d_inner      = 0
0.00.040.259 I print_info: ssm_d_state      = 0
0.00.040.259 I print_info: ssm_dt_rank      = 0
0.00.040.260 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.260 I print_info: model type       = 1.4B
0.00.040.260 I print_info: model params     = 1.41 B
0.00.040.260 I print_info: general.name     = 1.4B
0.00.040.261 I print_info: vocab type       = BPE
0.00.040.261 I print_info: n_vocab          = 50304
0.00.040.261 I print_info: n_merges         = 50009
0.00.040.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.266 I print_info: LF token         = 187 'Ċ'
0.00.040.266 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.266 I print_info: max token length = 1024
0.00.040.267 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.667.123 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.128 I load_tensors: offloading output layer to GPU
0.00.667.129 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.153 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.667.155 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.668.260 I llama_init_from_model: n_seq_max     = 1
0.00.668.262 I llama_init_from_model: n_ctx         = 128
0.00.668.263 I llama_init_from_model: n_ctx_per_seq = 128
0.00.668.263 I llama_init_from_model: n_batch       = 128
0.00.668.263 I llama_init_from_model: n_ubatch      = 128
0.00.668.263 I llama_init_from_model: flash_attn    = 0
0.00.668.264 I llama_init_from_model: freq_base     = 10000.0
0.00.668.265 I llama_init_from_model: freq_scale    = 1
0.00.668.265 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.668.266 I ggml_metal_init: allocating
0.00.668.280 I ggml_metal_init: found device: Apple M4
0.00.668.287 I ggml_metal_init: picking default device: Apple M4
0.00.669.561 I ggml_metal_init: using embedded metal library
0.00.675.438 I ggml_metal_init: GPU name:   Apple M4
0.00.675.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.443 I ggml_metal_init: simdgroup reduction   = true
0.00.675.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.443 I ggml_metal_init: has residency sets    = true
0.00.675.443 I ggml_metal_init: has bfloat            = true
0.00.675.444 I ggml_metal_init: use bfloat            = true
0.00.675.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.721 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.024 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.049 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.182 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.184 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.185 I llama_init_from_model: graph nodes  = 967
0.00.698.185 I llama_init_from_model: graph splits = 2
0.00.698.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.258 I 
0.00.734.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.350 I perplexity: tokenizing the input ..
0.00.741.425 I perplexity: tokenization took 7.072 ms
0.00.741.446 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.765 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.891.012 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.891.027 I llama_perf_context_print:        load time =     724.49 ms
0.00.891.028 I llama_perf_context_print: prompt eval time =     147.49 ms /   128 tokens (    1.15 ms per token,   867.85 tokens per second)
0.00.891.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.029 I llama_perf_context_print:       total time =     156.77 ms /   129 tokens
0.00.891.433 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.079s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.717 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.543 I llama_model_loader: - type  f32:  194 tensors
0.00.026.543 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.544 I print_info: file format = GGUF V3 (latest)
0.00.026.544 I print_info: file type   = Q6_K
0.00.026.545 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.627 I load: special tokens cache size = 25
0.00.040.438 I load: token to piece cache size = 0.2984 MB
0.00.040.443 I print_info: arch             = gptneox
0.00.040.444 I print_info: vocab_only       = 0
0.00.040.444 I print_info: n_ctx_train      = 2048
0.00.040.444 I print_info: n_embd           = 2048
0.00.040.444 I print_info: n_layer          = 24
0.00.040.447 I print_info: n_head           = 16
0.00.040.452 I print_info: n_head_kv        = 16
0.00.040.453 I print_info: n_rot            = 32
0.00.040.454 I print_info: n_swa            = 0
0.00.040.454 I print_info: n_embd_head_k    = 128
0.00.040.454 I print_info: n_embd_head_v    = 128
0.00.040.455 I print_info: n_gqa            = 1
0.00.040.456 I print_info: n_embd_k_gqa     = 2048
0.00.040.458 I print_info: n_embd_v_gqa     = 2048
0.00.040.458 I print_info: f_norm_eps       = 1.0e-05
0.00.040.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.459 I print_info: f_logit_scale    = 0.0e+00
0.00.040.460 I print_info: n_ff             = 8192
0.00.040.460 I print_info: n_expert         = 0
0.00.040.460 I print_info: n_expert_used    = 0
0.00.040.461 I print_info: causal attn      = 1
0.00.040.461 I print_info: pooling type     = 0
0.00.040.461 I print_info: rope type        = 2
0.00.040.461 I print_info: rope scaling     = linear
0.00.040.462 I print_info: freq_base_train  = 10000.0
0.00.040.462 I print_info: freq_scale_train = 1
0.00.040.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.462 I print_info: rope_finetuned   = unknown
0.00.040.462 I print_info: ssm_d_conv       = 0
0.00.040.463 I print_info: ssm_d_inner      = 0
0.00.040.463 I print_info: ssm_d_state      = 0
0.00.040.463 I print_info: ssm_dt_rank      = 0
0.00.040.463 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.463 I print_info: model type       = 1.4B
0.00.040.464 I print_info: model params     = 1.41 B
0.00.040.464 I print_info: general.name     = 1.4B
0.00.040.464 I print_info: vocab type       = BPE
0.00.040.465 I print_info: n_vocab          = 50304
0.00.040.465 I print_info: n_merges         = 50009
0.00.040.465 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: LF token         = 187 'Ċ'
0.00.040.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: max token length = 1024
0.00.040.468 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.238.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.238.924 I load_tensors: offloading output layer to GPU
0.00.238.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.238.948 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.238.950 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.239.902 I llama_init_from_model: n_seq_max     = 1
0.00.239.904 I llama_init_from_model: n_ctx         = 128
0.00.239.904 I llama_init_from_model: n_ctx_per_seq = 128
0.00.239.905 I llama_init_from_model: n_batch       = 128
0.00.239.905 I llama_init_from_model: n_ubatch      = 128
0.00.239.905 I llama_init_from_model: flash_attn    = 0
0.00.239.906 I llama_init_from_model: freq_base     = 10000.0
0.00.239.906 I llama_init_from_model: freq_scale    = 1
0.00.239.907 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.239.908 I ggml_metal_init: allocating
0.00.239.941 I ggml_metal_init: found device: Apple M4
0.00.239.950 I ggml_metal_init: picking default device: Apple M4
0.00.241.122 I ggml_metal_init: using embedded metal library
0.00.246.057 I ggml_metal_init: GPU name:   Apple M4
0.00.246.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.246.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.246.062 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.246.062 I ggml_metal_init: simdgroup reduction   = true
0.00.246.063 I ggml_metal_init: simdgroup matrix mul. = true
0.00.246.063 I ggml_metal_init: has residency sets    = true
0.00.246.063 I ggml_metal_init: has bfloat            = true
0.00.246.063 I ggml_metal_init: use bfloat            = true
0.00.246.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.246.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.259.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.262.212 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.262.215 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.262.233 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.264.443 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.264.445 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.264.446 I llama_init_from_model: graph nodes  = 967
0.00.264.446 I llama_init_from_model: graph splits = 2
0.00.264.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.264.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.297.129 I 
0.00.297.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.297.169 I perplexity: tokenizing the input ..
0.00.303.068 I perplexity: tokenization took 5.897 ms
0.00.303.083 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.442.577 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.443.869 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.443.888 I llama_perf_context_print:        load time =     286.37 ms
0.00.443.889 I llama_perf_context_print: prompt eval time =     139.20 ms /   128 tokens (    1.09 ms per token,   919.52 tokens per second)
0.00.443.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.443.890 I llama_perf_context_print:       total time =     146.76 ms /   129 tokens
0.00.444.296 I ggml_metal_free: deallocating

real	0m0.460s
user	0m0.073s
sys	0m0.093s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.307 I build: 4695 (fef0cbea) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.401 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.418 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.420 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.420 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.421 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.799 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.801 I llama_model_loader: - type  f32:  194 tensors
0.00.049.801 I llama_model_loader: - type  f16:   98 tensors
0.00.049.802 I print_info: file format = GGUF V3 (latest)
0.00.049.803 I print_info: file type   = all F32 (guessed)
0.00.049.804 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.046 I load: special tokens cache size = 25
0.00.070.416 I load: token to piece cache size = 0.2984 MB
0.00.070.419 I print_info: arch             = gptneox
0.00.070.420 I print_info: vocab_only       = 0
0.00.070.420 I print_info: n_ctx_train      = 2048
0.00.070.420 I print_info: n_embd           = 2048
0.00.070.420 I print_info: n_layer          = 24
0.00.070.423 I print_info: n_head           = 16
0.00.070.425 I print_info: n_head_kv        = 16
0.00.070.425 I print_info: n_rot            = 32
0.00.070.425 I print_info: n_swa            = 0
0.00.070.425 I print_info: n_embd_head_k    = 128
0.00.070.426 I print_info: n_embd_head_v    = 128
0.00.070.426 I print_info: n_gqa            = 1
0.00.070.427 I print_info: n_embd_k_gqa     = 2048
0.00.070.428 I print_info: n_embd_v_gqa     = 2048
0.00.070.429 I print_info: f_norm_eps       = 1.0e-05
0.00.070.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.430 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.430 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.430 I print_info: f_logit_scale    = 0.0e+00
0.00.070.431 I print_info: n_ff             = 8192
0.00.070.431 I print_info: n_expert         = 0
0.00.070.431 I print_info: n_expert_used    = 0
0.00.070.431 I print_info: causal attn      = 1
0.00.070.431 I print_info: pooling type     = 0
0.00.070.432 I print_info: rope type        = 2
0.00.070.432 I print_info: rope scaling     = linear
0.00.070.433 I print_info: freq_base_train  = 10000.0
0.00.070.433 I print_info: freq_scale_train = 1
0.00.070.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.433 I print_info: rope_finetuned   = unknown
0.00.070.434 I print_info: ssm_d_conv       = 0
0.00.070.434 I print_info: ssm_d_inner      = 0
0.00.070.434 I print_info: ssm_d_state      = 0
0.00.070.434 I print_info: ssm_dt_rank      = 0
0.00.070.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.434 I print_info: model type       = 1.4B
0.00.070.435 I print_info: model params     = 1.41 B
0.00.070.435 I print_info: general.name     = 1.4B
0.00.070.435 I print_info: vocab type       = BPE
0.00.070.436 I print_info: n_vocab          = 50304
0.00.070.436 I print_info: n_merges         = 50009
0.00.070.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.437 I print_info: LF token         = 187 'Ċ'
0.00.070.437 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.438 I print_info: max token length = 1024
0.00.070.438 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.482.274 I load_tensors: offloading 24 repeating layers to GPU
0.01.482.282 I load_tensors: offloading output layer to GPU
0.01.482.284 I load_tensors: offloaded 25/25 layers to GPU
0.01.482.307 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.482.309 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.483.040 I llama_init_from_model: n_seq_max     = 1
0.01.483.041 I llama_init_from_model: n_ctx         = 128
0.01.483.042 I llama_init_from_model: n_ctx_per_seq = 128
0.01.483.042 I llama_init_from_model: n_batch       = 128
0.01.483.042 I llama_init_from_model: n_ubatch      = 128
0.01.483.042 I llama_init_from_model: flash_attn    = 0
0.01.483.043 I llama_init_from_model: freq_base     = 10000.0
0.01.483.043 I llama_init_from_model: freq_scale    = 1
0.01.483.044 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.483.046 I ggml_metal_init: allocating
0.01.483.080 I ggml_metal_init: found device: Apple M4
0.01.483.087 I ggml_metal_init: picking default device: Apple M4
0.01.484.122 I ggml_metal_init: using embedded metal library
0.01.487.624 I ggml_metal_init: GPU name:   Apple M4
0.01.487.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.487.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.487.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.487.628 I ggml_metal_init: simdgroup reduction   = true
0.01.487.628 I ggml_metal_init: simdgroup matrix mul. = true
0.01.487.628 I ggml_metal_init: has residency sets    = true
0.01.487.628 I ggml_metal_init: has bfloat            = true
0.01.487.628 I ggml_metal_init: use bfloat            = true
0.01.487.629 I ggml_metal_init: hasUnifiedMemory      = true
0.01.487.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.500.425 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.502.052 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.502.054 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.502.067 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.503.676 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.503.677 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.503.678 I llama_init_from_model: graph nodes  = 967
0.01.503.678 I llama_init_from_model: graph splits = 2
0.01.503.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.503.680 I 
0.01.503.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.503.707 I compute_imatrix: tokenizing the input ..
0.01.507.469 I compute_imatrix: tokenization took 3.762 ms
0.01.507.471 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.769.735 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.772.228 I llama_perf_context_print:        load time =    1748.82 ms
0.01.772.230 I llama_perf_context_print: prompt eval time =     261.22 ms /   128 tokens (    2.04 ms per token,   490.02 tokens per second)
0.01.772.231 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.772.231 I llama_perf_context_print:       total time =    1751.31 ms /   129 tokens
0.01.772.736 I ggml_metal_free: deallocating

real	0m1.965s
user	0m0.126s
sys	0m0.356s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4695 (fef0cbea)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150209240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150209950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150209f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15020a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15020aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15020b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15020b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15020bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15020c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15020c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15020cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15020d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15020db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15020e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15020eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15020f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15020f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150210060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150210780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150210f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150211670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150211d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1502124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150212d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150213470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150213730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150213d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1502149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150214ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1502151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150215650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150215910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1502161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1502166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1502169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150216e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1502172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150217780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150217c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1502180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150218560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150218a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150218ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150219340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150219600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150219c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15021a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15021ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15021b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15021b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15021bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15021c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15021c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15021cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15021d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15021dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15021e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15021e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15021e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15021f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15021f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15021f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15021fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150220230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1502206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150220b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150221010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1502214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150221950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150221df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150222290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150222730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150222bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150223120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150223670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150223bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150224110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150224660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150224bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150225100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150225650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150225ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1502260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150226640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150226b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1502270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150227630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150227b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1502280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150228620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150228b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1502290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150229610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150229b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15022a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15022a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15022ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15021a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15022afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15022b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15022bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15022c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15022c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15022ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15022d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15022d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15022dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15022e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15022e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15022ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15022f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15022f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15022fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150230120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1502305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150230a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150230f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1502313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150231840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150231ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150232180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150232620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150232ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150232f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150233400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1502338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150233d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1502341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150234680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150234b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150234fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150235460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150235900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150235da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150236240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1502366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150236b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150237020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1502374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150237960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150237e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1502382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150238740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150238be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150239080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150239520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1502399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150239e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15023a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15023a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15023ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15023b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15023b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15023ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15023bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15023c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15023c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15023cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15023d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15023d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15023da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15023df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15023e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15023e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15023ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15023f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15023f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15023fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15023ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150240420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1502408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150240d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150241200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1502416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150241b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150241fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150242480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150242920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150242dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150243260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150243700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150243ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150244040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1502444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150244980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150244e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1502452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150245760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150245c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1502460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150246540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1502469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150246e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1502473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150247920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150247e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1502483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150248680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150248c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1502492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1502498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15024a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15024a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15024a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15024ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15024b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15024bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15024c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15024c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15024c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15024d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15024d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15024dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15024e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15024e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15024ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15024f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15024f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15024fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150250170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1502506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150250c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150251160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1502516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150251c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150252150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1502526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150252bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150253140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150253690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150253be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150254130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150254680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150254bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150255120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150255670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150255bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150256110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150256660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150256bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150257100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150257650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150257ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1502580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150258640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150258b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1502590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150259630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150259b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15025a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15025a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15025ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15025b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15025b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15025bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15025c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15025c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15025cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15025d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15025d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15025db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15025e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15025e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15025eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15025f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15025f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15025fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15025ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150260460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150260900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150260da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150261240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1502616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150261b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150262020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1502624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150262960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150262e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1502632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150263740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150263be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150264080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1502645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150264cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150265410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150265b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150266250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150266510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150266d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150266fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1502675d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.782.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.782.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150267280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150248f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150248940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150249560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15021c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15021c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15021e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15024b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1502139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15021a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15021ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15021b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1502198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15021ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1502129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1502087f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15021d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15021ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15022b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1502667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150215bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150215e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15024b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150249b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150214000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1502142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150214580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150267a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150267cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150267fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150268270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150268530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1502687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150268ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150268d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150269030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1502692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1502695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150269870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150269b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150269df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15026a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15026a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15026a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15026a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15026abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15026ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15026b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15026b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15026b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15026b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15026bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15026bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15026c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15026c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15026c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15026c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15026ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15026cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15026d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15026d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15026d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15026da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15026dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15026dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15026e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15026e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15026e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15026eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15026edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15026f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15026f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15026f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15026f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15026fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15026fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1502700f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1502703b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150270670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150270930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150270bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150270eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150271170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150271430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1502716f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1502719b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150271c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150271f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1502721f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1502724b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150272770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150272a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150272cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150272fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150273270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150273530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1502737f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150273ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150273d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150274030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1502742f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1502745b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150274870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150274b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150274df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1502750b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150275370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150275630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1502758f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150275bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150275e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150276130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1502763f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1502766b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150276970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150276c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150276ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1502771b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150277470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150277730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1502779f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150277cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150277f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150278230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1502784f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1502787b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150278a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150278d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150278ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1502792b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150279570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150279830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150279af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150279db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15027a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15027a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15027a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15027a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15027ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15027ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15027b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15027b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15027b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15027b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15027bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15027beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15027c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15027c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15027c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15027c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15027cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15027cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15027d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15027d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15027d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15027da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15027dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15027dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15027e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15027e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15027e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15027eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15027ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15027f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15027f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15027f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15027f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15027fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15027fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1502800b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150280370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150280630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1502808f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150280bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150280e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150281130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1502813f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1502816b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150281970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150281c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150281ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1502821b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150282470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150282730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1502829f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150282cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150282f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150283230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1502834f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1502837b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150283a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150283d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150283ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1502842b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150284570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150284830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150284af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150284db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150285070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150285330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1502855f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1502858b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150285b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150285e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1502860f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1502863b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150286670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150286930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150286bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150286eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150287480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150287740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150287a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150287cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150287f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150288240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150288500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1502887c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150288a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150288d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150289000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1502892c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150289580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150289840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150289b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150289dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15028a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15028a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15028a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15028a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15028ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15028ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15028b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15028b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15028b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15028b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15028bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15028c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15028c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15028cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15028d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15028d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15028dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15028e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15028e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15028ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15028f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15028f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15028fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150290110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150290660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150290bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150291100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150291650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150291ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1502920f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150292640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150292b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1502930e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150293630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150293b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1502940d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150294620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150294b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1502950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150295610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150295b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150295e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1502960e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1502963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150296810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150296c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1502970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150297560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1502979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150297e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1502982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150298720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150298b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150299000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150299470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1502998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150299d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15029a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15029aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15029b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15029bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15029bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15029c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15029ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15029d030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.842s
user	0m0.278s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4695 (fef0cbea)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13280d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13280db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13280e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13280e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13280ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13280f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13280f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13280fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132810300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132810800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132810d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132811200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132811d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1328124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132812ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132813400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132813b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132814240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132814960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132815130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132815850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132815f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132816690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132816f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132817650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132817910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132817f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132818b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1328190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132819390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132819830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132819af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13281a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13281a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13281ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13281b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13281b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13281b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13281be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13281c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13281c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13281cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13281d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13281d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13281d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13281ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13281e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13281ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13281f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13281f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13281ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132820560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132820b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132821180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132821970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132821e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1328222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132822570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132822b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132823370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132823630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132823f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132824410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1328248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132824d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1328251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132825690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132825b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132825fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132826470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132826910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132827300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132827850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132827da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1328282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132828840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132828d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1328292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132829830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132829d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13282a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13282a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13282ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13282b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13282b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13282bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13282c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13282c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13282cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13282d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13282d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13282dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13282e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13282e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13282ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13281ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13282f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13282f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13282fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1328303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132830940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132830e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1328313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132831930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132831e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1328323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132832920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132832e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1328333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132833910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132833e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132834300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1328347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1328350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132835580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132835a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132835ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132836360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132836800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132836ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132837140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1328375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132837a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132837f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1328383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132838860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132838d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1328391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132839640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132839ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132839f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13283a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13283a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13283ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13283b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13283b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13283bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13283bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13283c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13283c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13283cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13283d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13283d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13283dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13283e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13283e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13283e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13283ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13283f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13283f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13283fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1328400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132840540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1328409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132840e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132841320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1328417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132841c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132842100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1328425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132842a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132842ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132843380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132843820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132843cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132844160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132844600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132844aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132844f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1328453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132845880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132845d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1328461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132846660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132846b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132847440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1328478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132847d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1328486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132848b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132849000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1328494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132849940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132849de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13284a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13284a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13284abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13284b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13284b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13284bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13284c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13284c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13284c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13284ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13284d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13284da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13284e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13284e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13284e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13284eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13284f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13284fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132850290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132850730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132850bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132851380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1328518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132851e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132852370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1328528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132852e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132853360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1328538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132853e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132854350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1328548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132854df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132855340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132855890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132855de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132856330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132856880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132856dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132857320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132857870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132857dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132858310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132858860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132858db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132859300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132859850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132859da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13285a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13285a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13285ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13285b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13285b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13285bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13285c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13285c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13285cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13285d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13285d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13285dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13285e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13285e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13285ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13285f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13285f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13285fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132860290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1328607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132860d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132861280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1328617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132861d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132862270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1328627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132862d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132863260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1328637b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132863d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1328641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132864640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132864ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132864f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132865420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1328658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132865d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132866200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1328666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132866b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132866fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132867480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132867920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132867dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132868260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1328687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132868ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1328695f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132869d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13286a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13286a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13286aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13286b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13286b7b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.093.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f08960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f08dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f09240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f09b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f09f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f0a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f0a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f0ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f0e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f0f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f16b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f1b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f1cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f21020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f29f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f2d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f2e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f2fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f33e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f34300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f35ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f37f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f3a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f3c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f46b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f4a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f4af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f5d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f5e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f5e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f5f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f5f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f5fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f60580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f60ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f61360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f61fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f62e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f63530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131f63fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f648b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13286b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13284d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13284cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13284d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132820820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132820210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132822830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13284f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132817bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13281e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13281efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13281f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13281daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13281fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132816bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132822e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13282f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13286a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132819db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13281a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13284f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13284dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1328181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1328184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132818760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13286bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13286bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13286c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13286c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13286c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13286c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13286cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13286cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13286d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13286d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13286d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13286da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13286dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13286dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13286e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13286e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13286e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13286ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13286ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13286f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13286f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13286f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13286f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13286fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13286fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1328700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132870390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132870650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132870910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132870bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132870e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132871150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132871410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1328716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132871990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132871c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132871f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1328721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132872490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132872750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132872a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132872cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132872f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132873250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132873510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1328737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132873a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132873d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132874010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1328742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132874590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132874850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132874b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132874dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132875090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132875350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132875610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1328758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132875b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132875e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132876110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1328763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132876690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132876950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132876c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132876ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132877190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132877450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132877710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1328779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132877c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132877f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132878210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1328784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132878790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132878a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132878d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132878fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132879290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132879550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132879810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132879ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132879d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13287a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13287a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13287a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13287a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13287ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13287ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13287b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13287b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13287b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13287b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13287bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13287be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13287c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13287c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13287c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13287c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13287cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13287cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13287d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13287d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13287d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13287da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13287dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13287df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13287e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13287e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13287e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13287ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13287ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13287f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13287f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13287f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13287f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13287fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13287fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132880090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132880350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132880610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1328808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132880b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132880e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132881110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1328813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132881690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132881950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132881c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132881ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132882190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132882450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132882710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1328829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132882c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132882f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132883210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1328834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132883790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132883a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132883d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132883fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132884290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132884550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132884810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132884ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132884d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132885050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132885310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1328855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132885890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132885b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132885e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1328860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132886390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132886650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132886910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132886bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132886e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132887150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132887410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1328876d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132887990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132887c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132887f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1328881d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132888490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132888750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132888a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132888cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132888f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132889250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132889510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1328897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132889a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132889d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13288a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13288a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13288a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13288a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13288ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13288add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13288b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13288b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13288b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13288c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13288c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13288c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13288cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13288cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13288d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13288d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13288dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13288e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13288e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13288e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13288ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13288f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13288f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13288fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132890010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132890480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1328908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132890d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1328911d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132891640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132891ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132891f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132892390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132892800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132892c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1328930e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132893550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1328939c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132893e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1328942a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132894710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132894b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132894ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132895460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1328958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132895d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1328961b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132896620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132896a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132896f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132897370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1328977e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132897c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1328980c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132898530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1328989a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132898e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132899280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1328996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132899b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132899fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13289a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13289a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13289ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13289b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13289b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13289ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13289bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13289c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13289c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13289cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13289d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13289d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13289d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13289ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13289e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13289e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13289eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13289efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13289f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13289f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13289fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1328a0770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1328a0e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1328a15b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1328a1cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1328a1f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1328a2780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1328a2a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1328a3050 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.903s
user	0m0.229s
sys	0m0.141s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
