### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.66 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.20 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    2.43 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.75 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    1.02 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.37 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    1.09 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  177.47 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    1.05 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   25.99 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.38 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.25 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 225.10 sec*proc (27 tests)

Total Test time (real) = 225.11 sec

real	3m45.217s
user	7m32.948s
sys	0m6.362s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.97 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.24 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.48 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.42 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.07 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.22 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.65 sec*proc (27 tests)

Total Test time (real) =  51.66 sec

real	0m51.676s
user	1m11.760s
sys	0m5.738s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.104 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.724 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.639 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.651 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.652 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.654 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.654 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.656 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.657 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.657 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.658 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.659 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.663 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.664 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.665 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.665 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.666 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.667 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.668 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.597 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.599 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.600 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.600 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.601 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.601 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.601 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.602 I llama_model_loader: - type  f32:  124 tensors
0.00.032.603 I llama_model_loader: - type  f16:   73 tensors
0.00.037.059 I llm_load_vocab: special tokens cache size = 5
0.00.039.535 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.540 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.540 I llm_load_print_meta: arch             = bert
0.00.039.540 I llm_load_print_meta: vocab type       = WPM
0.00.039.541 I llm_load_print_meta: n_vocab          = 30522
0.00.039.541 I llm_load_print_meta: n_merges         = 0
0.00.039.541 I llm_load_print_meta: vocab_only       = 0
0.00.039.542 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.542 I llm_load_print_meta: n_embd           = 384
0.00.039.542 I llm_load_print_meta: n_layer          = 12
0.00.039.559 I llm_load_print_meta: n_head           = 12
0.00.039.560 I llm_load_print_meta: n_head_kv        = 12
0.00.039.560 I llm_load_print_meta: n_rot            = 32
0.00.039.561 I llm_load_print_meta: n_swa            = 0
0.00.039.561 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.561 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.562 I llm_load_print_meta: n_gqa            = 1
0.00.039.563 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.567 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.568 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.569 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.569 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.570 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.570 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.571 I llm_load_print_meta: n_ff             = 1536
0.00.039.571 I llm_load_print_meta: n_expert         = 0
0.00.039.572 I llm_load_print_meta: n_expert_used    = 0
0.00.039.572 I llm_load_print_meta: causal attn      = 0
0.00.039.572 I llm_load_print_meta: pooling type     = 2
0.00.039.572 I llm_load_print_meta: rope type        = 2
0.00.039.573 I llm_load_print_meta: rope scaling     = linear
0.00.039.573 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.574 I llm_load_print_meta: freq_scale_train = 1
0.00.039.574 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.575 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.576 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.576 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.577 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.577 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.577 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.577 I llm_load_print_meta: model type       = 33M
0.00.039.578 I llm_load_print_meta: model ftype      = F16
0.00.039.578 I llm_load_print_meta: model params     = 33.21 M
0.00.039.579 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.580 I llm_load_print_meta: general.name     = Bge Small
0.00.039.582 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.582 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.582 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.583 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.583 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.584 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.584 I llm_load_print_meta: max token length = 21
0.00.041.791 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.798 I llm_load_tensors: offloading output layer to GPU
0.00.041.799 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.826 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.827 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.396 I llama_new_context_with_model: n_seq_max     = 1
0.00.042.398 I llama_new_context_with_model: n_ctx         = 512
0.00.042.398 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.042.398 I llama_new_context_with_model: n_batch       = 2048
0.00.042.398 I llama_new_context_with_model: n_ubatch      = 2048
0.00.042.399 I llama_new_context_with_model: flash_attn    = 0
0.00.042.399 I llama_new_context_with_model: freq_base     = 10000.0
0.00.042.400 I llama_new_context_with_model: freq_scale    = 1
0.00.042.400 I ggml_metal_init: allocating
0.00.042.405 I ggml_metal_init: found device: Apple M4
0.00.042.407 I ggml_metal_init: picking default device: Apple M4
0.00.043.263 I ggml_metal_init: using embedded metal library
0.00.047.568 I ggml_metal_init: GPU name:   Apple M4
0.00.047.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.572 I ggml_metal_init: simdgroup reduction   = true
0.00.047.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.572 I ggml_metal_init: has bfloat            = true
0.00.047.573 I ggml_metal_init: use bfloat            = true
0.00.047.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.309 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.061.311 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.061.313 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.062.153 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.062.155 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.062.155 I llama_new_context_with_model: graph nodes  = 429
0.00.062.155 I llama_new_context_with_model: graph splits = 2
0.00.062.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.062.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.702 I 
0.00.068.728 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.069.397 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.385 I llama_perf_context_print:        load time =      47.97 ms
0.00.073.386 I llama_perf_context_print: prompt eval time =       3.86 ms /     9 tokens (    0.43 ms per token,  2332.21 tokens per second)
0.00.073.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.387 I llama_perf_context_print:       total time =       4.68 ms /    10 tokens
0.00.073.514 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.052s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.999 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.072 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.077 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.078 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.078 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.078 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.079 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.080 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.080 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.080 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.080 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.082 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.083 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.083 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.083 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.083 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.084 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.084 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.509 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.178 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.179 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.179 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.180 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.180 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.180 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.181 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.181 I llama_model_loader: - type  f32:  124 tensors
0.00.014.181 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.727 I llm_load_vocab: special tokens cache size = 5
0.00.018.058 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.061 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.062 I llm_load_print_meta: arch             = bert
0.00.018.062 I llm_load_print_meta: vocab type       = WPM
0.00.018.062 I llm_load_print_meta: n_vocab          = 30522
0.00.018.062 I llm_load_print_meta: n_merges         = 0
0.00.018.062 I llm_load_print_meta: vocab_only       = 0
0.00.018.062 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.063 I llm_load_print_meta: n_embd           = 384
0.00.018.063 I llm_load_print_meta: n_layer          = 12
0.00.018.072 I llm_load_print_meta: n_head           = 12
0.00.018.073 I llm_load_print_meta: n_head_kv        = 12
0.00.018.073 I llm_load_print_meta: n_rot            = 32
0.00.018.073 I llm_load_print_meta: n_swa            = 0
0.00.018.073 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.073 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.074 I llm_load_print_meta: n_gqa            = 1
0.00.018.074 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.075 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.075 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.076 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.076 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.076 I llm_load_print_meta: n_ff             = 1536
0.00.018.077 I llm_load_print_meta: n_expert         = 0
0.00.018.077 I llm_load_print_meta: n_expert_used    = 0
0.00.018.077 I llm_load_print_meta: causal attn      = 0
0.00.018.077 I llm_load_print_meta: pooling type     = 2
0.00.018.077 I llm_load_print_meta: rope type        = 2
0.00.018.077 I llm_load_print_meta: rope scaling     = linear
0.00.018.078 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.078 I llm_load_print_meta: freq_scale_train = 1
0.00.018.078 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.079 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.079 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.079 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.079 I llm_load_print_meta: model type       = 33M
0.00.018.079 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.080 I llm_load_print_meta: model params     = 33.21 M
0.00.018.080 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.080 I llm_load_print_meta: general.name     = Bge Small
0.00.018.081 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.081 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.081 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.081 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.081 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.081 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.082 I llm_load_print_meta: max token length = 21
0.00.019.383 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.383 I llm_load_tensors: offloading output layer to GPU
0.00.019.384 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.392 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.393 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.770 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.770 I llama_new_context_with_model: n_ctx         = 512
0.00.019.771 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.771 I llama_new_context_with_model: n_batch       = 2048
0.00.019.771 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.771 I llama_new_context_with_model: flash_attn    = 0
0.00.019.772 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.772 I llama_new_context_with_model: freq_scale    = 1
0.00.019.772 I ggml_metal_init: allocating
0.00.019.775 I ggml_metal_init: found device: Apple M4
0.00.019.776 I ggml_metal_init: picking default device: Apple M4
0.00.020.385 I ggml_metal_init: using embedded metal library
0.00.022.898 I ggml_metal_init: GPU name:   Apple M4
0.00.022.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.901 I ggml_metal_init: simdgroup reduction   = true
0.00.022.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.901 I ggml_metal_init: has bfloat            = true
0.00.022.901 I ggml_metal_init: use bfloat            = true
0.00.022.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.591 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.593 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.594 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.209 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.210 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.210 I llama_new_context_with_model: graph nodes  = 429
0.00.034.211 I llama_new_context_with_model: graph splits = 2
0.00.034.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.799 I 
0.00.038.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.366 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.695 I llama_perf_context_print:        load time =      29.80 ms
0.00.042.696 I llama_perf_context_print: prompt eval time =       3.21 ms /     9 tokens (    0.36 ms per token,  2807.24 tokens per second)
0.00.042.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.698 I llama_perf_context_print:       total time =       3.90 ms /    10 tokens
0.00.042.855 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.128 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.769 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.777 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.787 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.788 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.789 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.790 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.791 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.792 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.792 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.793 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.796 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.797 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.798 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.300 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.303 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.304 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.304 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.304 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.305 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.305 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.306 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.306 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.307 I llama_model_loader: - type  f32:   41 tensors
0.00.050.307 I llama_model_loader: - type  f16:   29 tensors
0.00.068.941 W llm_load_vocab: empty token at index 5
0.00.074.028 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.350 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.376 I llm_load_vocab: special tokens cache size = 5
0.00.342.646 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.342.661 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.342.661 I llm_load_print_meta: arch             = jina-bert-v2
0.00.342.662 I llm_load_print_meta: vocab type       = BPE
0.00.342.662 I llm_load_print_meta: n_vocab          = 61056
0.00.342.663 I llm_load_print_meta: n_merges         = 39382
0.00.342.663 I llm_load_print_meta: vocab_only       = 0
0.00.342.663 I llm_load_print_meta: n_ctx_train      = 8192
0.00.342.663 I llm_load_print_meta: n_embd           = 384
0.00.342.663 I llm_load_print_meta: n_layer          = 4
0.00.342.694 I llm_load_print_meta: n_head           = 12
0.00.342.695 I llm_load_print_meta: n_head_kv        = 12
0.00.342.695 I llm_load_print_meta: n_rot            = 32
0.00.342.696 I llm_load_print_meta: n_swa            = 0
0.00.342.696 I llm_load_print_meta: n_embd_head_k    = 32
0.00.342.696 I llm_load_print_meta: n_embd_head_v    = 32
0.00.342.698 I llm_load_print_meta: n_gqa            = 1
0.00.342.698 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.342.699 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.342.700 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.342.701 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.342.705 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.342.705 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.342.706 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.342.706 I llm_load_print_meta: n_ff             = 1536
0.00.342.706 I llm_load_print_meta: n_expert         = 0
0.00.342.707 I llm_load_print_meta: n_expert_used    = 0
0.00.342.707 I llm_load_print_meta: causal attn      = 0
0.00.342.707 I llm_load_print_meta: pooling type     = -1
0.00.342.707 I llm_load_print_meta: rope type        = -1
0.00.342.708 I llm_load_print_meta: rope scaling     = linear
0.00.342.708 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.342.708 I llm_load_print_meta: freq_scale_train = 1
0.00.342.710 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.342.710 I llm_load_print_meta: rope_finetuned   = unknown
0.00.342.710 I llm_load_print_meta: ssm_d_conv       = 0
0.00.342.710 I llm_load_print_meta: ssm_d_inner      = 0
0.00.342.710 I llm_load_print_meta: ssm_d_state      = 0
0.00.342.710 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.342.710 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.342.711 I llm_load_print_meta: model type       = 33M
0.00.342.711 I llm_load_print_meta: model ftype      = F16
0.00.342.711 I llm_load_print_meta: model params     = 32.90 M
0.00.342.712 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.342.712 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.342.713 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.342.713 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.342.713 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.342.713 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.342.713 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.342.713 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.342.714 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.342.714 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.342.714 I llm_load_print_meta: max token length = 45
0.00.343.644 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.343.644 I llm_load_tensors: offloading output layer to GPU
0.00.343.644 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.343.663 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.343.664 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.344.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.344.393 I llama_new_context_with_model: n_ctx         = 8192
0.00.344.394 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.344.394 I llama_new_context_with_model: n_batch       = 2048
0.00.344.394 I llama_new_context_with_model: n_ubatch      = 2048
0.00.344.394 I llama_new_context_with_model: flash_attn    = 0
0.00.344.395 I llama_new_context_with_model: freq_base     = 10000.0
0.00.344.395 I llama_new_context_with_model: freq_scale    = 1
0.00.344.396 I ggml_metal_init: allocating
0.00.344.399 I ggml_metal_init: found device: Apple M4
0.00.344.401 I ggml_metal_init: picking default device: Apple M4
0.00.345.070 I ggml_metal_init: using embedded metal library
0.00.347.621 I ggml_metal_init: GPU name:   Apple M4
0.00.347.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.624 I ggml_metal_init: simdgroup reduction   = true
0.00.347.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.624 I ggml_metal_init: has bfloat            = true
0.00.347.624 I ggml_metal_init: use bfloat            = true
0.00.347.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.360.334 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.360.339 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.360.341 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.360.933 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.360.934 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.360.934 I llama_new_context_with_model: graph nodes  = 154
0.00.360.934 I llama_new_context_with_model: graph splits = 2
0.00.360.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.360.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.372.560 I 
0.00.372.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.372.827 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.372.828 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.372.831 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.372.831 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.372.834 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.372.835 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.373.349 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.376.890 I llama_perf_context_print:        load time =     348.67 ms
0.00.376.891 I llama_perf_context_print: prompt eval time =       3.53 ms /    62 tokens (    0.06 ms per token, 17553.79 tokens per second)
0.00.376.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.376.892 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.377.104 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.349s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.152 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.278 I main: llama backend init
0.00.000.289 I main: load the model and apply lora adapter, if any
0.00.035.162 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.969 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.996 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.997 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.048.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.048.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.048.001 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.048.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.048.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.048.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.048.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.048.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.065.191 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.193 I llama_model_loader: - type  f32:  194 tensors
0.00.065.193 I llama_model_loader: - type  f16:   98 tensors
0.00.093.302 I llm_load_vocab: special tokens cache size = 25
0.00.099.886 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.888 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.889 I llm_load_print_meta: arch             = gptneox
0.00.099.889 I llm_load_print_meta: vocab type       = BPE
0.00.099.889 I llm_load_print_meta: n_vocab          = 50304
0.00.099.889 I llm_load_print_meta: n_merges         = 50009
0.00.099.890 I llm_load_print_meta: vocab_only       = 0
0.00.099.890 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.890 I llm_load_print_meta: n_embd           = 2048
0.00.099.890 I llm_load_print_meta: n_layer          = 24
0.00.099.903 I llm_load_print_meta: n_head           = 16
0.00.099.904 I llm_load_print_meta: n_head_kv        = 16
0.00.099.904 I llm_load_print_meta: n_rot            = 32
0.00.099.904 I llm_load_print_meta: n_swa            = 0
0.00.099.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.905 I llm_load_print_meta: n_gqa            = 1
0.00.099.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.906 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.907 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.907 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.907 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.907 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.908 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.908 I llm_load_print_meta: n_ff             = 8192
0.00.099.909 I llm_load_print_meta: n_expert         = 0
0.00.099.909 I llm_load_print_meta: n_expert_used    = 0
0.00.099.909 I llm_load_print_meta: causal attn      = 1
0.00.099.909 I llm_load_print_meta: pooling type     = 0
0.00.099.909 I llm_load_print_meta: rope type        = 2
0.00.099.909 I llm_load_print_meta: rope scaling     = linear
0.00.099.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.910 I llm_load_print_meta: freq_scale_train = 1
0.00.099.910 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.910 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.910 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.911 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.911 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.911 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.913 I llm_load_print_meta: model type       = 1.4B
0.00.099.913 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.913 I llm_load_print_meta: model params     = 1.41 B
0.00.099.914 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.914 I llm_load_print_meta: general.name     = 1.4B
0.00.099.914 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.914 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.915 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.916 I llm_load_print_meta: LF token         = 128 ''
0.00.099.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.917 I llm_load_print_meta: max token length = 1024
0.00.101.768 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.769 I llm_load_tensors: offloading output layer to GPU
0.00.101.769 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.787 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.788 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.664 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.664 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.664 I llama_new_context_with_model: n_batch       = 2048
0.00.102.664 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.665 I llama_new_context_with_model: flash_attn    = 0
0.00.102.665 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.665 I llama_new_context_with_model: freq_scale    = 1
0.00.102.666 I ggml_metal_init: allocating
0.00.102.669 I ggml_metal_init: found device: Apple M4
0.00.102.671 I ggml_metal_init: picking default device: Apple M4
0.00.103.321 I ggml_metal_init: using embedded metal library
0.00.112.624 I ggml_metal_init: GPU name:   Apple M4
0.00.112.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.112.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.112.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.112.627 I ggml_metal_init: simdgroup reduction   = true
0.00.112.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.112.627 I ggml_metal_init: has bfloat            = true
0.00.112.627 I ggml_metal_init: use bfloat            = true
0.00.112.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.112.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.160.662 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.667 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.688 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.646 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.648 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.649 I llama_new_context_with_model: graph nodes  = 967
0.00.161.649 I llama_new_context_with_model: graph splits = 2
0.00.161.689 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.245.819 I main: llama threadpool init, n_threads = 4
0.00.245.862 I 
0.00.245.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.245.899 I 
0.00.245.970 I sampler seed: 1234
0.00.245.974 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.246.007 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.246.009 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.246.009 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.094.414 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.02.094.415 I llama_perf_context_print:        load time =     210.64 ms
0.02.094.416 I llama_perf_context_print: prompt eval time =      43.95 ms /     7 tokens (    6.28 ms per token,   159.26 tokens per second)
0.02.094.417 I llama_perf_context_print:        eval time =    1801.56 ms /    63 runs   (   28.60 ms per token,    34.97 tokens per second)
0.02.094.418 I llama_perf_context_print:       total time =    1848.60 ms /    70 tokens
0.02.094.585 I ggml_metal_free: deallocating

real	0m2.446s
user	0m0.142s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.599 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.110 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.653 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.544 I llama_model_loader: - type  f32:  194 tensors
0.00.055.544 I llama_model_loader: - type  f16:   98 tensors
0.00.085.803 I llm_load_vocab: special tokens cache size = 25
0.00.092.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.504 I llm_load_print_meta: arch             = gptneox
0.00.092.505 I llm_load_print_meta: vocab type       = BPE
0.00.092.505 I llm_load_print_meta: n_vocab          = 50304
0.00.092.505 I llm_load_print_meta: n_merges         = 50009
0.00.092.505 I llm_load_print_meta: vocab_only       = 0
0.00.092.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.506 I llm_load_print_meta: n_embd           = 2048
0.00.092.506 I llm_load_print_meta: n_layer          = 24
0.00.092.520 I llm_load_print_meta: n_head           = 16
0.00.092.521 I llm_load_print_meta: n_head_kv        = 16
0.00.092.522 I llm_load_print_meta: n_rot            = 32
0.00.092.522 I llm_load_print_meta: n_swa            = 0
0.00.092.522 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.527 I llm_load_print_meta: n_gqa            = 1
0.00.092.528 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.530 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.530 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.530 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.530 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.531 I llm_load_print_meta: n_ff             = 8192
0.00.092.532 I llm_load_print_meta: n_expert         = 0
0.00.092.532 I llm_load_print_meta: n_expert_used    = 0
0.00.092.532 I llm_load_print_meta: causal attn      = 1
0.00.092.532 I llm_load_print_meta: pooling type     = 0
0.00.092.532 I llm_load_print_meta: rope type        = 2
0.00.092.532 I llm_load_print_meta: rope scaling     = linear
0.00.092.533 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.533 I llm_load_print_meta: freq_scale_train = 1
0.00.092.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.534 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.535 I llm_load_print_meta: model type       = 1.4B
0.00.092.536 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.536 I llm_load_print_meta: model params     = 1.41 B
0.00.092.536 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.537 I llm_load_print_meta: general.name     = 1.4B
0.00.092.537 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.538 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.539 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.539 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.539 I llm_load_print_meta: LF token         = 128 ''
0.00.092.539 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.539 I llm_load_print_meta: max token length = 1024
0.00.095.118 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.118 I llm_load_tensors: offloading output layer to GPU
0.00.095.118 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.129 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.130 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.060 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.060 I llama_new_context_with_model: n_ctx         = 128
0.00.096.060 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.061 I llama_new_context_with_model: n_batch       = 128
0.00.096.061 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.061 I llama_new_context_with_model: flash_attn    = 0
0.00.096.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.062 I llama_new_context_with_model: freq_scale    = 1
0.00.096.062 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.062 I ggml_metal_init: allocating
0.00.096.065 I ggml_metal_init: found device: Apple M4
0.00.096.067 I ggml_metal_init: picking default device: Apple M4
0.00.096.668 I ggml_metal_init: using embedded metal library
0.00.099.220 I ggml_metal_init: GPU name:   Apple M4
0.00.099.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.223 I ggml_metal_init: simdgroup reduction   = true
0.00.099.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.223 I ggml_metal_init: has bfloat            = true
0.00.099.223 I ggml_metal_init: use bfloat            = true
0.00.099.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.031 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.854 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.855 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.855 I llama_new_context_with_model: graph nodes  = 967
0.00.110.856 I llama_new_context_with_model: graph splits = 2
0.00.110.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.146.743 I 
0.01.146.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.146.836 I perplexity: tokenizing the input ..
0.01.160.367 I perplexity: tokenization took 13.528 ms
0.01.160.379 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.282.970 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.284.819 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.284.844 I llama_perf_context_print:        load time =    1121.61 ms
0.01.284.846 I llama_perf_context_print: prompt eval time =     121.70 ms /   128 tokens (    0.95 ms per token,  1051.76 tokens per second)
0.01.284.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.284.848 I llama_perf_context_print:       total time =     138.11 ms /   129 tokens
0.01.285.566 I ggml_metal_free: deallocating

real	0m1.501s
user	0m0.126s
sys	0m0.219s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.051 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.933 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.941 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.941 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.943 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.943 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.785 I llama_model_loader: - type  f32:  194 tensors
0.00.027.785 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.153 I llm_load_vocab: special tokens cache size = 25
0.00.055.166 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.170 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.170 I llm_load_print_meta: arch             = gptneox
0.00.055.171 I llm_load_print_meta: vocab type       = BPE
0.00.055.171 I llm_load_print_meta: n_vocab          = 50304
0.00.055.171 I llm_load_print_meta: n_merges         = 50009
0.00.055.172 I llm_load_print_meta: vocab_only       = 0
0.00.055.173 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.173 I llm_load_print_meta: n_embd           = 2048
0.00.055.174 I llm_load_print_meta: n_layer          = 24
0.00.055.192 I llm_load_print_meta: n_head           = 16
0.00.055.194 I llm_load_print_meta: n_head_kv        = 16
0.00.055.194 I llm_load_print_meta: n_rot            = 32
0.00.055.194 I llm_load_print_meta: n_swa            = 0
0.00.055.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.195 I llm_load_print_meta: n_gqa            = 1
0.00.055.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.196 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.197 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.197 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.198 I llm_load_print_meta: n_ff             = 8192
0.00.055.198 I llm_load_print_meta: n_expert         = 0
0.00.055.198 I llm_load_print_meta: n_expert_used    = 0
0.00.055.199 I llm_load_print_meta: causal attn      = 1
0.00.055.199 I llm_load_print_meta: pooling type     = 0
0.00.055.199 I llm_load_print_meta: rope type        = 2
0.00.055.200 I llm_load_print_meta: rope scaling     = linear
0.00.055.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.201 I llm_load_print_meta: freq_scale_train = 1
0.00.055.201 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.202 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.202 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.202 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.202 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.206 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.206 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.206 I llm_load_print_meta: model type       = 1.4B
0.00.055.207 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.207 I llm_load_print_meta: model params     = 1.41 B
0.00.055.208 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.208 I llm_load_print_meta: general.name     = 1.4B
0.00.055.208 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.208 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.208 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.208 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.210 I llm_load_print_meta: LF token         = 128 ''
0.00.055.210 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.210 I llm_load_print_meta: max token length = 1024
0.00.057.620 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.620 I llm_load_tensors: offloading output layer to GPU
0.00.057.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.632 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.633 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.596 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.597 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.597 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.597 I llama_new_context_with_model: n_batch       = 2048
0.00.058.597 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.598 I llama_new_context_with_model: flash_attn    = 0
0.00.058.598 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.598 I llama_new_context_with_model: freq_scale    = 1
0.00.058.599 I ggml_metal_init: allocating
0.00.058.605 I ggml_metal_init: found device: Apple M4
0.00.058.607 I ggml_metal_init: picking default device: Apple M4
0.00.059.307 I ggml_metal_init: using embedded metal library
0.00.061.882 I ggml_metal_init: GPU name:   Apple M4
0.00.061.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.884 I ggml_metal_init: simdgroup reduction   = true
0.00.061.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.885 I ggml_metal_init: has bfloat            = true
0.00.061.885 I ggml_metal_init: use bfloat            = true
0.00.061.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.164 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.177 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.204 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.284 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.286 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.286 I llama_new_context_with_model: graph nodes  = 967
0.00.099.287 I llama_new_context_with_model: graph splits = 2
0.00.099.314 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.233.278 I main: llama threadpool init, n_threads = 4
0.01.233.311 I 
0.01.233.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.233.341 I 
0.01.233.580 I sampler seed: 1234
0.01.233.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.233.600 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.233.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.233.600 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.330.464 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.02.330.465 I llama_perf_context_print:        load time =    1223.22 ms
0.02.330.467 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.82 tokens per second)
0.02.330.467 I llama_perf_context_print:        eval time =    1050.44 ms /    63 runs   (   16.67 ms per token,    59.97 tokens per second)
0.02.330.468 I llama_perf_context_print:       total time =    1097.19 ms /    70 tokens
0.02.330.687 I ggml_metal_free: deallocating

real	0m2.348s
user	0m0.111s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.137 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.982 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.126 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.126 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.127 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.503 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.477 I llama_model_loader: - type  f32:  194 tensors
0.00.033.477 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.869 I llm_load_vocab: special tokens cache size = 25
0.00.066.546 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.549 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.549 I llm_load_print_meta: arch             = gptneox
0.00.066.549 I llm_load_print_meta: vocab type       = BPE
0.00.066.549 I llm_load_print_meta: n_vocab          = 50304
0.00.066.550 I llm_load_print_meta: n_merges         = 50009
0.00.066.550 I llm_load_print_meta: vocab_only       = 0
0.00.066.550 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.550 I llm_load_print_meta: n_embd           = 2048
0.00.066.550 I llm_load_print_meta: n_layer          = 24
0.00.066.567 I llm_load_print_meta: n_head           = 16
0.00.066.568 I llm_load_print_meta: n_head_kv        = 16
0.00.066.568 I llm_load_print_meta: n_rot            = 32
0.00.066.568 I llm_load_print_meta: n_swa            = 0
0.00.066.569 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.569 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.569 I llm_load_print_meta: n_gqa            = 1
0.00.066.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.570 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.571 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.571 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.574 I llm_load_print_meta: n_ff             = 8192
0.00.066.575 I llm_load_print_meta: n_expert         = 0
0.00.066.576 I llm_load_print_meta: n_expert_used    = 0
0.00.066.576 I llm_load_print_meta: causal attn      = 1
0.00.066.576 I llm_load_print_meta: pooling type     = 0
0.00.066.577 I llm_load_print_meta: rope type        = 2
0.00.066.577 I llm_load_print_meta: rope scaling     = linear
0.00.066.577 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.577 I llm_load_print_meta: freq_scale_train = 1
0.00.066.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.578 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.578 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.578 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.578 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.583 I llm_load_print_meta: model type       = 1.4B
0.00.066.584 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.585 I llm_load_print_meta: model params     = 1.41 B
0.00.066.585 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.585 I llm_load_print_meta: general.name     = 1.4B
0.00.066.586 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.586 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.586 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.586 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.587 I llm_load_print_meta: LF token         = 128 ''
0.00.066.587 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.587 I llm_load_print_meta: max token length = 1024
0.00.068.999 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.999 I llm_load_tensors: offloading output layer to GPU
0.00.069.000 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.011 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.012 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.967 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.968 I llama_new_context_with_model: n_ctx         = 128
0.00.069.968 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.968 I llama_new_context_with_model: n_batch       = 128
0.00.069.968 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.968 I llama_new_context_with_model: flash_attn    = 0
0.00.069.969 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.969 I llama_new_context_with_model: freq_scale    = 1
0.00.069.970 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.970 I ggml_metal_init: allocating
0.00.069.977 I ggml_metal_init: found device: Apple M4
0.00.069.980 I ggml_metal_init: picking default device: Apple M4
0.00.070.662 I ggml_metal_init: using embedded metal library
0.00.073.408 I ggml_metal_init: GPU name:   Apple M4
0.00.073.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.411 I ggml_metal_init: simdgroup reduction   = true
0.00.073.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.411 I ggml_metal_init: has bfloat            = true
0.00.073.411 I ggml_metal_init: use bfloat            = true
0.00.073.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.414 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.195 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.214 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.212 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.213 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.213 I llama_new_context_with_model: graph nodes  = 967
0.00.086.214 I llama_new_context_with_model: graph splits = 2
0.00.086.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.051 I 
0.00.896.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.091 I perplexity: tokenizing the input ..
0.00.904.371 I perplexity: tokenization took 8.278 ms
0.00.904.374 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.029.163 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.030.407 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.030.419 I llama_perf_context_print:        load time =     884.07 ms
0.01.030.420 I llama_perf_context_print: prompt eval time =     124.55 ms /   128 tokens (    0.97 ms per token,  1027.69 tokens per second)
0.01.030.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.030.421 I llama_perf_context_print:       total time =     134.37 ms /   129 tokens
0.01.030.763 I ggml_metal_free: deallocating

real	0m1.049s
user	0m0.095s
sys	0m0.146s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.989 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.304 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.307 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.308 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.308 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.069 I llama_model_loader: - type  f32:  194 tensors
0.00.025.069 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.228 I llm_load_vocab: special tokens cache size = 25
0.00.052.216 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.219 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.219 I llm_load_print_meta: arch             = gptneox
0.00.052.219 I llm_load_print_meta: vocab type       = BPE
0.00.052.219 I llm_load_print_meta: n_vocab          = 50304
0.00.052.220 I llm_load_print_meta: n_merges         = 50009
0.00.052.220 I llm_load_print_meta: vocab_only       = 0
0.00.052.220 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.220 I llm_load_print_meta: n_embd           = 2048
0.00.052.220 I llm_load_print_meta: n_layer          = 24
0.00.052.235 I llm_load_print_meta: n_head           = 16
0.00.052.237 I llm_load_print_meta: n_head_kv        = 16
0.00.052.237 I llm_load_print_meta: n_rot            = 32
0.00.052.237 I llm_load_print_meta: n_swa            = 0
0.00.052.237 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.238 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.238 I llm_load_print_meta: n_gqa            = 1
0.00.052.243 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.244 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.245 I llm_load_print_meta: n_ff             = 8192
0.00.052.245 I llm_load_print_meta: n_expert         = 0
0.00.052.245 I llm_load_print_meta: n_expert_used    = 0
0.00.052.247 I llm_load_print_meta: causal attn      = 1
0.00.052.248 I llm_load_print_meta: pooling type     = 0
0.00.052.248 I llm_load_print_meta: rope type        = 2
0.00.052.248 I llm_load_print_meta: rope scaling     = linear
0.00.052.249 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.249 I llm_load_print_meta: freq_scale_train = 1
0.00.052.249 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.250 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.250 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.250 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.250 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.251 I llm_load_print_meta: model type       = 1.4B
0.00.052.251 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.251 I llm_load_print_meta: model params     = 1.41 B
0.00.052.251 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.252 I llm_load_print_meta: general.name     = 1.4B
0.00.052.252 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.252 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.252 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.252 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.253 I llm_load_print_meta: LF token         = 128 ''
0.00.052.253 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.253 I llm_load_print_meta: max token length = 1024
0.00.054.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.344 I llm_load_tensors: offloading output layer to GPU
0.00.054.344 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.354 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.355 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.258 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.259 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.259 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.259 I llama_new_context_with_model: n_batch       = 2048
0.00.055.259 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.259 I llama_new_context_with_model: flash_attn    = 0
0.00.055.260 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.260 I llama_new_context_with_model: freq_scale    = 1
0.00.055.261 I ggml_metal_init: allocating
0.00.055.267 I ggml_metal_init: found device: Apple M4
0.00.055.269 I ggml_metal_init: picking default device: Apple M4
0.00.055.872 I ggml_metal_init: using embedded metal library
0.00.058.213 I ggml_metal_init: GPU name:   Apple M4
0.00.058.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.215 I ggml_metal_init: simdgroup reduction   = true
0.00.058.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.216 I ggml_metal_init: has bfloat            = true
0.00.058.217 I ggml_metal_init: use bfloat            = true
0.00.058.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.321 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.336 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.368 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.369 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.370 I llama_new_context_with_model: graph nodes  = 967
0.00.090.370 I llama_new_context_with_model: graph splits = 2
0.00.090.396 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.608 I main: llama threadpool init, n_threads = 4
0.00.671.643 I 
0.00.671.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.672 I 
0.00.671.886 I sampler seed: 1234
0.00.671.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.671.907 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.671.907 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.671.907 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.357.021 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.357.022 I llama_perf_context_print:        load time =     661.61 ms
0.01.357.023 I llama_perf_context_print: prompt eval time =      45.22 ms /     7 tokens (    6.46 ms per token,   154.81 tokens per second)
0.01.357.023 I llama_perf_context_print:        eval time =     636.91 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.357.024 I llama_perf_context_print:       total time =     685.42 ms /    70 tokens
0.01.357.174 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.113s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.141 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.630 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.632 I llama_model_loader: - type  f32:  194 tensors
0.00.024.632 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.709 I llm_load_vocab: special tokens cache size = 25
0.00.050.594 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.597 I llm_load_print_meta: arch             = gptneox
0.00.050.597 I llm_load_print_meta: vocab type       = BPE
0.00.050.598 I llm_load_print_meta: n_vocab          = 50304
0.00.050.598 I llm_load_print_meta: n_merges         = 50009
0.00.050.598 I llm_load_print_meta: vocab_only       = 0
0.00.050.598 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.598 I llm_load_print_meta: n_embd           = 2048
0.00.050.598 I llm_load_print_meta: n_layer          = 24
0.00.050.612 I llm_load_print_meta: n_head           = 16
0.00.050.615 I llm_load_print_meta: n_head_kv        = 16
0.00.050.615 I llm_load_print_meta: n_rot            = 32
0.00.050.615 I llm_load_print_meta: n_swa            = 0
0.00.050.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.616 I llm_load_print_meta: n_gqa            = 1
0.00.050.617 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.618 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.618 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.620 I llm_load_print_meta: n_ff             = 8192
0.00.050.620 I llm_load_print_meta: n_expert         = 0
0.00.050.620 I llm_load_print_meta: n_expert_used    = 0
0.00.050.620 I llm_load_print_meta: causal attn      = 1
0.00.050.620 I llm_load_print_meta: pooling type     = 0
0.00.050.620 I llm_load_print_meta: rope type        = 2
0.00.050.621 I llm_load_print_meta: rope scaling     = linear
0.00.050.621 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.621 I llm_load_print_meta: freq_scale_train = 1
0.00.050.621 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.621 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.622 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.622 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.622 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.622 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.622 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.623 I llm_load_print_meta: model type       = 1.4B
0.00.050.623 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.623 I llm_load_print_meta: model params     = 1.41 B
0.00.050.624 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.624 I llm_load_print_meta: general.name     = 1.4B
0.00.050.624 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: LF token         = 128 ''
0.00.050.626 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: max token length = 1024
0.00.052.567 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.567 I llm_load_tensors: offloading output layer to GPU
0.00.052.567 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.577 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.578 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.449 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.450 I llama_new_context_with_model: n_ctx         = 128
0.00.053.450 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.451 I llama_new_context_with_model: n_batch       = 128
0.00.053.451 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.451 I llama_new_context_with_model: flash_attn    = 0
0.00.053.451 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.451 I llama_new_context_with_model: freq_scale    = 1
0.00.053.452 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.452 I ggml_metal_init: allocating
0.00.053.455 I ggml_metal_init: found device: Apple M4
0.00.053.457 I ggml_metal_init: picking default device: Apple M4
0.00.054.027 I ggml_metal_init: using embedded metal library
0.00.056.323 I ggml_metal_init: GPU name:   Apple M4
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.325 I ggml_metal_init: simdgroup reduction   = true
0.00.056.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.326 I ggml_metal_init: has bfloat            = true
0.00.056.326 I ggml_metal_init: use bfloat            = true
0.00.056.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.474 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.486 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.421 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.422 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.422 I llama_new_context_with_model: graph nodes  = 967
0.00.068.422 I llama_new_context_with_model: graph splits = 2
0.00.068.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.312 I 
0.00.599.351 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.363 I perplexity: tokenizing the input ..
0.00.607.204 I perplexity: tokenization took 7.839 ms
0.00.607.208 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.429 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.730.591 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.730.605 I llama_perf_context_print:        load time =     589.16 ms
0.00.730.606 I llama_perf_context_print: prompt eval time =     121.99 ms /   128 tokens (    0.95 ms per token,  1049.26 tokens per second)
0.00.730.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.608 I llama_perf_context_print:       total time =     131.30 ms /   129 tokens
0.00.731.042 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.078s
sys	0m0.092s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.249 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.995 I llama_model_loader: - type  f32:  194 tensors
0.00.023.995 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.415 I llm_load_vocab: special tokens cache size = 25
0.00.050.465 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.468 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.468 I llm_load_print_meta: arch             = gptneox
0.00.050.469 I llm_load_print_meta: vocab type       = BPE
0.00.050.469 I llm_load_print_meta: n_vocab          = 50304
0.00.050.469 I llm_load_print_meta: n_merges         = 50009
0.00.050.469 I llm_load_print_meta: vocab_only       = 0
0.00.050.469 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.470 I llm_load_print_meta: n_embd           = 2048
0.00.050.470 I llm_load_print_meta: n_layer          = 24
0.00.050.484 I llm_load_print_meta: n_head           = 16
0.00.050.486 I llm_load_print_meta: n_head_kv        = 16
0.00.050.486 I llm_load_print_meta: n_rot            = 32
0.00.050.486 I llm_load_print_meta: n_swa            = 0
0.00.050.486 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.486 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.487 I llm_load_print_meta: n_gqa            = 1
0.00.050.488 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.488 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.489 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.489 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.489 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.490 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.490 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.490 I llm_load_print_meta: n_ff             = 8192
0.00.050.491 I llm_load_print_meta: n_expert         = 0
0.00.050.491 I llm_load_print_meta: n_expert_used    = 0
0.00.050.497 I llm_load_print_meta: causal attn      = 1
0.00.050.498 I llm_load_print_meta: pooling type     = 0
0.00.050.498 I llm_load_print_meta: rope type        = 2
0.00.050.498 I llm_load_print_meta: rope scaling     = linear
0.00.050.499 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.499 I llm_load_print_meta: freq_scale_train = 1
0.00.050.499 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.506 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.507 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.507 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.507 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.507 I llm_load_print_meta: model type       = 1.4B
0.00.050.507 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.508 I llm_load_print_meta: model params     = 1.41 B
0.00.050.508 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.510 I llm_load_print_meta: general.name     = 1.4B
0.00.050.510 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.511 I llm_load_print_meta: LF token         = 128 ''
0.00.050.511 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.511 I llm_load_print_meta: max token length = 1024
0.00.052.464 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.464 I llm_load_tensors: offloading output layer to GPU
0.00.052.464 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.475 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.476 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.356 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.356 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.356 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.356 I llama_new_context_with_model: n_batch       = 2048
0.00.053.357 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.357 I llama_new_context_with_model: flash_attn    = 0
0.00.053.357 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.357 I llama_new_context_with_model: freq_scale    = 1
0.00.053.358 I ggml_metal_init: allocating
0.00.053.361 I ggml_metal_init: found device: Apple M4
0.00.053.363 I ggml_metal_init: picking default device: Apple M4
0.00.053.980 I ggml_metal_init: using embedded metal library
0.00.056.330 I ggml_metal_init: GPU name:   Apple M4
0.00.056.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.332 I ggml_metal_init: simdgroup reduction   = true
0.00.056.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.333 I ggml_metal_init: has bfloat            = true
0.00.056.334 I ggml_metal_init: use bfloat            = true
0.00.056.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.961 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.966 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.985 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.054 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.055 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.055 I llama_new_context_with_model: graph nodes  = 967
0.00.086.056 I llama_new_context_with_model: graph splits = 2
0.00.086.080 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.224 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.899 I main: llama threadpool init, n_threads = 4
0.00.711.941 I 
0.00.712.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.003 I 
0.00.712.247 I sampler seed: 1234
0.00.712.251 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.296 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.300 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.300 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.438.361 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63848.92 tokens per second)
0.01.438.361 I llama_perf_context_print:        load time =     703.17 ms
0.01.438.362 I llama_perf_context_print: prompt eval time =      45.53 ms /     7 tokens (    6.50 ms per token,   153.75 tokens per second)
0.01.438.363 I llama_perf_context_print:        eval time =     677.68 ms /    63 runs   (   10.76 ms per token,    92.96 tokens per second)
0.01.438.363 I llama_perf_context_print:       total time =     726.47 ms /    70 tokens
0.01.438.573 I ggml_metal_free: deallocating

real	0m1.455s
user	0m0.108s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.631 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.266 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.267 I llama_model_loader: - type  f32:  194 tensors
0.00.023.267 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.267 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.312 I llm_load_vocab: special tokens cache size = 25
0.00.050.263 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.265 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.266 I llm_load_print_meta: arch             = gptneox
0.00.050.266 I llm_load_print_meta: vocab type       = BPE
0.00.050.266 I llm_load_print_meta: n_vocab          = 50304
0.00.050.266 I llm_load_print_meta: n_merges         = 50009
0.00.050.267 I llm_load_print_meta: vocab_only       = 0
0.00.050.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.267 I llm_load_print_meta: n_embd           = 2048
0.00.050.267 I llm_load_print_meta: n_layer          = 24
0.00.050.281 I llm_load_print_meta: n_head           = 16
0.00.050.282 I llm_load_print_meta: n_head_kv        = 16
0.00.050.282 I llm_load_print_meta: n_rot            = 32
0.00.050.282 I llm_load_print_meta: n_swa            = 0
0.00.050.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.283 I llm_load_print_meta: n_gqa            = 1
0.00.050.284 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.285 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.285 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.286 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.286 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.287 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.287 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.289 I llm_load_print_meta: n_ff             = 8192
0.00.050.289 I llm_load_print_meta: n_expert         = 0
0.00.050.289 I llm_load_print_meta: n_expert_used    = 0
0.00.050.289 I llm_load_print_meta: causal attn      = 1
0.00.050.289 I llm_load_print_meta: pooling type     = 0
0.00.050.289 I llm_load_print_meta: rope type        = 2
0.00.050.289 I llm_load_print_meta: rope scaling     = linear
0.00.050.290 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.290 I llm_load_print_meta: freq_scale_train = 1
0.00.050.290 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.290 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.291 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.291 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.291 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.291 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.291 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.291 I llm_load_print_meta: model type       = 1.4B
0.00.050.292 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.292 I llm_load_print_meta: model params     = 1.41 B
0.00.050.292 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.293 I llm_load_print_meta: general.name     = 1.4B
0.00.050.293 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.294 I llm_load_print_meta: LF token         = 128 ''
0.00.050.294 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.294 I llm_load_print_meta: max token length = 1024
0.00.052.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.312 I llm_load_tensors: offloading output layer to GPU
0.00.052.312 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.323 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.324 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.277 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.278 I llama_new_context_with_model: n_ctx         = 128
0.00.053.278 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.279 I llama_new_context_with_model: n_batch       = 128
0.00.053.279 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.279 I llama_new_context_with_model: flash_attn    = 0
0.00.053.279 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.279 I llama_new_context_with_model: freq_scale    = 1
0.00.053.280 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.280 I ggml_metal_init: allocating
0.00.053.283 I ggml_metal_init: found device: Apple M4
0.00.053.285 I ggml_metal_init: picking default device: Apple M4
0.00.053.846 I ggml_metal_init: using embedded metal library
0.00.056.191 I ggml_metal_init: GPU name:   Apple M4
0.00.056.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.193 I ggml_metal_init: simdgroup reduction   = true
0.00.056.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.194 I ggml_metal_init: has bfloat            = true
0.00.056.194 I ggml_metal_init: use bfloat            = true
0.00.056.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.324 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.239 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.240 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.240 I llama_new_context_with_model: graph nodes  = 967
0.00.068.240 I llama_new_context_with_model: graph splits = 2
0.00.068.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.310 I 
0.00.673.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.356 I perplexity: tokenizing the input ..
0.00.681.600 I perplexity: tokenization took 8.242 ms
0.00.681.608 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.360 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.804.791 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.804.802 I llama_perf_context_print:        load time =     664.68 ms
0.00.804.803 I llama_perf_context_print: prompt eval time =     121.52 ms /   128 tokens (    0.95 ms per token,  1053.36 tokens per second)
0.00.804.804 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.804 I llama_perf_context_print:       total time =     131.49 ms /   129 tokens
0.00.805.093 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.079s
sys	0m0.108s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.213 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.486 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.054 I llama_model_loader: - type  f32:  194 tensors
0.00.026.054 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.449 I llm_load_vocab: special tokens cache size = 25
0.00.052.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.483 I llm_load_print_meta: arch             = gptneox
0.00.052.484 I llm_load_print_meta: vocab type       = BPE
0.00.052.484 I llm_load_print_meta: n_vocab          = 50304
0.00.052.484 I llm_load_print_meta: n_merges         = 50009
0.00.052.484 I llm_load_print_meta: vocab_only       = 0
0.00.052.484 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.484 I llm_load_print_meta: n_embd           = 2048
0.00.052.485 I llm_load_print_meta: n_layer          = 24
0.00.052.499 I llm_load_print_meta: n_head           = 16
0.00.052.500 I llm_load_print_meta: n_head_kv        = 16
0.00.052.500 I llm_load_print_meta: n_rot            = 32
0.00.052.500 I llm_load_print_meta: n_swa            = 0
0.00.052.500 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.500 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.501 I llm_load_print_meta: n_gqa            = 1
0.00.052.502 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.504 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.506 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.506 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.507 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.507 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.507 I llm_load_print_meta: n_ff             = 8192
0.00.052.508 I llm_load_print_meta: n_expert         = 0
0.00.052.508 I llm_load_print_meta: n_expert_used    = 0
0.00.052.509 I llm_load_print_meta: causal attn      = 1
0.00.052.510 I llm_load_print_meta: pooling type     = 0
0.00.052.510 I llm_load_print_meta: rope type        = 2
0.00.052.511 I llm_load_print_meta: rope scaling     = linear
0.00.052.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.511 I llm_load_print_meta: freq_scale_train = 1
0.00.052.511 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.511 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.511 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.512 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.512 I llm_load_print_meta: model type       = 1.4B
0.00.052.512 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.513 I llm_load_print_meta: model params     = 1.41 B
0.00.052.513 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.513 I llm_load_print_meta: general.name     = 1.4B
0.00.052.514 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.514 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.517 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.517 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.517 I llm_load_print_meta: LF token         = 128 ''
0.00.052.518 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.518 I llm_load_print_meta: max token length = 1024
0.00.054.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.527 I llm_load_tensors: offloading output layer to GPU
0.00.054.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.538 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.539 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.462 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.462 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.463 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.463 I llama_new_context_with_model: n_batch       = 2048
0.00.055.463 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.463 I llama_new_context_with_model: flash_attn    = 0
0.00.055.464 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.464 I llama_new_context_with_model: freq_scale    = 1
0.00.055.464 I ggml_metal_init: allocating
0.00.055.471 I ggml_metal_init: found device: Apple M4
0.00.055.473 I ggml_metal_init: picking default device: Apple M4
0.00.056.042 I ggml_metal_init: using embedded metal library
0.00.058.388 I ggml_metal_init: GPU name:   Apple M4
0.00.058.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.390 I ggml_metal_init: simdgroup reduction   = true
0.00.058.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.390 I ggml_metal_init: has bfloat            = true
0.00.058.390 I ggml_metal_init: use bfloat            = true
0.00.058.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.071 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.087 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.142 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.144 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.144 I llama_new_context_with_model: graph nodes  = 967
0.00.089.144 I llama_new_context_with_model: graph splits = 2
0.00.089.168 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.485 I main: llama threadpool init, n_threads = 4
0.00.769.521 I 
0.00.769.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.553 I 
0.00.769.782 I sampler seed: 1234
0.00.769.787 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.833 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.837 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.838 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.558.397 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.558.398 I llama_perf_context_print:        load time =     759.27 ms
0.01.558.398 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.32 tokens per second)
0.01.558.399 I llama_perf_context_print:        eval time =     742.37 ms /    63 runs   (   11.78 ms per token,    84.86 tokens per second)
0.01.558.399 I llama_perf_context_print:       total time =     788.91 ms /    70 tokens
0.01.558.593 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.915 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.727 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.729 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.735 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.747 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.720 I llama_model_loader: - type  f32:  194 tensors
0.00.025.720 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.670 I llm_load_vocab: special tokens cache size = 25
0.00.052.650 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.653 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.654 I llm_load_print_meta: arch             = gptneox
0.00.052.654 I llm_load_print_meta: vocab type       = BPE
0.00.052.654 I llm_load_print_meta: n_vocab          = 50304
0.00.052.654 I llm_load_print_meta: n_merges         = 50009
0.00.052.654 I llm_load_print_meta: vocab_only       = 0
0.00.052.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.655 I llm_load_print_meta: n_embd           = 2048
0.00.052.655 I llm_load_print_meta: n_layer          = 24
0.00.052.671 I llm_load_print_meta: n_head           = 16
0.00.052.673 I llm_load_print_meta: n_head_kv        = 16
0.00.052.673 I llm_load_print_meta: n_rot            = 32
0.00.052.673 I llm_load_print_meta: n_swa            = 0
0.00.052.673 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.673 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.674 I llm_load_print_meta: n_gqa            = 1
0.00.052.674 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.675 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.675 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.676 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.676 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.676 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.677 I llm_load_print_meta: n_ff             = 8192
0.00.052.677 I llm_load_print_meta: n_expert         = 0
0.00.052.677 I llm_load_print_meta: n_expert_used    = 0
0.00.052.677 I llm_load_print_meta: causal attn      = 1
0.00.052.677 I llm_load_print_meta: pooling type     = 0
0.00.052.677 I llm_load_print_meta: rope type        = 2
0.00.052.678 I llm_load_print_meta: rope scaling     = linear
0.00.052.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.678 I llm_load_print_meta: freq_scale_train = 1
0.00.052.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.682 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.682 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.682 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.682 I llm_load_print_meta: model type       = 1.4B
0.00.052.683 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.683 I llm_load_print_meta: model params     = 1.41 B
0.00.052.683 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.684 I llm_load_print_meta: general.name     = 1.4B
0.00.052.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.684 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.684 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.684 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.685 I llm_load_print_meta: LF token         = 128 ''
0.00.052.686 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.686 I llm_load_print_meta: max token length = 1024
0.00.054.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.679 I llm_load_tensors: offloading output layer to GPU
0.00.054.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.689 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.690 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.603 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.605 I llama_new_context_with_model: n_ctx         = 128
0.00.055.605 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.605 I llama_new_context_with_model: n_batch       = 128
0.00.055.605 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.605 I llama_new_context_with_model: flash_attn    = 0
0.00.055.606 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.606 I llama_new_context_with_model: freq_scale    = 1
0.00.055.607 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.607 I ggml_metal_init: allocating
0.00.055.610 I ggml_metal_init: found device: Apple M4
0.00.055.612 I ggml_metal_init: picking default device: Apple M4
0.00.056.198 I ggml_metal_init: using embedded metal library
0.00.058.562 I ggml_metal_init: GPU name:   Apple M4
0.00.058.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.564 I ggml_metal_init: simdgroup reduction   = true
0.00.058.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.565 I ggml_metal_init: has bfloat            = true
0.00.058.565 I ggml_metal_init: use bfloat            = true
0.00.058.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.968 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.982 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.906 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.907 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.908 I llama_new_context_with_model: graph nodes  = 967
0.00.070.908 I llama_new_context_with_model: graph splits = 2
0.00.070.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.025 I 
0.00.738.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.151 I perplexity: tokenizing the input ..
0.00.751.337 I perplexity: tokenization took 13.182 ms
0.00.751.347 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.279 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.891.421 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.891.454 I llama_perf_context_print:        load time =     728.10 ms
0.00.891.455 I llama_perf_context_print: prompt eval time =     135.68 ms /   128 tokens (    1.06 ms per token,   943.40 tokens per second)
0.00.891.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.457 I llama_perf_context_print:       total time =     153.43 ms /   129 tokens
0.00.892.261 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.104s
sys	0m0.121s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.681 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.018 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.019 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.021 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.021 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.658 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.660 I llama_model_loader: - type  f32:  194 tensors
0.00.024.661 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.661 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.011 I llm_load_vocab: special tokens cache size = 25
0.00.051.002 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.004 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.005 I llm_load_print_meta: arch             = gptneox
0.00.051.005 I llm_load_print_meta: vocab type       = BPE
0.00.051.005 I llm_load_print_meta: n_vocab          = 50304
0.00.051.005 I llm_load_print_meta: n_merges         = 50009
0.00.051.006 I llm_load_print_meta: vocab_only       = 0
0.00.051.006 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.006 I llm_load_print_meta: n_embd           = 2048
0.00.051.006 I llm_load_print_meta: n_layer          = 24
0.00.051.020 I llm_load_print_meta: n_head           = 16
0.00.051.021 I llm_load_print_meta: n_head_kv        = 16
0.00.051.022 I llm_load_print_meta: n_rot            = 32
0.00.051.022 I llm_load_print_meta: n_swa            = 0
0.00.051.022 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.022 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.023 I llm_load_print_meta: n_gqa            = 1
0.00.051.023 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.024 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.025 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.025 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.025 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.025 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.025 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.026 I llm_load_print_meta: n_ff             = 8192
0.00.051.026 I llm_load_print_meta: n_expert         = 0
0.00.051.027 I llm_load_print_meta: n_expert_used    = 0
0.00.051.029 I llm_load_print_meta: causal attn      = 1
0.00.051.030 I llm_load_print_meta: pooling type     = 0
0.00.051.030 I llm_load_print_meta: rope type        = 2
0.00.051.030 I llm_load_print_meta: rope scaling     = linear
0.00.051.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.031 I llm_load_print_meta: freq_scale_train = 1
0.00.051.031 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.034 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.034 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.035 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.035 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.036 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.036 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.036 I llm_load_print_meta: model type       = 1.4B
0.00.051.036 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.037 I llm_load_print_meta: model params     = 1.41 B
0.00.051.037 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.039 I llm_load_print_meta: general.name     = 1.4B
0.00.051.039 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.039 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.039 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.040 I llm_load_print_meta: LF token         = 128 ''
0.00.051.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.040 I llm_load_print_meta: max token length = 1024
0.00.052.999 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.000 I llm_load_tensors: offloading output layer to GPU
0.00.053.000 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.010 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.011 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.907 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.908 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.908 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.908 I llama_new_context_with_model: n_batch       = 2048
0.00.053.908 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.909 I llama_new_context_with_model: flash_attn    = 0
0.00.053.909 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.909 I llama_new_context_with_model: freq_scale    = 1
0.00.053.910 I ggml_metal_init: allocating
0.00.053.913 I ggml_metal_init: found device: Apple M4
0.00.053.915 I ggml_metal_init: picking default device: Apple M4
0.00.054.502 I ggml_metal_init: using embedded metal library
0.00.056.783 I ggml_metal_init: GPU name:   Apple M4
0.00.056.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.785 I ggml_metal_init: simdgroup reduction   = true
0.00.056.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.787 I ggml_metal_init: has bfloat            = true
0.00.056.787 I ggml_metal_init: use bfloat            = true
0.00.056.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.168 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.173 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.191 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.187 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.189 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.189 I llama_new_context_with_model: graph nodes  = 967
0.00.086.189 I llama_new_context_with_model: graph splits = 2
0.00.086.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.521 I main: llama threadpool init, n_threads = 4
0.00.746.564 I 
0.00.746.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.589 I 
0.00.746.824 I sampler seed: 1234
0.00.746.829 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.845 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.845 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.588.122 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.588.123 I llama_perf_context_print:        load time =     737.84 ms
0.01.588.124 I llama_perf_context_print: prompt eval time =      46.23 ms /     7 tokens (    6.60 ms per token,   151.42 tokens per second)
0.01.588.124 I llama_perf_context_print:        eval time =     791.94 ms /    63 runs   (   12.57 ms per token,    79.55 tokens per second)
0.01.588.125 I llama_perf_context_print:       total time =     841.60 ms /    70 tokens
0.01.588.310 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.110s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.718 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.426 I llama_model_loader: - type  f32:  194 tensors
0.00.023.426 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.982 I llm_load_vocab: special tokens cache size = 25
0.00.050.050 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.054 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.054 I llm_load_print_meta: arch             = gptneox
0.00.050.054 I llm_load_print_meta: vocab type       = BPE
0.00.050.055 I llm_load_print_meta: n_vocab          = 50304
0.00.050.055 I llm_load_print_meta: n_merges         = 50009
0.00.050.055 I llm_load_print_meta: vocab_only       = 0
0.00.050.056 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.056 I llm_load_print_meta: n_embd           = 2048
0.00.050.056 I llm_load_print_meta: n_layer          = 24
0.00.050.073 I llm_load_print_meta: n_head           = 16
0.00.050.075 I llm_load_print_meta: n_head_kv        = 16
0.00.050.075 I llm_load_print_meta: n_rot            = 32
0.00.050.075 I llm_load_print_meta: n_swa            = 0
0.00.050.075 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.075 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.076 I llm_load_print_meta: n_gqa            = 1
0.00.050.076 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.077 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.077 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.078 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.079 I llm_load_print_meta: n_ff             = 8192
0.00.050.079 I llm_load_print_meta: n_expert         = 0
0.00.050.079 I llm_load_print_meta: n_expert_used    = 0
0.00.050.079 I llm_load_print_meta: causal attn      = 1
0.00.050.079 I llm_load_print_meta: pooling type     = 0
0.00.050.079 I llm_load_print_meta: rope type        = 2
0.00.050.080 I llm_load_print_meta: rope scaling     = linear
0.00.050.080 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.080 I llm_load_print_meta: freq_scale_train = 1
0.00.050.080 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.081 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.081 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.081 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.081 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.081 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.081 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.082 I llm_load_print_meta: model type       = 1.4B
0.00.050.082 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.083 I llm_load_print_meta: model params     = 1.41 B
0.00.050.083 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.083 I llm_load_print_meta: general.name     = 1.4B
0.00.050.083 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: LF token         = 128 ''
0.00.050.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.085 I llm_load_print_meta: max token length = 1024
0.00.052.116 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.116 I llm_load_tensors: offloading output layer to GPU
0.00.052.116 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.127 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.128 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.084 I llama_new_context_with_model: n_ctx         = 128
0.00.053.084 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.084 I llama_new_context_with_model: n_batch       = 128
0.00.053.084 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.084 I llama_new_context_with_model: flash_attn    = 0
0.00.053.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.085 I llama_new_context_with_model: freq_scale    = 1
0.00.053.086 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.086 I ggml_metal_init: allocating
0.00.053.091 I ggml_metal_init: found device: Apple M4
0.00.053.093 I ggml_metal_init: picking default device: Apple M4
0.00.053.741 I ggml_metal_init: using embedded metal library
0.00.056.236 I ggml_metal_init: GPU name:   Apple M4
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.239 I ggml_metal_init: simdgroup reduction   = true
0.00.056.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.239 I ggml_metal_init: has bfloat            = true
0.00.056.239 I ggml_metal_init: use bfloat            = true
0.00.056.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.784 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.788 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.803 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.733 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.734 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.735 I llama_new_context_with_model: graph nodes  = 967
0.00.067.735 I llama_new_context_with_model: graph splits = 2
0.00.067.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.702 I 
0.00.655.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.796 I perplexity: tokenizing the input ..
0.00.664.555 I perplexity: tokenization took 8.758 ms
0.00.664.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.436 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.800.610 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.800.657 I llama_perf_context_print:        load time =     646.98 ms
0.00.800.658 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.63 tokens per second)
0.00.800.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.659 I llama_perf_context_print:       total time =     144.96 ms /   129 tokens
0.00.801.178 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.081s
sys	0m0.113s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.843 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.278 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.280 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.281 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.283 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.284 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.863 I llama_model_loader: - type  f32:  194 tensors
0.00.023.863 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.863 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.180 I llm_load_vocab: special tokens cache size = 25
0.00.049.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.939 I llm_load_print_meta: arch             = gptneox
0.00.049.940 I llm_load_print_meta: vocab type       = BPE
0.00.049.940 I llm_load_print_meta: n_vocab          = 50304
0.00.049.940 I llm_load_print_meta: n_merges         = 50009
0.00.049.940 I llm_load_print_meta: vocab_only       = 0
0.00.049.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.941 I llm_load_print_meta: n_embd           = 2048
0.00.049.941 I llm_load_print_meta: n_layer          = 24
0.00.049.955 I llm_load_print_meta: n_head           = 16
0.00.049.956 I llm_load_print_meta: n_head_kv        = 16
0.00.049.956 I llm_load_print_meta: n_rot            = 32
0.00.049.957 I llm_load_print_meta: n_swa            = 0
0.00.049.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.957 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.958 I llm_load_print_meta: n_gqa            = 1
0.00.049.958 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.959 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.960 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.960 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.960 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.961 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.961 I llm_load_print_meta: n_ff             = 8192
0.00.049.962 I llm_load_print_meta: n_expert         = 0
0.00.049.963 I llm_load_print_meta: n_expert_used    = 0
0.00.049.963 I llm_load_print_meta: causal attn      = 1
0.00.049.963 I llm_load_print_meta: pooling type     = 0
0.00.049.963 I llm_load_print_meta: rope type        = 2
0.00.049.963 I llm_load_print_meta: rope scaling     = linear
0.00.049.963 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.964 I llm_load_print_meta: freq_scale_train = 1
0.00.049.965 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.966 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.966 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.966 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.966 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.967 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.967 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.968 I llm_load_print_meta: model type       = 1.4B
0.00.049.968 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.968 I llm_load_print_meta: model params     = 1.41 B
0.00.049.968 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.969 I llm_load_print_meta: general.name     = 1.4B
0.00.049.969 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.969 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.970 I llm_load_print_meta: LF token         = 128 ''
0.00.049.970 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.971 I llm_load_print_meta: max token length = 1024
0.00.051.858 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.858 I llm_load_tensors: offloading output layer to GPU
0.00.051.859 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.869 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.871 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.769 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.769 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.769 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.770 I llama_new_context_with_model: n_batch       = 2048
0.00.052.770 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.770 I llama_new_context_with_model: flash_attn    = 0
0.00.052.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.771 I llama_new_context_with_model: freq_scale    = 1
0.00.052.771 I ggml_metal_init: allocating
0.00.052.774 I ggml_metal_init: found device: Apple M4
0.00.052.776 I ggml_metal_init: picking default device: Apple M4
0.00.053.351 I ggml_metal_init: using embedded metal library
0.00.055.680 I ggml_metal_init: GPU name:   Apple M4
0.00.055.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.682 I ggml_metal_init: simdgroup reduction   = true
0.00.055.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.682 I ggml_metal_init: has bfloat            = true
0.00.055.683 I ggml_metal_init: use bfloat            = true
0.00.055.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.321 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.333 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.351 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.415 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.416 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.417 I llama_new_context_with_model: graph nodes  = 967
0.00.085.417 I llama_new_context_with_model: graph splits = 2
0.00.085.442 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.706 I main: llama threadpool init, n_threads = 4
0.00.441.744 I 
0.00.441.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.778 I 
0.00.442.018 I sampler seed: 1234
0.00.442.024 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.442.068 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.442.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.442.070 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.122.026 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.122.026 I llama_perf_context_print:        load time =     431.86 ms
0.01.122.028 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.78 tokens per second)
0.01.122.029 I llama_perf_context_print:        eval time =     641.19 ms /    63 runs   (   10.18 ms per token,    98.26 tokens per second)
0.01.122.029 I llama_perf_context_print:       total time =     680.32 ms /    70 tokens
0.01.122.198 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.466 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.468 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.468 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.469 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.470 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.471 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.471 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.472 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.472 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.531 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.423 I llama_model_loader: - type  f32:  194 tensors
0.00.024.423 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.423 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.423 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.410 I llm_load_vocab: special tokens cache size = 25
0.00.051.357 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.359 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.359 I llm_load_print_meta: arch             = gptneox
0.00.051.360 I llm_load_print_meta: vocab type       = BPE
0.00.051.360 I llm_load_print_meta: n_vocab          = 50304
0.00.051.360 I llm_load_print_meta: n_merges         = 50009
0.00.051.361 I llm_load_print_meta: vocab_only       = 0
0.00.051.361 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.361 I llm_load_print_meta: n_embd           = 2048
0.00.051.361 I llm_load_print_meta: n_layer          = 24
0.00.051.375 I llm_load_print_meta: n_head           = 16
0.00.051.376 I llm_load_print_meta: n_head_kv        = 16
0.00.051.376 I llm_load_print_meta: n_rot            = 32
0.00.051.376 I llm_load_print_meta: n_swa            = 0
0.00.051.377 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.377 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.378 I llm_load_print_meta: n_gqa            = 1
0.00.051.380 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.382 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.382 I llm_load_print_meta: n_ff             = 8192
0.00.051.382 I llm_load_print_meta: n_expert         = 0
0.00.051.383 I llm_load_print_meta: n_expert_used    = 0
0.00.051.383 I llm_load_print_meta: causal attn      = 1
0.00.051.383 I llm_load_print_meta: pooling type     = 0
0.00.051.383 I llm_load_print_meta: rope type        = 2
0.00.051.383 I llm_load_print_meta: rope scaling     = linear
0.00.051.385 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.385 I llm_load_print_meta: freq_scale_train = 1
0.00.051.385 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.385 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.386 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.386 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.386 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.386 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.386 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.386 I llm_load_print_meta: model type       = 1.4B
0.00.051.387 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.387 I llm_load_print_meta: model params     = 1.41 B
0.00.051.388 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.388 I llm_load_print_meta: general.name     = 1.4B
0.00.051.389 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: LF token         = 128 ''
0.00.051.390 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.390 I llm_load_print_meta: max token length = 1024
0.00.053.326 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.326 I llm_load_tensors: offloading output layer to GPU
0.00.053.326 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.337 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.338 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.274 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.274 I llama_new_context_with_model: n_ctx         = 128
0.00.054.275 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.275 I llama_new_context_with_model: n_batch       = 128
0.00.054.275 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.275 I llama_new_context_with_model: flash_attn    = 0
0.00.054.276 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.276 I llama_new_context_with_model: freq_scale    = 1
0.00.054.276 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.277 I ggml_metal_init: allocating
0.00.054.280 I ggml_metal_init: found device: Apple M4
0.00.054.282 I ggml_metal_init: picking default device: Apple M4
0.00.054.846 I ggml_metal_init: using embedded metal library
0.00.057.157 I ggml_metal_init: GPU name:   Apple M4
0.00.057.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.160 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.160 I ggml_metal_init: simdgroup reduction   = true
0.00.057.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.160 I ggml_metal_init: has bfloat            = true
0.00.057.160 I ggml_metal_init: use bfloat            = true
0.00.057.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.161 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.346 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.359 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.326 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.327 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.328 I llama_new_context_with_model: graph nodes  = 967
0.00.069.328 I llama_new_context_with_model: graph splits = 2
0.00.069.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.841 I 
0.00.385.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.894 I perplexity: tokenizing the input ..
0.00.394.075 I perplexity: tokenization took 8.178 ms
0.00.394.082 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.526.250 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.527.408 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.527.434 I llama_perf_context_print:        load time =     375.92 ms
0.00.527.436 I llama_perf_context_print: prompt eval time =     131.94 ms /   128 tokens (    1.03 ms per token,   970.12 tokens per second)
0.00.527.436 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.527.437 I llama_perf_context_print:       total time =     141.59 ms /   129 tokens
0.00.527.896 I ggml_metal_free: deallocating

real	0m0.543s
user	0m0.079s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.009.547 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.014 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.018 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.019 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.019 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.019 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.022 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.022 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.917 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.917 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.919 I llama_model_loader: - type  f32:  194 tensors
0.00.024.919 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.919 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.919 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.030 I llm_load_vocab: special tokens cache size = 25
0.00.052.088 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.090 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.091 I llm_load_print_meta: arch             = gptneox
0.00.052.091 I llm_load_print_meta: vocab type       = BPE
0.00.052.091 I llm_load_print_meta: n_vocab          = 50304
0.00.052.092 I llm_load_print_meta: n_merges         = 50009
0.00.052.092 I llm_load_print_meta: vocab_only       = 0
0.00.052.092 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.092 I llm_load_print_meta: n_embd           = 2048
0.00.052.092 I llm_load_print_meta: n_layer          = 24
0.00.052.106 I llm_load_print_meta: n_head           = 16
0.00.052.107 I llm_load_print_meta: n_head_kv        = 16
0.00.052.107 I llm_load_print_meta: n_rot            = 32
0.00.052.107 I llm_load_print_meta: n_swa            = 0
0.00.052.107 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.108 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.109 I llm_load_print_meta: n_gqa            = 1
0.00.052.110 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.110 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.111 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.111 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.111 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.112 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.112 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.112 I llm_load_print_meta: n_ff             = 8192
0.00.052.113 I llm_load_print_meta: n_expert         = 0
0.00.052.113 I llm_load_print_meta: n_expert_used    = 0
0.00.052.113 I llm_load_print_meta: causal attn      = 1
0.00.052.113 I llm_load_print_meta: pooling type     = 0
0.00.052.113 I llm_load_print_meta: rope type        = 2
0.00.052.113 I llm_load_print_meta: rope scaling     = linear
0.00.052.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.114 I llm_load_print_meta: freq_scale_train = 1
0.00.052.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.115 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.115 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.115 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.116 I llm_load_print_meta: model type       = 1.4B
0.00.052.116 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.116 I llm_load_print_meta: model params     = 1.41 B
0.00.052.116 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.117 I llm_load_print_meta: general.name     = 1.4B
0.00.052.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.117 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.117 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.117 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.118 I llm_load_print_meta: LF token         = 128 ''
0.00.052.118 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.118 I llm_load_print_meta: max token length = 1024
0.00.053.764 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.765 I llm_load_tensors: offloading output layer to GPU
0.00.053.765 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.775 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.776 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.609 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.610 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.610 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.610 I llama_new_context_with_model: n_batch       = 2048
0.00.054.611 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.611 I llama_new_context_with_model: flash_attn    = 0
0.00.054.611 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.611 I llama_new_context_with_model: freq_scale    = 1
0.00.054.612 I ggml_metal_init: allocating
0.00.054.615 I ggml_metal_init: found device: Apple M4
0.00.054.617 I ggml_metal_init: picking default device: Apple M4
0.00.055.231 I ggml_metal_init: using embedded metal library
0.00.057.609 I ggml_metal_init: GPU name:   Apple M4
0.00.057.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.612 I ggml_metal_init: simdgroup reduction   = true
0.00.057.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.612 I ggml_metal_init: has bfloat            = true
0.00.057.612 I ggml_metal_init: use bfloat            = true
0.00.057.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.708 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.729 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.711 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.713 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.713 I llama_new_context_with_model: graph nodes  = 967
0.00.088.713 I llama_new_context_with_model: graph splits = 2
0.00.088.737 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.060 I main: llama threadpool init, n_threads = 4
0.00.543.096 I 
0.00.543.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.128 I 
0.00.543.356 I sampler seed: 1234
0.00.543.360 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.376 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.290.195 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.290.196 I llama_perf_context_print:        load time =     533.51 ms
0.01.290.196 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.01 tokens per second)
0.01.290.197 I llama_perf_context_print:        eval time =     699.99 ms /    63 runs   (   11.11 ms per token,    90.00 tokens per second)
0.01.290.197 I llama_perf_context_print:       total time =     747.14 ms /    70 tokens
0.01.290.397 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.111s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.417 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.255 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.848 I llama_model_loader: - type  f32:  194 tensors
0.00.022.848 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.849 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.849 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.849 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.934 I llm_load_vocab: special tokens cache size = 25
0.00.048.876 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.879 I llm_load_print_meta: arch             = gptneox
0.00.048.879 I llm_load_print_meta: vocab type       = BPE
0.00.048.880 I llm_load_print_meta: n_vocab          = 50304
0.00.048.880 I llm_load_print_meta: n_merges         = 50009
0.00.048.880 I llm_load_print_meta: vocab_only       = 0
0.00.048.880 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.880 I llm_load_print_meta: n_embd           = 2048
0.00.048.880 I llm_load_print_meta: n_layer          = 24
0.00.048.894 I llm_load_print_meta: n_head           = 16
0.00.048.894 I llm_load_print_meta: n_head_kv        = 16
0.00.048.894 I llm_load_print_meta: n_rot            = 32
0.00.048.895 I llm_load_print_meta: n_swa            = 0
0.00.048.897 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.897 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.898 I llm_load_print_meta: n_gqa            = 1
0.00.048.899 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.899 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.905 I llm_load_print_meta: n_ff             = 8192
0.00.048.905 I llm_load_print_meta: n_expert         = 0
0.00.048.905 I llm_load_print_meta: n_expert_used    = 0
0.00.048.905 I llm_load_print_meta: causal attn      = 1
0.00.048.905 I llm_load_print_meta: pooling type     = 0
0.00.048.905 I llm_load_print_meta: rope type        = 2
0.00.048.905 I llm_load_print_meta: rope scaling     = linear
0.00.048.906 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.906 I llm_load_print_meta: freq_scale_train = 1
0.00.048.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.906 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.907 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.907 I llm_load_print_meta: model type       = 1.4B
0.00.048.908 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.908 I llm_load_print_meta: model params     = 1.41 B
0.00.048.908 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.910 I llm_load_print_meta: general.name     = 1.4B
0.00.048.910 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.910 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.910 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.910 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.911 I llm_load_print_meta: LF token         = 128 ''
0.00.048.911 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.911 I llm_load_print_meta: max token length = 1024
0.00.050.487 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.488 I llm_load_tensors: offloading output layer to GPU
0.00.050.488 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.498 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.499 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.376 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.377 I llama_new_context_with_model: n_ctx         = 128
0.00.051.377 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.377 I llama_new_context_with_model: n_batch       = 128
0.00.051.377 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.377 I llama_new_context_with_model: flash_attn    = 0
0.00.051.378 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.378 I llama_new_context_with_model: freq_scale    = 1
0.00.051.378 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.379 I ggml_metal_init: allocating
0.00.051.382 I ggml_metal_init: found device: Apple M4
0.00.051.384 I ggml_metal_init: picking default device: Apple M4
0.00.051.927 I ggml_metal_init: using embedded metal library
0.00.054.272 I ggml_metal_init: GPU name:   Apple M4
0.00.054.274 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.275 I ggml_metal_init: simdgroup reduction   = true
0.00.054.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.275 I ggml_metal_init: has bfloat            = true
0.00.054.275 I ggml_metal_init: use bfloat            = true
0.00.054.276 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.276 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.849 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.851 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.864 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.754 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.755 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.755 I llama_new_context_with_model: graph nodes  = 967
0.00.065.756 I llama_new_context_with_model: graph splits = 2
0.00.065.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.691 I 
0.00.489.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.746 I perplexity: tokenizing the input ..
0.00.497.982 I perplexity: tokenization took 8.234 ms
0.00.497.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.139 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.631.298 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.631.319 I llama_perf_context_print:        load time =     481.27 ms
0.00.631.321 I llama_perf_context_print: prompt eval time =     131.92 ms /   128 tokens (    1.03 ms per token,   970.26 tokens per second)
0.00.631.322 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.631.322 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.631.759 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.078s
sys	0m0.090s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.436 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.736 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.738 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.739 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.564 I llama_model_loader: - type  f32:  194 tensors
0.00.026.564 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.565 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.565 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.911 I llm_load_vocab: special tokens cache size = 25
0.00.052.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.767 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.767 I llm_load_print_meta: arch             = gptneox
0.00.052.768 I llm_load_print_meta: vocab type       = BPE
0.00.052.768 I llm_load_print_meta: n_vocab          = 50304
0.00.052.768 I llm_load_print_meta: n_merges         = 50009
0.00.052.768 I llm_load_print_meta: vocab_only       = 0
0.00.052.768 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.769 I llm_load_print_meta: n_embd           = 2048
0.00.052.769 I llm_load_print_meta: n_layer          = 24
0.00.052.778 I llm_load_print_meta: n_head           = 16
0.00.052.779 I llm_load_print_meta: n_head_kv        = 16
0.00.052.779 I llm_load_print_meta: n_rot            = 32
0.00.052.779 I llm_load_print_meta: n_swa            = 0
0.00.052.781 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.782 I llm_load_print_meta: n_gqa            = 1
0.00.052.783 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.783 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.785 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.785 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.785 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.786 I llm_load_print_meta: n_ff             = 8192
0.00.052.786 I llm_load_print_meta: n_expert         = 0
0.00.052.786 I llm_load_print_meta: n_expert_used    = 0
0.00.052.786 I llm_load_print_meta: causal attn      = 1
0.00.052.786 I llm_load_print_meta: pooling type     = 0
0.00.052.787 I llm_load_print_meta: rope type        = 2
0.00.052.787 I llm_load_print_meta: rope scaling     = linear
0.00.052.787 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.787 I llm_load_print_meta: freq_scale_train = 1
0.00.052.788 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.788 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.788 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.788 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.788 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.788 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.788 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.789 I llm_load_print_meta: model type       = 1.4B
0.00.052.790 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.790 I llm_load_print_meta: model params     = 1.41 B
0.00.052.790 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.791 I llm_load_print_meta: general.name     = 1.4B
0.00.052.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.792 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.792 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.792 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.793 I llm_load_print_meta: LF token         = 128 ''
0.00.052.793 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.793 I llm_load_print_meta: max token length = 1024
0.00.054.555 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.555 I llm_load_tensors: offloading output layer to GPU
0.00.054.555 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.561 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.561 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.409 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.410 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.410 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.410 I llama_new_context_with_model: n_batch       = 2048
0.00.055.410 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.411 I llama_new_context_with_model: flash_attn    = 0
0.00.055.411 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.411 I llama_new_context_with_model: freq_scale    = 1
0.00.055.412 I ggml_metal_init: allocating
0.00.055.415 I ggml_metal_init: found device: Apple M4
0.00.055.417 I ggml_metal_init: picking default device: Apple M4
0.00.056.023 I ggml_metal_init: using embedded metal library
0.00.058.331 I ggml_metal_init: GPU name:   Apple M4
0.00.058.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.334 I ggml_metal_init: simdgroup reduction   = true
0.00.058.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.334 I ggml_metal_init: has bfloat            = true
0.00.058.334 I ggml_metal_init: use bfloat            = true
0.00.058.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.615 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.625 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.697 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.698 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.699 I llama_new_context_with_model: graph nodes  = 967
0.00.088.699 I llama_new_context_with_model: graph splits = 2
0.00.088.724 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.493 I main: llama threadpool init, n_threads = 4
0.00.617.537 I 
0.00.617.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.567 I 
0.00.617.802 I sampler seed: 1234
0.00.617.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.853 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.380.764 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.380.764 I llama_perf_context_print:        load time =     606.05 ms
0.01.380.765 I llama_perf_context_print: prompt eval time =      50.39 ms /     7 tokens (    7.20 ms per token,   138.91 tokens per second)
0.01.380.766 I llama_perf_context_print:        eval time =     709.40 ms /    63 runs   (   11.26 ms per token,    88.81 tokens per second)
0.01.380.766 I llama_perf_context_print:       total time =     763.27 ms /    70 tokens
0.01.380.970 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.706 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.513 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.514 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.057 I llama_model_loader: - type  f32:  194 tensors
0.00.023.057 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.057 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.057 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.169 I llm_load_vocab: special tokens cache size = 25
0.00.049.163 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.165 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.166 I llm_load_print_meta: arch             = gptneox
0.00.049.166 I llm_load_print_meta: vocab type       = BPE
0.00.049.166 I llm_load_print_meta: n_vocab          = 50304
0.00.049.166 I llm_load_print_meta: n_merges         = 50009
0.00.049.167 I llm_load_print_meta: vocab_only       = 0
0.00.049.167 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.167 I llm_load_print_meta: n_embd           = 2048
0.00.049.167 I llm_load_print_meta: n_layer          = 24
0.00.049.181 I llm_load_print_meta: n_head           = 16
0.00.049.183 I llm_load_print_meta: n_head_kv        = 16
0.00.049.183 I llm_load_print_meta: n_rot            = 32
0.00.049.183 I llm_load_print_meta: n_swa            = 0
0.00.049.183 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.184 I llm_load_print_meta: n_gqa            = 1
0.00.049.185 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.185 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.186 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.187 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.187 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.188 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.188 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.188 I llm_load_print_meta: n_ff             = 8192
0.00.049.189 I llm_load_print_meta: n_expert         = 0
0.00.049.189 I llm_load_print_meta: n_expert_used    = 0
0.00.049.189 I llm_load_print_meta: causal attn      = 1
0.00.049.190 I llm_load_print_meta: pooling type     = 0
0.00.049.190 I llm_load_print_meta: rope type        = 2
0.00.049.190 I llm_load_print_meta: rope scaling     = linear
0.00.049.190 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.191 I llm_load_print_meta: freq_scale_train = 1
0.00.049.191 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.191 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.191 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.191 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.191 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.192 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.193 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.193 I llm_load_print_meta: model type       = 1.4B
0.00.049.193 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.194 I llm_load_print_meta: model params     = 1.41 B
0.00.049.194 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.194 I llm_load_print_meta: general.name     = 1.4B
0.00.049.194 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.194 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.195 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.195 I llm_load_print_meta: LF token         = 128 ''
0.00.049.195 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.195 I llm_load_print_meta: max token length = 1024
0.00.051.166 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.167 I llm_load_tensors: offloading output layer to GPU
0.00.051.167 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.177 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.179 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.079 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.079 I llama_new_context_with_model: n_ctx         = 128
0.00.052.079 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.079 I llama_new_context_with_model: n_batch       = 128
0.00.052.080 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.080 I llama_new_context_with_model: flash_attn    = 0
0.00.052.080 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.080 I llama_new_context_with_model: freq_scale    = 1
0.00.052.081 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.081 I ggml_metal_init: allocating
0.00.052.084 I ggml_metal_init: found device: Apple M4
0.00.052.086 I ggml_metal_init: picking default device: Apple M4
0.00.052.651 I ggml_metal_init: using embedded metal library
0.00.054.945 I ggml_metal_init: GPU name:   Apple M4
0.00.054.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.948 I ggml_metal_init: simdgroup reduction   = true
0.00.054.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.948 I ggml_metal_init: has bfloat            = true
0.00.054.948 I ggml_metal_init: use bfloat            = true
0.00.054.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.735 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.737 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.752 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.712 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.713 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.714 I llama_new_context_with_model: graph nodes  = 967
0.00.066.714 I llama_new_context_with_model: graph splits = 2
0.00.066.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.038 I 
0.00.563.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.127 I perplexity: tokenizing the input ..
0.00.571.244 I perplexity: tokenization took 8.114 ms
0.00.571.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.779 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.029 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.049 I llama_perf_context_print:        load time =     554.33 ms
0.00.707.050 I llama_perf_context_print: prompt eval time =     134.30 ms /   128 tokens (    1.05 ms per token,   953.09 tokens per second)
0.00.707.051 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.051 I llama_perf_context_print:       total time =     144.01 ms /   129 tokens
0.00.707.564 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.077s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.579 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.664 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.524 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.525 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.469 I llm_load_vocab: special tokens cache size = 25
0.00.051.456 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.459 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.459 I llm_load_print_meta: arch             = gptneox
0.00.051.460 I llm_load_print_meta: vocab type       = BPE
0.00.051.460 I llm_load_print_meta: n_vocab          = 50304
0.00.051.460 I llm_load_print_meta: n_merges         = 50009
0.00.051.460 I llm_load_print_meta: vocab_only       = 0
0.00.051.460 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.461 I llm_load_print_meta: n_embd           = 2048
0.00.051.461 I llm_load_print_meta: n_layer          = 24
0.00.051.475 I llm_load_print_meta: n_head           = 16
0.00.051.475 I llm_load_print_meta: n_head_kv        = 16
0.00.051.476 I llm_load_print_meta: n_rot            = 32
0.00.051.476 I llm_load_print_meta: n_swa            = 0
0.00.051.476 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.476 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.477 I llm_load_print_meta: n_gqa            = 1
0.00.051.477 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.478 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.479 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.479 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.479 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.479 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.479 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.480 I llm_load_print_meta: n_ff             = 8192
0.00.051.480 I llm_load_print_meta: n_expert         = 0
0.00.051.480 I llm_load_print_meta: n_expert_used    = 0
0.00.051.481 I llm_load_print_meta: causal attn      = 1
0.00.051.484 I llm_load_print_meta: pooling type     = 0
0.00.051.484 I llm_load_print_meta: rope type        = 2
0.00.051.484 I llm_load_print_meta: rope scaling     = linear
0.00.051.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.485 I llm_load_print_meta: freq_scale_train = 1
0.00.051.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.485 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.485 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.487 I llm_load_print_meta: model type       = 1.4B
0.00.051.487 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.487 I llm_load_print_meta: model params     = 1.41 B
0.00.051.488 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.488 I llm_load_print_meta: general.name     = 1.4B
0.00.051.488 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.489 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.489 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.489 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.489 I llm_load_print_meta: LF token         = 128 ''
0.00.051.490 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.491 I llm_load_print_meta: max token length = 1024
0.00.053.160 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.160 I llm_load_tensors: offloading output layer to GPU
0.00.053.160 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.171 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.172 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.001 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.002 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.002 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.003 I llama_new_context_with_model: n_batch       = 2048
0.00.054.003 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.003 I llama_new_context_with_model: flash_attn    = 0
0.00.054.004 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.004 I llama_new_context_with_model: freq_scale    = 1
0.00.054.004 I ggml_metal_init: allocating
0.00.054.011 I ggml_metal_init: found device: Apple M4
0.00.054.013 I ggml_metal_init: picking default device: Apple M4
0.00.054.593 I ggml_metal_init: using embedded metal library
0.00.056.928 I ggml_metal_init: GPU name:   Apple M4
0.00.056.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.931 I ggml_metal_init: simdgroup reduction   = true
0.00.056.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.931 I ggml_metal_init: has bfloat            = true
0.00.056.932 I ggml_metal_init: use bfloat            = true
0.00.056.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.627 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.645 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.601 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.602 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.602 I llama_new_context_with_model: graph nodes  = 967
0.00.086.602 I llama_new_context_with_model: graph splits = 2
0.00.086.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.704 I main: llama threadpool init, n_threads = 4
0.00.710.744 I 
0.00.710.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.777 I 
0.00.711.109 I sampler seed: 1234
0.00.711.118 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.142 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.560.058 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.560.059 I llama_perf_context_print:        load time =     702.12 ms
0.01.560.060 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.38 ms per token,   135.59 tokens per second)
0.01.560.061 I llama_perf_context_print:        eval time =     794.52 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.560.061 I llama_perf_context_print:       total time =     849.36 ms /    70 tokens
0.01.560.291 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.105 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.921 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.886 I llama_model_loader: - type  f32:  194 tensors
0.00.024.886 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.886 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.898 I llm_load_vocab: special tokens cache size = 25
0.00.051.919 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.922 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.922 I llm_load_print_meta: arch             = gptneox
0.00.051.923 I llm_load_print_meta: vocab type       = BPE
0.00.051.923 I llm_load_print_meta: n_vocab          = 50304
0.00.051.923 I llm_load_print_meta: n_merges         = 50009
0.00.051.923 I llm_load_print_meta: vocab_only       = 0
0.00.051.923 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.924 I llm_load_print_meta: n_embd           = 2048
0.00.051.924 I llm_load_print_meta: n_layer          = 24
0.00.051.933 I llm_load_print_meta: n_head           = 16
0.00.051.934 I llm_load_print_meta: n_head_kv        = 16
0.00.051.934 I llm_load_print_meta: n_rot            = 32
0.00.051.934 I llm_load_print_meta: n_swa            = 0
0.00.051.935 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.935 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.938 I llm_load_print_meta: n_gqa            = 1
0.00.051.939 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.941 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.945 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.946 I llm_load_print_meta: n_ff             = 8192
0.00.051.946 I llm_load_print_meta: n_expert         = 0
0.00.051.946 I llm_load_print_meta: n_expert_used    = 0
0.00.051.946 I llm_load_print_meta: causal attn      = 1
0.00.051.946 I llm_load_print_meta: pooling type     = 0
0.00.051.946 I llm_load_print_meta: rope type        = 2
0.00.051.947 I llm_load_print_meta: rope scaling     = linear
0.00.051.947 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.947 I llm_load_print_meta: freq_scale_train = 1
0.00.051.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.947 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.949 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.949 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.949 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.949 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.949 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.950 I llm_load_print_meta: model type       = 1.4B
0.00.051.950 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.950 I llm_load_print_meta: model params     = 1.41 B
0.00.051.951 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.951 I llm_load_print_meta: general.name     = 1.4B
0.00.051.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.953 I llm_load_print_meta: LF token         = 128 ''
0.00.051.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.953 I llm_load_print_meta: max token length = 1024
0.00.053.697 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.697 I llm_load_tensors: offloading output layer to GPU
0.00.053.697 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.702 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.703 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.548 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.548 I llama_new_context_with_model: n_ctx         = 128
0.00.054.549 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.549 I llama_new_context_with_model: n_batch       = 128
0.00.054.549 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.549 I llama_new_context_with_model: flash_attn    = 0
0.00.054.549 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.550 I llama_new_context_with_model: freq_scale    = 1
0.00.054.550 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.550 I ggml_metal_init: allocating
0.00.054.556 I ggml_metal_init: found device: Apple M4
0.00.054.559 I ggml_metal_init: picking default device: Apple M4
0.00.055.117 I ggml_metal_init: using embedded metal library
0.00.057.413 I ggml_metal_init: GPU name:   Apple M4
0.00.057.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.416 I ggml_metal_init: simdgroup reduction   = true
0.00.057.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.416 I ggml_metal_init: has bfloat            = true
0.00.057.416 I ggml_metal_init: use bfloat            = true
0.00.057.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.110 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.971 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.972 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.973 I llama_new_context_with_model: graph nodes  = 967
0.00.068.973 I llama_new_context_with_model: graph splits = 2
0.00.068.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.470 I 
0.00.636.514 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.527 I perplexity: tokenizing the input ..
0.00.644.704 I perplexity: tokenization took 8.175 ms
0.00.644.707 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.572 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.850 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.866 I llama_perf_context_print:        load time =     626.36 ms
0.00.785.867 I llama_perf_context_print: prompt eval time =     139.63 ms /   128 tokens (    1.09 ms per token,   916.71 tokens per second)
0.00.785.868 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.868 I llama_perf_context_print:       total time =     149.40 ms /   129 tokens
0.00.786.340 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.078s
sys	0m0.114s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.802 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.257 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.257 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.258 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.043 I llama_model_loader: - type  f32:  194 tensors
0.00.025.043 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.188 I llm_load_vocab: special tokens cache size = 25
0.00.052.167 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.169 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.170 I llm_load_print_meta: arch             = gptneox
0.00.052.170 I llm_load_print_meta: vocab type       = BPE
0.00.052.170 I llm_load_print_meta: n_vocab          = 50304
0.00.052.170 I llm_load_print_meta: n_merges         = 50009
0.00.052.171 I llm_load_print_meta: vocab_only       = 0
0.00.052.171 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.171 I llm_load_print_meta: n_embd           = 2048
0.00.052.171 I llm_load_print_meta: n_layer          = 24
0.00.052.185 I llm_load_print_meta: n_head           = 16
0.00.052.187 I llm_load_print_meta: n_head_kv        = 16
0.00.052.187 I llm_load_print_meta: n_rot            = 32
0.00.052.187 I llm_load_print_meta: n_swa            = 0
0.00.052.188 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.188 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.189 I llm_load_print_meta: n_gqa            = 1
0.00.052.189 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.190 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.191 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.191 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.192 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.192 I llm_load_print_meta: n_ff             = 8192
0.00.052.193 I llm_load_print_meta: n_expert         = 0
0.00.052.193 I llm_load_print_meta: n_expert_used    = 0
0.00.052.193 I llm_load_print_meta: causal attn      = 1
0.00.052.194 I llm_load_print_meta: pooling type     = 0
0.00.052.196 I llm_load_print_meta: rope type        = 2
0.00.052.196 I llm_load_print_meta: rope scaling     = linear
0.00.052.196 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.196 I llm_load_print_meta: freq_scale_train = 1
0.00.052.197 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.197 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.197 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.197 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.197 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.197 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.197 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.198 I llm_load_print_meta: model type       = 1.4B
0.00.052.198 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.198 I llm_load_print_meta: model params     = 1.41 B
0.00.052.199 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.199 I llm_load_print_meta: general.name     = 1.4B
0.00.052.200 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.200 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.200 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.201 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.201 I llm_load_print_meta: LF token         = 128 ''
0.00.052.202 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.202 I llm_load_print_meta: max token length = 1024
0.00.054.319 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.319 I llm_load_tensors: offloading output layer to GPU
0.00.054.319 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.330 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.331 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.251 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.252 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.252 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.252 I llama_new_context_with_model: n_batch       = 2048
0.00.055.252 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.252 I llama_new_context_with_model: flash_attn    = 0
0.00.055.253 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.253 I llama_new_context_with_model: freq_scale    = 1
0.00.055.253 I ggml_metal_init: allocating
0.00.055.260 I ggml_metal_init: found device: Apple M4
0.00.055.262 I ggml_metal_init: picking default device: Apple M4
0.00.055.863 I ggml_metal_init: using embedded metal library
0.00.058.202 I ggml_metal_init: GPU name:   Apple M4
0.00.058.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.205 I ggml_metal_init: simdgroup reduction   = true
0.00.058.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.206 I ggml_metal_init: has bfloat            = true
0.00.058.207 I ggml_metal_init: use bfloat            = true
0.00.058.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.806 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.814 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.832 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.913 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.915 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.915 I llama_new_context_with_model: graph nodes  = 967
0.00.089.915 I llama_new_context_with_model: graph splits = 2
0.00.089.940 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.150 I main: llama threadpool init, n_threads = 4
0.00.768.185 I 
0.00.768.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.213 I 
0.00.768.473 I sampler seed: 1234
0.00.768.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.493 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.493 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.647.051 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.647.052 I llama_perf_context_print:        load time =     758.35 ms
0.01.647.053 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.87 tokens per second)
0.01.647.054 I llama_perf_context_print:        eval time =     821.21 ms /    63 runs   (   13.04 ms per token,    76.72 tokens per second)
0.01.647.055 I llama_perf_context_print:       total time =     878.90 ms /    70 tokens
0.01.647.243 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4342 (b1977975) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.423 I llama_model_loader: - type  f32:  194 tensors
0.00.023.424 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.474 I llm_load_vocab: special tokens cache size = 25
0.00.050.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.763 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.764 I llm_load_print_meta: arch             = gptneox
0.00.050.764 I llm_load_print_meta: vocab type       = BPE
0.00.050.764 I llm_load_print_meta: n_vocab          = 50304
0.00.050.764 I llm_load_print_meta: n_merges         = 50009
0.00.050.765 I llm_load_print_meta: vocab_only       = 0
0.00.050.765 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.765 I llm_load_print_meta: n_embd           = 2048
0.00.050.765 I llm_load_print_meta: n_layer          = 24
0.00.050.779 I llm_load_print_meta: n_head           = 16
0.00.050.781 I llm_load_print_meta: n_head_kv        = 16
0.00.050.781 I llm_load_print_meta: n_rot            = 32
0.00.050.781 I llm_load_print_meta: n_swa            = 0
0.00.050.781 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.782 I llm_load_print_meta: n_gqa            = 1
0.00.050.783 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.784 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.786 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.786 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.786 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.787 I llm_load_print_meta: n_ff             = 8192
0.00.050.787 I llm_load_print_meta: n_expert         = 0
0.00.050.787 I llm_load_print_meta: n_expert_used    = 0
0.00.050.787 I llm_load_print_meta: causal attn      = 1
0.00.050.788 I llm_load_print_meta: pooling type     = 0
0.00.050.788 I llm_load_print_meta: rope type        = 2
0.00.050.788 I llm_load_print_meta: rope scaling     = linear
0.00.050.788 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.788 I llm_load_print_meta: freq_scale_train = 1
0.00.050.789 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.789 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.789 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.789 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.789 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.789 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.789 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.790 I llm_load_print_meta: model type       = 1.4B
0.00.050.790 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.790 I llm_load_print_meta: model params     = 1.41 B
0.00.050.791 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.791 I llm_load_print_meta: general.name     = 1.4B
0.00.050.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.792 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.792 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.792 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.793 I llm_load_print_meta: LF token         = 128 ''
0.00.050.793 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.793 I llm_load_print_meta: max token length = 1024
0.00.052.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.655 I llm_load_tensors: offloading output layer to GPU
0.00.052.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.661 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.662 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.574 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.575 I llama_new_context_with_model: n_ctx         = 128
0.00.053.575 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.575 I llama_new_context_with_model: n_batch       = 128
0.00.053.576 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.576 I llama_new_context_with_model: flash_attn    = 0
0.00.053.576 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.576 I llama_new_context_with_model: freq_scale    = 1
0.00.053.577 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.577 I ggml_metal_init: allocating
0.00.053.581 I ggml_metal_init: found device: Apple M4
0.00.053.583 I ggml_metal_init: picking default device: Apple M4
0.00.054.136 I ggml_metal_init: using embedded metal library
0.00.056.519 I ggml_metal_init: GPU name:   Apple M4
0.00.056.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.521 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.521 I ggml_metal_init: simdgroup reduction   = true
0.00.056.521 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.521 I ggml_metal_init: has bfloat            = true
0.00.056.522 I ggml_metal_init: use bfloat            = true
0.00.056.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.695 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.710 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.656 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.657 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.657 I llama_new_context_with_model: graph nodes  = 967
0.00.068.658 I llama_new_context_with_model: graph splits = 2
0.00.068.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.578 I 
0.00.391.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.635 I perplexity: tokenizing the input ..
0.00.399.534 I perplexity: tokenization took 7.898 ms
0.00.399.542 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.539.597 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.540.789 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.540.812 I llama_perf_context_print:        load time =     382.84 ms
0.00.540.813 I llama_perf_context_print: prompt eval time =     139.83 ms /   128 tokens (    1.09 ms per token,   915.41 tokens per second)
0.00.540.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.540.814 I llama_perf_context_print:       total time =     149.24 ms /   129 tokens
0.00.541.228 I ggml_metal_free: deallocating

real	0m0.554s
user	0m0.080s
sys	0m0.085s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4342 (b1977975)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a407740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a407e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a408400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a4089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a408f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a409510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a409ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a40a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a40a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a40ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a40b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a40b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a40c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a40c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a40d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a40d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a40de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a40e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a40ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a40f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a40fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a410290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a4109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a411250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a411970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a411c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a412240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a412eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a4133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a4136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a413b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a413e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a4146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a414be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a414ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a415340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a4157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a415c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a416120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a4165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a416a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a416f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a4173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a417840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a417b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a418110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a418720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a419040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a419650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a419c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a41a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a41a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a41ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a41b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a41bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a41c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a41c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a41c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a41cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a41d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a41d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a41ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a41e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a41e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a41ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a41f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a41f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a41f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a41fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a4202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a420790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a420c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a4210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a421620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a421b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a4220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a422610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a422b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a4230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a423600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a423b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a4240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a4245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a424b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a425090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a4255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a425b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a426080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a4265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a426b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a427070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a4275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a427b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a428060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a4285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a428b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a429050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a418d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a4294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a429c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a42a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a42a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a42ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a42b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a42b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a42bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a42c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a42c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a42cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a42d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a42d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a42dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a42e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a42e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a42eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a42ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a42f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a42f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a42fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a4301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a430680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a430b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a430fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a431460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a431900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a431da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a432240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a4326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a432b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a433020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a4334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a433960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a433e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a4342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a434740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a434be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a435080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a435520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a4359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a435e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a436300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a4367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a436c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a4370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a437580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a437a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a437ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a438360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a438800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a438ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a439140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a4395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a439a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a439f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a43a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a43a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a43ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a43b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a43b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a43bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a43bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a43c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a43c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a43cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a43d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a43d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a43db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a43dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a43e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a43e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a43edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a43f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a43f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a43fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a440040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a4404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a440980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a440e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a4412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a441760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a441c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a4420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a442540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a4429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a442e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a443320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a4437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a443c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a444100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a4445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a444a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a444ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a445380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a4458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a445e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a446370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a4468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a446b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a447190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a4477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a447db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a4485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a448a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a448d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a449310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a449920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a44a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a44a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a44aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a44aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a44b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a44bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a44c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a44c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a44cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a44d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a44d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a44dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a44e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a44e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a44ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a44f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a44f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a44fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a450100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a450650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a450ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a4510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a451640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a451b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a4520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a452630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a452b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a4530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a453620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a453b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a4540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a454610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a454b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a4550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a455600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a455b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a4560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a4565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a456b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a457090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a4575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a457b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a458080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a4585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a458b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a459070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a4595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a459b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a45a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a45a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a45ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a45b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a45b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a45baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a45c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a45c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a45cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a45d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a45d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a45dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a45e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a45e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a45e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a45ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a45f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a45f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a45fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a460080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a460520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a4609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a460e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a461300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a4617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a461c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a4620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a462580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a462ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a4631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a463910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a464030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a464750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a464a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a465200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a4654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a465ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f904dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f905240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f9056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f905b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f905f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f906400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f906870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f906ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f907150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f9075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f907a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f908120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f908c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f9093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f909c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f90a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f90aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f90b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f90b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f90bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f90c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f90cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f90d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f90dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f90e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f90e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f90e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f90ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f90f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f90f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f90fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f90ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f910430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f9106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f910b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f910fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f911440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f9118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f911d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f912190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f912600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f912a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f912ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f913350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f9137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f913c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f9140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f914510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f914980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f914df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f915260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f9156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f915b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f915fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f916420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f916890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f916e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f917300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f917770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f917be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f918050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f9184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f918930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f918da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f919210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f919680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f919af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f919f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f91a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f91a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f91acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f91b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f91b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f91ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f91be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f91c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f91c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f91cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f91d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f91d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f91d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f91dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f91e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f91e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f91ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f91ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f91f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f91f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f91fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f920100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f920570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f9209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f920e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f9212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f921730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f921ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f922010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f922480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f9228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f922d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f9231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f923640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f923ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f923f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f924390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f924800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f924c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f9250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f925550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f9259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f925e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f9262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f926710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f926b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f926ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f927460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f9278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f927d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f9281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f928620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f928a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f928f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f929370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f9297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f929c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f92a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f92a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f92a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f92ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f92b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f92b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f92bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f92bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f92c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f92c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f92cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f92d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f92d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f92da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f92dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f92e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f92e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f92ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f92f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f92f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f92f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f92fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f930260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f9306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f930b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f930fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f931420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f931890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f931d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f932170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f9325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f932a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f932ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f933330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a5045d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a504a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a504eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a505320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a505790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a505c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a506070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a5064e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a506950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a506dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a507230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a5076a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a507b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a507f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a5083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a508860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a508cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a509140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a5095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a509a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a509e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a50a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a50a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a50abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a50b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a50b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a50b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a50bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a50c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a50c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a50caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a50cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a50d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a50d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a50dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a50e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a50e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a50ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a50ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a50f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a50f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a50fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a510030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a5104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a510910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a510d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a5111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a511660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a511ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a511f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a5123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a512f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a5131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a5134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a513920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a513d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a514200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a514670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a514ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a514f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a5153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a515830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a515ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a516110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a516580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a5169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a516e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a5172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a517740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a517bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a518020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a518490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a518900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a518d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a5191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a519650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a519ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a519f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a51a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a51a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a51ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a51b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a51b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a51b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a51be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a51c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a51c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a51cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a51d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a51d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a51d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a51dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a51e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a51e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a51eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a51ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a51f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a51f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a51fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a5200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a520540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a5209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a520e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a521290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a521700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a521b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a521fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a522450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a5228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a522d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a5231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a523610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a523a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a523ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a524360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a5247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a524c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a5250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a525520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a525990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a525e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a526270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a5266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a526b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a5275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a527ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a528400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a528b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a528de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a529250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a529850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a529e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f904ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f905150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f9055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f905a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f905ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f906310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f906780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f906bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f907060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f9074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f907940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f907f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f908810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f908f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f909770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f909e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f90a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f90ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f90b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f90bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f90c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f90ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f90d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f90d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f90df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f90e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f90e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f90ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f90f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f90f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f90fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f90fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f9102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f9105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f910a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f910e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f9112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f911760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f911bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f912040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f9124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f912920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f912d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f913200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f913670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f913ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f913f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f9143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f914830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f914ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f915110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f915580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f9159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f915e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f9162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f916740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f916bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f917020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f917490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f917900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f917d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f9181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f918650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f918ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f918f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f9193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f919810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f919c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f91a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f91a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f91a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f91ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f91b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f91b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f91bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f91c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f91c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f91c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f91cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f91d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f91d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f91daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f91df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f91e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f91e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f91ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f91f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f91f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f91f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f91fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f920290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f920700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f920b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f920fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f921450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f9218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f921d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f9221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f922610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f922a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f922ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f923360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f9237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f923c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f9240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f924520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f924990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f924e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f925270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f9256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f925b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f925fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f926430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f9268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f926d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f927180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f9275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f927a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f927ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f928340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f9287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f928c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f929090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f929500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f929970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f929de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f92a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f92a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f92ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f92afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f92b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f92b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f92bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f92c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f92c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f92ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f92ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f92d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f92d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f92dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f92e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f92e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f92e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f92edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f92f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f92f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f92fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f92ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f9303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f930860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f930cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f931140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f9315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f931a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f931e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f932300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f932770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f932be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f933050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f9334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f933a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f933f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f934450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f934960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f934e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f935380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f935890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f935da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f9362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f9367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f936cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f9371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f9376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f937c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f938110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f938620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f938b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f939040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f939550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f939a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f939f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f93a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f93a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f93aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f93b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f93b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f93bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f93c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f93c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f93cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f93d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f93d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f93dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f93e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f93e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f93ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f93f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f93f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f93fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f9402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f9408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f9410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f941540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f941800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f941e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f942420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f942c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f9430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f943550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f9439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f9441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f9446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f944c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f945190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f9456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f945c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f946180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f9466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f946c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f947170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f9476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f947c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f948160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f9486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f948c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f949150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f9496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f949bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f94a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f94a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f94abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f94b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f94b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f94bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f94c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f94c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f94cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f94d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f94d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f94dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f94e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f94e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f94eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f94f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f94f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f94fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f9500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f950630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f950b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f9510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f951620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f951b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f9520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f952610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f952b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f9530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f953600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f953b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f9540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f9545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f954b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f955090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f9555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f955b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f956080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f9565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f956b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f956fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f957460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f957900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f957da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f958240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f9586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f958b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f959020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f9594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f959960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f959e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f95a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f95a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f95abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f95b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f95b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f95bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f95c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f95cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f95d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f95d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f95dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f95dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f95e5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.819s
user	0m0.293s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4342 (b1977975)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15280ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15280b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15280b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15280bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15280c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15280caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15280d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15280d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15280dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15280e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15280e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15280eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15280f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15280fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1528105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152810d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152811420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152811b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152812260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152812a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152813870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152813f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152814830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152814f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152815820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152816490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1528169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1528173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1528181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152818480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152818920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152818dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152819260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152819700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152819ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15281a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15281a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15281a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15281ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15281b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15281b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15281bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15281c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15281cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15281d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15281d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15281de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15281e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15281ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15281f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15281f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15281fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15281fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152820480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152820c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152820f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1528213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152821870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152821d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1528221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152822650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152822af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152823430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1528238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152823d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152824210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1528246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1528256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152826140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152826690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152826be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152827680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152827bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152828120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152828670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152828bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152829660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152829bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15282a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15282a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15282aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15282b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15282b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15282bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15282c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15282c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15281c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15282caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15282d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15282d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15282dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15282e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15282e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15282ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15282f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15282f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15282fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152830220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152830770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152830cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152831210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152831760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152831c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1528320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152832540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1528329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152832e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152833320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1528337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152833c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152834100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1528345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152834a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152834ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152835380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152835820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152835cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152836160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152836aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152836f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1528373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152837880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152837d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1528381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152838660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152838b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152838fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1528398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152839d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15283a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15283a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15283ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15283b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15283b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15283b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15283bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15283c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15283c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15283cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15283d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15283d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15283d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15283de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15283e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15283e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15283ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15283f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15283f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15283fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15283fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152840340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1528407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152840c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152841120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1528415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152841f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1528423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152842840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152842ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152843ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152843f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1528448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152844d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1528451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152845680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152845b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152845fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152846460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152846900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152847240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1528476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152847b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152848020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1528484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152848960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152849400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152849950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152849ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15284a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15284a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15284ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15284b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15284bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15284c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15284c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15284c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15284cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15284d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15284db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15284e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15284e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15284ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15284f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15284f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15284fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1528501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152850710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152850c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1528511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152851700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152851c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1528521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1528526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152852c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152853190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1528536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152853c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152854180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1528546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152854c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152855170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1528556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152855c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152856160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1528566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152856c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152857150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1528576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152857bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152858140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152858690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152858be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152859130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152859680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152859bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15285a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15285a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15285abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15285b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15285b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15285bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15285c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15285c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15285cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15285d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15285d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15285db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15285e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15285e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15285eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15285f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15285f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15285fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1528600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152860610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152860b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1528610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152861600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152861f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1528623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152862880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152862d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1528631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152863660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152863b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152863fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152864440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1528648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152864d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152865220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1528656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152865b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1528660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1528667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152866ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152867610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152867d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152867ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1528687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152868aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1528690b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152a04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152a04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152a053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152a05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152a05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152a06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152a06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152a069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152a06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152a07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152a077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152a07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152a08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152a09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152a09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152a0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152a0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152a0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152a0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152a0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152a0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152a0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152a0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152a0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152a0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152a0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152a0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152a0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152a0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152a0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152a0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152a0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152a10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152a104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152a10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152a10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152a11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152a11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152a11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152a11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152a123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152a12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152a12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152a13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152a13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152a13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152a13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152a142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152a14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152a14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152a15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152a154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152a15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152a15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152a161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152a16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152a16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152a170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152a17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152a179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152a17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152a18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152a18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152a18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152a18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152a19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152a198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152a19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152a1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152a1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152a1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152a1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152a1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152a1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152a1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152a1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152a1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152a1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152a1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152a1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152a1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152a1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152a1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152a1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152a1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152a1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152a1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152a1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152a1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152a1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152a20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152a207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152a20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152a21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152a21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152a21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152a21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152a22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152a226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152a22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152a22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152a23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152a23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152a23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152a24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152a245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152a24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152a24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152a25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152a25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152a25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152a26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152a264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152a26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152a26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152a27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152a276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152a27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152a27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152a283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152a28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152a28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152a29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152a295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152a29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152a29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152a2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152a2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152a2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152a2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152a2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152a2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152a2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152a2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152a2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152a2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152a2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152a2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152a2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152a2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152a2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152a2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152a2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152a2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152a2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152a2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152a2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152a30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152a304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152a30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152a30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152a311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152a31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152a31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152a31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152a323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152a32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152a32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152a33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152a33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152a339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152a33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152a342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152a34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152a34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152a35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152a35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152a358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152a35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152a361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152a36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152a36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152a36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152a37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152a37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152a37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152a380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152a38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152a389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152a38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152a392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152a39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152a39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152a39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152a3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152a3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152a3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152a3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152a3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152a3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152a3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152a3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152a3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152a3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152a3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152a3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152a3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152a3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152a3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152a3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152a3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152a3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152a3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152a3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152a3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152a40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152a40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152a40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152a41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152a41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152a41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152a42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152a42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152a429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152a42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152a43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152a43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152a43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152a43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152a44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152a448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152a44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152a451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152a45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152a45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152a45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152a46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152a467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152a46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152a470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152a47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152a47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152a47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152a48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152a486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152a48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152a48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152a49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152a498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152a49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152a4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152a4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152a4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152a4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152a4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152a4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152a4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152a4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152a4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152a4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152a4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152a4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152a4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152a4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152a4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152a4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152a4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152a4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152a4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152a4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152a4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152a4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152a50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152a50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152a50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152a51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152a514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152a51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152a51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152a526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152a52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152a52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152a533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152a53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152a53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152a54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152a545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152a54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152a54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152a55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152a55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152a55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152a56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152a56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152a57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152a57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152a57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152a582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152a588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152a58ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151e04f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151e05380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151e057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151e05c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151e060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151e06540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151e069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151e06e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151e07290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151e07800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151e07c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151e082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151e08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151e095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151e09dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151e0a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151e0ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151e0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151e0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151e0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151e0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151e0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151e0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151e0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151e0e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151e0e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151e0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151e0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151e0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151e0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151e0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151e10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151e106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151e10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151e10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151e11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151e116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151e11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151e11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151e12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151e12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151e12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151e13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151e135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151e13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151e13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151e14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151e14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151e14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151e15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151e154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151e15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151e15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151e16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151e16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151e16b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151e17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151e17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151e179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151e17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151e182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151e18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151e18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151e19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151e19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151e198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151e19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151e1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151e1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151e1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151e1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151e1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151e1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151e1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151e1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151e1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151e1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151e1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151e1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151e1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151e1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151e1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151e1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151e1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151e1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151e1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151e1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151e1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151e1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151e20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151e207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151e20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151e210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151e21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151e219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151e21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151e22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151e226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151e22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151e22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151e23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151e238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151e23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151e24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151e24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151e24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151e24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151e25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151e257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151e25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151e260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151e26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151e26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151e26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151e27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151e276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151e27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151e27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151e28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151e28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151e28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151e29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151e295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151e29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151e29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151e2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151e2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151e2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151e2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151e2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151e2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151e2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151e2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151e2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151e2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151e2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151e2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151e2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151e2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151e2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151e2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151e2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151e2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151e2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151e2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151e30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151e304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151e30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151e31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151e31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151e31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151e31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151e323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151e32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151e32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151e33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151e335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151e33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151e33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151e342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151e34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151e34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151e35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151e354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151e35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151e35d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151e36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151e36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151e36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151e36f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151e373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151e37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151e37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151e38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151e38580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151e389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151e38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151e392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151e39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151e3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151e3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151e3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151e3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151e3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151e3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151e3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151e3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151e3c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151e3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151e3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151e3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151e3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151e3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151e3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151e3e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151e3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151e3eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151e3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151e3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151e3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151e3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151e401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151e40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151e40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151e41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151e414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151e41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151e42460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151e42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151e429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151e42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151e432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151e43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151e43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151e44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151e44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151e448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151e44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151e451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151e45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151e45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151e45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151e46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151e46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151e46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151e470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151e47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151e479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151e47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151e482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151e48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151e48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151e48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151e49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151e498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151e49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151e4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151e4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151e4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151e4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151e4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151e4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151e4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151e4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151e4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151e4ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151e4d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151e4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151e4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151e4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151e4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151e4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151e4ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151e4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151e4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151e4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151e4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151e50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151e506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151e50b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151e50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151e513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151e51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151e51cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151e52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151e525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151e52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151e52e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151e53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151e53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151e53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151e54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151e544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151e54da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151e55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151e55680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151e55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151e55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151e563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151e56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151e57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151e57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151e58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151e58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151e590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151e596e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.245s
sys	0m0.143s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.60 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.62 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.23 sec*proc (2 tests)

Total Test time (real) =   1.23 sec
        1.25 real         0.75 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.29 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.57 sec*proc (2 tests)

Total Test time (real) =   0.58 sec
        0.58 real         0.15 user         0.05 sys
```
