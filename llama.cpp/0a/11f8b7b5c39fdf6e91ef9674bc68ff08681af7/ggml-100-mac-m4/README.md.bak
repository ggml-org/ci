### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.25 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.49 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.57 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.09 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.45 sec*proc (28 tests)

Total Test time (real) = 222.46 sec

real	3m42.498s
user	7m44.540s
sys	0m6.155s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.10 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.30 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   27.68 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.29 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.25 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  49.66 sec*proc (28 tests)

Total Test time (real) =  49.67 sec

real	0m49.684s
user	1m11.778s
sys	0m4.765s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.068 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.295 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.304 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.306 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.306 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.307 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.309 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.309 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.310 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.311 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.311 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.318 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.318 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.319 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.320 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.320 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.324 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.324 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.526 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.529 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.529 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.530 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.530 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.531 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.531 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.532 I llama_model_loader: - type  f32:  124 tensors
0.00.028.532 I llama_model_loader: - type  f16:   73 tensors
0.00.032.968 I llm_load_vocab: special tokens cache size = 5
0.00.035.333 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.337 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.338 I llm_load_print_meta: arch             = bert
0.00.035.338 I llm_load_print_meta: vocab type       = WPM
0.00.035.339 I llm_load_print_meta: n_vocab          = 30522
0.00.035.339 I llm_load_print_meta: n_merges         = 0
0.00.035.339 I llm_load_print_meta: vocab_only       = 0
0.00.035.339 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.340 I llm_load_print_meta: n_embd           = 384
0.00.035.340 I llm_load_print_meta: n_layer          = 12
0.00.035.343 I llm_load_print_meta: n_head           = 12
0.00.035.344 I llm_load_print_meta: n_head_kv        = 12
0.00.035.344 I llm_load_print_meta: n_rot            = 32
0.00.035.345 I llm_load_print_meta: n_swa            = 0
0.00.035.345 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.345 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.346 I llm_load_print_meta: n_gqa            = 1
0.00.035.347 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.348 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.348 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.350 I llm_load_print_meta: n_ff             = 1536
0.00.035.350 I llm_load_print_meta: n_expert         = 0
0.00.035.351 I llm_load_print_meta: n_expert_used    = 0
0.00.035.351 I llm_load_print_meta: causal attn      = 0
0.00.035.351 I llm_load_print_meta: pooling type     = 2
0.00.035.351 I llm_load_print_meta: rope type        = 2
0.00.035.351 I llm_load_print_meta: rope scaling     = linear
0.00.035.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.353 I llm_load_print_meta: freq_scale_train = 1
0.00.035.353 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.354 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.355 I llm_load_print_meta: model type       = 33M
0.00.035.355 I llm_load_print_meta: model ftype      = F16
0.00.035.356 I llm_load_print_meta: model params     = 33.21 M
0.00.035.357 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.357 I llm_load_print_meta: general.name     = Bge Small
0.00.035.357 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.358 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.358 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.358 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.359 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.359 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.359 I llm_load_print_meta: max token length = 21
0.00.037.427 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.428 I llm_load_tensors: offloading output layer to GPU
0.00.037.428 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.452 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.453 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.997 I llama_new_context_with_model: n_seq_max     = 1
0.00.037.998 I llama_new_context_with_model: n_ctx         = 512
0.00.037.998 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.037.999 I llama_new_context_with_model: n_batch       = 2048
0.00.037.999 I llama_new_context_with_model: n_ubatch      = 2048
0.00.037.999 I llama_new_context_with_model: flash_attn    = 0
0.00.038.000 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.000 I llama_new_context_with_model: freq_scale    = 1
0.00.038.001 I ggml_metal_init: allocating
0.00.038.005 I ggml_metal_init: found device: Apple M4
0.00.038.008 I ggml_metal_init: picking default device: Apple M4
0.00.038.860 I ggml_metal_init: using embedded metal library
0.00.043.231 I ggml_metal_init: GPU name:   Apple M4
0.00.043.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.234 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.235 I ggml_metal_init: simdgroup reduction   = true
0.00.043.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.235 I ggml_metal_init: has bfloat            = true
0.00.043.236 I ggml_metal_init: use bfloat            = true
0.00.043.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.055.909 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.056.557 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.560 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.561 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.482 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.483 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.484 I llama_new_context_with_model: graph nodes  = 429
0.00.057.484 I llama_new_context_with_model: graph splits = 2
0.00.057.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.057.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.064.055 I 
0.00.064.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.064.733 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.468 I llama_perf_context_print:        load time =      46.05 ms
0.00.069.469 I llama_perf_context_print: prompt eval time =       4.57 ms /     9 tokens (    0.51 ms per token,  1968.07 tokens per second)
0.00.069.470 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.471 I llama_perf_context_print:       total time =       5.41 ms /    10 tokens
0.00.069.606 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.050s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.837 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.946 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.951 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.953 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.953 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.954 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.955 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.955 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.955 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.956 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.956 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.959 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.959 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.959 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.960 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.960 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.960 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.960 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.265 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.267 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.267 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.267 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.268 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.016.268 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.268 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.016.269 I llama_model_loader: - type  f32:  124 tensors
0.00.016.269 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.808 I llm_load_vocab: special tokens cache size = 5
0.00.020.159 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.161 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.161 I llm_load_print_meta: arch             = bert
0.00.020.162 I llm_load_print_meta: vocab type       = WPM
0.00.020.162 I llm_load_print_meta: n_vocab          = 30522
0.00.020.162 I llm_load_print_meta: n_merges         = 0
0.00.020.162 I llm_load_print_meta: vocab_only       = 0
0.00.020.162 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.163 I llm_load_print_meta: n_embd           = 384
0.00.020.163 I llm_load_print_meta: n_layer          = 12
0.00.020.165 I llm_load_print_meta: n_head           = 12
0.00.020.166 I llm_load_print_meta: n_head_kv        = 12
0.00.020.166 I llm_load_print_meta: n_rot            = 32
0.00.020.167 I llm_load_print_meta: n_swa            = 0
0.00.020.167 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.167 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.168 I llm_load_print_meta: n_gqa            = 1
0.00.020.168 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.169 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.169 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.170 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.170 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.170 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.170 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.171 I llm_load_print_meta: n_ff             = 1536
0.00.020.171 I llm_load_print_meta: n_expert         = 0
0.00.020.171 I llm_load_print_meta: n_expert_used    = 0
0.00.020.171 I llm_load_print_meta: causal attn      = 0
0.00.020.171 I llm_load_print_meta: pooling type     = 2
0.00.020.171 I llm_load_print_meta: rope type        = 2
0.00.020.172 I llm_load_print_meta: rope scaling     = linear
0.00.020.172 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.174 I llm_load_print_meta: freq_scale_train = 1
0.00.020.174 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.174 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.175 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.175 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.175 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.175 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.175 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.175 I llm_load_print_meta: model type       = 33M
0.00.020.176 I llm_load_print_meta: model ftype      = Q8_0
0.00.020.176 I llm_load_print_meta: model params     = 33.21 M
0.00.020.177 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.020.178 I llm_load_print_meta: general.name     = Bge Small
0.00.020.178 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.178 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.178 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.179 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.179 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.179 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.179 I llm_load_print_meta: max token length = 21
0.00.021.544 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.021.545 I llm_load_tensors: offloading output layer to GPU
0.00.021.545 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.021.551 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.551 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.926 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.927 I llama_new_context_with_model: n_ctx         = 512
0.00.021.927 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.927 I llama_new_context_with_model: n_batch       = 2048
0.00.021.928 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.928 I llama_new_context_with_model: flash_attn    = 0
0.00.021.928 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.929 I llama_new_context_with_model: freq_scale    = 1
0.00.021.929 I ggml_metal_init: allocating
0.00.021.936 I ggml_metal_init: found device: Apple M4
0.00.021.939 I ggml_metal_init: picking default device: Apple M4
0.00.022.637 I ggml_metal_init: using embedded metal library
0.00.025.212 I ggml_metal_init: GPU name:   Apple M4
0.00.025.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.216 I ggml_metal_init: simdgroup reduction   = true
0.00.025.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.216 I ggml_metal_init: has bfloat            = true
0.00.025.216 I ggml_metal_init: use bfloat            = true
0.00.025.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.481 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.035.957 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.959 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.961 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.036.551 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.036.553 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.036.553 I llama_new_context_with_model: graph nodes  = 429
0.00.036.553 I llama_new_context_with_model: graph splits = 2
0.00.036.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.041 I 
0.00.041.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.583 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.026 I llama_perf_context_print:        load time =      30.20 ms
0.00.046.027 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2088.17 tokens per second)
0.00.046.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.028 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.046.223 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.183 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.933 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.864 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.869 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.871 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.872 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.880 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.881 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.881 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.883 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.884 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.884 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.887 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.888 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.891 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.891 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.892 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.893 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.893 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.693 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.694 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.694 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.695 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.695 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.695 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.696 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.696 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.696 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.697 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.697 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.697 I llama_model_loader: - type  f32:   40 tensors
0.00.047.698 I llama_model_loader: - type  f16:   30 tensors
0.00.066.156 W llm_load_vocab: empty token at index 5
0.00.070.798 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.166 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.197 I llm_load_vocab: special tokens cache size = 5
0.00.330.421 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.330.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.427 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.428 I llm_load_print_meta: vocab type       = BPE
0.00.330.428 I llm_load_print_meta: n_vocab          = 61056
0.00.330.428 I llm_load_print_meta: n_merges         = 39382
0.00.330.428 I llm_load_print_meta: vocab_only       = 0
0.00.330.428 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.428 I llm_load_print_meta: n_embd           = 384
0.00.330.430 I llm_load_print_meta: n_layer          = 4
0.00.330.434 I llm_load_print_meta: n_head           = 12
0.00.330.435 I llm_load_print_meta: n_head_kv        = 12
0.00.330.435 I llm_load_print_meta: n_rot            = 32
0.00.330.436 I llm_load_print_meta: n_swa            = 0
0.00.330.436 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.437 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.437 I llm_load_print_meta: n_gqa            = 1
0.00.330.440 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.440 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.441 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.442 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.442 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.443 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.443 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.444 I llm_load_print_meta: n_ff             = 1536
0.00.330.444 I llm_load_print_meta: n_expert         = 0
0.00.330.446 I llm_load_print_meta: n_expert_used    = 0
0.00.330.446 I llm_load_print_meta: causal attn      = 0
0.00.330.446 I llm_load_print_meta: pooling type     = -1
0.00.330.446 I llm_load_print_meta: rope type        = -1
0.00.330.447 I llm_load_print_meta: rope scaling     = linear
0.00.330.447 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.447 I llm_load_print_meta: freq_scale_train = 1
0.00.330.447 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.448 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.449 I llm_load_print_meta: model type       = 33M
0.00.330.450 I llm_load_print_meta: model ftype      = F16
0.00.330.451 I llm_load_print_meta: model params     = 32.90 M
0.00.330.451 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.451 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.452 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.330.452 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.330.452 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.330.452 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.330.453 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.330.453 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.330.453 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.330.454 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.330.454 I llm_load_print_meta: max token length = 45
0.00.331.664 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.665 I llm_load_tensors: offloading output layer to GPU
0.00.331.665 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.688 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.689 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.332.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.332.381 I llama_new_context_with_model: n_ctx         = 8192
0.00.332.381 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.332.381 I llama_new_context_with_model: n_batch       = 2048
0.00.332.382 I llama_new_context_with_model: n_ubatch      = 2048
0.00.332.382 I llama_new_context_with_model: flash_attn    = 0
0.00.332.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.332.383 I llama_new_context_with_model: freq_scale    = 1
0.00.332.383 I ggml_metal_init: allocating
0.00.332.386 I ggml_metal_init: found device: Apple M4
0.00.332.388 I ggml_metal_init: picking default device: Apple M4
0.00.333.363 I ggml_metal_init: using embedded metal library
0.00.336.266 I ggml_metal_init: GPU name:   Apple M4
0.00.336.268 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.268 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.269 I ggml_metal_init: simdgroup reduction   = true
0.00.336.269 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.269 I ggml_metal_init: has bfloat            = true
0.00.336.269 I ggml_metal_init: use bfloat            = true
0.00.336.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.694 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.348.110 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.112 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.113 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.647 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.648 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.648 I llama_new_context_with_model: graph nodes  = 154
0.00.348.648 I llama_new_context_with_model: graph splits = 2
0.00.348.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.963 I 
0.00.357.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.145 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.146 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.149 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.149 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.153 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.154 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.671 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.334 I llama_perf_context_print:        load time =     336.02 ms
0.00.362.336 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16963.06 tokens per second)
0.00.362.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.337 I llama_perf_context_print:       total time =       4.37 ms /    63 tokens
0.00.362.580 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.341s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.256 I main: llama backend init
0.00.000.264 I main: load the model and apply lora adapter, if any
0.00.054.966 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.071.248 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.071.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.071.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.071.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.071.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.071.262 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.071.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.265 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.266 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.269 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.090.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.090.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.090.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.090.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.090.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.090.360 I llama_model_loader: - type  f32:  194 tensors
0.00.090.361 I llama_model_loader: - type  f16:   98 tensors
0.00.123.310 I llm_load_vocab: special tokens cache size = 25
0.00.130.243 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.130.246 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.130.246 I llm_load_print_meta: arch             = gptneox
0.00.130.246 I llm_load_print_meta: vocab type       = BPE
0.00.130.246 I llm_load_print_meta: n_vocab          = 50304
0.00.130.247 I llm_load_print_meta: n_merges         = 50009
0.00.130.247 I llm_load_print_meta: vocab_only       = 0
0.00.130.247 I llm_load_print_meta: n_ctx_train      = 2048
0.00.130.247 I llm_load_print_meta: n_embd           = 2048
0.00.130.247 I llm_load_print_meta: n_layer          = 24
0.00.130.251 I llm_load_print_meta: n_head           = 16
0.00.130.252 I llm_load_print_meta: n_head_kv        = 16
0.00.130.252 I llm_load_print_meta: n_rot            = 32
0.00.130.252 I llm_load_print_meta: n_swa            = 0
0.00.130.254 I llm_load_print_meta: n_embd_head_k    = 128
0.00.130.254 I llm_load_print_meta: n_embd_head_v    = 128
0.00.130.255 I llm_load_print_meta: n_gqa            = 1
0.00.130.255 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.130.256 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.130.263 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.130.264 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.130.264 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.130.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.130.264 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.130.271 I llm_load_print_meta: n_ff             = 8192
0.00.130.272 I llm_load_print_meta: n_expert         = 0
0.00.130.272 I llm_load_print_meta: n_expert_used    = 0
0.00.130.273 I llm_load_print_meta: causal attn      = 1
0.00.130.273 I llm_load_print_meta: pooling type     = 0
0.00.130.273 I llm_load_print_meta: rope type        = 2
0.00.130.273 I llm_load_print_meta: rope scaling     = linear
0.00.130.274 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.130.274 I llm_load_print_meta: freq_scale_train = 1
0.00.130.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.130.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.130.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.130.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.130.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.130.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.130.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.130.276 I llm_load_print_meta: model type       = 1.4B
0.00.130.277 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.130.277 I llm_load_print_meta: model params     = 1.41 B
0.00.130.280 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.130.280 I llm_load_print_meta: general.name     = 1.4B
0.00.130.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.130.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.130.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.130.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.130.281 I llm_load_print_meta: LF token         = 128 ''
0.00.130.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.130.282 I llm_load_print_meta: max token length = 1024
0.00.132.310 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.132.310 I llm_load_tensors: offloading output layer to GPU
0.00.132.311 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.132.328 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.132.329 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.133.260 I llama_new_context_with_model: n_seq_max     = 1
0.00.133.261 I llama_new_context_with_model: n_ctx         = 2048
0.00.133.261 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.133.261 I llama_new_context_with_model: n_batch       = 2048
0.00.133.261 I llama_new_context_with_model: n_ubatch      = 512
0.00.133.261 I llama_new_context_with_model: flash_attn    = 0
0.00.133.262 I llama_new_context_with_model: freq_base     = 10000.0
0.00.133.262 I llama_new_context_with_model: freq_scale    = 1
0.00.133.263 I ggml_metal_init: allocating
0.00.133.272 I ggml_metal_init: found device: Apple M4
0.00.133.274 I ggml_metal_init: picking default device: Apple M4
0.00.133.981 I ggml_metal_init: using embedded metal library
0.00.160.834 I ggml_metal_init: GPU name:   Apple M4
0.00.160.836 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.837 I ggml_metal_init: simdgroup reduction   = true
0.00.160.837 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.837 I ggml_metal_init: has bfloat            = true
0.00.160.837 I ggml_metal_init: use bfloat            = true
0.00.160.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.252.691 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.271.762 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.271.768 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.271.791 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.272.729 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.272.731 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.272.732 I llama_new_context_with_model: graph nodes  = 967
0.00.272.732 I llama_new_context_with_model: graph splits = 2
0.00.272.756 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.272.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.272.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.346.441 I main: llama threadpool init, n_threads = 4
0.00.346.473 I 
0.00.346.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.346.505 I 
0.00.346.575 I sampler seed: 1234
0.00.346.579 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.346.613 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.346.615 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.346.615 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.154.247 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.02.154.248 I llama_perf_context_print:        load time =     291.46 ms
0.02.154.249 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.98 tokens per second)
0.02.154.250 I llama_perf_context_print:        eval time =    1760.63 ms /    63 runs   (   27.95 ms per token,    35.78 tokens per second)
0.02.154.251 I llama_perf_context_print:       total time =    1807.81 ms /    70 tokens
0.02.154.432 I ggml_metal_free: deallocating

real	0m2.465s
user	0m0.157s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.660 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.520 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.786 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.220 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.590 I llama_model_loader: - type  f32:  194 tensors
0.00.052.590 I llama_model_loader: - type  f16:   98 tensors
0.00.081.135 I llm_load_vocab: special tokens cache size = 25
0.00.087.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.557 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.557 I llm_load_print_meta: arch             = gptneox
0.00.087.558 I llm_load_print_meta: vocab type       = BPE
0.00.087.558 I llm_load_print_meta: n_vocab          = 50304
0.00.087.558 I llm_load_print_meta: n_merges         = 50009
0.00.087.558 I llm_load_print_meta: vocab_only       = 0
0.00.087.558 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.558 I llm_load_print_meta: n_embd           = 2048
0.00.087.558 I llm_load_print_meta: n_layer          = 24
0.00.087.561 I llm_load_print_meta: n_head           = 16
0.00.087.561 I llm_load_print_meta: n_head_kv        = 16
0.00.087.562 I llm_load_print_meta: n_rot            = 32
0.00.087.562 I llm_load_print_meta: n_swa            = 0
0.00.087.562 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.563 I llm_load_print_meta: n_gqa            = 1
0.00.087.564 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.564 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.565 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.565 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.565 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.565 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.569 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.570 I llm_load_print_meta: n_ff             = 8192
0.00.087.570 I llm_load_print_meta: n_expert         = 0
0.00.087.570 I llm_load_print_meta: n_expert_used    = 0
0.00.087.571 I llm_load_print_meta: causal attn      = 1
0.00.087.571 I llm_load_print_meta: pooling type     = 0
0.00.087.571 I llm_load_print_meta: rope type        = 2
0.00.087.571 I llm_load_print_meta: rope scaling     = linear
0.00.087.571 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.572 I llm_load_print_meta: freq_scale_train = 1
0.00.087.572 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.572 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.572 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.572 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.572 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.572 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.572 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.573 I llm_load_print_meta: model type       = 1.4B
0.00.087.573 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.574 I llm_load_print_meta: model params     = 1.41 B
0.00.087.574 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.574 I llm_load_print_meta: general.name     = 1.4B
0.00.087.574 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.575 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.575 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.575 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.575 I llm_load_print_meta: LF token         = 128 ''
0.00.087.575 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.575 I llm_load_print_meta: max token length = 1024
0.00.089.497 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.498 I llm_load_tensors: offloading output layer to GPU
0.00.089.498 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.507 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.509 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.381 I llama_new_context_with_model: n_ctx         = 128
0.00.090.381 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.382 I llama_new_context_with_model: n_batch       = 128
0.00.090.382 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.382 I llama_new_context_with_model: flash_attn    = 0
0.00.090.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.383 I llama_new_context_with_model: freq_scale    = 1
0.00.090.383 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.383 I ggml_metal_init: allocating
0.00.090.389 I ggml_metal_init: found device: Apple M4
0.00.090.391 I ggml_metal_init: picking default device: Apple M4
0.00.090.969 I ggml_metal_init: using embedded metal library
0.00.093.533 I ggml_metal_init: GPU name:   Apple M4
0.00.093.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.536 I ggml_metal_init: simdgroup reduction   = true
0.00.093.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.536 I ggml_metal_init: has bfloat            = true
0.00.093.536 I ggml_metal_init: use bfloat            = true
0.00.093.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.578 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.855 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.858 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.871 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.721 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.722 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.722 I llama_new_context_with_model: graph nodes  = 967
0.00.105.722 I llama_new_context_with_model: graph splits = 2
0.00.105.734 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.526 I 
0.00.928.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.584 I perplexity: tokenizing the input ..
0.00.939.041 I perplexity: tokenization took 10.455 ms
0.00.939.051 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.060.171 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.061.822 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.061.838 I llama_perf_context_print:        load time =     907.00 ms
0.01.061.842 I llama_perf_context_print: prompt eval time =     120.74 ms /   128 tokens (    0.94 ms per token,  1060.16 tokens per second)
0.01.061.844 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.061.848 I llama_perf_context_print:       total time =     133.31 ms /   129 tokens
0.01.062.440 I ggml_metal_free: deallocating

real	0m1.266s
user	0m0.125s
sys	0m0.230s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.162 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.624 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.631 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.636 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.637 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.576 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.569 I llama_model_loader: - type  f32:  194 tensors
0.00.027.570 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.866 I llm_load_vocab: special tokens cache size = 25
0.00.055.686 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.691 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.691 I llm_load_print_meta: arch             = gptneox
0.00.055.692 I llm_load_print_meta: vocab type       = BPE
0.00.055.692 I llm_load_print_meta: n_vocab          = 50304
0.00.055.692 I llm_load_print_meta: n_merges         = 50009
0.00.055.692 I llm_load_print_meta: vocab_only       = 0
0.00.055.693 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.693 I llm_load_print_meta: n_embd           = 2048
0.00.055.693 I llm_load_print_meta: n_layer          = 24
0.00.055.697 I llm_load_print_meta: n_head           = 16
0.00.055.698 I llm_load_print_meta: n_head_kv        = 16
0.00.055.699 I llm_load_print_meta: n_rot            = 32
0.00.055.699 I llm_load_print_meta: n_swa            = 0
0.00.055.699 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.699 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.700 I llm_load_print_meta: n_gqa            = 1
0.00.055.701 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.703 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.704 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.704 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.705 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.705 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.705 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.706 I llm_load_print_meta: n_ff             = 8192
0.00.055.706 I llm_load_print_meta: n_expert         = 0
0.00.055.706 I llm_load_print_meta: n_expert_used    = 0
0.00.055.706 I llm_load_print_meta: causal attn      = 1
0.00.055.706 I llm_load_print_meta: pooling type     = 0
0.00.055.707 I llm_load_print_meta: rope type        = 2
0.00.055.707 I llm_load_print_meta: rope scaling     = linear
0.00.055.708 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.708 I llm_load_print_meta: freq_scale_train = 1
0.00.055.708 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.708 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.709 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.709 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.711 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.711 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.711 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.712 I llm_load_print_meta: model type       = 1.4B
0.00.055.712 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.712 I llm_load_print_meta: model params     = 1.41 B
0.00.055.713 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.713 I llm_load_print_meta: general.name     = 1.4B
0.00.055.713 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.713 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.713 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.714 I llm_load_print_meta: LF token         = 128 ''
0.00.055.714 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.714 I llm_load_print_meta: max token length = 1024
0.00.058.132 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.132 I llm_load_tensors: offloading output layer to GPU
0.00.058.132 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.143 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.144 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.100 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.102 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.102 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.102 I llama_new_context_with_model: n_batch       = 2048
0.00.059.103 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.103 I llama_new_context_with_model: flash_attn    = 0
0.00.059.103 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.103 I llama_new_context_with_model: freq_scale    = 1
0.00.059.104 I ggml_metal_init: allocating
0.00.059.108 I ggml_metal_init: found device: Apple M4
0.00.059.110 I ggml_metal_init: picking default device: Apple M4
0.00.059.895 I ggml_metal_init: using embedded metal library
0.00.062.515 I ggml_metal_init: GPU name:   Apple M4
0.00.062.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.517 I ggml_metal_init: simdgroup reduction   = true
0.00.062.518 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.518 I ggml_metal_init: has bfloat            = true
0.00.062.518 I ggml_metal_init: use bfloat            = true
0.00.062.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.097.990 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.002 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.028 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.170 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.173 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.173 I llama_new_context_with_model: graph nodes  = 967
0.00.099.173 I llama_new_context_with_model: graph splits = 2
0.00.099.191 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.333 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.252.015 I main: llama threadpool init, n_threads = 4
0.01.252.049 I 
0.01.252.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.252.104 I 
0.01.252.266 I sampler seed: 1234
0.01.252.270 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.252.317 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.252.319 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.252.319 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.337.114 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63906.39 tokens per second)
0.02.337.115 I llama_perf_context_print:        load time =    1241.85 ms
0.02.337.117 I llama_perf_context_print: prompt eval time =      39.77 ms /     7 tokens (    5.68 ms per token,   176.02 tokens per second)
0.02.337.117 I llama_perf_context_print:        eval time =    1042.20 ms /    63 runs   (   16.54 ms per token,    60.45 tokens per second)
0.02.337.118 I llama_perf_context_print:       total time =    1085.10 ms /    70 tokens
0.02.337.312 I ggml_metal_free: deallocating

real	0m2.354s
user	0m0.113s
sys	0m0.271s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.146 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.648 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.267 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.275 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.281 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.726 I llama_model_loader: - type  f32:  194 tensors
0.00.037.727 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.189 I llm_load_vocab: special tokens cache size = 25
0.00.069.348 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.352 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.352 I llm_load_print_meta: arch             = gptneox
0.00.069.353 I llm_load_print_meta: vocab type       = BPE
0.00.069.353 I llm_load_print_meta: n_vocab          = 50304
0.00.069.353 I llm_load_print_meta: n_merges         = 50009
0.00.069.353 I llm_load_print_meta: vocab_only       = 0
0.00.069.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.354 I llm_load_print_meta: n_embd           = 2048
0.00.069.354 I llm_load_print_meta: n_layer          = 24
0.00.069.358 I llm_load_print_meta: n_head           = 16
0.00.069.359 I llm_load_print_meta: n_head_kv        = 16
0.00.069.359 I llm_load_print_meta: n_rot            = 32
0.00.069.359 I llm_load_print_meta: n_swa            = 0
0.00.069.361 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.361 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.363 I llm_load_print_meta: n_gqa            = 1
0.00.069.364 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.365 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.365 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.366 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.366 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.366 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.366 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.374 I llm_load_print_meta: n_ff             = 8192
0.00.069.375 I llm_load_print_meta: n_expert         = 0
0.00.069.375 I llm_load_print_meta: n_expert_used    = 0
0.00.069.375 I llm_load_print_meta: causal attn      = 1
0.00.069.375 I llm_load_print_meta: pooling type     = 0
0.00.069.375 I llm_load_print_meta: rope type        = 2
0.00.069.376 I llm_load_print_meta: rope scaling     = linear
0.00.069.376 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.376 I llm_load_print_meta: freq_scale_train = 1
0.00.069.377 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.377 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.379 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.379 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.380 I llm_load_print_meta: model type       = 1.4B
0.00.069.380 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.380 I llm_load_print_meta: model params     = 1.41 B
0.00.069.381 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.381 I llm_load_print_meta: general.name     = 1.4B
0.00.069.381 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.381 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.384 I llm_load_print_meta: LF token         = 128 ''
0.00.069.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.385 I llm_load_print_meta: max token length = 1024
0.00.071.850 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.851 I llm_load_tensors: offloading output layer to GPU
0.00.071.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.862 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.863 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.741 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.742 I llama_new_context_with_model: n_ctx         = 128
0.00.072.742 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.742 I llama_new_context_with_model: n_batch       = 128
0.00.072.742 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.743 I llama_new_context_with_model: flash_attn    = 0
0.00.072.743 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.743 I llama_new_context_with_model: freq_scale    = 1
0.00.072.744 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.745 I ggml_metal_init: allocating
0.00.072.750 I ggml_metal_init: found device: Apple M4
0.00.072.753 I ggml_metal_init: picking default device: Apple M4
0.00.073.481 I ggml_metal_init: using embedded metal library
0.00.076.147 I ggml_metal_init: GPU name:   Apple M4
0.00.076.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.149 I ggml_metal_init: simdgroup reduction   = true
0.00.076.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.150 I ggml_metal_init: has bfloat            = true
0.00.076.150 I ggml_metal_init: use bfloat            = true
0.00.076.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.618 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.132 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.088.135 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.088.153 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.052 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.089.054 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.089.055 I llama_new_context_with_model: graph nodes  = 967
0.00.089.055 I llama_new_context_with_model: graph splits = 2
0.00.089.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.089.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.016.608 I 
0.01.016.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.016.672 I perplexity: tokenizing the input ..
0.01.024.336 I perplexity: tokenization took 7.663 ms
0.01.024.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.149.332 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.150.427 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.150.444 I llama_perf_context_print:        load time =    1003.96 ms
0.01.150.445 I llama_perf_context_print: prompt eval time =     124.75 ms /   128 tokens (    0.97 ms per token,  1026.09 tokens per second)
0.01.150.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.150.447 I llama_perf_context_print:       total time =     133.84 ms /   129 tokens
0.01.150.855 I ggml_metal_free: deallocating

real	0m1.168s
user	0m0.098s
sys	0m0.211s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.959 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.865 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.793 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.857 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.858 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.859 I llama_model_loader: - type  f32:  194 tensors
0.00.034.859 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.729 I llm_load_vocab: special tokens cache size = 25
0.00.061.666 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.669 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.669 I llm_load_print_meta: arch             = gptneox
0.00.061.670 I llm_load_print_meta: vocab type       = BPE
0.00.061.670 I llm_load_print_meta: n_vocab          = 50304
0.00.061.670 I llm_load_print_meta: n_merges         = 50009
0.00.061.670 I llm_load_print_meta: vocab_only       = 0
0.00.061.670 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.671 I llm_load_print_meta: n_embd           = 2048
0.00.061.671 I llm_load_print_meta: n_layer          = 24
0.00.061.675 I llm_load_print_meta: n_head           = 16
0.00.061.676 I llm_load_print_meta: n_head_kv        = 16
0.00.061.676 I llm_load_print_meta: n_rot            = 32
0.00.061.676 I llm_load_print_meta: n_swa            = 0
0.00.061.677 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.679 I llm_load_print_meta: n_gqa            = 1
0.00.061.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.680 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.681 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.681 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.681 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.682 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.682 I llm_load_print_meta: n_ff             = 8192
0.00.061.682 I llm_load_print_meta: n_expert         = 0
0.00.061.683 I llm_load_print_meta: n_expert_used    = 0
0.00.061.683 I llm_load_print_meta: causal attn      = 1
0.00.061.683 I llm_load_print_meta: pooling type     = 0
0.00.061.683 I llm_load_print_meta: rope type        = 2
0.00.061.684 I llm_load_print_meta: rope scaling     = linear
0.00.061.687 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.687 I llm_load_print_meta: freq_scale_train = 1
0.00.061.688 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.688 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.688 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.688 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.688 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.688 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.688 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.689 I llm_load_print_meta: model type       = 1.4B
0.00.061.689 I llm_load_print_meta: model ftype      = Q4_0
0.00.061.689 I llm_load_print_meta: model params     = 1.41 B
0.00.061.690 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.061.690 I llm_load_print_meta: general.name     = 1.4B
0.00.061.690 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.691 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.691 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.691 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.691 I llm_load_print_meta: LF token         = 128 ''
0.00.061.691 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.692 I llm_load_print_meta: max token length = 1024
0.00.063.748 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.749 I llm_load_tensors: offloading output layer to GPU
0.00.063.749 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.759 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.063.760 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.064.647 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.648 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.648 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.648 I llama_new_context_with_model: n_batch       = 2048
0.00.064.648 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.649 I llama_new_context_with_model: flash_attn    = 0
0.00.064.649 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.649 I llama_new_context_with_model: freq_scale    = 1
0.00.064.650 I ggml_metal_init: allocating
0.00.064.656 I ggml_metal_init: found device: Apple M4
0.00.064.659 I ggml_metal_init: picking default device: Apple M4
0.00.065.408 I ggml_metal_init: using embedded metal library
0.00.068.045 I ggml_metal_init: GPU name:   Apple M4
0.00.068.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.051 I ggml_metal_init: simdgroup reduction   = true
0.00.068.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.051 I ggml_metal_init: has bfloat            = true
0.00.068.051 I ggml_metal_init: use bfloat            = true
0.00.068.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.037 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.009 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.020 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.052 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.268 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.269 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.270 I llama_new_context_with_model: graph nodes  = 967
0.00.109.270 I llama_new_context_with_model: graph splits = 2
0.00.109.287 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.451 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.776 I main: llama threadpool init, n_threads = 4
0.00.803.818 I 
0.00.803.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.854 I 
0.00.804.016 I sampler seed: 1234
0.00.804.022 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.036 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.036 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.818 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.483.819 I llama_perf_context_print:        load time =     792.81 ms
0.01.483.820 I llama_perf_context_print: prompt eval time =      40.03 ms /     7 tokens (    5.72 ms per token,   174.86 tokens per second)
0.01.483.821 I llama_perf_context_print:        eval time =     636.57 ms /    63 runs   (   10.10 ms per token,    98.97 tokens per second)
0.01.483.821 I llama_perf_context_print:       total time =     680.05 ms /    70 tokens
0.01.483.991 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.112s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.733 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.016 I llama_model_loader: - type  f32:  194 tensors
0.00.024.016 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.937 I llm_load_vocab: special tokens cache size = 25
0.00.050.908 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.913 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.914 I llm_load_print_meta: arch             = gptneox
0.00.050.914 I llm_load_print_meta: vocab type       = BPE
0.00.050.914 I llm_load_print_meta: n_vocab          = 50304
0.00.050.914 I llm_load_print_meta: n_merges         = 50009
0.00.050.915 I llm_load_print_meta: vocab_only       = 0
0.00.050.915 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.915 I llm_load_print_meta: n_embd           = 2048
0.00.050.915 I llm_load_print_meta: n_layer          = 24
0.00.050.918 I llm_load_print_meta: n_head           = 16
0.00.050.918 I llm_load_print_meta: n_head_kv        = 16
0.00.050.918 I llm_load_print_meta: n_rot            = 32
0.00.050.919 I llm_load_print_meta: n_swa            = 0
0.00.050.919 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.919 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.920 I llm_load_print_meta: n_gqa            = 1
0.00.050.921 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.921 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.922 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.922 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.923 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.923 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.923 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.923 I llm_load_print_meta: n_ff             = 8192
0.00.050.924 I llm_load_print_meta: n_expert         = 0
0.00.050.924 I llm_load_print_meta: n_expert_used    = 0
0.00.050.924 I llm_load_print_meta: causal attn      = 1
0.00.050.924 I llm_load_print_meta: pooling type     = 0
0.00.050.924 I llm_load_print_meta: rope type        = 2
0.00.050.925 I llm_load_print_meta: rope scaling     = linear
0.00.050.925 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.925 I llm_load_print_meta: freq_scale_train = 1
0.00.050.926 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.926 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.926 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.926 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.926 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.927 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.927 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.927 I llm_load_print_meta: model type       = 1.4B
0.00.050.927 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.928 I llm_load_print_meta: model params     = 1.41 B
0.00.050.928 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.928 I llm_load_print_meta: general.name     = 1.4B
0.00.050.929 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: LF token         = 128 ''
0.00.050.931 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.931 I llm_load_print_meta: max token length = 1024
0.00.052.704 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.705 I llm_load_tensors: offloading output layer to GPU
0.00.052.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.715 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.716 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.552 I llama_new_context_with_model: n_ctx         = 128
0.00.053.553 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.553 I llama_new_context_with_model: n_batch       = 128
0.00.053.553 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.553 I llama_new_context_with_model: flash_attn    = 0
0.00.053.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.554 I llama_new_context_with_model: freq_scale    = 1
0.00.053.554 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.555 I ggml_metal_init: allocating
0.00.053.560 I ggml_metal_init: found device: Apple M4
0.00.053.562 I ggml_metal_init: picking default device: Apple M4
0.00.054.118 I ggml_metal_init: using embedded metal library
0.00.056.485 I ggml_metal_init: GPU name:   Apple M4
0.00.056.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.487 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.488 I ggml_metal_init: simdgroup reduction   = true
0.00.056.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.488 I ggml_metal_init: has bfloat            = true
0.00.056.488 I ggml_metal_init: use bfloat            = true
0.00.056.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.871 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.125 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.141 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.966 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.967 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.967 I llama_new_context_with_model: graph nodes  = 967
0.00.067.968 I llama_new_context_with_model: graph splits = 2
0.00.067.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.620 I 
0.00.658.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.665 I perplexity: tokenizing the input ..
0.00.666.193 I perplexity: tokenization took 7.526 ms
0.00.666.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.189 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.790.278 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.790.291 I llama_perf_context_print:        load time =     648.88 ms
0.00.790.292 I llama_perf_context_print: prompt eval time =     122.77 ms /   128 tokens (    0.96 ms per token,  1042.59 tokens per second)
0.00.790.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.293 I llama_perf_context_print:       total time =     131.67 ms /   129 tokens
0.00.790.680 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.079s
sys	0m0.131s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.093 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.307 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.156 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.157 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.158 I llama_model_loader: - type  f32:  194 tensors
0.00.030.158 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.158 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.099 I llm_load_vocab: special tokens cache size = 25
0.00.057.036 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.039 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.039 I llm_load_print_meta: arch             = gptneox
0.00.057.040 I llm_load_print_meta: vocab type       = BPE
0.00.057.040 I llm_load_print_meta: n_vocab          = 50304
0.00.057.040 I llm_load_print_meta: n_merges         = 50009
0.00.057.040 I llm_load_print_meta: vocab_only       = 0
0.00.057.041 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.041 I llm_load_print_meta: n_embd           = 2048
0.00.057.041 I llm_load_print_meta: n_layer          = 24
0.00.057.044 I llm_load_print_meta: n_head           = 16
0.00.057.045 I llm_load_print_meta: n_head_kv        = 16
0.00.057.045 I llm_load_print_meta: n_rot            = 32
0.00.057.045 I llm_load_print_meta: n_swa            = 0
0.00.057.047 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.047 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.050 I llm_load_print_meta: n_gqa            = 1
0.00.057.050 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.051 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.052 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.052 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.054 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.054 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.054 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.055 I llm_load_print_meta: n_ff             = 8192
0.00.057.055 I llm_load_print_meta: n_expert         = 0
0.00.057.055 I llm_load_print_meta: n_expert_used    = 0
0.00.057.057 I llm_load_print_meta: causal attn      = 1
0.00.057.057 I llm_load_print_meta: pooling type     = 0
0.00.057.057 I llm_load_print_meta: rope type        = 2
0.00.057.057 I llm_load_print_meta: rope scaling     = linear
0.00.057.058 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.058 I llm_load_print_meta: freq_scale_train = 1
0.00.057.058 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.058 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.058 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.059 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.059 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.063 I llm_load_print_meta: model type       = 1.4B
0.00.057.064 I llm_load_print_meta: model ftype      = Q4_1
0.00.057.064 I llm_load_print_meta: model params     = 1.41 B
0.00.057.065 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.057.065 I llm_load_print_meta: general.name     = 1.4B
0.00.057.065 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.065 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.065 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.066 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.066 I llm_load_print_meta: LF token         = 128 ''
0.00.057.066 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.066 I llm_load_print_meta: max token length = 1024
0.00.058.868 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.869 I llm_load_tensors: offloading output layer to GPU
0.00.058.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.879 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.880 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.059.729 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.730 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.730 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.730 I llama_new_context_with_model: n_batch       = 2048
0.00.059.730 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.730 I llama_new_context_with_model: flash_attn    = 0
0.00.059.731 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.731 I llama_new_context_with_model: freq_scale    = 1
0.00.059.731 I ggml_metal_init: allocating
0.00.059.737 I ggml_metal_init: found device: Apple M4
0.00.059.739 I ggml_metal_init: picking default device: Apple M4
0.00.060.356 I ggml_metal_init: using embedded metal library
0.00.062.690 I ggml_metal_init: GPU name:   Apple M4
0.00.062.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.692 I ggml_metal_init: simdgroup reduction   = true
0.00.062.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.693 I ggml_metal_init: has bfloat            = true
0.00.062.693 I ggml_metal_init: use bfloat            = true
0.00.062.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.174 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.996 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.019 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.146 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.146 I llama_new_context_with_model: graph nodes  = 967
0.00.092.147 I llama_new_context_with_model: graph splits = 2
0.00.092.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.199 I main: llama threadpool init, n_threads = 4
0.00.818.235 I 
0.00.818.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.265 I 
0.00.818.461 I sampler seed: 1234
0.00.818.465 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.478 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.479 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.479 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.549.884 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.549.885 I llama_perf_context_print:        load time =     809.10 ms
0.01.549.885 I llama_perf_context_print: prompt eval time =      44.82 ms /     7 tokens (    6.40 ms per token,   156.17 tokens per second)
0.01.549.886 I llama_perf_context_print:        eval time =     683.59 ms /    63 runs   (   10.85 ms per token,    92.16 tokens per second)
0.01.549.886 I llama_perf_context_print:       total time =     731.69 ms /    70 tokens
0.01.550.054 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.111s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.072 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.985 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.987 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.621 I llama_model_loader: - type  f32:  194 tensors
0.00.023.621 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.621 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.679 I llm_load_vocab: special tokens cache size = 25
0.00.049.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.442 I llm_load_print_meta: arch             = gptneox
0.00.049.442 I llm_load_print_meta: vocab type       = BPE
0.00.049.442 I llm_load_print_meta: n_vocab          = 50304
0.00.049.442 I llm_load_print_meta: n_merges         = 50009
0.00.049.443 I llm_load_print_meta: vocab_only       = 0
0.00.049.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.443 I llm_load_print_meta: n_embd           = 2048
0.00.049.443 I llm_load_print_meta: n_layer          = 24
0.00.049.446 I llm_load_print_meta: n_head           = 16
0.00.049.446 I llm_load_print_meta: n_head_kv        = 16
0.00.049.447 I llm_load_print_meta: n_rot            = 32
0.00.049.447 I llm_load_print_meta: n_swa            = 0
0.00.049.447 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.447 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.448 I llm_load_print_meta: n_gqa            = 1
0.00.049.449 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.449 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.451 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.452 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.452 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.453 I llm_load_print_meta: n_ff             = 8192
0.00.049.453 I llm_load_print_meta: n_expert         = 0
0.00.049.453 I llm_load_print_meta: n_expert_used    = 0
0.00.049.453 I llm_load_print_meta: causal attn      = 1
0.00.049.453 I llm_load_print_meta: pooling type     = 0
0.00.049.453 I llm_load_print_meta: rope type        = 2
0.00.049.454 I llm_load_print_meta: rope scaling     = linear
0.00.049.454 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.454 I llm_load_print_meta: freq_scale_train = 1
0.00.049.455 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.455 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.455 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.455 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.455 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.455 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.455 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.456 I llm_load_print_meta: model type       = 1.4B
0.00.049.456 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.461 I llm_load_print_meta: model params     = 1.41 B
0.00.049.462 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.464 I llm_load_print_meta: general.name     = 1.4B
0.00.049.464 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.464 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.465 I llm_load_print_meta: LF token         = 128 ''
0.00.049.465 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.465 I llm_load_print_meta: max token length = 1024
0.00.051.215 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.216 I llm_load_tensors: offloading output layer to GPU
0.00.051.216 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.226 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.227 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.044 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.044 I llama_new_context_with_model: n_ctx         = 128
0.00.052.045 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.045 I llama_new_context_with_model: n_batch       = 128
0.00.052.045 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.045 I llama_new_context_with_model: flash_attn    = 0
0.00.052.046 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.046 I llama_new_context_with_model: freq_scale    = 1
0.00.052.046 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.047 I ggml_metal_init: allocating
0.00.052.052 I ggml_metal_init: found device: Apple M4
0.00.052.054 I ggml_metal_init: picking default device: Apple M4
0.00.052.603 I ggml_metal_init: using embedded metal library
0.00.054.940 I ggml_metal_init: GPU name:   Apple M4
0.00.054.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.943 I ggml_metal_init: simdgroup reduction   = true
0.00.054.943 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.943 I ggml_metal_init: has bfloat            = true
0.00.054.943 I ggml_metal_init: use bfloat            = true
0.00.054.943 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.944 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.245 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.464 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.467 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.313 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.314 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.315 I llama_new_context_with_model: graph nodes  = 967
0.00.066.315 I llama_new_context_with_model: graph splits = 2
0.00.066.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.660 I 
0.00.720.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.708 I perplexity: tokenizing the input ..
0.00.728.241 I perplexity: tokenization took 7.532 ms
0.00.728.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.208 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.852.302 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.852.317 I llama_perf_context_print:        load time =     711.58 ms
0.00.852.318 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.85 tokens per second)
0.00.852.319 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.319 I llama_perf_context_print:       total time =     131.66 ms /   129 tokens
0.00.852.718 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.077s
sys	0m0.146s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.014.042 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.110 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.112 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.113 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.114 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.811 I llama_model_loader: - type  f32:  194 tensors
0.00.043.811 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.224 I llm_load_vocab: special tokens cache size = 25
0.00.078.438 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.441 I llm_load_print_meta: arch             = gptneox
0.00.078.441 I llm_load_print_meta: vocab type       = BPE
0.00.078.441 I llm_load_print_meta: n_vocab          = 50304
0.00.078.442 I llm_load_print_meta: n_merges         = 50009
0.00.078.442 I llm_load_print_meta: vocab_only       = 0
0.00.078.442 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.442 I llm_load_print_meta: n_embd           = 2048
0.00.078.442 I llm_load_print_meta: n_layer          = 24
0.00.078.445 I llm_load_print_meta: n_head           = 16
0.00.078.446 I llm_load_print_meta: n_head_kv        = 16
0.00.078.446 I llm_load_print_meta: n_rot            = 32
0.00.078.446 I llm_load_print_meta: n_swa            = 0
0.00.078.446 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.446 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.447 I llm_load_print_meta: n_gqa            = 1
0.00.078.448 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.449 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.449 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.450 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.450 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.450 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.450 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.451 I llm_load_print_meta: n_ff             = 8192
0.00.078.451 I llm_load_print_meta: n_expert         = 0
0.00.078.451 I llm_load_print_meta: n_expert_used    = 0
0.00.078.452 I llm_load_print_meta: causal attn      = 1
0.00.078.453 I llm_load_print_meta: pooling type     = 0
0.00.078.453 I llm_load_print_meta: rope type        = 2
0.00.078.453 I llm_load_print_meta: rope scaling     = linear
0.00.078.453 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.454 I llm_load_print_meta: freq_scale_train = 1
0.00.078.454 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.456 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.457 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.457 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.457 I llm_load_print_meta: model type       = 1.4B
0.00.078.458 I llm_load_print_meta: model ftype      = Q5_0
0.00.078.458 I llm_load_print_meta: model params     = 1.41 B
0.00.078.458 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.078.459 I llm_load_print_meta: general.name     = 1.4B
0.00.078.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.460 I llm_load_print_meta: LF token         = 128 ''
0.00.078.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.460 I llm_load_print_meta: max token length = 1024
0.00.080.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.595 I llm_load_tensors: offloading output layer to GPU
0.00.080.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.605 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.080.606 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.081.608 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.609 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.609 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.609 I llama_new_context_with_model: n_batch       = 2048
0.00.081.610 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.610 I llama_new_context_with_model: flash_attn    = 0
0.00.081.610 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.611 I llama_new_context_with_model: freq_scale    = 1
0.00.081.611 I ggml_metal_init: allocating
0.00.081.614 I ggml_metal_init: found device: Apple M4
0.00.081.617 I ggml_metal_init: picking default device: Apple M4
0.00.082.328 I ggml_metal_init: using embedded metal library
0.00.085.321 I ggml_metal_init: GPU name:   Apple M4
0.00.085.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.324 I ggml_metal_init: simdgroup reduction   = true
0.00.085.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.325 I ggml_metal_init: has bfloat            = true
0.00.085.325 I ggml_metal_init: use bfloat            = true
0.00.085.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.109 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.121.518 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.525 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.543 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.617 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.620 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.620 I llama_new_context_with_model: graph nodes  = 967
0.00.122.620 I llama_new_context_with_model: graph splits = 2
0.00.122.636 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.767 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.164 I main: llama threadpool init, n_threads = 4
0.00.884.200 I 
0.00.884.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.230 I 
0.00.884.427 I sampler seed: 1234
0.00.884.431 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.884.444 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.884.445 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.884.445 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.670.077 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.670.078 I llama_perf_context_print:        load time =     870.12 ms
0.01.670.079 I llama_perf_context_print: prompt eval time =      43.46 ms /     7 tokens (    6.21 ms per token,   161.08 tokens per second)
0.01.670.079 I llama_perf_context_print:        eval time =     739.27 ms /    63 runs   (   11.73 ms per token,    85.22 tokens per second)
0.01.670.080 I llama_perf_context_print:       total time =     785.91 ms /    70 tokens
0.01.670.262 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.124s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.715 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.717 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.720 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.536 I llama_model_loader: - type  f32:  194 tensors
0.00.025.536 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.537 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.382 I llm_load_vocab: special tokens cache size = 25
0.00.052.325 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.328 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.328 I llm_load_print_meta: arch             = gptneox
0.00.052.329 I llm_load_print_meta: vocab type       = BPE
0.00.052.329 I llm_load_print_meta: n_vocab          = 50304
0.00.052.329 I llm_load_print_meta: n_merges         = 50009
0.00.052.329 I llm_load_print_meta: vocab_only       = 0
0.00.052.329 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.329 I llm_load_print_meta: n_embd           = 2048
0.00.052.330 I llm_load_print_meta: n_layer          = 24
0.00.052.334 I llm_load_print_meta: n_head           = 16
0.00.052.335 I llm_load_print_meta: n_head_kv        = 16
0.00.052.335 I llm_load_print_meta: n_rot            = 32
0.00.052.335 I llm_load_print_meta: n_swa            = 0
0.00.052.335 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.336 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.336 I llm_load_print_meta: n_gqa            = 1
0.00.052.337 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.338 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.339 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.339 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.339 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.339 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.340 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.340 I llm_load_print_meta: n_ff             = 8192
0.00.052.341 I llm_load_print_meta: n_expert         = 0
0.00.052.341 I llm_load_print_meta: n_expert_used    = 0
0.00.052.341 I llm_load_print_meta: causal attn      = 1
0.00.052.341 I llm_load_print_meta: pooling type     = 0
0.00.052.341 I llm_load_print_meta: rope type        = 2
0.00.052.342 I llm_load_print_meta: rope scaling     = linear
0.00.052.342 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.342 I llm_load_print_meta: freq_scale_train = 1
0.00.052.342 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.343 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.343 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.343 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.343 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.344 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.345 I llm_load_print_meta: model type       = 1.4B
0.00.052.346 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.346 I llm_load_print_meta: model params     = 1.41 B
0.00.052.347 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.347 I llm_load_print_meta: general.name     = 1.4B
0.00.052.347 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.347 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.348 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.348 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.348 I llm_load_print_meta: LF token         = 128 ''
0.00.052.349 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.349 I llm_load_print_meta: max token length = 1024
0.00.054.138 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.139 I llm_load_tensors: offloading output layer to GPU
0.00.054.139 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.149 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.150 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.981 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.982 I llama_new_context_with_model: n_ctx         = 128
0.00.054.983 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.983 I llama_new_context_with_model: n_batch       = 128
0.00.054.983 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.983 I llama_new_context_with_model: flash_attn    = 0
0.00.054.984 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.984 I llama_new_context_with_model: freq_scale    = 1
0.00.054.984 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.985 I ggml_metal_init: allocating
0.00.054.988 I ggml_metal_init: found device: Apple M4
0.00.054.990 I ggml_metal_init: picking default device: Apple M4
0.00.055.560 I ggml_metal_init: using embedded metal library
0.00.057.880 I ggml_metal_init: GPU name:   Apple M4
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.882 I ggml_metal_init: simdgroup reduction   = true
0.00.057.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.883 I ggml_metal_init: has bfloat            = true
0.00.057.883 I ggml_metal_init: use bfloat            = true
0.00.057.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.646 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.048 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.051 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.066 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.987 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.988 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.989 I llama_new_context_with_model: graph nodes  = 967
0.00.069.989 I llama_new_context_with_model: graph splits = 2
0.00.070.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.931 I 
0.00.749.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.976 I perplexity: tokenizing the input ..
0.00.757.862 I perplexity: tokenization took 7.884 ms
0.00.757.868 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.893.105 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.894.197 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.894.211 I llama_perf_context_print:        load time =     739.19 ms
0.00.894.212 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   948.02 tokens per second)
0.00.894.213 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.894.214 I llama_perf_context_print:       total time =     144.28 ms /   129 tokens
0.00.894.647 I ggml_metal_free: deallocating

real	0m0.911s
user	0m0.079s
sys	0m0.151s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.028 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.057 I main: llama backend init
0.00.000.059 I main: load the model and apply lora adapter, if any
0.00.015.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.208 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.208 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.039.938 I llama_model_loader: - type  f32:  194 tensors
0.00.039.939 I llama_model_loader: - type q5_1:   97 tensors
0.00.039.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.132 I llm_load_vocab: special tokens cache size = 25
0.00.076.222 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.226 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.226 I llm_load_print_meta: arch             = gptneox
0.00.076.226 I llm_load_print_meta: vocab type       = BPE
0.00.076.227 I llm_load_print_meta: n_vocab          = 50304
0.00.076.227 I llm_load_print_meta: n_merges         = 50009
0.00.076.227 I llm_load_print_meta: vocab_only       = 0
0.00.076.227 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.228 I llm_load_print_meta: n_embd           = 2048
0.00.076.228 I llm_load_print_meta: n_layer          = 24
0.00.076.230 I llm_load_print_meta: n_head           = 16
0.00.076.234 I llm_load_print_meta: n_head_kv        = 16
0.00.076.234 I llm_load_print_meta: n_rot            = 32
0.00.076.235 I llm_load_print_meta: n_swa            = 0
0.00.076.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.237 I llm_load_print_meta: n_gqa            = 1
0.00.076.238 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.239 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.240 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.240 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.240 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.241 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.241 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.244 I llm_load_print_meta: n_ff             = 8192
0.00.076.244 I llm_load_print_meta: n_expert         = 0
0.00.076.244 I llm_load_print_meta: n_expert_used    = 0
0.00.076.245 I llm_load_print_meta: causal attn      = 1
0.00.076.245 I llm_load_print_meta: pooling type     = 0
0.00.076.245 I llm_load_print_meta: rope type        = 2
0.00.076.245 I llm_load_print_meta: rope scaling     = linear
0.00.076.246 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.246 I llm_load_print_meta: freq_scale_train = 1
0.00.076.246 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.247 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.247 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.247 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.248 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.248 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.248 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.248 I llm_load_print_meta: model type       = 1.4B
0.00.076.249 I llm_load_print_meta: model ftype      = Q5_1
0.00.076.249 I llm_load_print_meta: model params     = 1.41 B
0.00.076.250 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.076.250 I llm_load_print_meta: general.name     = 1.4B
0.00.076.251 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.253 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.253 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.253 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.254 I llm_load_print_meta: LF token         = 128 ''
0.00.076.254 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.254 I llm_load_print_meta: max token length = 1024
0.00.078.924 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.925 I llm_load_tensors: offloading output layer to GPU
0.00.078.925 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.936 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.078.936 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.080.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.342 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.342 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.342 I llama_new_context_with_model: n_batch       = 2048
0.00.080.343 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.343 I llama_new_context_with_model: flash_attn    = 0
0.00.080.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.344 I llama_new_context_with_model: freq_scale    = 1
0.00.080.345 I ggml_metal_init: allocating
0.00.080.354 I ggml_metal_init: found device: Apple M4
0.00.080.358 I ggml_metal_init: picking default device: Apple M4
0.00.081.218 I ggml_metal_init: using embedded metal library
0.00.085.639 I ggml_metal_init: GPU name:   Apple M4
0.00.085.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.643 I ggml_metal_init: simdgroup reduction   = true
0.00.085.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.643 I ggml_metal_init: has bfloat            = true
0.00.085.643 I ggml_metal_init: use bfloat            = true
0.00.085.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.977 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.122.964 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.974 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.994 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.082 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.088 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.088 I llama_new_context_with_model: graph nodes  = 967
0.00.124.088 I llama_new_context_with_model: graph splits = 2
0.00.124.105 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.846 I main: llama threadpool init, n_threads = 4
0.00.875.921 I 
0.00.875.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.987 I 
0.00.876.272 I sampler seed: 1234
0.00.876.279 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.876.303 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.876.304 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.876.304 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.712.681 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.712.681 I llama_perf_context_print:        load time =     860.04 ms
0.01.712.682 I llama_perf_context_print: prompt eval time =      43.19 ms /     7 tokens (    6.17 ms per token,   162.06 tokens per second)
0.01.712.683 I llama_perf_context_print:        eval time =     790.11 ms /    63 runs   (   12.54 ms per token,    79.74 tokens per second)
0.01.712.683 I llama_perf_context_print:       total time =     836.84 ms /    70 tokens
0.01.712.846 I ggml_metal_free: deallocating

real	0m1.734s
user	0m0.131s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.089 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.001 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.002 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.002 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.003 I llama_model_loader: - type  f32:  194 tensors
0.00.023.003 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.003 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.031 I llm_load_vocab: special tokens cache size = 25
0.00.048.824 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.826 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.826 I llm_load_print_meta: arch             = gptneox
0.00.048.827 I llm_load_print_meta: vocab type       = BPE
0.00.048.827 I llm_load_print_meta: n_vocab          = 50304
0.00.048.827 I llm_load_print_meta: n_merges         = 50009
0.00.048.827 I llm_load_print_meta: vocab_only       = 0
0.00.048.827 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.827 I llm_load_print_meta: n_embd           = 2048
0.00.048.828 I llm_load_print_meta: n_layer          = 24
0.00.048.830 I llm_load_print_meta: n_head           = 16
0.00.048.831 I llm_load_print_meta: n_head_kv        = 16
0.00.048.831 I llm_load_print_meta: n_rot            = 32
0.00.048.831 I llm_load_print_meta: n_swa            = 0
0.00.048.831 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.831 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.832 I llm_load_print_meta: n_gqa            = 1
0.00.048.833 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.833 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.834 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.834 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.835 I llm_load_print_meta: n_ff             = 8192
0.00.048.835 I llm_load_print_meta: n_expert         = 0
0.00.048.835 I llm_load_print_meta: n_expert_used    = 0
0.00.048.836 I llm_load_print_meta: causal attn      = 1
0.00.048.836 I llm_load_print_meta: pooling type     = 0
0.00.048.838 I llm_load_print_meta: rope type        = 2
0.00.048.838 I llm_load_print_meta: rope scaling     = linear
0.00.048.838 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.839 I llm_load_print_meta: freq_scale_train = 1
0.00.048.839 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.839 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.839 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.839 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.839 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.840 I llm_load_print_meta: model type       = 1.4B
0.00.048.843 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.843 I llm_load_print_meta: model params     = 1.41 B
0.00.048.844 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.844 I llm_load_print_meta: general.name     = 1.4B
0.00.048.844 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.846 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: LF token         = 128 ''
0.00.048.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.847 I llm_load_print_meta: max token length = 1024
0.00.050.570 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.571 I llm_load_tensors: offloading output layer to GPU
0.00.050.571 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.580 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.581 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.449 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.450 I llama_new_context_with_model: n_ctx         = 128
0.00.051.450 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.450 I llama_new_context_with_model: n_batch       = 128
0.00.051.450 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.451 I llama_new_context_with_model: flash_attn    = 0
0.00.051.451 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.451 I llama_new_context_with_model: freq_scale    = 1
0.00.051.452 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.452 I ggml_metal_init: allocating
0.00.051.455 I ggml_metal_init: found device: Apple M4
0.00.051.457 I ggml_metal_init: picking default device: Apple M4
0.00.052.015 I ggml_metal_init: using embedded metal library
0.00.054.331 I ggml_metal_init: GPU name:   Apple M4
0.00.054.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.334 I ggml_metal_init: simdgroup reduction   = true
0.00.054.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.334 I ggml_metal_init: has bfloat            = true
0.00.054.334 I ggml_metal_init: use bfloat            = true
0.00.054.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.819 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.122 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.135 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.071 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.072 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.073 I llama_new_context_with_model: graph nodes  = 967
0.00.066.073 I llama_new_context_with_model: graph splits = 2
0.00.066.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.348 I 
0.00.723.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.393 I perplexity: tokenizing the input ..
0.00.731.248 I perplexity: tokenization took 7.854 ms
0.00.731.252 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.262 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.867.350 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.867.363 I llama_perf_context_print:        load time =     714.25 ms
0.00.867.364 I llama_perf_context_print: prompt eval time =     134.79 ms /   128 tokens (    1.05 ms per token,   949.63 tokens per second)
0.00.867.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.366 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.867.759 I ggml_metal_free: deallocating

real	0m0.880s
user	0m0.077s
sys	0m0.164s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.805 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.122 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.124 I llama_model_loader: - type  f32:  194 tensors
0.00.024.124 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.124 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.125 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.233 I llm_load_vocab: special tokens cache size = 25
0.00.050.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.062 I llm_load_print_meta: arch             = gptneox
0.00.050.062 I llm_load_print_meta: vocab type       = BPE
0.00.050.063 I llm_load_print_meta: n_vocab          = 50304
0.00.050.063 I llm_load_print_meta: n_merges         = 50009
0.00.050.063 I llm_load_print_meta: vocab_only       = 0
0.00.050.063 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.063 I llm_load_print_meta: n_embd           = 2048
0.00.050.064 I llm_load_print_meta: n_layer          = 24
0.00.050.066 I llm_load_print_meta: n_head           = 16
0.00.050.067 I llm_load_print_meta: n_head_kv        = 16
0.00.050.067 I llm_load_print_meta: n_rot            = 32
0.00.050.067 I llm_load_print_meta: n_swa            = 0
0.00.050.067 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.067 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.069 I llm_load_print_meta: n_gqa            = 1
0.00.050.070 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.072 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.073 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.075 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.075 I llm_load_print_meta: n_ff             = 8192
0.00.050.076 I llm_load_print_meta: n_expert         = 0
0.00.050.076 I llm_load_print_meta: n_expert_used    = 0
0.00.050.076 I llm_load_print_meta: causal attn      = 1
0.00.050.076 I llm_load_print_meta: pooling type     = 0
0.00.050.076 I llm_load_print_meta: rope type        = 2
0.00.050.077 I llm_load_print_meta: rope scaling     = linear
0.00.050.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.077 I llm_load_print_meta: freq_scale_train = 1
0.00.050.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.079 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.080 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.080 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.080 I llm_load_print_meta: model type       = 1.4B
0.00.050.080 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.081 I llm_load_print_meta: model params     = 1.41 B
0.00.050.081 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.081 I llm_load_print_meta: general.name     = 1.4B
0.00.050.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.082 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.082 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.082 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: LF token         = 128 ''
0.00.050.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.084 I llm_load_print_meta: max token length = 1024
0.00.051.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.826 I llm_load_tensors: offloading output layer to GPU
0.00.051.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.836 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.837 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.647 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.648 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.648 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.648 I llama_new_context_with_model: n_batch       = 2048
0.00.052.648 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.648 I llama_new_context_with_model: flash_attn    = 0
0.00.052.649 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.649 I llama_new_context_with_model: freq_scale    = 1
0.00.052.649 I ggml_metal_init: allocating
0.00.052.652 I ggml_metal_init: found device: Apple M4
0.00.052.654 I ggml_metal_init: picking default device: Apple M4
0.00.053.240 I ggml_metal_init: using embedded metal library
0.00.055.554 I ggml_metal_init: GPU name:   Apple M4
0.00.055.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.557 I ggml_metal_init: simdgroup reduction   = true
0.00.055.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.557 I ggml_metal_init: has bfloat            = true
0.00.055.557 I ggml_metal_init: use bfloat            = true
0.00.055.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.244 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.145 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.150 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.300 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.302 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.302 I llama_new_context_with_model: graph nodes  = 967
0.00.086.303 I llama_new_context_with_model: graph splits = 2
0.00.086.318 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.485.897 I main: llama threadpool init, n_threads = 4
0.00.485.941 I 
0.00.485.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.485.973 I 
0.00.486.120 I sampler seed: 1234
0.00.486.125 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.486.139 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.486.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.486.141 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.164.151 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.164.151 I llama_perf_context_print:        load time =     476.09 ms
0.01.164.152 I llama_perf_context_print: prompt eval time =      36.16 ms /     7 tokens (    5.17 ms per token,   193.61 tokens per second)
0.01.164.152 I llama_perf_context_print:        eval time =     638.83 ms /    63 runs   (   10.14 ms per token,    98.62 tokens per second)
0.01.164.153 I llama_perf_context_print:       total time =     678.26 ms /    70 tokens
0.01.164.339 I ggml_metal_free: deallocating

real	0m1.180s
user	0m0.109s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.195 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.969 I llama_model_loader: - type  f32:  194 tensors
0.00.023.969 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.969 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.841 I llm_load_vocab: special tokens cache size = 25
0.00.049.631 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.633 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.634 I llm_load_print_meta: arch             = gptneox
0.00.049.634 I llm_load_print_meta: vocab type       = BPE
0.00.049.634 I llm_load_print_meta: n_vocab          = 50304
0.00.049.635 I llm_load_print_meta: n_merges         = 50009
0.00.049.635 I llm_load_print_meta: vocab_only       = 0
0.00.049.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.635 I llm_load_print_meta: n_embd           = 2048
0.00.049.635 I llm_load_print_meta: n_layer          = 24
0.00.049.638 I llm_load_print_meta: n_head           = 16
0.00.049.638 I llm_load_print_meta: n_head_kv        = 16
0.00.049.639 I llm_load_print_meta: n_rot            = 32
0.00.049.639 I llm_load_print_meta: n_swa            = 0
0.00.049.639 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.641 I llm_load_print_meta: n_gqa            = 1
0.00.049.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.642 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.643 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.643 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.643 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.643 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.644 I llm_load_print_meta: n_ff             = 8192
0.00.049.644 I llm_load_print_meta: n_expert         = 0
0.00.049.644 I llm_load_print_meta: n_expert_used    = 0
0.00.049.644 I llm_load_print_meta: causal attn      = 1
0.00.049.644 I llm_load_print_meta: pooling type     = 0
0.00.049.645 I llm_load_print_meta: rope type        = 2
0.00.049.645 I llm_load_print_meta: rope scaling     = linear
0.00.049.647 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.647 I llm_load_print_meta: freq_scale_train = 1
0.00.049.647 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.648 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.648 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.648 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.648 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.648 I llm_load_print_meta: model type       = 1.4B
0.00.049.649 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.649 I llm_load_print_meta: model params     = 1.41 B
0.00.049.650 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.650 I llm_load_print_meta: general.name     = 1.4B
0.00.049.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.652 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.652 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.652 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.653 I llm_load_print_meta: LF token         = 128 ''
0.00.049.653 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.653 I llm_load_print_meta: max token length = 1024
0.00.051.391 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.392 I llm_load_tensors: offloading output layer to GPU
0.00.051.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.402 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.403 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.229 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.230 I llama_new_context_with_model: n_ctx         = 128
0.00.052.230 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.230 I llama_new_context_with_model: n_batch       = 128
0.00.052.230 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.230 I llama_new_context_with_model: flash_attn    = 0
0.00.052.231 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.231 I llama_new_context_with_model: freq_scale    = 1
0.00.052.231 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.232 I ggml_metal_init: allocating
0.00.052.238 I ggml_metal_init: found device: Apple M4
0.00.052.240 I ggml_metal_init: picking default device: Apple M4
0.00.052.801 I ggml_metal_init: using embedded metal library
0.00.055.103 I ggml_metal_init: GPU name:   Apple M4
0.00.055.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.106 I ggml_metal_init: simdgroup reduction   = true
0.00.055.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.106 I ggml_metal_init: has bfloat            = true
0.00.055.106 I ggml_metal_init: use bfloat            = true
0.00.055.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.572 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.800 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.804 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.818 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.687 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.688 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.688 I llama_new_context_with_model: graph nodes  = 967
0.00.066.688 I llama_new_context_with_model: graph splits = 2
0.00.066.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.984 I 
0.00.411.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.411.024 I perplexity: tokenizing the input ..
0.00.418.759 I perplexity: tokenization took 7.734 ms
0.00.418.763 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.339 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.446 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.466 I llama_perf_context_print:        load time =     401.23 ms
0.00.552.466 I llama_perf_context_print: prompt eval time =     132.35 ms /   128 tokens (    1.03 ms per token,   967.10 tokens per second)
0.00.552.467 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.468 I llama_perf_context_print:       total time =     141.48 ms /   129 tokens
0.00.552.932 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.078s
sys	0m0.087s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.867 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.163 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.166 I llama_model_loader: - type  f32:  194 tensors
0.00.025.166 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.166 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.167 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.167 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.441 I llm_load_vocab: special tokens cache size = 25
0.00.051.375 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.379 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.379 I llm_load_print_meta: arch             = gptneox
0.00.051.379 I llm_load_print_meta: vocab type       = BPE
0.00.051.379 I llm_load_print_meta: n_vocab          = 50304
0.00.051.380 I llm_load_print_meta: n_merges         = 50009
0.00.051.380 I llm_load_print_meta: vocab_only       = 0
0.00.051.380 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.380 I llm_load_print_meta: n_embd           = 2048
0.00.051.380 I llm_load_print_meta: n_layer          = 24
0.00.051.383 I llm_load_print_meta: n_head           = 16
0.00.051.384 I llm_load_print_meta: n_head_kv        = 16
0.00.051.384 I llm_load_print_meta: n_rot            = 32
0.00.051.384 I llm_load_print_meta: n_swa            = 0
0.00.051.384 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.385 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.385 I llm_load_print_meta: n_gqa            = 1
0.00.051.386 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.387 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.387 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.388 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.388 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.391 I llm_load_print_meta: n_ff             = 8192
0.00.051.391 I llm_load_print_meta: n_expert         = 0
0.00.051.392 I llm_load_print_meta: n_expert_used    = 0
0.00.051.392 I llm_load_print_meta: causal attn      = 1
0.00.051.392 I llm_load_print_meta: pooling type     = 0
0.00.051.392 I llm_load_print_meta: rope type        = 2
0.00.051.392 I llm_load_print_meta: rope scaling     = linear
0.00.051.394 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.394 I llm_load_print_meta: freq_scale_train = 1
0.00.051.394 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.394 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.394 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.395 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.395 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.395 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.395 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.395 I llm_load_print_meta: model type       = 1.4B
0.00.051.396 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.396 I llm_load_print_meta: model params     = 1.41 B
0.00.051.397 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.397 I llm_load_print_meta: general.name     = 1.4B
0.00.051.397 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.397 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.397 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: LF token         = 128 ''
0.00.051.398 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: max token length = 1024
0.00.053.173 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.174 I llm_load_tensors: offloading output layer to GPU
0.00.053.174 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.184 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.185 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.012 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.013 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.013 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.014 I llama_new_context_with_model: n_batch       = 2048
0.00.054.014 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.014 I llama_new_context_with_model: flash_attn    = 0
0.00.054.014 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.015 I llama_new_context_with_model: freq_scale    = 1
0.00.054.015 I ggml_metal_init: allocating
0.00.054.021 I ggml_metal_init: found device: Apple M4
0.00.054.023 I ggml_metal_init: picking default device: Apple M4
0.00.054.595 I ggml_metal_init: using embedded metal library
0.00.056.935 I ggml_metal_init: GPU name:   Apple M4
0.00.056.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.937 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.937 I ggml_metal_init: simdgroup reduction   = true
0.00.056.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.938 I ggml_metal_init: has bfloat            = true
0.00.056.938 I ggml_metal_init: use bfloat            = true
0.00.056.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.571 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.407 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.411 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.429 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.381 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.382 I llama_new_context_with_model: graph nodes  = 967
0.00.087.383 I llama_new_context_with_model: graph splits = 2
0.00.087.397 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.274 I main: llama threadpool init, n_threads = 4
0.00.580.311 I 
0.00.580.339 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.340 I 
0.00.580.511 I sampler seed: 1234
0.00.580.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.580.546 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.580.547 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.580.547 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.323.936 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.323.937 I llama_perf_context_print:        load time =     571.40 ms
0.01.323.938 I llama_perf_context_print: prompt eval time =      40.30 ms /     7 tokens (    5.76 ms per token,   173.68 tokens per second)
0.01.323.939 I llama_perf_context_print:        eval time =     700.12 ms /    63 runs   (   11.11 ms per token,    89.98 tokens per second)
0.01.323.939 I llama_perf_context_print:       total time =     743.66 ms /    70 tokens
0.01.324.133 I ggml_metal_free: deallocating

real	0m1.340s
user	0m0.109s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.014 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.896 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.904 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.909 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.694 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.694 I llama_model_loader: - type  f32:  194 tensors
0.00.023.694 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.694 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.695 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.695 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.410 I llm_load_vocab: special tokens cache size = 25
0.00.050.323 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.326 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.326 I llm_load_print_meta: arch             = gptneox
0.00.050.327 I llm_load_print_meta: vocab type       = BPE
0.00.050.327 I llm_load_print_meta: n_vocab          = 50304
0.00.050.327 I llm_load_print_meta: n_merges         = 50009
0.00.050.327 I llm_load_print_meta: vocab_only       = 0
0.00.050.327 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.327 I llm_load_print_meta: n_embd           = 2048
0.00.050.328 I llm_load_print_meta: n_layer          = 24
0.00.050.330 I llm_load_print_meta: n_head           = 16
0.00.050.331 I llm_load_print_meta: n_head_kv        = 16
0.00.050.331 I llm_load_print_meta: n_rot            = 32
0.00.050.331 I llm_load_print_meta: n_swa            = 0
0.00.050.331 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.331 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.332 I llm_load_print_meta: n_gqa            = 1
0.00.050.333 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.334 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.339 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.339 I llm_load_print_meta: n_ff             = 8192
0.00.050.339 I llm_load_print_meta: n_expert         = 0
0.00.050.340 I llm_load_print_meta: n_expert_used    = 0
0.00.050.340 I llm_load_print_meta: causal attn      = 1
0.00.050.340 I llm_load_print_meta: pooling type     = 0
0.00.050.340 I llm_load_print_meta: rope type        = 2
0.00.050.340 I llm_load_print_meta: rope scaling     = linear
0.00.050.341 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.341 I llm_load_print_meta: freq_scale_train = 1
0.00.050.341 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.342 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.342 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.342 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.343 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.343 I llm_load_print_meta: model type       = 1.4B
0.00.050.343 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.344 I llm_load_print_meta: model params     = 1.41 B
0.00.050.344 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.344 I llm_load_print_meta: general.name     = 1.4B
0.00.050.345 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.345 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.345 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: LF token         = 128 ''
0.00.050.347 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: max token length = 1024
0.00.052.131 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.132 I llm_load_tensors: offloading output layer to GPU
0.00.052.132 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.142 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.143 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.958 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.958 I llama_new_context_with_model: n_ctx         = 128
0.00.052.959 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.959 I llama_new_context_with_model: n_batch       = 128
0.00.052.959 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.959 I llama_new_context_with_model: flash_attn    = 0
0.00.052.960 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.960 I llama_new_context_with_model: freq_scale    = 1
0.00.052.960 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.961 I ggml_metal_init: allocating
0.00.052.966 I ggml_metal_init: found device: Apple M4
0.00.052.968 I ggml_metal_init: picking default device: Apple M4
0.00.053.561 I ggml_metal_init: using embedded metal library
0.00.055.888 I ggml_metal_init: GPU name:   Apple M4
0.00.055.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.891 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.891 I ggml_metal_init: simdgroup reduction   = true
0.00.055.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.891 I ggml_metal_init: has bfloat            = true
0.00.055.891 I ggml_metal_init: use bfloat            = true
0.00.055.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.269 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.493 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.507 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.384 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.386 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.386 I llama_new_context_with_model: graph nodes  = 967
0.00.067.386 I llama_new_context_with_model: graph splits = 2
0.00.067.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.523.004 I 
0.00.523.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.523.079 I perplexity: tokenizing the input ..
0.00.530.895 I perplexity: tokenization took 7.815 ms
0.00.530.899 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.313 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.664.400 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.664.412 I llama_perf_context_print:        load time =     513.99 ms
0.00.664.413 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.27 tokens per second)
0.00.664.414 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.414 I llama_perf_context_print:       total time =     141.41 ms /   129 tokens
0.00.664.859 I ggml_metal_free: deallocating

real	0m0.677s
user	0m0.079s
sys	0m0.113s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.491 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.278 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.278 I llama_model_loader: - type  f32:  194 tensors
0.00.025.278 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.279 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.279 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.539 I llm_load_vocab: special tokens cache size = 25
0.00.051.531 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.534 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.535 I llm_load_print_meta: arch             = gptneox
0.00.051.535 I llm_load_print_meta: vocab type       = BPE
0.00.051.535 I llm_load_print_meta: n_vocab          = 50304
0.00.051.535 I llm_load_print_meta: n_merges         = 50009
0.00.051.535 I llm_load_print_meta: vocab_only       = 0
0.00.051.536 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.536 I llm_load_print_meta: n_embd           = 2048
0.00.051.536 I llm_load_print_meta: n_layer          = 24
0.00.051.539 I llm_load_print_meta: n_head           = 16
0.00.051.539 I llm_load_print_meta: n_head_kv        = 16
0.00.051.540 I llm_load_print_meta: n_rot            = 32
0.00.051.540 I llm_load_print_meta: n_swa            = 0
0.00.051.540 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.540 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.541 I llm_load_print_meta: n_gqa            = 1
0.00.051.542 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.542 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.543 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.543 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.544 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.544 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.545 I llm_load_print_meta: n_ff             = 8192
0.00.051.545 I llm_load_print_meta: n_expert         = 0
0.00.051.545 I llm_load_print_meta: n_expert_used    = 0
0.00.051.545 I llm_load_print_meta: causal attn      = 1
0.00.051.545 I llm_load_print_meta: pooling type     = 0
0.00.051.545 I llm_load_print_meta: rope type        = 2
0.00.051.548 I llm_load_print_meta: rope scaling     = linear
0.00.051.548 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.548 I llm_load_print_meta: freq_scale_train = 1
0.00.051.549 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.549 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.549 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.549 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.549 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.549 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.551 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.551 I llm_load_print_meta: model type       = 1.4B
0.00.051.551 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.552 I llm_load_print_meta: model params     = 1.41 B
0.00.051.552 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.552 I llm_load_print_meta: general.name     = 1.4B
0.00.051.553 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: LF token         = 128 ''
0.00.051.554 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.554 I llm_load_print_meta: max token length = 1024
0.00.053.256 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.257 I llm_load_tensors: offloading output layer to GPU
0.00.053.257 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.267 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.268 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.132 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.133 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.133 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.134 I llama_new_context_with_model: n_batch       = 2048
0.00.054.134 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.134 I llama_new_context_with_model: flash_attn    = 0
0.00.054.134 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.135 I llama_new_context_with_model: freq_scale    = 1
0.00.054.135 I ggml_metal_init: allocating
0.00.054.141 I ggml_metal_init: found device: Apple M4
0.00.054.143 I ggml_metal_init: picking default device: Apple M4
0.00.054.723 I ggml_metal_init: using embedded metal library
0.00.057.081 I ggml_metal_init: GPU name:   Apple M4
0.00.057.082 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.083 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.083 I ggml_metal_init: simdgroup reduction   = true
0.00.057.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.084 I ggml_metal_init: has bfloat            = true
0.00.057.084 I ggml_metal_init: use bfloat            = true
0.00.057.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.664 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.196 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.256 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.257 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.258 I llama_new_context_with_model: graph nodes  = 967
0.00.087.258 I llama_new_context_with_model: graph splits = 2
0.00.087.274 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.541 I main: llama threadpool init, n_threads = 4
0.00.659.585 I 
0.00.659.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.610 I 
0.00.659.757 I sampler seed: 1234
0.00.659.761 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.659.803 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.659.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.659.808 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.413.404 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.413.405 I llama_perf_context_print:        load time =     649.84 ms
0.01.413.405 I llama_perf_context_print: prompt eval time =      47.48 ms /     7 tokens (    6.78 ms per token,   147.43 tokens per second)
0.01.413.406 I llama_perf_context_print:        eval time =     702.99 ms /    63 runs   (   11.16 ms per token,    89.62 tokens per second)
0.01.413.407 I llama_perf_context_print:       total time =     753.86 ms /    70 tokens
0.01.413.602 I ggml_metal_free: deallocating

real	0m1.430s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.554 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.862 I llama_model_loader: - type  f32:  194 tensors
0.00.024.863 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.863 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.863 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.771 I llm_load_vocab: special tokens cache size = 25
0.00.050.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.751 I llm_load_print_meta: arch             = gptneox
0.00.050.752 I llm_load_print_meta: vocab type       = BPE
0.00.050.752 I llm_load_print_meta: n_vocab          = 50304
0.00.050.752 I llm_load_print_meta: n_merges         = 50009
0.00.050.752 I llm_load_print_meta: vocab_only       = 0
0.00.050.752 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.753 I llm_load_print_meta: n_embd           = 2048
0.00.050.753 I llm_load_print_meta: n_layer          = 24
0.00.050.756 I llm_load_print_meta: n_head           = 16
0.00.050.756 I llm_load_print_meta: n_head_kv        = 16
0.00.050.757 I llm_load_print_meta: n_rot            = 32
0.00.050.757 I llm_load_print_meta: n_swa            = 0
0.00.050.757 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.758 I llm_load_print_meta: n_gqa            = 1
0.00.050.759 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.759 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.760 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.760 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.761 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.762 I llm_load_print_meta: n_ff             = 8192
0.00.050.763 I llm_load_print_meta: n_expert         = 0
0.00.050.764 I llm_load_print_meta: n_expert_used    = 0
0.00.050.764 I llm_load_print_meta: causal attn      = 1
0.00.050.764 I llm_load_print_meta: pooling type     = 0
0.00.050.764 I llm_load_print_meta: rope type        = 2
0.00.050.764 I llm_load_print_meta: rope scaling     = linear
0.00.050.767 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.767 I llm_load_print_meta: freq_scale_train = 1
0.00.050.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.768 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.768 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.768 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.768 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.768 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.768 I llm_load_print_meta: model type       = 1.4B
0.00.050.769 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.769 I llm_load_print_meta: model params     = 1.41 B
0.00.050.770 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.770 I llm_load_print_meta: general.name     = 1.4B
0.00.050.770 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.770 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.771 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: LF token         = 128 ''
0.00.050.775 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.776 I llm_load_print_meta: max token length = 1024
0.00.052.491 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.492 I llm_load_tensors: offloading output layer to GPU
0.00.052.492 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.502 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.503 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.317 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.318 I llama_new_context_with_model: n_ctx         = 128
0.00.053.318 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.318 I llama_new_context_with_model: n_batch       = 128
0.00.053.319 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.319 I llama_new_context_with_model: flash_attn    = 0
0.00.053.319 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.319 I llama_new_context_with_model: freq_scale    = 1
0.00.053.320 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.320 I ggml_metal_init: allocating
0.00.053.326 I ggml_metal_init: found device: Apple M4
0.00.053.328 I ggml_metal_init: picking default device: Apple M4
0.00.053.895 I ggml_metal_init: using embedded metal library
0.00.056.250 I ggml_metal_init: GPU name:   Apple M4
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.253 I ggml_metal_init: simdgroup reduction   = true
0.00.056.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.253 I ggml_metal_init: has bfloat            = true
0.00.056.253 I ggml_metal_init: use bfloat            = true
0.00.056.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.683 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.063 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.066 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.927 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.929 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.929 I llama_new_context_with_model: graph nodes  = 967
0.00.069.929 I llama_new_context_with_model: graph splits = 2
0.00.069.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.942 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.854 I 
0.00.646.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.900 I perplexity: tokenizing the input ..
0.00.654.472 I perplexity: tokenization took 7.571 ms
0.00.654.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.894 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.789.984 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.789.996 I llama_perf_context_print:        load time =     636.29 ms
0.00.789.997 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.81 tokens per second)
0.00.789.998 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.999 I llama_perf_context_print:       total time =     143.14 ms /   129 tokens
0.00.790.416 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.078s
sys	0m0.147s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.172 I llama_model_loader: - type  f32:  194 tensors
0.00.024.173 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.173 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.536 I llm_load_vocab: special tokens cache size = 25
0.00.050.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.443 I llm_load_print_meta: arch             = gptneox
0.00.050.443 I llm_load_print_meta: vocab type       = BPE
0.00.050.443 I llm_load_print_meta: n_vocab          = 50304
0.00.050.443 I llm_load_print_meta: n_merges         = 50009
0.00.050.444 I llm_load_print_meta: vocab_only       = 0
0.00.050.444 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.444 I llm_load_print_meta: n_embd           = 2048
0.00.050.444 I llm_load_print_meta: n_layer          = 24
0.00.050.447 I llm_load_print_meta: n_head           = 16
0.00.050.447 I llm_load_print_meta: n_head_kv        = 16
0.00.050.447 I llm_load_print_meta: n_rot            = 32
0.00.050.448 I llm_load_print_meta: n_swa            = 0
0.00.050.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.449 I llm_load_print_meta: n_gqa            = 1
0.00.050.450 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.450 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.452 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.452 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.453 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.453 I llm_load_print_meta: n_ff             = 8192
0.00.050.454 I llm_load_print_meta: n_expert         = 0
0.00.050.454 I llm_load_print_meta: n_expert_used    = 0
0.00.050.455 I llm_load_print_meta: causal attn      = 1
0.00.050.456 I llm_load_print_meta: pooling type     = 0
0.00.050.456 I llm_load_print_meta: rope type        = 2
0.00.050.456 I llm_load_print_meta: rope scaling     = linear
0.00.050.457 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.457 I llm_load_print_meta: freq_scale_train = 1
0.00.050.457 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.457 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.458 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.458 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.458 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.458 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.458 I llm_load_print_meta: model type       = 1.4B
0.00.050.459 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.459 I llm_load_print_meta: model params     = 1.41 B
0.00.050.460 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.460 I llm_load_print_meta: general.name     = 1.4B
0.00.050.460 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.460 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.462 I llm_load_print_meta: LF token         = 128 ''
0.00.050.462 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.462 I llm_load_print_meta: max token length = 1024
0.00.052.196 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.197 I llm_load_tensors: offloading output layer to GPU
0.00.052.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.207 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.208 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.058 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.059 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.059 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.059 I llama_new_context_with_model: n_batch       = 2048
0.00.053.059 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.059 I llama_new_context_with_model: flash_attn    = 0
0.00.053.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.060 I llama_new_context_with_model: freq_scale    = 1
0.00.053.061 I ggml_metal_init: allocating
0.00.053.066 I ggml_metal_init: found device: Apple M4
0.00.053.068 I ggml_metal_init: picking default device: Apple M4
0.00.053.642 I ggml_metal_init: using embedded metal library
0.00.055.960 I ggml_metal_init: GPU name:   Apple M4
0.00.055.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.963 I ggml_metal_init: simdgroup reduction   = true
0.00.055.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.964 I ggml_metal_init: has bfloat            = true
0.00.055.964 I ggml_metal_init: use bfloat            = true
0.00.055.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.719 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.171 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.189 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.269 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.270 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.271 I llama_new_context_with_model: graph nodes  = 967
0.00.086.271 I llama_new_context_with_model: graph splits = 2
0.00.086.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.451 I main: llama threadpool init, n_threads = 4
0.00.746.488 I 
0.00.746.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.535 I 
0.00.746.697 I sampler seed: 1234
0.00.746.702 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.716 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.717 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.717 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.592.050 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.592.051 I llama_perf_context_print:        load time =     737.55 ms
0.01.592.053 I llama_perf_context_print: prompt eval time =      51.91 ms /     7 tokens (    7.42 ms per token,   134.84 tokens per second)
0.01.592.053 I llama_perf_context_print:        eval time =     790.55 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.592.054 I llama_perf_context_print:       total time =     845.60 ms /    70 tokens
0.01.592.241 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.109s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.881 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.580 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.580 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.581 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.581 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.247 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.248 I llama_model_loader: - type  f32:  194 tensors
0.00.023.248 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.248 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.137 I llm_load_vocab: special tokens cache size = 25
0.00.048.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.906 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.907 I llm_load_print_meta: arch             = gptneox
0.00.048.907 I llm_load_print_meta: vocab type       = BPE
0.00.048.907 I llm_load_print_meta: n_vocab          = 50304
0.00.048.907 I llm_load_print_meta: n_merges         = 50009
0.00.048.907 I llm_load_print_meta: vocab_only       = 0
0.00.048.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.908 I llm_load_print_meta: n_embd           = 2048
0.00.048.908 I llm_load_print_meta: n_layer          = 24
0.00.048.911 I llm_load_print_meta: n_head           = 16
0.00.048.912 I llm_load_print_meta: n_head_kv        = 16
0.00.048.912 I llm_load_print_meta: n_rot            = 32
0.00.048.913 I llm_load_print_meta: n_swa            = 0
0.00.048.913 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.914 I llm_load_print_meta: n_gqa            = 1
0.00.048.915 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.916 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.916 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.917 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.917 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.917 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.917 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.918 I llm_load_print_meta: n_ff             = 8192
0.00.048.918 I llm_load_print_meta: n_expert         = 0
0.00.048.918 I llm_load_print_meta: n_expert_used    = 0
0.00.048.919 I llm_load_print_meta: causal attn      = 1
0.00.048.919 I llm_load_print_meta: pooling type     = 0
0.00.048.919 I llm_load_print_meta: rope type        = 2
0.00.048.919 I llm_load_print_meta: rope scaling     = linear
0.00.048.920 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.920 I llm_load_print_meta: freq_scale_train = 1
0.00.048.921 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.922 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.922 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.922 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.922 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.922 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.922 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.922 I llm_load_print_meta: model type       = 1.4B
0.00.048.923 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.923 I llm_load_print_meta: model params     = 1.41 B
0.00.048.924 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.924 I llm_load_print_meta: general.name     = 1.4B
0.00.048.924 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.930 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.932 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.932 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.932 I llm_load_print_meta: LF token         = 128 ''
0.00.048.933 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.933 I llm_load_print_meta: max token length = 1024
0.00.050.744 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.745 I llm_load_tensors: offloading output layer to GPU
0.00.050.745 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.755 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.756 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.563 I llama_new_context_with_model: n_ctx         = 128
0.00.051.564 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.564 I llama_new_context_with_model: n_batch       = 128
0.00.051.564 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.564 I llama_new_context_with_model: flash_attn    = 0
0.00.051.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.565 I llama_new_context_with_model: freq_scale    = 1
0.00.051.565 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.566 I ggml_metal_init: allocating
0.00.051.572 I ggml_metal_init: found device: Apple M4
0.00.051.574 I ggml_metal_init: picking default device: Apple M4
0.00.052.137 I ggml_metal_init: using embedded metal library
0.00.054.469 I ggml_metal_init: GPU name:   Apple M4
0.00.054.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.472 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.472 I ggml_metal_init: simdgroup reduction   = true
0.00.054.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.472 I ggml_metal_init: has bfloat            = true
0.00.054.472 I ggml_metal_init: use bfloat            = true
0.00.054.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.991 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.205 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.212 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.228 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.100 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.101 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.101 I llama_new_context_with_model: graph nodes  = 967
0.00.066.102 I llama_new_context_with_model: graph splits = 2
0.00.066.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.126 I 
0.00.774.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.171 I perplexity: tokenizing the input ..
0.00.781.834 I perplexity: tokenization took 7.661 ms
0.00.781.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.711 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.923.808 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.923.819 I llama_perf_context_print:        load time =     765.24 ms
0.00.923.820 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.06 tokens per second)
0.00.923.824 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.824 I llama_perf_context_print:       total time =     149.70 ms /   129 tokens
0.00.924.291 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.078s
sys	0m0.170s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.878 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.555 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.437 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.437 I llama_model_loader: - type  f32:  194 tensors
0.00.025.438 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.656 I llm_load_vocab: special tokens cache size = 25
0.00.051.469 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.472 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.472 I llm_load_print_meta: arch             = gptneox
0.00.051.473 I llm_load_print_meta: vocab type       = BPE
0.00.051.473 I llm_load_print_meta: n_vocab          = 50304
0.00.051.473 I llm_load_print_meta: n_merges         = 50009
0.00.051.473 I llm_load_print_meta: vocab_only       = 0
0.00.051.474 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.474 I llm_load_print_meta: n_embd           = 2048
0.00.051.474 I llm_load_print_meta: n_layer          = 24
0.00.051.476 I llm_load_print_meta: n_head           = 16
0.00.051.477 I llm_load_print_meta: n_head_kv        = 16
0.00.051.477 I llm_load_print_meta: n_rot            = 32
0.00.051.478 I llm_load_print_meta: n_swa            = 0
0.00.051.478 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.478 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.479 I llm_load_print_meta: n_gqa            = 1
0.00.051.480 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.481 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.482 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.482 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.482 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.483 I llm_load_print_meta: n_ff             = 8192
0.00.051.484 I llm_load_print_meta: n_expert         = 0
0.00.051.484 I llm_load_print_meta: n_expert_used    = 0
0.00.051.484 I llm_load_print_meta: causal attn      = 1
0.00.051.484 I llm_load_print_meta: pooling type     = 0
0.00.051.484 I llm_load_print_meta: rope type        = 2
0.00.051.484 I llm_load_print_meta: rope scaling     = linear
0.00.051.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.487 I llm_load_print_meta: freq_scale_train = 1
0.00.051.487 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.488 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.488 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.488 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.488 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.488 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.488 I llm_load_print_meta: model type       = 1.4B
0.00.051.489 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.489 I llm_load_print_meta: model params     = 1.41 B
0.00.051.490 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.490 I llm_load_print_meta: general.name     = 1.4B
0.00.051.490 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.491 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.491 I llm_load_print_meta: LF token         = 128 ''
0.00.051.491 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.491 I llm_load_print_meta: max token length = 1024
0.00.053.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.293 I llm_load_tensors: offloading output layer to GPU
0.00.053.293 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.303 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.304 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.117 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.119 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.119 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.119 I llama_new_context_with_model: n_batch       = 2048
0.00.054.119 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.119 I llama_new_context_with_model: flash_attn    = 0
0.00.054.120 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.120 I llama_new_context_with_model: freq_scale    = 1
0.00.054.121 I ggml_metal_init: allocating
0.00.054.124 I ggml_metal_init: found device: Apple M4
0.00.054.126 I ggml_metal_init: picking default device: Apple M4
0.00.054.694 I ggml_metal_init: using embedded metal library
0.00.057.043 I ggml_metal_init: GPU name:   Apple M4
0.00.057.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.045 I ggml_metal_init: simdgroup reduction   = true
0.00.057.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.046 I ggml_metal_init: has bfloat            = true
0.00.057.046 I ggml_metal_init: use bfloat            = true
0.00.057.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.047 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.111 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.118 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.139 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.260 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.262 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.263 I llama_new_context_with_model: graph nodes  = 967
0.00.089.263 I llama_new_context_with_model: graph splits = 2
0.00.089.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.656 I main: llama threadpool init, n_threads = 4
0.00.820.697 I 
0.00.820.732 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.732 I 
0.00.820.884 I sampler seed: 1234
0.00.820.889 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.820.932 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.820.933 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.820.936 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.688.555 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.688.555 I llama_perf_context_print:        load time =     810.77 ms
0.01.688.556 I llama_perf_context_print: prompt eval time =      54.81 ms /     7 tokens (    7.83 ms per token,   127.71 tokens per second)
0.01.688.557 I llama_perf_context_print:        eval time =     809.82 ms /    63 runs   (   12.85 ms per token,    77.80 tokens per second)
0.01.688.557 I llama_perf_context_print:       total time =     867.90 ms /    70 tokens
0.01.688.721 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4368 (0a11f8b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.392 I llama_model_loader: - type  f32:  194 tensors
0.00.024.392 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.120 I llm_load_vocab: special tokens cache size = 25
0.00.051.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.030 I llm_load_print_meta: arch             = gptneox
0.00.051.030 I llm_load_print_meta: vocab type       = BPE
0.00.051.030 I llm_load_print_meta: n_vocab          = 50304
0.00.051.031 I llm_load_print_meta: n_merges         = 50009
0.00.051.031 I llm_load_print_meta: vocab_only       = 0
0.00.051.031 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.031 I llm_load_print_meta: n_embd           = 2048
0.00.051.031 I llm_load_print_meta: n_layer          = 24
0.00.051.034 I llm_load_print_meta: n_head           = 16
0.00.051.034 I llm_load_print_meta: n_head_kv        = 16
0.00.051.035 I llm_load_print_meta: n_rot            = 32
0.00.051.035 I llm_load_print_meta: n_swa            = 0
0.00.051.035 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.035 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.036 I llm_load_print_meta: n_gqa            = 1
0.00.051.037 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.037 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.038 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.038 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.038 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.039 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.039 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.039 I llm_load_print_meta: n_ff             = 8192
0.00.051.040 I llm_load_print_meta: n_expert         = 0
0.00.051.040 I llm_load_print_meta: n_expert_used    = 0
0.00.051.040 I llm_load_print_meta: causal attn      = 1
0.00.051.040 I llm_load_print_meta: pooling type     = 0
0.00.051.040 I llm_load_print_meta: rope type        = 2
0.00.051.041 I llm_load_print_meta: rope scaling     = linear
0.00.051.042 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.042 I llm_load_print_meta: freq_scale_train = 1
0.00.051.042 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.043 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.043 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.043 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.043 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.045 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.045 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.045 I llm_load_print_meta: model type       = 1.4B
0.00.051.045 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.046 I llm_load_print_meta: model params     = 1.41 B
0.00.051.046 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.046 I llm_load_print_meta: general.name     = 1.4B
0.00.051.047 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: LF token         = 128 ''
0.00.051.052 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: max token length = 1024
0.00.052.832 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.833 I llm_load_tensors: offloading output layer to GPU
0.00.052.833 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.843 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.844 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.664 I llama_new_context_with_model: n_ctx         = 128
0.00.053.664 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.664 I llama_new_context_with_model: n_batch       = 128
0.00.053.665 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.665 I llama_new_context_with_model: flash_attn    = 0
0.00.053.665 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.665 I llama_new_context_with_model: freq_scale    = 1
0.00.053.666 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.666 I ggml_metal_init: allocating
0.00.053.672 I ggml_metal_init: found device: Apple M4
0.00.053.674 I ggml_metal_init: picking default device: Apple M4
0.00.054.238 I ggml_metal_init: using embedded metal library
0.00.056.554 I ggml_metal_init: GPU name:   Apple M4
0.00.056.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.557 I ggml_metal_init: simdgroup reduction   = true
0.00.056.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.557 I ggml_metal_init: has bfloat            = true
0.00.056.557 I ggml_metal_init: use bfloat            = true
0.00.056.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.314 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.174 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.175 I llama_new_context_with_model: graph nodes  = 967
0.00.068.175 I llama_new_context_with_model: graph splits = 2
0.00.068.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.135.485 I 
0.00.135.523 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.135.536 I perplexity: tokenizing the input ..
0.00.142.998 I perplexity: tokenization took 7.459 ms
0.00.143.003 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.281.530 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.282.620 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.282.631 I llama_perf_context_print:        load time =     125.52 ms
0.00.282.632 I llama_perf_context_print: prompt eval time =     138.30 ms /   128 tokens (    1.08 ms per token,   925.53 tokens per second)
0.00.282.633 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.282.633 I llama_perf_context_print:       total time =     147.15 ms /   129 tokens
0.00.283.110 I ggml_metal_free: deallocating

real	0m0.298s
user	0m0.079s
sys	0m0.037s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4368 (0a11f8b7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13bb0abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13bb0b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13bb0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13bb0be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13bb0c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13bb0c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13bb0cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13bb0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13bb0dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13bb0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13bb0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13bb0e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13bb0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13bb0fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13bb10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13bb10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13bb112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13bb119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13bb12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13bb128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13bb13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13bb13720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13bb13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13bb146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13bb14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13bb150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13bb156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13bb16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13bb16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13bb16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13bb16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13bb172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13bb17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13bb18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13bb18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13bb187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13bb18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13bb19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13bb195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13bb19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13bb19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13bb1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13bb1a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13bb1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13bb1af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13bb1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13bb1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13bb1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13bb1cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13bb1d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13bb1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13bb1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13bb1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13bb1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13bb1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13bb1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13bb1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13bb1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13bb20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13bb20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13bb20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13bb21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13bb21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13bb21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13bb22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13bb22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13bb229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13bb22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13bb232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13bb23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13bb23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13bb240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13bb24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13bb24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13bb25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13bb25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13bb25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13bb25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13bb26540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13bb26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13bb26fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13bb27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13bb27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13bb27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13bb28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13bb28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13bb28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13bb29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13bb29a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13bb29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13bb2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13bb2aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13bb2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13bb2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13bb2ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13bb2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13bb2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13bb1c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13bb2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13bb2d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13bb2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13bb2dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13bb2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13bb2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13bb2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13bb2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13bb2f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13bb2fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13bb300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13bb30620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13bb30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13bb310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13bb31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13bb31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13bb31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13bb323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13bb32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13bb32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13bb331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13bb33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13bb33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13bb33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13bb34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13bb348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13bb34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13bb35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13bb356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13bb35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13bb36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13bb364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13bb36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13bb36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13bb37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13bb37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13bb37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13bb38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13bb38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13bb389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13bb38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13bb392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13bb39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13bb39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13bb3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13bb3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13bb3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13bb3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13bb3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13bb3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13bb3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13bb3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13bb3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13bb3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13bb3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13bb3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13bb3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13bb3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13bb3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13bb3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13bb3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13bb3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13bb3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13bb3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13bb3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13bb401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13bb40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13bb40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13bb40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13bb41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13bb41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13bb41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13bb42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13bb426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13bb42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13bb43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13bb434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13bb43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13bb43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13bb442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13bb44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13bb44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13bb45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13bb45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13bb459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13bb45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13bb46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13bb467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13bb46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13bb470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13bb47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13bb47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13bb47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13bb48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13bb48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13bb48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13bb492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13bb49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13bb49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13bb4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13bb4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13bb4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13bb4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13bb4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13bb4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13bb4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13bb4c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13bb4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13bb4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13bb4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13bb4dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13bb4e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13bb4eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13bb4f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13bb4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13bb4fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13bb50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13bb505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13bb50b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13bb51060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13bb515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13bb51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13bb52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13bb525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13bb52af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13bb53040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13bb53590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13bb53ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13bb54030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13bb54580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13bb54ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13bb55020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13bb55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13bb55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13bb56010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13bb56560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13bb56ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13bb57000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13bb57550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13bb57aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13bb57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13bb58540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13bb58a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13bb58fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13bb59530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13bb59a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13bb59fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13bb5a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13bb5aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13bb5afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13bb5b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13bb5ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13bb5bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13bb5c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13bb5ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13bb5cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13bb5d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13bb5da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13bb5df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13bb5e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13bb5ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13bb5ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13bb5f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13bb5fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13bb5ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13bb604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13bb60a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13bb60f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13bb614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13bb61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13bb61df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13bb62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13bb62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13bb62bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13bb63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13bb63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13bb639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13bb63e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13bb642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13bb64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13bb64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13bb650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13bb65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13bb65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13bb65f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13bb66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13bb66da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13bb674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13bb67be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13bb67ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13bb68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13bb68950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13bb68f60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.173.808 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.173.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12db04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12db05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12db056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12db05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12db05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12db06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12db06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12db06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12db07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12db075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12db07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12db08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12db08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12db093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12db09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12db0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12db0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12db0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12db0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12db0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12db0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12db0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12db0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12db0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12db0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12db0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12db0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12db0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12db0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12db0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12db0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12db0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12db10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12db106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12db10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12db10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12db11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12db118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12db11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12db12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12db12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12db12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12db12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12db13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12db137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12db13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12db140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12db14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12db14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12db14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12db15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12db156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12db15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12db15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12db16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12db16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12db16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12db17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12db17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12db17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12db18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12db184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12db18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12db18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12db19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12db19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12db19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12db19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12db1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12db1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12db1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12db1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12db1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12db1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12db1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12db1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12db1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12db1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12db1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12db1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12db1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12db1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12db1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12db1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12db1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12db1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12db1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12db1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12db1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12db20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12db20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12db209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12db20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12db212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12db21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12db21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12db22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12db22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12db228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12db22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12db231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12db23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12db23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12db23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12db24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12db24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12db24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12db250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12db25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12db259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12db25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12db262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12db26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12db26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12db26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12db27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12db278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12db27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12db281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12db28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12db28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12db28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12db29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12db297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12db29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12db2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12db2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12db2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12db2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12db2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12db2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12db2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12db2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12db2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12db2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12db2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12db2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12db2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12db2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12db2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12db2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12db2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12db2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12db2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12db2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12db2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12db2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12db30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12db306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12db30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12db30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12db31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12db31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12db31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12db32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12db325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12db32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12db32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12db33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12db337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12db33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12db34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12db344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12db34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12db34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12db35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12db356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12db35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12db35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12db36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12db36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12db36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12db37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12db375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12db37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12db37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12db38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12db38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12db38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12db39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12db394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12db39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12db39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12db3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12db3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12db3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12db3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12db3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12db3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12db3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12db3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12db3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12db3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12db3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12db3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12db3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12db3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12db3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12db3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12db3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12db3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12db3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12db3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12db3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12db3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12db403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12db40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12db40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12db41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12db416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12db421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12db424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12db42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12db42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12db43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12db434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12db43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12db43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12db44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12db44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12db44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12db44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12db453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12db45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12db45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12db46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12db46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12db46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12db46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12db472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12db47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12db47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12db48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12db484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12db48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12db48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12db491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12db49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12db49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12db49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12db4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12db4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12db4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12db4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12db4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12db4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12db4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12db4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12db4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12db4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12db4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12db4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12db4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12db4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12db4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12db4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12db4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12db4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12db4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12db4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12db4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12db500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12db50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12db509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12db50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12db512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12db51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12db51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12db51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12db52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12db528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12db52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12db531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12db53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12db53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12db53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12db54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12db547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12db54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12db550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12db55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12db559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12db55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12db56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12db56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12db576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12db57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12db580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12db58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12db58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12db59120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12db04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12db05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12db055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12db05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12db05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12db06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12db06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12db06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12db07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12db074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12db07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12db07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12db08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12db08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12db09770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12db09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12db0a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12db0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12db0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12db0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12db0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12db0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12db0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12db0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12db0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12db0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12db0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12db0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12db0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12db0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12db0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12db0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12db102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12db105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12db10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12db10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12db112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12db11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12db11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12db12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12db124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12db12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12db12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12db13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12db13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12db13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12db13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12db143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12db14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12db14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12db15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12db15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12db159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12db15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12db162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12db16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12db16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12db17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12db17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12db17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12db17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12db181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12db18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12db18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12db18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12db193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12db19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12db19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12db1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12db1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12db1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12db1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12db1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12db1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12db1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12db1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12db1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12db1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12db1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12db1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12db1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12db1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12db1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12db1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12db1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12db1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12db1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12db1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12db1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12db1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12db20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12db20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12db20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12db20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12db21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12db218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12db21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12db221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12db22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12db22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12db22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12db23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12db237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12db23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12db240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12db24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12db24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12db24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12db25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12db256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12db25b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12db25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12db26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12db268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12db26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12db27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12db275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12db27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12db27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12db28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12db287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12db28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12db29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12db29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12db29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12db29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12db2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12db2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12db2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12db2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12db2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12db2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12db2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12db2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12db2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12db2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12db2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12db2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12db2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12db2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12db2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12db2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12db2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12db2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12db2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12db2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12db2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12db2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12db303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12db30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12db30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12db31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12db315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12db31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12db31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12db32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12db32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12db32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12db33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12db334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12db33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12db33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12db34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12db34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12db34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12db34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12db353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12db35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12db35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12db36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12db36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12db36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12db36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12db372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12db37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12db37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12db38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12db384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12db38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12db38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12db391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12db39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12db39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12db39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12db3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12db3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12db3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12db3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12db3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12db3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12db3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12db3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12db3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12db3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12db3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12db3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12db3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12db3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12db3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12db3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12db3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12db3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12db3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12db3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12db3fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12db400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12db40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12db409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12db40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12db412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12db41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12db41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12db42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12db42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12db42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12db43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12db434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12db43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12db43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12db44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12db44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12db44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12db44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12db453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12db45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12db45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12db46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12db46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12db46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12db46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12db472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12db47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12db47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12db48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12db484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12db48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12db48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12db491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12db49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12db49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12db49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12db4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12db4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12db4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12db4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12db4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12db4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12db4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12db4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12db4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12db4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12db4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12db4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12db4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12db4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12db4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12db4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12db4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12db4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12db4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12db4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12db4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12db500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12db50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12db509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12db50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12db512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12db51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12db51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12db51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12db52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12db528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12db52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12db531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12db53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12db53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12db53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12db54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12db547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12db54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12db550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12db55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12db559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12db56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12db568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12db56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12db576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12db57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12db57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12db58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12db58890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.924s
user	0m0.311s
sys	0m0.311s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4368 (0a11f8b7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c60bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c60c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c60cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c60d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c60d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c60dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c60e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c60e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c60ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c60f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c60f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c60fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c610860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c611010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c612660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c612d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c6134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c613c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c614390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c614ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c6151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c615a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c616190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c616450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c616a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c6176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c617c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c617ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c618630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c619400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c6196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c619b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c61a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c61a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c61ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c61b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c61b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c61bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c61c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c61c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c61c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c61cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c61d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c61de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c61e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c61ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c61f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c61f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c61fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c6204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c620950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c6210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c621eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c622170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c622ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c622f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c6233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c623890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c6241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c624670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c624b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c624fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c625450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c6258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c625e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c626390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c6268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c626e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c627380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c6278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c627e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c628370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c6288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c628e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c6298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c629e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c62a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c62a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c62adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c62b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c62b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c62bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c62c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c62c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c62cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c62d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c62d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c61d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c62dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c62e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c62e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c62ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c62f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c62f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c62ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c630470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c6309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c630f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c6319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c631f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c632450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c6329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c632e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c6332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c633780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c633c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c6340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c634560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c634ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c635340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c6357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c635c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c6365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c636a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c636f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c6373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c637ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c638180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c638ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c638f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c639400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c6398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c63a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c63ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c63afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c63b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c63b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c63bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c63c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c63c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c63cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c63d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c63d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c63d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c63de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c63e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c63e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c63ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c63f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c63f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c63f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c63fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c6407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c640c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c6410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c641580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c642ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c643140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c6435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c643a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c643f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c6443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c644860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c644d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c6451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c645ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c645f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c646420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c6468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c646d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c6476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c647fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c648480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c648920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c648dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c649260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c649ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c64a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c64a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c64ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c64b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c64b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c64b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c64bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c64c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c64cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c64d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c64db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c64e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c64e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c64edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c64f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c64f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c64fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c650410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c650960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c650eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c651400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c651950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c651ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c6523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c652940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c652e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c6533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c653e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c6543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c654920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c654e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c6553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c655910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c655e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c6563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c656900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c656e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c6573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c6578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c657e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c658390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c6588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c658e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c659380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c6598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c659e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c65a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c65a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c65ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c65b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c65b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c65be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c65c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c65c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c65cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c65d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c65d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c65dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c65e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c65e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c65edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c65f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c65f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c65fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c660310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c660860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c660db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c661300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c661850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c661da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c6622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c662840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c662ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c663180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c663620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c663ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c663f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c664400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c6648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c664d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c6651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c665680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c665b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c665fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c666460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c666900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c666da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c6672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c667a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c668130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c668850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c669a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c669ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c66a2f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e0064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e0083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e009390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e009eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e00a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e00ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e00b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e00bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e00c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e00caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e00d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e00d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e00e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e00e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e00ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e00f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e00f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e00fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e010050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e0104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e010930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e010da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e0112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e011740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e011a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e0122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e012750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e012bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e013030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e0134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e013910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e013d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e0141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e014ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e014f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e0153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e015820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e015c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e016100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e0169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e0172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e017730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e017ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e018110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e018610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e019360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e0197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e019c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e01a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e01a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e01a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e01ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e01b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e01b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e01bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e01bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e01c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e01c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e01cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e01d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e01d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e01da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e01ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e01e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e01e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e01ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e01f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e01f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e01f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e01fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e020250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e0206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e020b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e020fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e021410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e021880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e021cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e0225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e022a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e023320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e023790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e023c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e024070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e0244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e024950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e024dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e025230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e0256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e025b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e0263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e026860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e026cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e027140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e0275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e027e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e028770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e028be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e029050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e0294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e029930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e029da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e02a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e02a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e02aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e02af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e02b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e02b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e02bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e02c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e02c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e02ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e02ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e02d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e02d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e02dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e02e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e02e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e02e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e02ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e02f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e02f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e02fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e02ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e0303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e030820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e030c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e031100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e031570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e0319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e031e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e0322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e032730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e032ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e033010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e033480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e0338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e033d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e0341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e034640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e034ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e034f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e035390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e035800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e035c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e0360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e036550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e0369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e036e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e0372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e037710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e037b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e037ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e038460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e0388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e038d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e0391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e039a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e039f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e03a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e03a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e03ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e03b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e03b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e03b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e03be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e03c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e03c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e03cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e03cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e03d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e03d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e03dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e03e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e03e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e03ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e03eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e03f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e03f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e03fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e0400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e040510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e040980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e040df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e041260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e0416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e041b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e0420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e042540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e0429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e043500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e0437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e043a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e043ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e044360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e0447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e044c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e0450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e045520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e045990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e045e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e046270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e0466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e046b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e046fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e047430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e0478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e047d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e048180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e0485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e048a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e048ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e0497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e049c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e04a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e04a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e04a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e04ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e04b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e04b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e04bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e04bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e04c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e04c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e04ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e04d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e04d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e04da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e04deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e04e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e04e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e04ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e04f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e04f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e04f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e04fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e050230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e0506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e050b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e050f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e0513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e051860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e051cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e052140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e0525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e052a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e052e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e053300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e053770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e053be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e054050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e0544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e054930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e054da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e055210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e055680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e055af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e055f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e0563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e056840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e056cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e057120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e057b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e0582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e0589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e0590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e0593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e059820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e059e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e05a430 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e0064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e0083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e009290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e009b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e00a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e00aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e00b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e00b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e00bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e00c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e00d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e00d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e00de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e00e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e00ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e00f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e00f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e00fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e0121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e0133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e0149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e0152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e0168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e016d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e0171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e017640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e017ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e017f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e018390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e018800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e018c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e0190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e019550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e0199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e01a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e01a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e01ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e01aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e01b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e01b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e01bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e01c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e01c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e01ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e01cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e01d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e01d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e01dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e01e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e01e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e01e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e01ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e01f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e01f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e01fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e01ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e020440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e0208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e021190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e021600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e021a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e021ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e022350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e0227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e022c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e0230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e023510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e023980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e023df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e024260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e0246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e024b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e024fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e025420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e025890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e025d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e026170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e0265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e026a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e026ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e027330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e0277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e027c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e028080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e0284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e028960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e028dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e029240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e0296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e029b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e029f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e02a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e02a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e02ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e02b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e02b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e02ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e02bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e02c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e02c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e02cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e02d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e02d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e02d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e02ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e02e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e02e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e02eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e02ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e02f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e02f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e02fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e030130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e0305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e030a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e030e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e0312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e031760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e031bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e032040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e0324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e032920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e032d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e033200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e033670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e033ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e033f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e0343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e034830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e034ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e035110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e035580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e0359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e035e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e0362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e036740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e036bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e037020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e037490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e037900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e037d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e0381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e038650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e038ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e038f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e0393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e039810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e039c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e03a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e03a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e03a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e03ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e03b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e03b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e03bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e03c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e03c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e03c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e03cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e03d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e03d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e03daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e03df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e03e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e03e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e03ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e03f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e03f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e03f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e03fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e040290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e040700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e040b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e040fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e041450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e0418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e041d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e0421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e042610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e042d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e043200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e043670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e043ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e043f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e0443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e044830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e044ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e045110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e045580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e0459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e045e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e0462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e046740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e046bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e047020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e047490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e047900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e047d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e0481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e048650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e048ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e048f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e0493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e049810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e049c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e04a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e04a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e04a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e04ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e04b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e04b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e04bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e04c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e04c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e04c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e04cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e04d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e04d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e04daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e04df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e04e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e04e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e04ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e04f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e04f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e04f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e04fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e050290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e050700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e050b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e050fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e0518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e051d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e0521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e052610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e052a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e052ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e053360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e0537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e053c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e0540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e054520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e054990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e054e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e055270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e0556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e055fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e056430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e0568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e056d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e057570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e057c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e058350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e058a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e058eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e059320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e059c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.925s
user	0m0.244s
sys	0m0.135s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.26 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
