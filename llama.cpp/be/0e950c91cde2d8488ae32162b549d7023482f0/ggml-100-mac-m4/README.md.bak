### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.87 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.71 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.51 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.04 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.35 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.22 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.33 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.22 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.82 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.82 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    1.05 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.93 sec*proc (28 tests)

Total Test time (real) = 222.94 sec

real	3m42.971s
user	7m32.207s
sys	0m6.373s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.34 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.12 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.34 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.45 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.43 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.76 sec*proc (28 tests)

Total Test time (real) =  51.77 sec

real	0m51.780s
user	1m12.186s
sys	0m5.699s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.070 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.198 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.005 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.014 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.016 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.017 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.017 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.018 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.019 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.022 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.023 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.023 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.026 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.029 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.030 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.030 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.031 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.031 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.032 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.650 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.652 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.653 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.653 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.654 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.655 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.656 I llama_model_loader: - type  f32:  124 tensors
0.00.025.656 I llama_model_loader: - type  f16:   73 tensors
0.00.029.823 I llm_load_vocab: special tokens cache size = 5
0.00.031.832 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.031.835 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.031.836 I llm_load_print_meta: arch             = bert
0.00.031.836 I llm_load_print_meta: vocab type       = WPM
0.00.031.837 I llm_load_print_meta: n_vocab          = 30522
0.00.031.837 I llm_load_print_meta: n_merges         = 0
0.00.031.837 I llm_load_print_meta: vocab_only       = 0
0.00.031.837 I llm_load_print_meta: n_ctx_train      = 512
0.00.031.838 I llm_load_print_meta: n_embd           = 384
0.00.031.838 I llm_load_print_meta: n_layer          = 12
0.00.031.841 I llm_load_print_meta: n_head           = 12
0.00.031.842 I llm_load_print_meta: n_head_kv        = 12
0.00.031.842 I llm_load_print_meta: n_rot            = 32
0.00.031.842 I llm_load_print_meta: n_swa            = 0
0.00.031.843 I llm_load_print_meta: n_embd_head_k    = 32
0.00.031.845 I llm_load_print_meta: n_embd_head_v    = 32
0.00.031.845 I llm_load_print_meta: n_gqa            = 1
0.00.031.846 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.031.847 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.031.848 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.031.849 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.031.849 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.031.849 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.031.849 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.031.850 I llm_load_print_meta: n_ff             = 1536
0.00.031.850 I llm_load_print_meta: n_expert         = 0
0.00.031.851 I llm_load_print_meta: n_expert_used    = 0
0.00.031.851 I llm_load_print_meta: causal attn      = 0
0.00.031.851 I llm_load_print_meta: pooling type     = 2
0.00.031.851 I llm_load_print_meta: rope type        = 2
0.00.031.851 I llm_load_print_meta: rope scaling     = linear
0.00.031.852 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.031.852 I llm_load_print_meta: freq_scale_train = 1
0.00.031.853 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.031.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.031.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.031.853 I llm_load_print_meta: ssm_d_inner      = 0
0.00.031.853 I llm_load_print_meta: ssm_d_state      = 0
0.00.031.854 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.031.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.031.854 I llm_load_print_meta: model type       = 33M
0.00.031.855 I llm_load_print_meta: model ftype      = F16
0.00.031.856 I llm_load_print_meta: model params     = 33.21 M
0.00.031.856 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.031.857 I llm_load_print_meta: general.name     = Bge Small
0.00.031.857 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.031.857 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.031.857 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.031.858 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.031.858 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.031.858 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.031.858 I llm_load_print_meta: max token length = 21
0.00.033.830 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.033.830 I llm_load_tensors: offloading output layer to GPU
0.00.033.831 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.033.857 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.858 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.095 I llama_new_context_with_model: n_ctx         = 512
0.00.034.095 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.095 I llama_new_context_with_model: n_batch       = 2048
0.00.034.096 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.096 I llama_new_context_with_model: flash_attn    = 0
0.00.034.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.097 I llama_new_context_with_model: freq_scale    = 1
0.00.034.097 I ggml_metal_init: allocating
0.00.034.101 I ggml_metal_init: found device: Apple M4
0.00.034.104 I ggml_metal_init: picking default device: Apple M4
0.00.034.897 I ggml_metal_init: using embedded metal library
0.00.038.947 I ggml_metal_init: GPU name:   Apple M4
0.00.038.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.951 I ggml_metal_init: simdgroup reduction   = true
0.00.038.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.952 I ggml_metal_init: has bfloat            = true
0.00.038.952 I ggml_metal_init: use bfloat            = true
0.00.038.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.737 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.275 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.277 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.278 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.052.075 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.052.076 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.052.076 I llama_new_context_with_model: graph nodes  = 429
0.00.052.077 I llama_new_context_with_model: graph splits = 2
0.00.052.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.057.818 I 
0.00.057.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.484 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.061.994 I llama_perf_context_print:        load time =      42.61 ms
0.00.061.996 I llama_perf_context_print: prompt eval time =       3.38 ms /     9 tokens (    0.38 ms per token,  2660.36 tokens per second)
0.00.061.997 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.061.998 I llama_perf_context_print:       total time =       4.18 ms /    10 tokens
0.00.062.128 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.029 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.486 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.491 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.493 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.493 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.493 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.494 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.495 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.495 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.495 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.496 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.497 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.498 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.498 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.498 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.499 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.499 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.912 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.543 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.544 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.545 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.545 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.545 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.545 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.546 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.014.546 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.014.546 I llama_model_loader: - type  f32:  124 tensors
0.00.014.547 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.068 I llm_load_vocab: special tokens cache size = 5
0.00.018.350 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.353 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.353 I llm_load_print_meta: arch             = bert
0.00.018.354 I llm_load_print_meta: vocab type       = WPM
0.00.018.354 I llm_load_print_meta: n_vocab          = 30522
0.00.018.354 I llm_load_print_meta: n_merges         = 0
0.00.018.354 I llm_load_print_meta: vocab_only       = 0
0.00.018.354 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.355 I llm_load_print_meta: n_embd           = 384
0.00.018.355 I llm_load_print_meta: n_layer          = 12
0.00.018.358 I llm_load_print_meta: n_head           = 12
0.00.018.359 I llm_load_print_meta: n_head_kv        = 12
0.00.018.359 I llm_load_print_meta: n_rot            = 32
0.00.018.359 I llm_load_print_meta: n_swa            = 0
0.00.018.359 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.359 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.360 I llm_load_print_meta: n_gqa            = 1
0.00.018.360 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.361 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.361 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.362 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.362 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.362 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.362 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.363 I llm_load_print_meta: n_ff             = 1536
0.00.018.364 I llm_load_print_meta: n_expert         = 0
0.00.018.364 I llm_load_print_meta: n_expert_used    = 0
0.00.018.364 I llm_load_print_meta: causal attn      = 0
0.00.018.364 I llm_load_print_meta: pooling type     = 2
0.00.018.365 I llm_load_print_meta: rope type        = 2
0.00.018.365 I llm_load_print_meta: rope scaling     = linear
0.00.018.365 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.365 I llm_load_print_meta: freq_scale_train = 1
0.00.018.367 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.368 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.368 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.368 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.368 I llm_load_print_meta: model type       = 33M
0.00.018.369 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.369 I llm_load_print_meta: model params     = 33.21 M
0.00.018.370 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.374 I llm_load_print_meta: general.name     = Bge Small
0.00.018.375 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.375 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.375 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.376 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.376 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.376 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.376 I llm_load_print_meta: max token length = 21
0.00.019.698 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.699 I llm_load_tensors: offloading output layer to GPU
0.00.019.699 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.707 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.708 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.864 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.865 I llama_new_context_with_model: n_ctx         = 512
0.00.019.865 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.865 I llama_new_context_with_model: n_batch       = 2048
0.00.019.865 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.866 I llama_new_context_with_model: flash_attn    = 0
0.00.019.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.866 I llama_new_context_with_model: freq_scale    = 1
0.00.019.867 I ggml_metal_init: allocating
0.00.019.869 I ggml_metal_init: found device: Apple M4
0.00.019.871 I ggml_metal_init: picking default device: Apple M4
0.00.020.471 I ggml_metal_init: using embedded metal library
0.00.022.901 I ggml_metal_init: GPU name:   Apple M4
0.00.022.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.904 I ggml_metal_init: simdgroup reduction   = true
0.00.022.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.904 I ggml_metal_init: has bfloat            = true
0.00.022.904 I ggml_metal_init: use bfloat            = true
0.00.022.905 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.906 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.070 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.579 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.582 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.585 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.245 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.246 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.247 I llama_new_context_with_model: graph nodes  = 429
0.00.034.247 I llama_new_context_with_model: graph splits = 2
0.00.034.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.857 I 
0.00.038.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.410 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.830 I llama_perf_context_print:        load time =      29.95 ms
0.00.042.831 I llama_perf_context_print: prompt eval time =       3.29 ms /     9 tokens (    0.37 ms per token,  2735.56 tokens per second)
0.00.042.832 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.832 I llama_perf_context_print:       total time =       3.97 ms /    10 tokens
0.00.043.009 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.178 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.070 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.252 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.259 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.261 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.262 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.262 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.264 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.268 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.269 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.269 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.270 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.275 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.275 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.276 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.709 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.078 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.078 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.078 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.079 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.079 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.079 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.080 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.080 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.081 I llama_model_loader: - type  f32:   40 tensors
0.00.051.083 I llama_model_loader: - type  f16:   30 tensors
0.00.068.972 W llm_load_vocab: empty token at index 5
0.00.073.602 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.975 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.006 I llm_load_vocab: special tokens cache size = 5
0.00.339.498 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.519 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.520 I llm_load_print_meta: vocab type       = BPE
0.00.339.520 I llm_load_print_meta: n_vocab          = 61056
0.00.339.520 I llm_load_print_meta: n_merges         = 39382
0.00.339.520 I llm_load_print_meta: vocab_only       = 0
0.00.339.521 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.521 I llm_load_print_meta: n_embd           = 384
0.00.339.521 I llm_load_print_meta: n_layer          = 4
0.00.339.527 I llm_load_print_meta: n_head           = 12
0.00.339.528 I llm_load_print_meta: n_head_kv        = 12
0.00.339.529 I llm_load_print_meta: n_rot            = 32
0.00.339.529 I llm_load_print_meta: n_swa            = 0
0.00.339.529 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.529 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.534 I llm_load_print_meta: n_gqa            = 1
0.00.339.536 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.536 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.537 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.538 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.538 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.538 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.538 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.539 I llm_load_print_meta: n_ff             = 1536
0.00.339.539 I llm_load_print_meta: n_expert         = 0
0.00.339.539 I llm_load_print_meta: n_expert_used    = 0
0.00.339.540 I llm_load_print_meta: causal attn      = 0
0.00.339.540 I llm_load_print_meta: pooling type     = -1
0.00.339.540 I llm_load_print_meta: rope type        = -1
0.00.339.540 I llm_load_print_meta: rope scaling     = linear
0.00.339.540 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.541 I llm_load_print_meta: freq_scale_train = 1
0.00.339.541 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.541 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.541 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.542 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.545 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.545 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.545 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.546 I llm_load_print_meta: model type       = 33M
0.00.339.546 I llm_load_print_meta: model ftype      = F16
0.00.339.547 I llm_load_print_meta: model params     = 32.90 M
0.00.339.547 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.547 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.548 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.548 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.549 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.549 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.549 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.549 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.549 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.549 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.550 I llm_load_print_meta: max token length = 45
0.00.340.800 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.340.800 I llm_load_tensors: offloading output layer to GPU
0.00.340.800 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.340.819 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.820 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.341.269 I llama_new_context_with_model: n_seq_max     = 1
0.00.341.269 I llama_new_context_with_model: n_ctx         = 8192
0.00.341.270 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.341.270 I llama_new_context_with_model: n_batch       = 2048
0.00.341.270 I llama_new_context_with_model: n_ubatch      = 2048
0.00.341.270 I llama_new_context_with_model: flash_attn    = 0
0.00.341.271 I llama_new_context_with_model: freq_base     = 10000.0
0.00.341.271 I llama_new_context_with_model: freq_scale    = 1
0.00.341.271 I ggml_metal_init: allocating
0.00.341.274 I ggml_metal_init: found device: Apple M4
0.00.341.276 I ggml_metal_init: picking default device: Apple M4
0.00.342.207 I ggml_metal_init: using embedded metal library
0.00.345.114 I ggml_metal_init: GPU name:   Apple M4
0.00.345.116 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.117 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.117 I ggml_metal_init: simdgroup reduction   = true
0.00.345.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.117 I ggml_metal_init: has bfloat            = true
0.00.345.117 I ggml_metal_init: use bfloat            = true
0.00.345.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.854 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.357.355 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.357 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.358 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.357.967 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.357.968 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.357.968 I llama_new_context_with_model: graph nodes  = 154
0.00.357.968 I llama_new_context_with_model: graph splits = 2
0.00.357.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.369.589 I 
0.00.369.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.369.913 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.369.914 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.369.921 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.369.922 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.369.927 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.369.928 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.370.451 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.374.209 I llama_perf_context_print:        load time =     345.51 ms
0.00.374.210 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16533.33 tokens per second)
0.00.374.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.374.211 I llama_perf_context_print:       total time =       4.62 ms /    63 tokens
0.00.374.447 I ggml_metal_free: deallocating

real	0m1.108s
user	0m0.346s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.154 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.289 I main: llama backend init
0.00.000.296 I main: load the model and apply lora adapter, if any
0.00.039.581 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.592 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.623 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.624 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.061.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.071.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.916 I llama_model_loader: - type  f32:  194 tensors
0.00.071.916 I llama_model_loader: - type  f16:   98 tensors
0.00.107.021 I llm_load_vocab: special tokens cache size = 25
0.00.114.190 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.114.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.114.194 I llm_load_print_meta: arch             = gptneox
0.00.114.194 I llm_load_print_meta: vocab type       = BPE
0.00.114.194 I llm_load_print_meta: n_vocab          = 50304
0.00.114.194 I llm_load_print_meta: n_merges         = 50009
0.00.114.194 I llm_load_print_meta: vocab_only       = 0
0.00.114.195 I llm_load_print_meta: n_ctx_train      = 2048
0.00.114.195 I llm_load_print_meta: n_embd           = 2048
0.00.114.195 I llm_load_print_meta: n_layer          = 24
0.00.114.198 I llm_load_print_meta: n_head           = 16
0.00.114.200 I llm_load_print_meta: n_head_kv        = 16
0.00.114.200 I llm_load_print_meta: n_rot            = 32
0.00.114.200 I llm_load_print_meta: n_swa            = 0
0.00.114.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.114.201 I llm_load_print_meta: n_embd_head_v    = 128
0.00.114.201 I llm_load_print_meta: n_gqa            = 1
0.00.114.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.114.203 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.114.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.114.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.114.204 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.114.205 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.114.205 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.114.206 I llm_load_print_meta: n_ff             = 8192
0.00.114.206 I llm_load_print_meta: n_expert         = 0
0.00.114.206 I llm_load_print_meta: n_expert_used    = 0
0.00.114.206 I llm_load_print_meta: causal attn      = 1
0.00.114.207 I llm_load_print_meta: pooling type     = 0
0.00.114.207 I llm_load_print_meta: rope type        = 2
0.00.114.207 I llm_load_print_meta: rope scaling     = linear
0.00.114.207 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.114.207 I llm_load_print_meta: freq_scale_train = 1
0.00.114.208 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.114.208 I llm_load_print_meta: rope_finetuned   = unknown
0.00.114.208 I llm_load_print_meta: ssm_d_conv       = 0
0.00.114.208 I llm_load_print_meta: ssm_d_inner      = 0
0.00.114.208 I llm_load_print_meta: ssm_d_state      = 0
0.00.114.208 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.114.209 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.114.209 I llm_load_print_meta: model type       = 1.4B
0.00.114.209 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.114.210 I llm_load_print_meta: model params     = 1.41 B
0.00.114.210 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.114.211 I llm_load_print_meta: general.name     = 1.4B
0.00.114.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.114.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.114.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.114.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.114.212 I llm_load_print_meta: LF token         = 128 ''
0.00.114.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.114.212 I llm_load_print_meta: max token length = 1024
0.00.116.920 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.116.921 I llm_load_tensors: offloading output layer to GPU
0.00.116.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.116.940 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.116.941 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.117.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.117.301 I llama_new_context_with_model: n_ctx         = 2048
0.00.117.302 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.117.302 I llama_new_context_with_model: n_batch       = 2048
0.00.117.302 I llama_new_context_with_model: n_ubatch      = 512
0.00.117.302 I llama_new_context_with_model: flash_attn    = 0
0.00.117.303 I llama_new_context_with_model: freq_base     = 10000.0
0.00.117.303 I llama_new_context_with_model: freq_scale    = 1
0.00.117.303 I ggml_metal_init: allocating
0.00.117.307 I ggml_metal_init: found device: Apple M4
0.00.117.309 I ggml_metal_init: picking default device: Apple M4
0.00.118.010 I ggml_metal_init: using embedded metal library
0.00.164.712 I ggml_metal_init: GPU name:   Apple M4
0.00.164.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.164.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.164.717 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.164.718 I ggml_metal_init: simdgroup reduction   = true
0.00.164.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.164.718 I ggml_metal_init: has bfloat            = true
0.00.164.718 I ggml_metal_init: use bfloat            = true
0.00.164.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.164.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.267.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.289.477 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.289.484 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.289.507 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.290.459 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.290.460 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.290.461 I llama_new_context_with_model: graph nodes  = 967
0.00.290.461 I llama_new_context_with_model: graph splits = 2
0.00.290.464 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.290.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.290.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.369.941 I main: llama threadpool init, n_threads = 4
0.00.369.994 I 
0.00.370.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.370.019 I 
0.00.370.095 I sampler seed: 1234
0.00.370.099 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.370.135 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.370.137 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.370.137 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.231.912 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.02.231.913 I llama_perf_context_print:        load time =     330.35 ms
0.02.231.914 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.02.231.914 I llama_perf_context_print:        eval time =    1804.59 ms /    63 runs   (   28.64 ms per token,    34.91 tokens per second)
0.02.231.915 I llama_perf_context_print:       total time =    1861.97 ms /    70 tokens
0.02.232.157 I ggml_metal_free: deallocating

real	0m2.564s
user	0m0.155s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.549 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.246 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.446 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.051 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.761 I llama_model_loader: - type  f32:  194 tensors
0.00.053.761 I llama_model_loader: - type  f16:   98 tensors
0.00.082.026 I llm_load_vocab: special tokens cache size = 25
0.00.088.481 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.484 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.484 I llm_load_print_meta: arch             = gptneox
0.00.088.485 I llm_load_print_meta: vocab type       = BPE
0.00.088.485 I llm_load_print_meta: n_vocab          = 50304
0.00.088.485 I llm_load_print_meta: n_merges         = 50009
0.00.088.485 I llm_load_print_meta: vocab_only       = 0
0.00.088.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.485 I llm_load_print_meta: n_embd           = 2048
0.00.088.485 I llm_load_print_meta: n_layer          = 24
0.00.088.488 I llm_load_print_meta: n_head           = 16
0.00.088.489 I llm_load_print_meta: n_head_kv        = 16
0.00.088.489 I llm_load_print_meta: n_rot            = 32
0.00.088.489 I llm_load_print_meta: n_swa            = 0
0.00.088.489 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.490 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.491 I llm_load_print_meta: n_gqa            = 1
0.00.088.492 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.492 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.493 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.500 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.501 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.501 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.502 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.508 I llm_load_print_meta: n_ff             = 8192
0.00.088.509 I llm_load_print_meta: n_expert         = 0
0.00.088.509 I llm_load_print_meta: n_expert_used    = 0
0.00.088.509 I llm_load_print_meta: causal attn      = 1
0.00.088.510 I llm_load_print_meta: pooling type     = 0
0.00.088.510 I llm_load_print_meta: rope type        = 2
0.00.088.511 I llm_load_print_meta: rope scaling     = linear
0.00.088.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.511 I llm_load_print_meta: freq_scale_train = 1
0.00.088.511 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.512 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.512 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.512 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.513 I llm_load_print_meta: model type       = 1.4B
0.00.088.513 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.514 I llm_load_print_meta: model params     = 1.41 B
0.00.088.514 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.514 I llm_load_print_meta: general.name     = 1.4B
0.00.088.515 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.515 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.515 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.515 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.515 I llm_load_print_meta: LF token         = 128 ''
0.00.088.517 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.517 I llm_load_print_meta: max token length = 1024
0.00.091.040 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.040 I llm_load_tensors: offloading output layer to GPU
0.00.091.040 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.051 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.052 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.366 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.367 I llama_new_context_with_model: n_ctx         = 128
0.00.091.367 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.367 I llama_new_context_with_model: n_batch       = 128
0.00.091.367 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.367 I llama_new_context_with_model: flash_attn    = 0
0.00.091.368 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.368 I llama_new_context_with_model: freq_scale    = 1
0.00.091.368 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.369 I ggml_metal_init: allocating
0.00.091.373 I ggml_metal_init: found device: Apple M4
0.00.091.375 I ggml_metal_init: picking default device: Apple M4
0.00.091.952 I ggml_metal_init: using embedded metal library
0.00.094.432 I ggml_metal_init: GPU name:   Apple M4
0.00.094.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.435 I ggml_metal_init: simdgroup reduction   = true
0.00.094.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.435 I ggml_metal_init: has bfloat            = true
0.00.094.435 I ggml_metal_init: use bfloat            = true
0.00.094.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.582 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.954 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.958 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.974 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.864 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.865 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.866 I llama_new_context_with_model: graph nodes  = 967
0.00.105.866 I llama_new_context_with_model: graph splits = 2
0.00.105.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.314.270 I 
0.01.314.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.314.352 I perplexity: tokenizing the input ..
0.01.328.791 I perplexity: tokenization took 14.43 ms
0.01.328.798 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.448.882 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.450.330 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.450.359 I llama_perf_context_print:        load time =    1293.01 ms
0.01.450.360 I llama_perf_context_print: prompt eval time =     119.10 ms /   128 tokens (    0.93 ms per token,  1074.74 tokens per second)
0.01.450.360 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.450.360 I llama_perf_context_print:       total time =     136.10 ms /   129 tokens
0.01.450.721 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.119s
sys	0m0.225s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.959 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.957 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.958 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.872 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.819 I llama_model_loader: - type  f32:  194 tensors
0.00.034.820 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.372 I llm_load_vocab: special tokens cache size = 25
0.00.062.347 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.351 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.351 I llm_load_print_meta: arch             = gptneox
0.00.062.352 I llm_load_print_meta: vocab type       = BPE
0.00.062.352 I llm_load_print_meta: n_vocab          = 50304
0.00.062.352 I llm_load_print_meta: n_merges         = 50009
0.00.062.352 I llm_load_print_meta: vocab_only       = 0
0.00.062.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.355 I llm_load_print_meta: n_embd           = 2048
0.00.062.356 I llm_load_print_meta: n_layer          = 24
0.00.062.362 I llm_load_print_meta: n_head           = 16
0.00.062.363 I llm_load_print_meta: n_head_kv        = 16
0.00.062.363 I llm_load_print_meta: n_rot            = 32
0.00.062.363 I llm_load_print_meta: n_swa            = 0
0.00.062.365 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.365 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.368 I llm_load_print_meta: n_gqa            = 1
0.00.062.369 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.369 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.370 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.371 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.371 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.371 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.372 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.373 I llm_load_print_meta: n_ff             = 8192
0.00.062.373 I llm_load_print_meta: n_expert         = 0
0.00.062.373 I llm_load_print_meta: n_expert_used    = 0
0.00.062.373 I llm_load_print_meta: causal attn      = 1
0.00.062.373 I llm_load_print_meta: pooling type     = 0
0.00.062.374 I llm_load_print_meta: rope type        = 2
0.00.062.374 I llm_load_print_meta: rope scaling     = linear
0.00.062.374 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.375 I llm_load_print_meta: freq_scale_train = 1
0.00.062.375 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.375 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.375 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.375 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.375 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.375 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.375 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.376 I llm_load_print_meta: model type       = 1.4B
0.00.062.376 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.377 I llm_load_print_meta: model params     = 1.41 B
0.00.062.377 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.377 I llm_load_print_meta: general.name     = 1.4B
0.00.062.377 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.378 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.378 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.378 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.378 I llm_load_print_meta: LF token         = 128 ''
0.00.062.379 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.379 I llm_load_print_meta: max token length = 1024
0.00.064.812 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.812 I llm_load_tensors: offloading output layer to GPU
0.00.064.812 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.824 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.825 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.237 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.237 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.237 I llama_new_context_with_model: n_batch       = 2048
0.00.065.238 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.238 I llama_new_context_with_model: flash_attn    = 0
0.00.065.238 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.238 I llama_new_context_with_model: freq_scale    = 1
0.00.065.239 I ggml_metal_init: allocating
0.00.065.243 I ggml_metal_init: found device: Apple M4
0.00.065.247 I ggml_metal_init: picking default device: Apple M4
0.00.065.973 I ggml_metal_init: using embedded metal library
0.00.068.557 I ggml_metal_init: GPU name:   Apple M4
0.00.068.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.563 I ggml_metal_init: simdgroup reduction   = true
0.00.068.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.563 I ggml_metal_init: has bfloat            = true
0.00.068.563 I ggml_metal_init: use bfloat            = true
0.00.068.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.162 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.187 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.486 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.487 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.488 I llama_new_context_with_model: graph nodes  = 967
0.00.105.488 I llama_new_context_with_model: graph splits = 2
0.00.105.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.546.500 I main: llama threadpool init, n_threads = 4
0.01.546.563 I 
0.01.546.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.546.619 I 
0.01.547.231 I sampler seed: 1234
0.01.547.238 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.547.288 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.547.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.547.290 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.651.227 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.02.651.227 I llama_perf_context_print:        load time =    1536.53 ms
0.02.651.228 I llama_perf_context_print: prompt eval time =      50.00 ms /     7 tokens (    7.14 ms per token,   139.99 tokens per second)
0.02.651.229 I llama_perf_context_print:        eval time =    1050.96 ms /    63 runs   (   16.68 ms per token,    59.95 tokens per second)
0.02.651.232 I llama_perf_context_print:       total time =    1104.73 ms /    70 tokens
0.02.651.465 I ggml_metal_free: deallocating

real	0m2.677s
user	0m0.123s
sys	0m0.282s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.267 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.236 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.087 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.102 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.103 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.104 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.107 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.799 I llama_model_loader: - type  f32:  194 tensors
0.00.026.800 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.780 I llm_load_vocab: special tokens cache size = 25
0.00.053.850 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.854 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.855 I llm_load_print_meta: arch             = gptneox
0.00.053.855 I llm_load_print_meta: vocab type       = BPE
0.00.053.855 I llm_load_print_meta: n_vocab          = 50304
0.00.053.855 I llm_load_print_meta: n_merges         = 50009
0.00.053.856 I llm_load_print_meta: vocab_only       = 0
0.00.053.856 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.856 I llm_load_print_meta: n_embd           = 2048
0.00.053.856 I llm_load_print_meta: n_layer          = 24
0.00.053.860 I llm_load_print_meta: n_head           = 16
0.00.053.861 I llm_load_print_meta: n_head_kv        = 16
0.00.053.861 I llm_load_print_meta: n_rot            = 32
0.00.053.861 I llm_load_print_meta: n_swa            = 0
0.00.053.861 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.862 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.862 I llm_load_print_meta: n_gqa            = 1
0.00.053.863 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.864 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.864 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.864 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.864 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.865 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.865 I llm_load_print_meta: n_ff             = 8192
0.00.053.865 I llm_load_print_meta: n_expert         = 0
0.00.053.866 I llm_load_print_meta: n_expert_used    = 0
0.00.053.866 I llm_load_print_meta: causal attn      = 1
0.00.053.866 I llm_load_print_meta: pooling type     = 0
0.00.053.866 I llm_load_print_meta: rope type        = 2
0.00.053.866 I llm_load_print_meta: rope scaling     = linear
0.00.053.866 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.868 I llm_load_print_meta: freq_scale_train = 1
0.00.053.870 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.870 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.870 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.871 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.871 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.871 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.871 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.871 I llm_load_print_meta: model type       = 1.4B
0.00.053.872 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.872 I llm_load_print_meta: model params     = 1.41 B
0.00.053.872 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.872 I llm_load_print_meta: general.name     = 1.4B
0.00.053.874 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.874 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.874 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.874 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.875 I llm_load_print_meta: LF token         = 128 ''
0.00.053.875 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.875 I llm_load_print_meta: max token length = 1024
0.00.056.081 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.081 I llm_load_tensors: offloading output layer to GPU
0.00.056.081 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.092 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.094 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.528 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.529 I llama_new_context_with_model: n_ctx         = 128
0.00.056.529 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.529 I llama_new_context_with_model: n_batch       = 128
0.00.056.530 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.530 I llama_new_context_with_model: flash_attn    = 0
0.00.056.530 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.531 I llama_new_context_with_model: freq_scale    = 1
0.00.056.531 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.532 I ggml_metal_init: allocating
0.00.056.536 I ggml_metal_init: found device: Apple M4
0.00.056.540 I ggml_metal_init: picking default device: Apple M4
0.00.057.137 I ggml_metal_init: using embedded metal library
0.00.059.785 I ggml_metal_init: GPU name:   Apple M4
0.00.059.786 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.787 I ggml_metal_init: simdgroup reduction   = true
0.00.059.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.788 I ggml_metal_init: has bfloat            = true
0.00.059.788 I ggml_metal_init: use bfloat            = true
0.00.059.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.990 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.523 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.527 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.542 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.549 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.550 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.550 I llama_new_context_with_model: graph nodes  = 967
0.00.072.550 I llama_new_context_with_model: graph splits = 2
0.00.072.552 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.411 I 
0.00.928.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.447 I perplexity: tokenizing the input ..
0.00.936.414 I perplexity: tokenization took 7.965 ms
0.00.936.417 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.060.218 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.061.458 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.061.481 I llama_perf_context_print:        load time =     918.17 ms
0.01.061.482 I llama_perf_context_print: prompt eval time =     123.56 ms /   128 tokens (    0.97 ms per token,  1035.92 tokens per second)
0.01.061.483 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.061.483 I llama_perf_context_print:       total time =     133.07 ms /   129 tokens
0.01.061.914 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.080s
sys	0m0.159s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.063 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.017.778 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.853 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.049.792 I llama_model_loader: - type  f32:  194 tensors
0.00.049.792 I llama_model_loader: - type q4_0:   97 tensors
0.00.049.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.091.824 I llm_load_vocab: special tokens cache size = 25
0.00.100.715 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.719 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.719 I llm_load_print_meta: arch             = gptneox
0.00.100.720 I llm_load_print_meta: vocab type       = BPE
0.00.100.720 I llm_load_print_meta: n_vocab          = 50304
0.00.100.721 I llm_load_print_meta: n_merges         = 50009
0.00.100.721 I llm_load_print_meta: vocab_only       = 0
0.00.100.722 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.722 I llm_load_print_meta: n_embd           = 2048
0.00.100.724 I llm_load_print_meta: n_layer          = 24
0.00.100.727 I llm_load_print_meta: n_head           = 16
0.00.100.729 I llm_load_print_meta: n_head_kv        = 16
0.00.100.729 I llm_load_print_meta: n_rot            = 32
0.00.100.729 I llm_load_print_meta: n_swa            = 0
0.00.100.729 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.730 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.730 I llm_load_print_meta: n_gqa            = 1
0.00.100.731 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.733 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.734 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.738 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.739 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.739 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.740 I llm_load_print_meta: n_ff             = 8192
0.00.100.740 I llm_load_print_meta: n_expert         = 0
0.00.100.740 I llm_load_print_meta: n_expert_used    = 0
0.00.100.740 I llm_load_print_meta: causal attn      = 1
0.00.100.740 I llm_load_print_meta: pooling type     = 0
0.00.100.740 I llm_load_print_meta: rope type        = 2
0.00.100.741 I llm_load_print_meta: rope scaling     = linear
0.00.100.741 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.741 I llm_load_print_meta: freq_scale_train = 1
0.00.100.742 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.742 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.742 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.743 I llm_load_print_meta: model type       = 1.4B
0.00.100.744 I llm_load_print_meta: model ftype      = Q4_0
0.00.100.744 I llm_load_print_meta: model params     = 1.41 B
0.00.100.745 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.100.745 I llm_load_print_meta: general.name     = 1.4B
0.00.100.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.746 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.746 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.746 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.746 I llm_load_print_meta: LF token         = 128 ''
0.00.100.747 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.747 I llm_load_print_meta: max token length = 1024
0.00.103.355 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.356 I llm_load_tensors: offloading output layer to GPU
0.00.103.356 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.368 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.103.369 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.103.823 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.824 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.825 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.825 I llama_new_context_with_model: n_batch       = 2048
0.00.103.825 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.826 I llama_new_context_with_model: flash_attn    = 0
0.00.103.826 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.826 I llama_new_context_with_model: freq_scale    = 1
0.00.103.827 I ggml_metal_init: allocating
0.00.103.831 I ggml_metal_init: found device: Apple M4
0.00.103.833 I ggml_metal_init: picking default device: Apple M4
0.00.104.708 I ggml_metal_init: using embedded metal library
0.00.107.975 I ggml_metal_init: GPU name:   Apple M4
0.00.107.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.979 I ggml_metal_init: simdgroup reduction   = true
0.00.107.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.979 I ggml_metal_init: has bfloat            = true
0.00.107.979 I ggml_metal_init: use bfloat            = true
0.00.107.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.673 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.143.479 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.143.493 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.143.527 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.144.649 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.144.651 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.144.651 I llama_new_context_with_model: graph nodes  = 967
0.00.144.652 I llama_new_context_with_model: graph splits = 2
0.00.144.655 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.144.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.106 I main: llama threadpool init, n_threads = 4
0.00.775.180 I 
0.00.775.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.247 I 
0.00.775.576 I sampler seed: 1234
0.00.775.584 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.630 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.468.850 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.468.850 I llama_perf_context_print:        load time =     757.32 ms
0.01.468.851 I llama_perf_context_print: prompt eval time =      50.17 ms /     7 tokens (    7.17 ms per token,   139.51 tokens per second)
0.01.468.852 I llama_perf_context_print:        eval time =     640.01 ms /    63 runs   (   10.16 ms per token,    98.44 tokens per second)
0.01.468.852 I llama_perf_context_print:       total time =     693.75 ms /    70 tokens
0.01.469.090 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.156s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.356 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.134 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.134 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.988 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.829 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.829 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.830 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.830 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.830 I llama_model_loader: - type  f32:  194 tensors
0.00.026.831 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.772 I llm_load_vocab: special tokens cache size = 25
0.00.053.647 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.650 I llm_load_print_meta: arch             = gptneox
0.00.053.651 I llm_load_print_meta: vocab type       = BPE
0.00.053.651 I llm_load_print_meta: n_vocab          = 50304
0.00.053.651 I llm_load_print_meta: n_merges         = 50009
0.00.053.651 I llm_load_print_meta: vocab_only       = 0
0.00.053.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.652 I llm_load_print_meta: n_embd           = 2048
0.00.053.652 I llm_load_print_meta: n_layer          = 24
0.00.053.654 I llm_load_print_meta: n_head           = 16
0.00.053.655 I llm_load_print_meta: n_head_kv        = 16
0.00.053.655 I llm_load_print_meta: n_rot            = 32
0.00.053.655 I llm_load_print_meta: n_swa            = 0
0.00.053.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.656 I llm_load_print_meta: n_gqa            = 1
0.00.053.657 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.658 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.660 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.661 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.661 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.661 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.662 I llm_load_print_meta: n_ff             = 8192
0.00.053.662 I llm_load_print_meta: n_expert         = 0
0.00.053.662 I llm_load_print_meta: n_expert_used    = 0
0.00.053.662 I llm_load_print_meta: causal attn      = 1
0.00.053.662 I llm_load_print_meta: pooling type     = 0
0.00.053.662 I llm_load_print_meta: rope type        = 2
0.00.053.663 I llm_load_print_meta: rope scaling     = linear
0.00.053.663 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.663 I llm_load_print_meta: freq_scale_train = 1
0.00.053.664 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.664 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.664 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.664 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.664 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.666 I llm_load_print_meta: model type       = 1.4B
0.00.053.667 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.667 I llm_load_print_meta: model params     = 1.41 B
0.00.053.668 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.668 I llm_load_print_meta: general.name     = 1.4B
0.00.053.668 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.668 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.668 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.668 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.669 I llm_load_print_meta: LF token         = 128 ''
0.00.053.670 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.670 I llm_load_print_meta: max token length = 1024
0.00.055.408 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.408 I llm_load_tensors: offloading output layer to GPU
0.00.055.409 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.414 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.414 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.737 I llama_new_context_with_model: n_ctx         = 128
0.00.055.738 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.738 I llama_new_context_with_model: n_batch       = 128
0.00.055.738 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.738 I llama_new_context_with_model: flash_attn    = 0
0.00.055.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.739 I llama_new_context_with_model: freq_scale    = 1
0.00.055.739 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.740 I ggml_metal_init: allocating
0.00.055.743 I ggml_metal_init: found device: Apple M4
0.00.055.744 I ggml_metal_init: picking default device: Apple M4
0.00.056.309 I ggml_metal_init: using embedded metal library
0.00.058.641 I ggml_metal_init: GPU name:   Apple M4
0.00.058.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.643 I ggml_metal_init: simdgroup reduction   = true
0.00.058.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.643 I ggml_metal_init: has bfloat            = true
0.00.058.643 I ggml_metal_init: use bfloat            = true
0.00.058.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.314 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.679 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.694 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.543 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.544 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.544 I llama_new_context_with_model: graph nodes  = 967
0.00.070.544 I llama_new_context_with_model: graph splits = 2
0.00.070.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.091 I 
0.00.631.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.146 I perplexity: tokenizing the input ..
0.00.639.119 I perplexity: tokenization took 7.972 ms
0.00.639.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.526 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.762.694 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.762.723 I llama_perf_context_print:        load time =     620.73 ms
0.00.762.726 I llama_perf_context_print: prompt eval time =     122.18 ms /   128 tokens (    0.95 ms per token,  1047.64 tokens per second)
0.00.762.728 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.729 I llama_perf_context_print:       total time =     131.64 ms /   129 tokens
0.00.763.246 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.080s
sys	0m0.103s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.795 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.493 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.494 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.496 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.498 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.982 I llama_model_loader: - type  f32:  194 tensors
0.00.024.982 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.982 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.233 I llm_load_vocab: special tokens cache size = 25
0.00.051.100 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.102 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.103 I llm_load_print_meta: arch             = gptneox
0.00.051.103 I llm_load_print_meta: vocab type       = BPE
0.00.051.103 I llm_load_print_meta: n_vocab          = 50304
0.00.051.103 I llm_load_print_meta: n_merges         = 50009
0.00.051.104 I llm_load_print_meta: vocab_only       = 0
0.00.051.104 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.104 I llm_load_print_meta: n_embd           = 2048
0.00.051.104 I llm_load_print_meta: n_layer          = 24
0.00.051.107 I llm_load_print_meta: n_head           = 16
0.00.051.108 I llm_load_print_meta: n_head_kv        = 16
0.00.051.108 I llm_load_print_meta: n_rot            = 32
0.00.051.108 I llm_load_print_meta: n_swa            = 0
0.00.051.108 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.108 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.109 I llm_load_print_meta: n_gqa            = 1
0.00.051.110 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.111 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.112 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.112 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.112 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.112 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.114 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.115 I llm_load_print_meta: n_ff             = 8192
0.00.051.115 I llm_load_print_meta: n_expert         = 0
0.00.051.115 I llm_load_print_meta: n_expert_used    = 0
0.00.051.115 I llm_load_print_meta: causal attn      = 1
0.00.051.116 I llm_load_print_meta: pooling type     = 0
0.00.051.116 I llm_load_print_meta: rope type        = 2
0.00.051.116 I llm_load_print_meta: rope scaling     = linear
0.00.051.116 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.117 I llm_load_print_meta: freq_scale_train = 1
0.00.051.117 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.117 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.118 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.118 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.118 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.118 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.119 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.119 I llm_load_print_meta: model type       = 1.4B
0.00.051.120 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.121 I llm_load_print_meta: model params     = 1.41 B
0.00.051.122 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.122 I llm_load_print_meta: general.name     = 1.4B
0.00.051.122 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.122 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.123 I llm_load_print_meta: LF token         = 128 ''
0.00.051.123 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.123 I llm_load_print_meta: max token length = 1024
0.00.053.110 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.110 I llm_load_tensors: offloading output layer to GPU
0.00.053.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.121 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.123 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.471 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.471 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.471 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.472 I llama_new_context_with_model: n_batch       = 2048
0.00.053.472 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.472 I llama_new_context_with_model: flash_attn    = 0
0.00.053.472 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.473 I llama_new_context_with_model: freq_scale    = 1
0.00.053.473 I ggml_metal_init: allocating
0.00.053.476 I ggml_metal_init: found device: Apple M4
0.00.053.478 I ggml_metal_init: picking default device: Apple M4
0.00.054.061 I ggml_metal_init: using embedded metal library
0.00.056.380 I ggml_metal_init: GPU name:   Apple M4
0.00.056.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.382 I ggml_metal_init: simdgroup reduction   = true
0.00.056.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.382 I ggml_metal_init: has bfloat            = true
0.00.056.383 I ggml_metal_init: use bfloat            = true
0.00.056.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.039 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.839 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.865 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.981 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.982 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.983 I llama_new_context_with_model: graph nodes  = 967
0.00.088.983 I llama_new_context_with_model: graph splits = 2
0.00.088.985 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.133 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.134 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.330 I main: llama threadpool init, n_threads = 4
0.00.717.366 I 
0.00.717.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.389 I 
0.00.717.627 I sampler seed: 1234
0.00.717.632 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.658 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.658 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.451.515 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.451.516 I llama_perf_context_print:        load time =     708.53 ms
0.01.451.517 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.59 tokens per second)
0.01.451.518 I llama_perf_context_print:        eval time =     687.59 ms /    63 runs   (   10.91 ms per token,    91.63 tokens per second)
0.01.451.518 I llama_perf_context_print:       total time =     734.19 ms /    70 tokens
0.01.451.739 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.763 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.304 I llama_model_loader: - type  f32:  194 tensors
0.00.024.305 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.434 I llm_load_vocab: special tokens cache size = 25
0.00.050.313 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.316 I llm_load_print_meta: arch             = gptneox
0.00.050.317 I llm_load_print_meta: vocab type       = BPE
0.00.050.317 I llm_load_print_meta: n_vocab          = 50304
0.00.050.317 I llm_load_print_meta: n_merges         = 50009
0.00.050.317 I llm_load_print_meta: vocab_only       = 0
0.00.050.317 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.318 I llm_load_print_meta: n_embd           = 2048
0.00.050.318 I llm_load_print_meta: n_layer          = 24
0.00.050.320 I llm_load_print_meta: n_head           = 16
0.00.050.321 I llm_load_print_meta: n_head_kv        = 16
0.00.050.321 I llm_load_print_meta: n_rot            = 32
0.00.050.321 I llm_load_print_meta: n_swa            = 0
0.00.050.322 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.322 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.323 I llm_load_print_meta: n_gqa            = 1
0.00.050.323 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.324 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.324 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.326 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.327 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.328 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.328 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.328 I llm_load_print_meta: n_ff             = 8192
0.00.050.329 I llm_load_print_meta: n_expert         = 0
0.00.050.329 I llm_load_print_meta: n_expert_used    = 0
0.00.050.329 I llm_load_print_meta: causal attn      = 1
0.00.050.329 I llm_load_print_meta: pooling type     = 0
0.00.050.330 I llm_load_print_meta: rope type        = 2
0.00.050.330 I llm_load_print_meta: rope scaling     = linear
0.00.050.331 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.331 I llm_load_print_meta: freq_scale_train = 1
0.00.050.331 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.331 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.332 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.332 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.332 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.332 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.332 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.334 I llm_load_print_meta: model type       = 1.4B
0.00.050.334 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.334 I llm_load_print_meta: model params     = 1.41 B
0.00.050.335 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.335 I llm_load_print_meta: general.name     = 1.4B
0.00.050.337 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: LF token         = 128 ''
0.00.050.338 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: max token length = 1024
0.00.052.295 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.295 I llm_load_tensors: offloading output layer to GPU
0.00.052.295 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.306 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.307 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.620 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.621 I llama_new_context_with_model: n_ctx         = 128
0.00.052.621 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.621 I llama_new_context_with_model: n_batch       = 128
0.00.052.621 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.621 I llama_new_context_with_model: flash_attn    = 0
0.00.052.622 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.622 I llama_new_context_with_model: freq_scale    = 1
0.00.052.622 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.623 I ggml_metal_init: allocating
0.00.052.626 I ggml_metal_init: found device: Apple M4
0.00.052.627 I ggml_metal_init: picking default device: Apple M4
0.00.053.190 I ggml_metal_init: using embedded metal library
0.00.055.551 I ggml_metal_init: GPU name:   Apple M4
0.00.055.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.553 I ggml_metal_init: simdgroup reduction   = true
0.00.055.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.554 I ggml_metal_init: has bfloat            = true
0.00.055.554 I ggml_metal_init: use bfloat            = true
0.00.055.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.163 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.688 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.704 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.577 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.578 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.579 I llama_new_context_with_model: graph nodes  = 967
0.00.067.579 I llama_new_context_with_model: graph splits = 2
0.00.067.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.788 I 
0.00.647.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.834 I perplexity: tokenizing the input ..
0.00.655.751 I perplexity: tokenization took 7.916 ms
0.00.655.755 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.501 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.779.744 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.779.765 I llama_perf_context_print:        load time =     638.91 ms
0.00.779.766 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.72 tokens per second)
0.00.779.769 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.769 I llama_perf_context_print:       total time =     131.98 ms /   129 tokens
0.00.780.208 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.078s
sys	0m0.096s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.186 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.754 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.770 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.775 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.390 I llama_model_loader: - type  f32:  194 tensors
0.00.026.391 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.589 I llm_load_vocab: special tokens cache size = 25
0.00.052.594 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.597 I llm_load_print_meta: arch             = gptneox
0.00.052.598 I llm_load_print_meta: vocab type       = BPE
0.00.052.598 I llm_load_print_meta: n_vocab          = 50304
0.00.052.598 I llm_load_print_meta: n_merges         = 50009
0.00.052.598 I llm_load_print_meta: vocab_only       = 0
0.00.052.598 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.599 I llm_load_print_meta: n_embd           = 2048
0.00.052.599 I llm_load_print_meta: n_layer          = 24
0.00.052.601 I llm_load_print_meta: n_head           = 16
0.00.052.602 I llm_load_print_meta: n_head_kv        = 16
0.00.052.602 I llm_load_print_meta: n_rot            = 32
0.00.052.602 I llm_load_print_meta: n_swa            = 0
0.00.052.603 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.603 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.604 I llm_load_print_meta: n_gqa            = 1
0.00.052.604 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.605 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.605 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.606 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.606 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.606 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.606 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.607 I llm_load_print_meta: n_ff             = 8192
0.00.052.607 I llm_load_print_meta: n_expert         = 0
0.00.052.607 I llm_load_print_meta: n_expert_used    = 0
0.00.052.608 I llm_load_print_meta: causal attn      = 1
0.00.052.608 I llm_load_print_meta: pooling type     = 0
0.00.052.610 I llm_load_print_meta: rope type        = 2
0.00.052.610 I llm_load_print_meta: rope scaling     = linear
0.00.052.610 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.611 I llm_load_print_meta: freq_scale_train = 1
0.00.052.611 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.611 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.611 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.612 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.612 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.612 I llm_load_print_meta: model type       = 1.4B
0.00.052.613 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.613 I llm_load_print_meta: model params     = 1.41 B
0.00.052.614 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.614 I llm_load_print_meta: general.name     = 1.4B
0.00.052.614 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.615 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.615 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.615 I llm_load_print_meta: LF token         = 128 ''
0.00.052.615 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.616 I llm_load_print_meta: max token length = 1024
0.00.054.520 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.520 I llm_load_tensors: offloading output layer to GPU
0.00.054.521 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.532 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.533 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.870 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.870 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.871 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.871 I llama_new_context_with_model: n_batch       = 2048
0.00.054.871 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.871 I llama_new_context_with_model: flash_attn    = 0
0.00.054.872 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.872 I llama_new_context_with_model: freq_scale    = 1
0.00.054.872 I ggml_metal_init: allocating
0.00.054.879 I ggml_metal_init: found device: Apple M4
0.00.054.881 I ggml_metal_init: picking default device: Apple M4
0.00.055.488 I ggml_metal_init: using embedded metal library
0.00.057.817 I ggml_metal_init: GPU name:   Apple M4
0.00.057.819 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.819 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.820 I ggml_metal_init: simdgroup reduction   = true
0.00.057.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.820 I ggml_metal_init: has bfloat            = true
0.00.057.820 I ggml_metal_init: use bfloat            = true
0.00.057.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.319 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.001 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.022 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.104 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.106 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.106 I llama_new_context_with_model: graph nodes  = 967
0.00.088.106 I llama_new_context_with_model: graph splits = 2
0.00.088.109 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.250 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.495 I main: llama threadpool init, n_threads = 4
0.00.796.540 I 
0.00.796.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.570 I 
0.00.796.801 I sampler seed: 1234
0.00.796.806 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.841 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.843 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.578.747 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.578.748 I llama_perf_context_print:        load time =     786.30 ms
0.01.578.749 I llama_perf_context_print: prompt eval time =      46.07 ms /     7 tokens (    6.58 ms per token,   151.94 tokens per second)
0.01.578.749 I llama_perf_context_print:        eval time =     732.98 ms /    63 runs   (   11.63 ms per token,    85.95 tokens per second)
0.01.578.749 I llama_perf_context_print:       total time =     782.26 ms /    70 tokens
0.01.578.955 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.363 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.600 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.611 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.612 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.612 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.046 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.046 I llama_model_loader: - type  f32:  194 tensors
0.00.026.047 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.047 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.102 I llm_load_vocab: special tokens cache size = 25
0.00.053.170 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.173 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.173 I llm_load_print_meta: arch             = gptneox
0.00.053.174 I llm_load_print_meta: vocab type       = BPE
0.00.053.174 I llm_load_print_meta: n_vocab          = 50304
0.00.053.174 I llm_load_print_meta: n_merges         = 50009
0.00.053.174 I llm_load_print_meta: vocab_only       = 0
0.00.053.174 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.175 I llm_load_print_meta: n_embd           = 2048
0.00.053.175 I llm_load_print_meta: n_layer          = 24
0.00.053.178 I llm_load_print_meta: n_head           = 16
0.00.053.179 I llm_load_print_meta: n_head_kv        = 16
0.00.053.180 I llm_load_print_meta: n_rot            = 32
0.00.053.180 I llm_load_print_meta: n_swa            = 0
0.00.053.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.180 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.181 I llm_load_print_meta: n_gqa            = 1
0.00.053.182 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.183 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.191 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.192 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.193 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.193 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.199 I llm_load_print_meta: n_ff             = 8192
0.00.053.200 I llm_load_print_meta: n_expert         = 0
0.00.053.200 I llm_load_print_meta: n_expert_used    = 0
0.00.053.200 I llm_load_print_meta: causal attn      = 1
0.00.053.201 I llm_load_print_meta: pooling type     = 0
0.00.053.201 I llm_load_print_meta: rope type        = 2
0.00.053.201 I llm_load_print_meta: rope scaling     = linear
0.00.053.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.202 I llm_load_print_meta: freq_scale_train = 1
0.00.053.202 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.202 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.202 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.202 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.204 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.204 I llm_load_print_meta: model type       = 1.4B
0.00.053.204 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.205 I llm_load_print_meta: model params     = 1.41 B
0.00.053.205 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.205 I llm_load_print_meta: general.name     = 1.4B
0.00.053.206 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.207 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.207 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.207 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: LF token         = 128 ''
0.00.053.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: max token length = 1024
0.00.055.195 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.196 I llm_load_tensors: offloading output layer to GPU
0.00.055.196 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.206 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.208 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.618 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.618 I llama_new_context_with_model: n_ctx         = 128
0.00.055.618 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.619 I llama_new_context_with_model: n_batch       = 128
0.00.055.619 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.619 I llama_new_context_with_model: flash_attn    = 0
0.00.055.619 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.619 I llama_new_context_with_model: freq_scale    = 1
0.00.055.620 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.620 I ggml_metal_init: allocating
0.00.055.624 I ggml_metal_init: found device: Apple M4
0.00.055.626 I ggml_metal_init: picking default device: Apple M4
0.00.056.189 I ggml_metal_init: using embedded metal library
0.00.058.528 I ggml_metal_init: GPU name:   Apple M4
0.00.058.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.530 I ggml_metal_init: simdgroup reduction   = true
0.00.058.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.531 I ggml_metal_init: has bfloat            = true
0.00.058.531 I ggml_metal_init: use bfloat            = true
0.00.058.531 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.476 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.478 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.410 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.411 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.411 I llama_new_context_with_model: graph nodes  = 967
0.00.070.411 I llama_new_context_with_model: graph splits = 2
0.00.070.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.230 I 
0.00.710.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.306 I perplexity: tokenizing the input ..
0.00.718.361 I perplexity: tokenization took 8.053 ms
0.00.718.367 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.620 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.834 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.861 I llama_perf_context_print:        load time =     699.86 ms
0.00.854.862 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   947.98 tokens per second)
0.00.854.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.863 I llama_perf_context_print:       total time =     144.63 ms /   129 tokens
0.00.855.387 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.080s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.910 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.439 I llama_model_loader: - type  f32:  194 tensors
0.00.025.439 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.440 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.430 I llm_load_vocab: special tokens cache size = 25
0.00.052.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.501 I llm_load_print_meta: arch             = gptneox
0.00.052.501 I llm_load_print_meta: vocab type       = BPE
0.00.052.501 I llm_load_print_meta: n_vocab          = 50304
0.00.052.502 I llm_load_print_meta: n_merges         = 50009
0.00.052.502 I llm_load_print_meta: vocab_only       = 0
0.00.052.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.502 I llm_load_print_meta: n_embd           = 2048
0.00.052.502 I llm_load_print_meta: n_layer          = 24
0.00.052.505 I llm_load_print_meta: n_head           = 16
0.00.052.507 I llm_load_print_meta: n_head_kv        = 16
0.00.052.507 I llm_load_print_meta: n_rot            = 32
0.00.052.507 I llm_load_print_meta: n_swa            = 0
0.00.052.507 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.507 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.508 I llm_load_print_meta: n_gqa            = 1
0.00.052.509 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.510 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.510 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.511 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.511 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.512 I llm_load_print_meta: n_ff             = 8192
0.00.052.512 I llm_load_print_meta: n_expert         = 0
0.00.052.512 I llm_load_print_meta: n_expert_used    = 0
0.00.052.513 I llm_load_print_meta: causal attn      = 1
0.00.052.515 I llm_load_print_meta: pooling type     = 0
0.00.052.515 I llm_load_print_meta: rope type        = 2
0.00.052.515 I llm_load_print_meta: rope scaling     = linear
0.00.052.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.516 I llm_load_print_meta: freq_scale_train = 1
0.00.052.516 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.517 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.517 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.517 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.517 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.517 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.517 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.518 I llm_load_print_meta: model type       = 1.4B
0.00.052.519 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.519 I llm_load_print_meta: model params     = 1.41 B
0.00.052.520 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.520 I llm_load_print_meta: general.name     = 1.4B
0.00.052.520 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.521 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.521 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.521 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.521 I llm_load_print_meta: LF token         = 128 ''
0.00.052.522 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.522 I llm_load_print_meta: max token length = 1024
0.00.054.289 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.289 I llm_load_tensors: offloading output layer to GPU
0.00.054.289 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.295 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.295 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.809 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.809 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.810 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.810 I llama_new_context_with_model: n_batch       = 2048
0.00.054.810 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.810 I llama_new_context_with_model: flash_attn    = 0
0.00.054.810 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.811 I llama_new_context_with_model: freq_scale    = 1
0.00.054.811 I ggml_metal_init: allocating
0.00.054.819 I ggml_metal_init: found device: Apple M4
0.00.054.821 I ggml_metal_init: picking default device: Apple M4
0.00.055.395 I ggml_metal_init: using embedded metal library
0.00.057.692 I ggml_metal_init: GPU name:   Apple M4
0.00.057.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.694 I ggml_metal_init: simdgroup reduction   = true
0.00.057.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.696 I ggml_metal_init: has bfloat            = true
0.00.057.696 I ggml_metal_init: use bfloat            = true
0.00.057.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.068 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.241 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.259 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.381 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.382 I llama_new_context_with_model: graph nodes  = 967
0.00.088.382 I llama_new_context_with_model: graph splits = 2
0.00.088.388 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.682 I main: llama threadpool init, n_threads = 4
0.00.705.718 I 
0.00.705.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.737 I 
0.00.705.985 I sampler seed: 1234
0.00.705.989 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.004 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.006 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.006 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.544.166 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.544.166 I llama_perf_context_print:        load time =     696.77 ms
0.01.544.167 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.69 tokens per second)
0.01.544.168 I llama_perf_context_print:        eval time =     792.98 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.544.168 I llama_perf_context_print:       total time =     838.49 ms /    70 tokens
0.01.544.456 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.112s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.856 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.347 I llama_model_loader: - type  f32:  194 tensors
0.00.024.347 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.167 I llm_load_vocab: special tokens cache size = 25
0.00.051.177 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.180 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.180 I llm_load_print_meta: arch             = gptneox
0.00.051.181 I llm_load_print_meta: vocab type       = BPE
0.00.051.181 I llm_load_print_meta: n_vocab          = 50304
0.00.051.181 I llm_load_print_meta: n_merges         = 50009
0.00.051.181 I llm_load_print_meta: vocab_only       = 0
0.00.051.181 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.182 I llm_load_print_meta: n_embd           = 2048
0.00.051.182 I llm_load_print_meta: n_layer          = 24
0.00.051.184 I llm_load_print_meta: n_head           = 16
0.00.051.185 I llm_load_print_meta: n_head_kv        = 16
0.00.051.185 I llm_load_print_meta: n_rot            = 32
0.00.051.185 I llm_load_print_meta: n_swa            = 0
0.00.051.185 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.186 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.186 I llm_load_print_meta: n_gqa            = 1
0.00.051.187 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.188 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.188 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.189 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.190 I llm_load_print_meta: n_ff             = 8192
0.00.051.190 I llm_load_print_meta: n_expert         = 0
0.00.051.190 I llm_load_print_meta: n_expert_used    = 0
0.00.051.191 I llm_load_print_meta: causal attn      = 1
0.00.051.191 I llm_load_print_meta: pooling type     = 0
0.00.051.191 I llm_load_print_meta: rope type        = 2
0.00.051.192 I llm_load_print_meta: rope scaling     = linear
0.00.051.192 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.193 I llm_load_print_meta: freq_scale_train = 1
0.00.051.193 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.193 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.193 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.195 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.195 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.196 I llm_load_print_meta: model type       = 1.4B
0.00.051.196 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.196 I llm_load_print_meta: model params     = 1.41 B
0.00.051.197 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.197 I llm_load_print_meta: general.name     = 1.4B
0.00.051.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.203 I llm_load_print_meta: LF token         = 128 ''
0.00.051.203 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.203 I llm_load_print_meta: max token length = 1024
0.00.053.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.198 I llm_load_tensors: offloading output layer to GPU
0.00.053.198 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.209 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.211 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.546 I llama_new_context_with_model: n_ctx         = 128
0.00.053.547 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.547 I llama_new_context_with_model: n_batch       = 128
0.00.053.547 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.547 I llama_new_context_with_model: flash_attn    = 0
0.00.053.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.548 I llama_new_context_with_model: freq_scale    = 1
0.00.053.548 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.548 I ggml_metal_init: allocating
0.00.053.552 I ggml_metal_init: found device: Apple M4
0.00.053.554 I ggml_metal_init: picking default device: Apple M4
0.00.054.152 I ggml_metal_init: using embedded metal library
0.00.056.530 I ggml_metal_init: GPU name:   Apple M4
0.00.056.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.533 I ggml_metal_init: simdgroup reduction   = true
0.00.056.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.533 I ggml_metal_init: has bfloat            = true
0.00.056.533 I ggml_metal_init: use bfloat            = true
0.00.056.534 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.369 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.637 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.640 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.656 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.613 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.614 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.614 I llama_new_context_with_model: graph nodes  = 967
0.00.068.615 I llama_new_context_with_model: graph splits = 2
0.00.068.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.621 I 
0.00.634.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.667 I perplexity: tokenizing the input ..
0.00.642.894 I perplexity: tokenization took 8.224 ms
0.00.642.898 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.937 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.779.247 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.779.279 I llama_perf_context_print:        load time =     625.76 ms
0.00.779.280 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.51 tokens per second)
0.00.779.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.281 I llama_perf_context_print:       total time =     144.66 ms /   129 tokens
0.00.779.805 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.079s
sys	0m0.111s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.682 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.386 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.061 I llama_model_loader: - type  f32:  194 tensors
0.00.026.061 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.061 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.941 I llm_load_vocab: special tokens cache size = 25
0.00.052.965 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.967 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.968 I llm_load_print_meta: arch             = gptneox
0.00.052.968 I llm_load_print_meta: vocab type       = BPE
0.00.052.968 I llm_load_print_meta: n_vocab          = 50304
0.00.052.969 I llm_load_print_meta: n_merges         = 50009
0.00.052.969 I llm_load_print_meta: vocab_only       = 0
0.00.052.969 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.969 I llm_load_print_meta: n_embd           = 2048
0.00.052.969 I llm_load_print_meta: n_layer          = 24
0.00.052.972 I llm_load_print_meta: n_head           = 16
0.00.052.973 I llm_load_print_meta: n_head_kv        = 16
0.00.052.975 I llm_load_print_meta: n_rot            = 32
0.00.052.975 I llm_load_print_meta: n_swa            = 0
0.00.052.976 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.976 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.976 I llm_load_print_meta: n_gqa            = 1
0.00.052.977 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.982 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.983 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.983 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.984 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.985 I llm_load_print_meta: n_ff             = 8192
0.00.052.985 I llm_load_print_meta: n_expert         = 0
0.00.052.985 I llm_load_print_meta: n_expert_used    = 0
0.00.052.985 I llm_load_print_meta: causal attn      = 1
0.00.052.986 I llm_load_print_meta: pooling type     = 0
0.00.052.986 I llm_load_print_meta: rope type        = 2
0.00.052.986 I llm_load_print_meta: rope scaling     = linear
0.00.052.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.988 I llm_load_print_meta: freq_scale_train = 1
0.00.052.988 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.989 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.989 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.989 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.989 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.989 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.989 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.990 I llm_load_print_meta: model type       = 1.4B
0.00.052.990 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.991 I llm_load_print_meta: model params     = 1.41 B
0.00.052.991 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.991 I llm_load_print_meta: general.name     = 1.4B
0.00.052.991 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.991 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.992 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.992 I llm_load_print_meta: LF token         = 128 ''
0.00.052.992 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.992 I llm_load_print_meta: max token length = 1024
0.00.054.880 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.880 I llm_load_tensors: offloading output layer to GPU
0.00.054.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.891 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.892 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.225 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.225 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.225 I llama_new_context_with_model: n_batch       = 2048
0.00.055.225 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.225 I llama_new_context_with_model: flash_attn    = 0
0.00.055.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.226 I llama_new_context_with_model: freq_scale    = 1
0.00.055.227 I ggml_metal_init: allocating
0.00.055.232 I ggml_metal_init: found device: Apple M4
0.00.055.234 I ggml_metal_init: picking default device: Apple M4
0.00.055.819 I ggml_metal_init: using embedded metal library
0.00.058.159 I ggml_metal_init: GPU name:   Apple M4
0.00.058.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.161 I ggml_metal_init: simdgroup reduction   = true
0.00.058.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.162 I ggml_metal_init: has bfloat            = true
0.00.058.162 I ggml_metal_init: use bfloat            = true
0.00.058.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.819 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.235 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.242 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.270 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.362 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.363 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.363 I llama_new_context_with_model: graph nodes  = 967
0.00.088.364 I llama_new_context_with_model: graph splits = 2
0.00.088.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.639 I main: llama threadpool init, n_threads = 4
0.00.437.683 I 
0.00.437.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.713 I 
0.00.437.953 I sampler seed: 1234
0.00.437.962 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.980 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.980 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.981 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.114.292 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.114.292 I llama_perf_context_print:        load time =     426.95 ms
0.01.114.293 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.63 tokens per second)
0.01.114.294 I llama_perf_context_print:        eval time =     637.85 ms /    63 runs   (   10.12 ms per token,    98.77 tokens per second)
0.01.114.294 I llama_perf_context_print:       total time =     676.66 ms /    70 tokens
0.01.114.569 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.111s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.078 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.312 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.313 I llama_model_loader: - type  f32:  194 tensors
0.00.025.313 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.313 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.314 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.387 I llm_load_vocab: special tokens cache size = 25
0.00.051.335 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.338 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.338 I llm_load_print_meta: arch             = gptneox
0.00.051.338 I llm_load_print_meta: vocab type       = BPE
0.00.051.339 I llm_load_print_meta: n_vocab          = 50304
0.00.051.339 I llm_load_print_meta: n_merges         = 50009
0.00.051.339 I llm_load_print_meta: vocab_only       = 0
0.00.051.339 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.339 I llm_load_print_meta: n_embd           = 2048
0.00.051.340 I llm_load_print_meta: n_layer          = 24
0.00.051.342 I llm_load_print_meta: n_head           = 16
0.00.051.343 I llm_load_print_meta: n_head_kv        = 16
0.00.051.343 I llm_load_print_meta: n_rot            = 32
0.00.051.343 I llm_load_print_meta: n_swa            = 0
0.00.051.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.347 I llm_load_print_meta: n_gqa            = 1
0.00.051.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.350 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.351 I llm_load_print_meta: n_ff             = 8192
0.00.051.352 I llm_load_print_meta: n_expert         = 0
0.00.051.352 I llm_load_print_meta: n_expert_used    = 0
0.00.051.352 I llm_load_print_meta: causal attn      = 1
0.00.051.352 I llm_load_print_meta: pooling type     = 0
0.00.051.352 I llm_load_print_meta: rope type        = 2
0.00.051.352 I llm_load_print_meta: rope scaling     = linear
0.00.051.353 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.353 I llm_load_print_meta: freq_scale_train = 1
0.00.051.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.354 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.356 I llm_load_print_meta: model type       = 1.4B
0.00.051.357 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.357 I llm_load_print_meta: model params     = 1.41 B
0.00.051.359 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.359 I llm_load_print_meta: general.name     = 1.4B
0.00.051.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: LF token         = 128 ''
0.00.051.364 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.364 I llm_load_print_meta: max token length = 1024
0.00.053.253 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.253 I llm_load_tensors: offloading output layer to GPU
0.00.053.253 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.264 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.265 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.593 I llama_new_context_with_model: n_ctx         = 128
0.00.053.594 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.594 I llama_new_context_with_model: n_batch       = 128
0.00.053.594 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.594 I llama_new_context_with_model: flash_attn    = 0
0.00.053.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.594 I llama_new_context_with_model: freq_scale    = 1
0.00.053.595 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.595 I ggml_metal_init: allocating
0.00.053.598 I ggml_metal_init: found device: Apple M4
0.00.053.600 I ggml_metal_init: picking default device: Apple M4
0.00.054.157 I ggml_metal_init: using embedded metal library
0.00.056.483 I ggml_metal_init: GPU name:   Apple M4
0.00.056.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.485 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.485 I ggml_metal_init: simdgroup reduction   = true
0.00.056.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.486 I ggml_metal_init: has bfloat            = true
0.00.056.486 I ggml_metal_init: use bfloat            = true
0.00.056.486 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.287 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.289 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.303 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.230 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.231 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.232 I llama_new_context_with_model: graph nodes  = 967
0.00.068.232 I llama_new_context_with_model: graph splits = 2
0.00.068.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.370.488 I 
0.00.370.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.370.548 I perplexity: tokenizing the input ..
0.00.378.232 I perplexity: tokenization took 7.682 ms
0.00.378.236 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.510.603 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.511.761 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.511.787 I llama_perf_context_print:        load time =     360.41 ms
0.00.511.788 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.69 tokens per second)
0.00.511.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.511.791 I llama_perf_context_print:       total time =     141.30 ms /   129 tokens
0.00.512.295 I ggml_metal_free: deallocating

real	0m0.527s
user	0m0.079s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.040 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.568 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.446 I llama_model_loader: - type  f32:  194 tensors
0.00.025.447 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.447 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.447 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.447 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.346 I llm_load_vocab: special tokens cache size = 25
0.00.053.231 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.235 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.236 I llm_load_print_meta: arch             = gptneox
0.00.053.236 I llm_load_print_meta: vocab type       = BPE
0.00.053.236 I llm_load_print_meta: n_vocab          = 50304
0.00.053.237 I llm_load_print_meta: n_merges         = 50009
0.00.053.237 I llm_load_print_meta: vocab_only       = 0
0.00.053.239 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.239 I llm_load_print_meta: n_embd           = 2048
0.00.053.239 I llm_load_print_meta: n_layer          = 24
0.00.053.243 I llm_load_print_meta: n_head           = 16
0.00.053.244 I llm_load_print_meta: n_head_kv        = 16
0.00.053.244 I llm_load_print_meta: n_rot            = 32
0.00.053.244 I llm_load_print_meta: n_swa            = 0
0.00.053.244 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.244 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.245 I llm_load_print_meta: n_gqa            = 1
0.00.053.246 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.246 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.247 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.247 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.248 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.248 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.248 I llm_load_print_meta: n_ff             = 8192
0.00.053.248 I llm_load_print_meta: n_expert         = 0
0.00.053.249 I llm_load_print_meta: n_expert_used    = 0
0.00.053.249 I llm_load_print_meta: causal attn      = 1
0.00.053.249 I llm_load_print_meta: pooling type     = 0
0.00.053.249 I llm_load_print_meta: rope type        = 2
0.00.053.249 I llm_load_print_meta: rope scaling     = linear
0.00.053.251 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.251 I llm_load_print_meta: freq_scale_train = 1
0.00.053.251 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.252 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.252 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.252 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.252 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.252 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.252 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.252 I llm_load_print_meta: model type       = 1.4B
0.00.053.253 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.253 I llm_load_print_meta: model params     = 1.41 B
0.00.053.254 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.254 I llm_load_print_meta: general.name     = 1.4B
0.00.053.254 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.254 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.254 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.255 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.255 I llm_load_print_meta: LF token         = 128 ''
0.00.053.255 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.255 I llm_load_print_meta: max token length = 1024
0.00.055.286 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.286 I llm_load_tensors: offloading output layer to GPU
0.00.055.287 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.297 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.299 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.616 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.617 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.617 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.617 I llama_new_context_with_model: n_batch       = 2048
0.00.055.617 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.618 I llama_new_context_with_model: flash_attn    = 0
0.00.055.618 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.618 I llama_new_context_with_model: freq_scale    = 1
0.00.055.619 I ggml_metal_init: allocating
0.00.055.622 I ggml_metal_init: found device: Apple M4
0.00.055.623 I ggml_metal_init: picking default device: Apple M4
0.00.056.258 I ggml_metal_init: using embedded metal library
0.00.059.730 I ggml_metal_init: GPU name:   Apple M4
0.00.059.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.733 I ggml_metal_init: simdgroup reduction   = true
0.00.059.734 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.734 I ggml_metal_init: has bfloat            = true
0.00.059.734 I ggml_metal_init: use bfloat            = true
0.00.059.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.929 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.822 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.835 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.859 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.834 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.836 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.836 I llama_new_context_with_model: graph nodes  = 967
0.00.089.836 I llama_new_context_with_model: graph splits = 2
0.00.089.839 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.161 I main: llama threadpool init, n_threads = 4
0.00.518.212 I 
0.00.518.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.238 I 
0.00.518.461 I sampler seed: 1234
0.00.518.467 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.518.515 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.518.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.518.518 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.259.815 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.259.816 I llama_perf_context_print:        load time =     509.11 ms
0.01.259.817 I llama_perf_context_print: prompt eval time =      40.54 ms /     7 tokens (    5.79 ms per token,   172.67 tokens per second)
0.01.259.817 I llama_perf_context_print:        eval time =     698.21 ms /    63 runs   (   11.08 ms per token,    90.23 tokens per second)
0.01.259.818 I llama_perf_context_print:       total time =     741.66 ms /    70 tokens
0.01.260.095 I ggml_metal_free: deallocating

real	0m1.278s
user	0m0.111s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.776 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.237 I llama_model_loader: - type  f32:  194 tensors
0.00.024.237 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.238 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.238 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.238 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.269 I llm_load_vocab: special tokens cache size = 25
0.00.050.222 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.224 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.225 I llm_load_print_meta: arch             = gptneox
0.00.050.225 I llm_load_print_meta: vocab type       = BPE
0.00.050.225 I llm_load_print_meta: n_vocab          = 50304
0.00.050.225 I llm_load_print_meta: n_merges         = 50009
0.00.050.226 I llm_load_print_meta: vocab_only       = 0
0.00.050.226 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.226 I llm_load_print_meta: n_embd           = 2048
0.00.050.226 I llm_load_print_meta: n_layer          = 24
0.00.050.228 I llm_load_print_meta: n_head           = 16
0.00.050.229 I llm_load_print_meta: n_head_kv        = 16
0.00.050.229 I llm_load_print_meta: n_rot            = 32
0.00.050.230 I llm_load_print_meta: n_swa            = 0
0.00.050.232 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.232 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.233 I llm_load_print_meta: n_gqa            = 1
0.00.050.234 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.234 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.235 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.235 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.235 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.236 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.236 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.236 I llm_load_print_meta: n_ff             = 8192
0.00.050.236 I llm_load_print_meta: n_expert         = 0
0.00.050.237 I llm_load_print_meta: n_expert_used    = 0
0.00.050.237 I llm_load_print_meta: causal attn      = 1
0.00.050.237 I llm_load_print_meta: pooling type     = 0
0.00.050.237 I llm_load_print_meta: rope type        = 2
0.00.050.237 I llm_load_print_meta: rope scaling     = linear
0.00.050.238 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.238 I llm_load_print_meta: freq_scale_train = 1
0.00.050.238 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.238 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.239 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.239 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.239 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.239 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.239 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.239 I llm_load_print_meta: model type       = 1.4B
0.00.050.240 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.240 I llm_load_print_meta: model params     = 1.41 B
0.00.050.242 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.242 I llm_load_print_meta: general.name     = 1.4B
0.00.050.242 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.243 I llm_load_print_meta: LF token         = 128 ''
0.00.050.243 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.244 I llm_load_print_meta: max token length = 1024
0.00.052.123 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.123 I llm_load_tensors: offloading output layer to GPU
0.00.052.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.134 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.135 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.482 I llama_new_context_with_model: n_ctx         = 128
0.00.052.483 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.483 I llama_new_context_with_model: n_batch       = 128
0.00.052.483 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.483 I llama_new_context_with_model: flash_attn    = 0
0.00.052.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.484 I llama_new_context_with_model: freq_scale    = 1
0.00.052.484 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.485 I ggml_metal_init: allocating
0.00.052.490 I ggml_metal_init: found device: Apple M4
0.00.052.492 I ggml_metal_init: picking default device: Apple M4
0.00.053.036 I ggml_metal_init: using embedded metal library
0.00.055.349 I ggml_metal_init: GPU name:   Apple M4
0.00.055.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.351 I ggml_metal_init: simdgroup reduction   = true
0.00.055.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.351 I ggml_metal_init: has bfloat            = true
0.00.055.352 I ggml_metal_init: use bfloat            = true
0.00.055.352 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.352 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.824 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.144 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.147 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.060 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.062 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.062 I llama_new_context_with_model: graph nodes  = 967
0.00.067.062 I llama_new_context_with_model: graph splits = 2
0.00.067.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.945 I 
0.00.483.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.483.993 I perplexity: tokenizing the input ..
0.00.491.965 I perplexity: tokenization took 7.971 ms
0.00.491.969 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.624.429 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.625.588 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.625.611 I llama_perf_context_print:        load time =     475.16 ms
0.00.625.612 I llama_perf_context_print: prompt eval time =     132.23 ms /   128 tokens (    1.03 ms per token,   968.00 tokens per second)
0.00.625.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.613 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.626.169 I ggml_metal_free: deallocating

real	0m0.640s
user	0m0.078s
sys	0m0.092s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.762 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.404 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.022 I llama_model_loader: - type  f32:  194 tensors
0.00.025.023 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.023 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.023 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.323 I llm_load_vocab: special tokens cache size = 25
0.00.051.300 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.303 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.303 I llm_load_print_meta: arch             = gptneox
0.00.051.304 I llm_load_print_meta: vocab type       = BPE
0.00.051.304 I llm_load_print_meta: n_vocab          = 50304
0.00.051.304 I llm_load_print_meta: n_merges         = 50009
0.00.051.304 I llm_load_print_meta: vocab_only       = 0
0.00.051.304 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.304 I llm_load_print_meta: n_embd           = 2048
0.00.051.305 I llm_load_print_meta: n_layer          = 24
0.00.051.307 I llm_load_print_meta: n_head           = 16
0.00.051.308 I llm_load_print_meta: n_head_kv        = 16
0.00.051.308 I llm_load_print_meta: n_rot            = 32
0.00.051.309 I llm_load_print_meta: n_swa            = 0
0.00.051.309 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.309 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.310 I llm_load_print_meta: n_gqa            = 1
0.00.051.312 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.314 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.314 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.314 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.316 I llm_load_print_meta: n_ff             = 8192
0.00.051.316 I llm_load_print_meta: n_expert         = 0
0.00.051.318 I llm_load_print_meta: n_expert_used    = 0
0.00.051.320 I llm_load_print_meta: causal attn      = 1
0.00.051.320 I llm_load_print_meta: pooling type     = 0
0.00.051.320 I llm_load_print_meta: rope type        = 2
0.00.051.320 I llm_load_print_meta: rope scaling     = linear
0.00.051.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.321 I llm_load_print_meta: freq_scale_train = 1
0.00.051.321 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.326 I llm_load_print_meta: model type       = 1.4B
0.00.051.326 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.327 I llm_load_print_meta: model params     = 1.41 B
0.00.051.327 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.328 I llm_load_print_meta: general.name     = 1.4B
0.00.051.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.328 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: LF token         = 128 ''
0.00.051.329 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: max token length = 1024
0.00.053.304 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.304 I llm_load_tensors: offloading output layer to GPU
0.00.053.304 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.315 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.316 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.691 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.691 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.692 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.692 I llama_new_context_with_model: n_batch       = 2048
0.00.053.692 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.692 I llama_new_context_with_model: flash_attn    = 0
0.00.053.693 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.693 I llama_new_context_with_model: freq_scale    = 1
0.00.053.693 I ggml_metal_init: allocating
0.00.053.700 I ggml_metal_init: found device: Apple M4
0.00.053.703 I ggml_metal_init: picking default device: Apple M4
0.00.054.327 I ggml_metal_init: using embedded metal library
0.00.056.651 I ggml_metal_init: GPU name:   Apple M4
0.00.056.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.655 I ggml_metal_init: simdgroup reduction   = true
0.00.056.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.656 I ggml_metal_init: has bfloat            = true
0.00.056.656 I ggml_metal_init: use bfloat            = true
0.00.056.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.358 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.068 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.077 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.134 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.136 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.136 I llama_new_context_with_model: graph nodes  = 967
0.00.086.136 I llama_new_context_with_model: graph splits = 2
0.00.086.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.354 I main: llama threadpool init, n_threads = 4
0.00.609.394 I 
0.00.609.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.419 I 
0.00.609.652 I sampler seed: 1234
0.00.609.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.690 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.693 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.308 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.373.309 I llama_perf_context_print:        load time =     600.59 ms
0.01.373.309 I llama_perf_context_print: prompt eval time =      51.01 ms /     7 tokens (    7.29 ms per token,   137.23 tokens per second)
0.01.373.310 I llama_perf_context_print:        eval time =     709.54 ms /    63 runs   (   11.26 ms per token,    88.79 tokens per second)
0.01.373.310 I llama_perf_context_print:       total time =     763.96 ms /    70 tokens
0.01.373.498 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.110s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.059 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.687 I llama_model_loader: - type  f32:  194 tensors
0.00.024.687 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.687 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.688 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.492 I llm_load_vocab: special tokens cache size = 25
0.00.051.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.393 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.394 I llm_load_print_meta: arch             = gptneox
0.00.051.394 I llm_load_print_meta: vocab type       = BPE
0.00.051.394 I llm_load_print_meta: n_vocab          = 50304
0.00.051.394 I llm_load_print_meta: n_merges         = 50009
0.00.051.395 I llm_load_print_meta: vocab_only       = 0
0.00.051.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.395 I llm_load_print_meta: n_embd           = 2048
0.00.051.395 I llm_load_print_meta: n_layer          = 24
0.00.051.398 I llm_load_print_meta: n_head           = 16
0.00.051.400 I llm_load_print_meta: n_head_kv        = 16
0.00.051.401 I llm_load_print_meta: n_rot            = 32
0.00.051.401 I llm_load_print_meta: n_swa            = 0
0.00.051.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.401 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.402 I llm_load_print_meta: n_gqa            = 1
0.00.051.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.403 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.404 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.404 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.404 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.404 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.405 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.405 I llm_load_print_meta: n_ff             = 8192
0.00.051.405 I llm_load_print_meta: n_expert         = 0
0.00.051.406 I llm_load_print_meta: n_expert_used    = 0
0.00.051.406 I llm_load_print_meta: causal attn      = 1
0.00.051.406 I llm_load_print_meta: pooling type     = 0
0.00.051.407 I llm_load_print_meta: rope type        = 2
0.00.051.407 I llm_load_print_meta: rope scaling     = linear
0.00.051.407 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.408 I llm_load_print_meta: freq_scale_train = 1
0.00.051.408 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.408 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.408 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.408 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.409 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.409 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.409 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.409 I llm_load_print_meta: model type       = 1.4B
0.00.051.410 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.410 I llm_load_print_meta: model params     = 1.41 B
0.00.051.411 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.411 I llm_load_print_meta: general.name     = 1.4B
0.00.051.411 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.412 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.412 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.412 I llm_load_print_meta: LF token         = 128 ''
0.00.051.412 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: max token length = 1024
0.00.053.398 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.398 I llm_load_tensors: offloading output layer to GPU
0.00.053.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.409 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.410 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.740 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.740 I llama_new_context_with_model: n_ctx         = 128
0.00.053.741 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.741 I llama_new_context_with_model: n_batch       = 128
0.00.053.741 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.741 I llama_new_context_with_model: flash_attn    = 0
0.00.053.741 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.742 I llama_new_context_with_model: freq_scale    = 1
0.00.053.742 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.743 I ggml_metal_init: allocating
0.00.053.751 I ggml_metal_init: found device: Apple M4
0.00.053.753 I ggml_metal_init: picking default device: Apple M4
0.00.054.324 I ggml_metal_init: using embedded metal library
0.00.056.662 I ggml_metal_init: GPU name:   Apple M4
0.00.056.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.665 I ggml_metal_init: simdgroup reduction   = true
0.00.056.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.665 I ggml_metal_init: has bfloat            = true
0.00.056.665 I ggml_metal_init: use bfloat            = true
0.00.056.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.311 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.650 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.653 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.667 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.513 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.514 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.515 I llama_new_context_with_model: graph nodes  = 967
0.00.068.515 I llama_new_context_with_model: graph splits = 2
0.00.068.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.412 I 
0.00.547.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.468 I perplexity: tokenizing the input ..
0.00.555.280 I perplexity: tokenization took 7.81 ms
0.00.555.284 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.531 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.690.795 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.690.816 I llama_perf_context_print:        load time =     538.60 ms
0.00.690.817 I llama_perf_context_print: prompt eval time =     134.02 ms /   128 tokens (    1.05 ms per token,   955.08 tokens per second)
0.00.690.818 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.819 I llama_perf_context_print:       total time =     143.41 ms /   129 tokens
0.00.691.363 I ggml_metal_free: deallocating

real	0m0.705s
user	0m0.080s
sys	0m0.090s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.093 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.807 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.591 I llama_model_loader: - type  f32:  194 tensors
0.00.026.591 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.591 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.503 I llm_load_vocab: special tokens cache size = 25
0.00.053.275 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.278 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.278 I llm_load_print_meta: arch             = gptneox
0.00.053.279 I llm_load_print_meta: vocab type       = BPE
0.00.053.279 I llm_load_print_meta: n_vocab          = 50304
0.00.053.279 I llm_load_print_meta: n_merges         = 50009
0.00.053.280 I llm_load_print_meta: vocab_only       = 0
0.00.053.280 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.280 I llm_load_print_meta: n_embd           = 2048
0.00.053.280 I llm_load_print_meta: n_layer          = 24
0.00.053.283 I llm_load_print_meta: n_head           = 16
0.00.053.284 I llm_load_print_meta: n_head_kv        = 16
0.00.053.284 I llm_load_print_meta: n_rot            = 32
0.00.053.284 I llm_load_print_meta: n_swa            = 0
0.00.053.284 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.285 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.285 I llm_load_print_meta: n_gqa            = 1
0.00.053.286 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.287 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.287 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.288 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.288 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.288 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.288 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.289 I llm_load_print_meta: n_ff             = 8192
0.00.053.298 I llm_load_print_meta: n_expert         = 0
0.00.053.300 I llm_load_print_meta: n_expert_used    = 0
0.00.053.302 I llm_load_print_meta: causal attn      = 1
0.00.053.302 I llm_load_print_meta: pooling type     = 0
0.00.053.303 I llm_load_print_meta: rope type        = 2
0.00.053.303 I llm_load_print_meta: rope scaling     = linear
0.00.053.303 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.304 I llm_load_print_meta: freq_scale_train = 1
0.00.053.304 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.304 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.304 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.304 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.304 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.304 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.305 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.305 I llm_load_print_meta: model type       = 1.4B
0.00.053.305 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.306 I llm_load_print_meta: model params     = 1.41 B
0.00.053.306 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.306 I llm_load_print_meta: general.name     = 1.4B
0.00.053.307 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.309 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.310 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.310 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.310 I llm_load_print_meta: LF token         = 128 ''
0.00.053.311 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.311 I llm_load_print_meta: max token length = 1024
0.00.055.336 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.336 I llm_load_tensors: offloading output layer to GPU
0.00.055.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.347 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.348 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.677 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.678 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.678 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.678 I llama_new_context_with_model: n_batch       = 2048
0.00.055.678 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.679 I llama_new_context_with_model: flash_attn    = 0
0.00.055.679 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.679 I llama_new_context_with_model: freq_scale    = 1
0.00.055.680 I ggml_metal_init: allocating
0.00.055.686 I ggml_metal_init: found device: Apple M4
0.00.055.689 I ggml_metal_init: picking default device: Apple M4
0.00.056.296 I ggml_metal_init: using embedded metal library
0.00.058.617 I ggml_metal_init: GPU name:   Apple M4
0.00.058.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.621 I ggml_metal_init: simdgroup reduction   = true
0.00.058.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.621 I ggml_metal_init: has bfloat            = true
0.00.058.621 I ggml_metal_init: use bfloat            = true
0.00.058.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.397 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.414 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.423 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.451 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.432 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.434 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.434 I llama_new_context_with_model: graph nodes  = 967
0.00.090.434 I llama_new_context_with_model: graph splits = 2
0.00.090.436 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.654 I main: llama threadpool init, n_threads = 4
0.00.695.693 I 
0.00.695.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.721 I 
0.00.695.953 I sampler seed: 1234
0.00.695.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.000 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.005 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.538.722 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.538.722 I llama_perf_context_print:        load time =     685.56 ms
0.01.538.723 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.538.724 I llama_perf_context_print:        eval time =     788.29 ms /    63 runs   (   12.51 ms per token,    79.92 tokens per second)
0.01.538.727 I llama_perf_context_print:       total time =     843.07 ms /    70 tokens
0.01.538.984 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.112s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.177 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.943 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.707 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.379 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.380 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.155 I llm_load_vocab: special tokens cache size = 25
0.00.052.031 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.034 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.034 I llm_load_print_meta: arch             = gptneox
0.00.052.034 I llm_load_print_meta: vocab type       = BPE
0.00.052.034 I llm_load_print_meta: n_vocab          = 50304
0.00.052.035 I llm_load_print_meta: n_merges         = 50009
0.00.052.035 I llm_load_print_meta: vocab_only       = 0
0.00.052.035 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.035 I llm_load_print_meta: n_embd           = 2048
0.00.052.035 I llm_load_print_meta: n_layer          = 24
0.00.052.038 I llm_load_print_meta: n_head           = 16
0.00.052.039 I llm_load_print_meta: n_head_kv        = 16
0.00.052.039 I llm_load_print_meta: n_rot            = 32
0.00.052.039 I llm_load_print_meta: n_swa            = 0
0.00.052.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.040 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.040 I llm_load_print_meta: n_gqa            = 1
0.00.052.041 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.042 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.043 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.045 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.046 I llm_load_print_meta: n_ff             = 8192
0.00.052.046 I llm_load_print_meta: n_expert         = 0
0.00.052.046 I llm_load_print_meta: n_expert_used    = 0
0.00.052.046 I llm_load_print_meta: causal attn      = 1
0.00.052.048 I llm_load_print_meta: pooling type     = 0
0.00.052.048 I llm_load_print_meta: rope type        = 2
0.00.052.048 I llm_load_print_meta: rope scaling     = linear
0.00.052.049 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.049 I llm_load_print_meta: freq_scale_train = 1
0.00.052.049 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.050 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.050 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.050 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.050 I llm_load_print_meta: model type       = 1.4B
0.00.052.055 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.056 I llm_load_print_meta: model params     = 1.41 B
0.00.052.057 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.057 I llm_load_print_meta: general.name     = 1.4B
0.00.052.058 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.058 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.058 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.058 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.060 I llm_load_print_meta: LF token         = 128 ''
0.00.052.060 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.060 I llm_load_print_meta: max token length = 1024
0.00.054.015 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.015 I llm_load_tensors: offloading output layer to GPU
0.00.054.015 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.026 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.027 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.373 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.374 I llama_new_context_with_model: n_ctx         = 128
0.00.054.374 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.374 I llama_new_context_with_model: n_batch       = 128
0.00.054.374 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.374 I llama_new_context_with_model: flash_attn    = 0
0.00.054.375 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.375 I llama_new_context_with_model: freq_scale    = 1
0.00.054.375 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.376 I ggml_metal_init: allocating
0.00.054.382 I ggml_metal_init: found device: Apple M4
0.00.054.384 I ggml_metal_init: picking default device: Apple M4
0.00.054.962 I ggml_metal_init: using embedded metal library
0.00.057.298 I ggml_metal_init: GPU name:   Apple M4
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.301 I ggml_metal_init: simdgroup reduction   = true
0.00.057.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.301 I ggml_metal_init: has bfloat            = true
0.00.057.301 I ggml_metal_init: use bfloat            = true
0.00.057.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.837 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.845 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.863 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.691 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.693 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.693 I llama_new_context_with_model: graph nodes  = 967
0.00.068.693 I llama_new_context_with_model: graph splits = 2
0.00.068.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.549 I 
0.00.641.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.592 I perplexity: tokenizing the input ..
0.00.649.416 I perplexity: tokenization took 7.822 ms
0.00.649.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.306 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.791.464 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.791.493 I llama_perf_context_print:        load time =     631.37 ms
0.00.791.494 I llama_perf_context_print: prompt eval time =     140.66 ms /   128 tokens (    1.10 ms per token,   909.99 tokens per second)
0.00.791.494 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.495 I llama_perf_context_print:       total time =     149.95 ms /   129 tokens
0.00.792.071 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.079s
sys	0m0.116s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.803 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.370 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.376 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.377 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.067 I llama_model_loader: - type  f32:  194 tensors
0.00.026.067 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.180 I llm_load_vocab: special tokens cache size = 25
0.00.053.088 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.091 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.092 I llm_load_print_meta: arch             = gptneox
0.00.053.092 I llm_load_print_meta: vocab type       = BPE
0.00.053.092 I llm_load_print_meta: n_vocab          = 50304
0.00.053.092 I llm_load_print_meta: n_merges         = 50009
0.00.053.092 I llm_load_print_meta: vocab_only       = 0
0.00.053.093 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.093 I llm_load_print_meta: n_embd           = 2048
0.00.053.093 I llm_load_print_meta: n_layer          = 24
0.00.053.096 I llm_load_print_meta: n_head           = 16
0.00.053.097 I llm_load_print_meta: n_head_kv        = 16
0.00.053.097 I llm_load_print_meta: n_rot            = 32
0.00.053.097 I llm_load_print_meta: n_swa            = 0
0.00.053.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.100 I llm_load_print_meta: n_gqa            = 1
0.00.053.101 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.101 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.102 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.102 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.104 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.104 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.104 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.105 I llm_load_print_meta: n_ff             = 8192
0.00.053.105 I llm_load_print_meta: n_expert         = 0
0.00.053.105 I llm_load_print_meta: n_expert_used    = 0
0.00.053.106 I llm_load_print_meta: causal attn      = 1
0.00.053.106 I llm_load_print_meta: pooling type     = 0
0.00.053.106 I llm_load_print_meta: rope type        = 2
0.00.053.106 I llm_load_print_meta: rope scaling     = linear
0.00.053.107 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.107 I llm_load_print_meta: freq_scale_train = 1
0.00.053.107 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.107 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.107 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.108 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.108 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.108 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.109 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.109 I llm_load_print_meta: model type       = 1.4B
0.00.053.109 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.110 I llm_load_print_meta: model params     = 1.41 B
0.00.053.110 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.110 I llm_load_print_meta: general.name     = 1.4B
0.00.053.111 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.111 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.111 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.111 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.113 I llm_load_print_meta: LF token         = 128 ''
0.00.053.113 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.113 I llm_load_print_meta: max token length = 1024
0.00.055.213 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.213 I llm_load_tensors: offloading output layer to GPU
0.00.055.214 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.224 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.226 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.570 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.570 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.570 I llama_new_context_with_model: n_batch       = 2048
0.00.055.570 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.570 I llama_new_context_with_model: flash_attn    = 0
0.00.055.571 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.571 I llama_new_context_with_model: freq_scale    = 1
0.00.055.572 I ggml_metal_init: allocating
0.00.055.574 I ggml_metal_init: found device: Apple M4
0.00.055.576 I ggml_metal_init: picking default device: Apple M4
0.00.056.170 I ggml_metal_init: using embedded metal library
0.00.058.550 I ggml_metal_init: GPU name:   Apple M4
0.00.058.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.552 I ggml_metal_init: simdgroup reduction   = true
0.00.058.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.552 I ggml_metal_init: has bfloat            = true
0.00.058.553 I ggml_metal_init: use bfloat            = true
0.00.058.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.560 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.680 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.699 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.795 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.797 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.797 I llama_new_context_with_model: graph nodes  = 967
0.00.089.797 I llama_new_context_with_model: graph splits = 2
0.00.089.800 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.929 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.120 I main: llama threadpool init, n_threads = 4
0.00.758.160 I 
0.00.758.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.214 I 
0.00.758.452 I sampler seed: 1234
0.00.758.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.473 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.475 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.475 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.641.398 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.641.399 I llama_perf_context_print:        load time =     749.31 ms
0.01.641.399 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.73 tokens per second)
0.01.641.400 I llama_perf_context_print:        eval time =     825.49 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.641.402 I llama_perf_context_print:       total time =     883.28 ms /    70 tokens
0.01.641.634 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4452 (be0e950c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.835 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.727 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.729 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.730 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.742 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.311 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.312 I llama_model_loader: - type  f32:  194 tensors
0.00.024.312 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.398 I llm_load_vocab: special tokens cache size = 25
0.00.050.303 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.306 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.306 I llm_load_print_meta: arch             = gptneox
0.00.050.307 I llm_load_print_meta: vocab type       = BPE
0.00.050.307 I llm_load_print_meta: n_vocab          = 50304
0.00.050.307 I llm_load_print_meta: n_merges         = 50009
0.00.050.307 I llm_load_print_meta: vocab_only       = 0
0.00.050.307 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.308 I llm_load_print_meta: n_embd           = 2048
0.00.050.308 I llm_load_print_meta: n_layer          = 24
0.00.050.310 I llm_load_print_meta: n_head           = 16
0.00.050.311 I llm_load_print_meta: n_head_kv        = 16
0.00.050.311 I llm_load_print_meta: n_rot            = 32
0.00.050.312 I llm_load_print_meta: n_swa            = 0
0.00.050.312 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.312 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.313 I llm_load_print_meta: n_gqa            = 1
0.00.050.314 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.315 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.316 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.316 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.316 I llm_load_print_meta: n_ff             = 8192
0.00.050.317 I llm_load_print_meta: n_expert         = 0
0.00.050.317 I llm_load_print_meta: n_expert_used    = 0
0.00.050.317 I llm_load_print_meta: causal attn      = 1
0.00.050.317 I llm_load_print_meta: pooling type     = 0
0.00.050.319 I llm_load_print_meta: rope type        = 2
0.00.050.319 I llm_load_print_meta: rope scaling     = linear
0.00.050.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.320 I llm_load_print_meta: freq_scale_train = 1
0.00.050.320 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.320 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.320 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.321 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.321 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.321 I llm_load_print_meta: model type       = 1.4B
0.00.050.321 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.322 I llm_load_print_meta: model params     = 1.41 B
0.00.050.322 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.323 I llm_load_print_meta: general.name     = 1.4B
0.00.050.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.324 I llm_load_print_meta: LF token         = 128 ''
0.00.050.326 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.326 I llm_load_print_meta: max token length = 1024
0.00.052.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.292 I llm_load_tensors: offloading output layer to GPU
0.00.052.292 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.302 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.303 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.641 I llama_new_context_with_model: n_ctx         = 128
0.00.052.642 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.642 I llama_new_context_with_model: n_batch       = 128
0.00.052.642 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.642 I llama_new_context_with_model: flash_attn    = 0
0.00.052.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.643 I llama_new_context_with_model: freq_scale    = 1
0.00.052.643 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.644 I ggml_metal_init: allocating
0.00.052.646 I ggml_metal_init: found device: Apple M4
0.00.052.648 I ggml_metal_init: picking default device: Apple M4
0.00.053.218 I ggml_metal_init: using embedded metal library
0.00.055.564 I ggml_metal_init: GPU name:   Apple M4
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.566 I ggml_metal_init: simdgroup reduction   = true
0.00.055.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.567 I ggml_metal_init: has bfloat            = true
0.00.055.567 I ggml_metal_init: use bfloat            = true
0.00.055.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.155 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.418 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.421 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.435 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.296 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.297 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.298 I llama_new_context_with_model: graph nodes  = 967
0.00.067.298 I llama_new_context_with_model: graph splits = 2
0.00.067.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.242 I 
0.00.541.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.284 I perplexity: tokenizing the input ..
0.00.549.422 I perplexity: tokenization took 8.137 ms
0.00.549.425 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.365 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.690.586 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.690.611 I llama_perf_context_print:        load time =     532.40 ms
0.00.690.614 I llama_perf_context_print: prompt eval time =     139.71 ms /   128 tokens (    1.09 ms per token,   916.20 tokens per second)
0.00.690.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.616 I llama_perf_context_print:       total time =     149.37 ms /   129 tokens
0.00.690.948 I ggml_metal_free: deallocating

real	0m0.705s
user	0m0.078s
sys	0m0.105s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4452 (be0e950c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101a04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101a04a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101a04e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101a052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101a05750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101a05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101a06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101a064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101a06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101a06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101a071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101a07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101a083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101a08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101a09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101a09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101a0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101a0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101a0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101a0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101a0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101a0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101a0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101a0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101a0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101a0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101a0e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101a0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101a0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101a0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101a0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101a0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101a10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101a10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101a10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101a11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101a11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101a11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101a11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101a12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101a124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101a12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101a12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101a13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101a13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101a13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101a13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101a14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101a14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101a150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101a15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101a159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101a15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101a16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101a166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101a16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101a17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101a17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101a17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101a18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101a18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101a18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101a18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101a19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101a19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101a19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101a1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101a1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101a1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101a1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101a1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101a1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101a1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101a1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101a1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101a1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101a1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101a1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101a1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101a1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101a1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101a1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101a1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101a20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101a20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101a20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101a212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101a21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101a21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101a223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101a22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101a22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101a234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101a23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101a24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101a245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101a14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101a24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101a251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101a25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101a25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101a26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101a26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101a26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101a27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101a27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101a27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101a28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101a28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101a28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101a294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101a29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101a2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101a2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101a2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101a2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101a2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101a2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101a2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101a2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101a2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101a2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101a2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101a2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101a2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101a2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101a2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101a2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101a2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101a2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101a2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101a2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101a30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101a30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101a30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101a31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101a31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101a32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101a32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101a32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101a33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101a33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101a33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101a34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101a34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101a34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101a34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101a35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101a35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101a35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101a36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101a36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101a36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101a37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101a37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101a37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101a38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101a38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101a38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101a39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101a39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101a39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101a39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101a3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101a3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101a3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101a3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101a3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101a3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101a3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101a3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101a3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101a3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101a3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101a3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101a3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101a3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101a3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101a3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101a3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101a3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101a3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101a40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101a40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101a40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101a41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101a41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101a41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101a42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101a42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101a42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101a43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101a435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101a43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101a44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101a446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101a44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101a452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101a460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101a46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101a46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101a46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101a47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101a47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101a480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101a48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101a48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101a491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101a49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101a49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101a4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101a4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101a4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101a4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101a4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101a4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101a4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101a4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101a4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101a4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101a4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101a4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101a4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101a4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101a4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101a4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101a4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101a4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101a50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101a506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101a50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101a51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101a51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101a52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101a526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101a52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101a53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101a53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101a53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101a54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101a54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101a54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101a55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101a55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101a55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101a56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101a56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101a56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101a57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101a57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101a57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101a58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101a58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101a590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101a59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101a59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101a5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101a5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101a5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101a5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101a5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101a5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101a5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101a5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101a5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101a5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101a5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101a5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101a5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101a5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101a5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101a5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101a5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101a5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101a5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101a5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101a600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101a60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101a60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101a61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101a61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101a62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101a62550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101a62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101a63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101a63610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.120.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104508320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104508790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104508c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104509070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1045094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104509950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104509dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10450a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10450a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10450ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10450af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10450b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10450c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10450c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10450d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10450d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10450dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10450e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10450ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10450f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10450fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104510380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104510aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1045111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1045118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104511ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104511e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1045122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104512740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104512bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1045130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1045135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104513a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104513cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104514160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1045145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104514b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104515030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104515530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104515a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104515f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104516430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104516930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104516e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104517330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1045177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104517c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104518080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1045184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104518960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104518dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104519240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1045196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104519b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104519f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10451a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10451ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10451aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10451b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10451bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10451c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10451c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10451caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10451cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10451d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10451d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10451dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10451e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10451e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10451eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10451efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10451f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10451f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10451fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104520380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1045208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104520e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104521370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1045218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104521e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104522360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1045228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104522e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104523350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1045238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104523df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104524340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104524890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104524de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104525330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104525880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104525dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104526320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104526870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104526dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104527310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104527860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104527db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104528300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104528850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104528da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1045292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104529840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104529d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10452a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10452a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10452ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10452b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10452b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10452bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10452c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10452c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10452cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10452d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10452d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10452db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10452dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10452e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10452e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10452edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10452f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10452f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10452fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104530040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1045304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104530980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104530e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1045312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104531760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104531c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1045320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104532540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1045329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104532e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104533320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1045337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104533c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104534100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1045345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104534a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104534ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104535380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104535820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104535cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104536160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104536600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104536aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104536f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1045373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104537880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104537d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1045381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104538660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104538b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104538fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104539440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1045398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104539d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10453a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10453a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10453ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10453b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10453b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10453b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10453bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10453c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10453c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10453cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10453d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10453d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10453d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10453de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10453e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10453e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10453ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10453f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10453f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10453fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10453fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104540340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1045407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104540c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104541120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1045415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104541a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104541f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1045423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104542840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104542ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104543180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104543620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104543ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104543f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1045444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104544a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104544f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1045454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104545760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104545d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104546380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104546990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104547180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104547620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1045478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104547ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104548500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104548cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104549190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104549630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104549ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10454a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10454a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10454ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10454b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10454b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10454bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10454c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10454c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10454cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10454d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10454d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10454dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10454e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10454e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10454ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10454f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10454f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10454fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104550220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104550770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104550cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104551210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104551760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104551cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104552200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104552750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104552ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1045531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104553740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104553c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1045541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104554730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104554c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1045551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104555720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104555c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1045561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104556710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104556c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1045571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104557700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104557c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1045581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1045586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104558c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104559190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1045596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104559c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10455a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10455a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10455ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10455b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10455b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10455bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10455c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10455c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10455cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10455d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10455d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10455d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10455de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10455e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10455e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10455ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10455f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10455f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10455fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10455fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104560380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104560820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104560cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104561160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1045616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104561dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1045624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104562c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104563330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1045635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104563de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1045640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1045646b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105f06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105f07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105f07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105f08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105f08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105f09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105f09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105f0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105f0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105f0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105f0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105f0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105f0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105f0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105f0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105f0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105f0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105f0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105f0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105f0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105f0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105f0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105f102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105f10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105f10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105f11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105f118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105f12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105f12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105f13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105f137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105f13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105f14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105f149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105f14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105f156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105f15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105f15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105f16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105f17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105f17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105f17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105f18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105f184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105f18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105f18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105f19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105f196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105f19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105f19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105f1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105f1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105f1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105f1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105f1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105f1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105f1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105f1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105f1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105f1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105f1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105f1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105f20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105f20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105f212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105f21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105f22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105f224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105f22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105f22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105f23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105f23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105f241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105f24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105f24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105f24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105f25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105f257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105f25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105f260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105f26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105f269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105f26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105f27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105f276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105f27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105f27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105f28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105f288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105f28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105f29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105f29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105f29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105f2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105f2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105f2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105f2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105f2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105f2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105f2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105f2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105f2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105f2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105f2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105f2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105f2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105f2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105f2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105f2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105f2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105f2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105f2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105f2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105f30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105f304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105f30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105f30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105f31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105f316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105f31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105f31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105f32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105f32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105f32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105f33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105f335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105f33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105f33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105f34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105f34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105f34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105f35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105f354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105f35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105f35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105f36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105f36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105f36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105f36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105f373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105f37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105f37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105f38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105f385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105f38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105f392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105f39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105f39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105f3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105f3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105f3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105f3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105f3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105f3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105f3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105f3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105f3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105f3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105f3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105f3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105f3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105f3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105f3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105f3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105f3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105f3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105f3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105f3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105f3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105f3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105f401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105f40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105f40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105f40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105f41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105f41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105f42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105f424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105f42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105f42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105f431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105f43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105f43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105f443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105f44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105f44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105f45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105f45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105f459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105f45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105f462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105f46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105f47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105f47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105f478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105f47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105f481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105f48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105f48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105f48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105f49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105f49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105f49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105f4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105f4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105f4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105f4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105f4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105f4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105f4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105f4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105f4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105f4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105f4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105f4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105f4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105f4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105f4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105f4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105f4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105f4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105f4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105f4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105f4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105f4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105f50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105f506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105f50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105f50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105f51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105f518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105f51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105f52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105f52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105f52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105f52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105f53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105f537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105f53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105f540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105f54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105f54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105f54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105f55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105f556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105f56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105f56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105f56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105f57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105f57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105f583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105f589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.702s
user	0m0.272s
sys	0m0.236s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4452 (be0e950c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e60b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e60bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e60c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e60c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e60cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e60d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e60d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e60de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e6105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e611500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e6171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e6189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e6195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e619f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e61a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e61ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e61b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e61b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e61b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e61bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e61ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e61d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e61da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e61e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e61ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e61f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e61fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e61ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e6203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e622070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e622510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e6229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e6232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e623c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e6240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e6263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e6273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e6283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e6293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e62a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e62a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e62ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e62b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e62b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e62be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e62c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e62c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e62ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e61cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e62d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e62da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e62dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e62e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e62ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e62ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e62f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e62fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e62ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e6304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e6314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e6328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e6331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e6356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e6364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e6372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e6389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e6397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e63a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e63a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e63aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e63aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e63b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e63b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e63bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e63c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e63c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e63ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e63d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e63d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e63dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e63e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e63e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e63eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e63ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e63f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e63f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e63fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e6406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e6434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e6442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e6450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e6459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e6467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e647100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e6475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e6496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e649c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e64a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e64a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e64a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e64af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e64b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e64bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e64c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e64c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e64cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e64d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e64d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e64def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e64e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e64e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e64ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e64f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e64f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e64ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e6509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e6519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e6529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e6583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e6593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e65a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e65a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e65ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e65b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e65b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e65be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e65c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e65c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e65ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e65d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e65d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e65de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e65e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e65e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e65ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e65f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e65f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e65fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e6608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e6618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e6622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e6639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e6647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e6650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e6668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e6676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e667e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e6687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e6692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e6698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e70d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e70d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e70d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e70e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e70eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e70f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e70faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e710210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e711050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e711770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e712660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e712d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e7134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e713bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e7142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e7145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e714860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e714cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e715140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e7155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e715ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e715fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e716430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e7166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e716b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e716fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e718e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e719330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e719830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e71d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e71d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e71ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e71e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e71eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e71f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e71f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e71f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e71fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e720720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e720bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e721060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e721500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e7219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e721e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e7222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e722830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e722d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e723d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e7242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e724d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e7252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e725800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e725d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e7262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e7267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e727290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e7277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e727d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e728280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e7287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e728d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e7297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e729d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e72a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e72a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e72ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e72b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e72b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e72bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e72c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e72c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e72cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e72d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e72d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e72dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e72e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e72e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e72ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e72f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e72f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e72fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e7300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e730540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e7309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e730e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e7317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e731c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e732100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e7325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e732a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e732ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e733380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e733820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e734160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e734600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e734aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e734f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e7353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e735880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e735d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e7361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e736660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e736b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e736fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e737440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e7378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e737d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e738220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e7386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e738b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e739000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e7394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e739de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e73a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e73a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e73abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e73b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e73b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e73b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e73be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e73c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e73c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e73cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e73d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e73d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e73da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e73dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e73e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e73e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e73ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e73f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e73f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e73fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e73ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e7403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e740840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e740ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e741180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e741620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e741ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e741f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e7428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e742d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e7431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e743680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e743b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e743fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e744460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e744da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e745240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e7456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e745b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e746020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e7464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e746960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e746eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e747400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e747950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e747ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e748160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e748770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e748d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e74a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e74a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e74af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e74b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e74bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e74c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e74c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e74cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e74d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e74d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e74dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e74e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e74e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e74ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e74f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e74f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e74fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e7501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e7506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e751190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e7516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e751c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e752180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e7526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e752c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e753170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e7536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e753c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e754160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e7546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e754c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e755150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e7556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e755bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e756140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e756690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e756be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e757130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e757680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e757bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e758120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e758670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e758bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e759110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e759660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e759bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e75a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e75a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e75aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e75b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e75b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e75bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e75c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e75c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e75cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e75d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e75d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e75db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e75e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e75e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e75eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e75f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e75f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e75faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e75ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e7603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e760880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e760d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e7611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e761660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e761b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e761fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e762440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e7628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e762d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e763220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e7636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e763b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e7640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e7647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e764ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e765610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e765d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e765ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e7667e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e766aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e7670b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e669560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e64cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e64ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e64b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e61e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e61e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e64d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e61c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e61d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e61d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e61bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e61dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e614cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e620f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e62d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e668ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e617eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e618170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e64be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e6162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e6165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e616860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e669d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e669fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e66a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e66a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e66a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e66aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e66ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e66b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e66b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e66b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e66b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e66bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e66be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e66c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e66c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e66c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e66c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e66cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e66ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e66d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e66d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e66d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e66d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e66dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e66df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e66e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e66e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e66e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e66ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e66ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e66ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e66f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e66f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e66f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e66fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e66fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e670010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e6702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e670590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e670850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e670b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e670dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e671090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e671350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e671610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e6718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e671b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e671e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e672110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e6723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e672690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e672950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e672c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e672ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e673190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e673450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e673710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e6739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e673c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e673f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e674210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e6744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e674790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e674a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e674d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e674fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e675290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e675550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e675810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e675ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e675d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e676050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e676310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e6765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e676890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e676b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e676e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e6770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e677390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e677650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e677910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e677bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e677e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e678150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e678410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e6786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e678990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e678c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e678f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e6791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e679490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e679750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e679a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e679cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e679f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e67a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e67a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e67a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e67aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e67ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e67b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e67b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e67b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e67b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e67bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e67bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e67c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e67c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e67c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e67c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e67cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e67ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e67d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e67d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e67d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e67d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e67dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e67ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e67e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e67e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e67e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e67e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e67ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e67ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e67f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e67f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e67f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e67fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e67fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e67ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e680290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e680550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e680810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e680ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e680d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e681050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e681310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e6815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e681890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e681b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e681e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e6820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e682390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e682650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e682910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e682bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e682e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e683150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e683410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e6836d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e683990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e683c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e683f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e6841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e684490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e684750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e684a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e684cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e684f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e685250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e685510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e6857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e685a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e685d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e686010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e6862d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e686590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e686850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e686b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e686dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e687090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e687350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e687610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e6878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e687b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e687e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e688110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e6883d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e688690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e688950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e688c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e688ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e689190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e689450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e689710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e689ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e689fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e68a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e68a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e68ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e68b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e68b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e68bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e68c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e68c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e68cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e68d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e68d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e68dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e68e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e68e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e68ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e68f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e68f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e68fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e690200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e690750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e690ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e6911f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e691740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e691c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e6921e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e692730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e692c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e6931d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e693720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e693c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e6941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e694710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e694c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e6951b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e695700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e695c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e6961a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e6966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e696c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e697190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e6976e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e697c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e698180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e6986d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e698c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e699170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e6996c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e699c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e69a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e69a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e69ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e69b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e69b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e69bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e69c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e69c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e69c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e69c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e69cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e69d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e69d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e69db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e69dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e69e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e69e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e69ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e69f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e69f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e69fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e69fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e6a0330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e6a07a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e6a1490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e6a1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e6a22d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e6a2590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e6a2a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e6a3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e6a3610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.921s
user	0m0.243s
sys	0m0.137s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.57 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.18 sec*proc (2 tests)

Total Test time (real) =   1.18 sec
        1.20 real         0.76 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.15 user         0.04 sys
```
