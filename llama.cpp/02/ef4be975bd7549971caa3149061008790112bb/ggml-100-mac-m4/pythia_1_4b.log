Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.622s
user	0m0.896s
sys	0m1.289s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target test-c
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Built target test-arg-parser
[ 61%] Built target test-gguf
[ 61%] Built target test-chat-template
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-embedding
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookahead
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-parallel
[ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-speculative
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Built target llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-speculative
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-gen-docs
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-llava-clip-quantize-cli
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.157s
user	0m6.598s
sys	0m10.041s

main: quantize time =  5225.71 ms
main:    total time =  5225.71 ms

main: quantize time =  4294.13 ms
main:    total time =  4294.13 ms

main: quantize time =  3016.35 ms
main:    total time =  3016.35 ms

main: quantize time =  3302.95 ms
main:    total time =  3302.95 ms

main: quantize time =  2010.72 ms
main:    total time =  2010.72 ms

main: quantize time =  5900.12 ms
main:    total time =  5900.12 ms

main: quantize time =  5705.55 ms
main:    total time =  5705.55 ms

main: quantize time =  7244.91 ms
main:    total time =  7244.91 ms

main: quantize time =  6230.22 ms
main:    total time =  6230.22 ms

main: quantize time =  4969.56 ms
main:    total time =  4969.56 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.223 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.383 I main: llama backend init
0.00.000.389 I main: load the model and apply lora adapter, if any
0.00.083.928 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.096.296 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.096.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.096.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.096.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.096.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.096.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.096.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.096.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.096.326 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.096.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.096.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.096.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.096.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.096.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.096.335 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.096.336 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.096.337 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.103.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.105.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.111.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.111.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.112.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.112.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.112.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.112.003 I llama_model_loader: - type  f32:  194 tensors
0.00.112.003 I llama_model_loader: - type  f16:   98 tensors
0.00.112.005 I print_info: file format = GGUF V3 (latest)
0.00.112.006 I print_info: file type   = all F32 (guessed)
0.00.112.010 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.130.205 I load: special tokens cache size = 25
0.00.140.398 I load: token to piece cache size = 0.2984 MB
0.00.140.404 I print_info: arch             = gptneox
0.00.140.404 I print_info: vocab_only       = 0
0.00.140.405 I print_info: n_ctx_train      = 2048
0.00.140.405 I print_info: n_embd           = 2048
0.00.140.407 I print_info: n_layer          = 24
0.00.140.412 I print_info: n_head           = 16
0.00.140.413 I print_info: n_head_kv        = 16
0.00.140.413 I print_info: n_rot            = 32
0.00.140.414 I print_info: n_swa            = 0
0.00.140.414 I print_info: n_embd_head_k    = 128
0.00.140.414 I print_info: n_embd_head_v    = 128
0.00.140.415 I print_info: n_gqa            = 1
0.00.140.416 I print_info: n_embd_k_gqa     = 2048
0.00.140.417 I print_info: n_embd_v_gqa     = 2048
0.00.140.417 I print_info: f_norm_eps       = 1.0e-05
0.00.140.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.140.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.140.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.140.420 I print_info: f_logit_scale    = 0.0e+00
0.00.140.421 I print_info: n_ff             = 8192
0.00.140.422 I print_info: n_expert         = 0
0.00.140.422 I print_info: n_expert_used    = 0
0.00.140.422 I print_info: causal attn      = 1
0.00.140.422 I print_info: pooling type     = 0
0.00.140.422 I print_info: rope type        = 2
0.00.140.423 I print_info: rope scaling     = linear
0.00.140.423 I print_info: freq_base_train  = 10000.0
0.00.140.424 I print_info: freq_scale_train = 1
0.00.140.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.140.424 I print_info: rope_finetuned   = unknown
0.00.140.424 I print_info: ssm_d_conv       = 0
0.00.140.424 I print_info: ssm_d_inner      = 0
0.00.140.424 I print_info: ssm_d_state      = 0
0.00.140.425 I print_info: ssm_dt_rank      = 0
0.00.140.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.140.425 I print_info: model type       = 1.4B
0.00.140.426 I print_info: model params     = 1.41 B
0.00.140.426 I print_info: general.name     = 1.4B
0.00.140.426 I print_info: vocab type       = BPE
0.00.140.427 I print_info: n_vocab          = 50304
0.00.140.427 I print_info: n_merges         = 50009
0.00.140.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.140.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.140.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.140.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.140.428 I print_info: LF token         = 187 'Ċ'
0.00.140.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.140.429 I print_info: max token length = 1024
0.00.140.429 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.197.902 I load_tensors: offloading 24 repeating layers to GPU
0.00.197.906 I load_tensors: offloading output layer to GPU
0.00.197.906 I load_tensors: offloaded 25/25 layers to GPU
0.00.197.933 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.197.934 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.198.581 I llama_context_unified: n_seq_max     = 1
0.00.198.583 I llama_context_unified: n_ctx         = 2048
0.00.198.583 I llama_context_unified: n_ctx_per_seq = 2048
0.00.198.583 I llama_context_unified: n_batch       = 2048
0.00.198.583 I llama_context_unified: n_ubatch      = 512
0.00.198.583 I llama_context_unified: flash_attn    = 0
0.00.198.584 I llama_context_unified: freq_base     = 10000.0
0.00.198.584 I llama_context_unified: freq_scale    = 1
0.00.198.586 I ggml_metal_init: allocating
0.00.198.642 I ggml_metal_init: found device: Apple M4
0.00.198.649 I ggml_metal_init: picking default device: Apple M4
0.00.199.293 I ggml_metal_init: using embedded metal library
0.00.212.077 I ggml_metal_init: GPU name:   Apple M4
0.00.212.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.212.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.212.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.212.079 I ggml_metal_init: simdgroup reduction   = true
0.00.212.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.212.080 I ggml_metal_init: has residency sets    = true
0.00.212.080 I ggml_metal_init: has bfloat            = true
0.00.212.080 I ggml_metal_init: use bfloat            = true
0.00.212.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.212.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.249.781 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.278.622 I init:      Metal KV buffer size =   384.00 MiB
0.00.278.628 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.278.649 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.283.078 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.283.080 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.283.080 I llama_context_unified: graph nodes  = 967
0.00.283.081 I llama_context_unified: graph splits = 2
0.00.283.086 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.283.219 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.283.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.349.531 I main: llama threadpool init, n_threads = 4
0.00.349.572 I 
0.00.349.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.349.608 I 
0.00.349.785 I sampler seed: 1234
0.00.349.789 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.349.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.349.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.349.841 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.176.157 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.176.158 I llama_perf_context_print:        load time =     264.75 ms
0.02.176.159 I llama_perf_context_print: prompt eval time =      44.00 ms /     7 tokens (    6.29 ms per token,   159.10 tokens per second)
0.02.176.160 I llama_perf_context_print:        eval time =    1779.41 ms /    63 runs   (   28.24 ms per token,    35.40 tokens per second)
0.02.176.160 I llama_perf_context_print:       total time =    1827.46 ms /    70 tokens
0.02.179.963 I ggml_metal_free: deallocating

real	0m2.474s
user	0m0.133s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.008.062 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.233 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.235 I llama_model_loader: - type  f32:  194 tensors
0.00.033.235 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.236 I print_info: file format = GGUF V3 (latest)
0.00.033.236 I print_info: file type   = Q8_0
0.00.033.237 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.584 I load: special tokens cache size = 25
0.00.047.889 I load: token to piece cache size = 0.2984 MB
0.00.047.892 I print_info: arch             = gptneox
0.00.047.892 I print_info: vocab_only       = 0
0.00.047.893 I print_info: n_ctx_train      = 2048
0.00.047.893 I print_info: n_embd           = 2048
0.00.047.893 I print_info: n_layer          = 24
0.00.047.898 I print_info: n_head           = 16
0.00.047.899 I print_info: n_head_kv        = 16
0.00.047.899 I print_info: n_rot            = 32
0.00.047.899 I print_info: n_swa            = 0
0.00.047.900 I print_info: n_embd_head_k    = 128
0.00.047.900 I print_info: n_embd_head_v    = 128
0.00.047.905 I print_info: n_gqa            = 1
0.00.047.905 I print_info: n_embd_k_gqa     = 2048
0.00.047.906 I print_info: n_embd_v_gqa     = 2048
0.00.047.907 I print_info: f_norm_eps       = 1.0e-05
0.00.047.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.908 I print_info: f_logit_scale    = 0.0e+00
0.00.047.909 I print_info: n_ff             = 8192
0.00.047.909 I print_info: n_expert         = 0
0.00.047.909 I print_info: n_expert_used    = 0
0.00.047.910 I print_info: causal attn      = 1
0.00.047.910 I print_info: pooling type     = 0
0.00.047.910 I print_info: rope type        = 2
0.00.047.910 I print_info: rope scaling     = linear
0.00.047.911 I print_info: freq_base_train  = 10000.0
0.00.047.912 I print_info: freq_scale_train = 1
0.00.047.912 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.913 I print_info: rope_finetuned   = unknown
0.00.047.913 I print_info: ssm_d_conv       = 0
0.00.047.913 I print_info: ssm_d_inner      = 0
0.00.047.913 I print_info: ssm_d_state      = 0
0.00.047.913 I print_info: ssm_dt_rank      = 0
0.00.047.913 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.914 I print_info: model type       = 1.4B
0.00.047.914 I print_info: model params     = 1.41 B
0.00.047.914 I print_info: general.name     = 1.4B
0.00.047.916 I print_info: vocab type       = BPE
0.00.047.916 I print_info: n_vocab          = 50304
0.00.047.916 I print_info: n_merges         = 50009
0.00.047.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.917 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.917 I print_info: LF token         = 187 'Ċ'
0.00.047.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.918 I print_info: max token length = 1024
0.00.047.919 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.355.420 I load_tensors: offloading 24 repeating layers to GPU
0.01.355.426 I load_tensors: offloading output layer to GPU
0.01.355.428 I load_tensors: offloaded 25/25 layers to GPU
0.01.355.451 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.355.453 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.356.561 I llama_context_unified: n_seq_max     = 1
0.01.356.562 I llama_context_unified: n_ctx         = 2048
0.01.356.563 I llama_context_unified: n_ctx_per_seq = 2048
0.01.356.563 I llama_context_unified: n_batch       = 2048
0.01.356.563 I llama_context_unified: n_ubatch      = 512
0.01.356.564 I llama_context_unified: flash_attn    = 0
0.01.356.565 I llama_context_unified: freq_base     = 10000.0
0.01.356.565 I llama_context_unified: freq_scale    = 1
0.01.356.566 I ggml_metal_init: allocating
0.01.356.576 I ggml_metal_init: found device: Apple M4
0.01.356.583 I ggml_metal_init: picking default device: Apple M4
0.01.357.819 I ggml_metal_init: using embedded metal library
0.01.363.116 I ggml_metal_init: GPU name:   Apple M4
0.01.363.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.363.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.363.121 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.363.121 I ggml_metal_init: simdgroup reduction   = true
0.01.363.121 I ggml_metal_init: simdgroup matrix mul. = true
0.01.363.122 I ggml_metal_init: has residency sets    = true
0.01.363.122 I ggml_metal_init: has bfloat            = true
0.01.363.122 I ggml_metal_init: use bfloat            = true
0.01.363.123 I ggml_metal_init: hasUnifiedMemory      = true
0.01.363.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.379.413 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.432.782 I init:      Metal KV buffer size =   384.00 MiB
0.01.432.791 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.432.819 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.436.865 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.01.436.867 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.01.436.867 I llama_context_unified: graph nodes  = 967
0.01.436.867 I llama_context_unified: graph splits = 2
0.01.436.873 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.436.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.436.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.493.957 I main: llama threadpool init, n_threads = 4
0.01.494.015 I 
0.01.494.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.494.038 I 
0.01.494.192 I sampler seed: 1234
0.01.494.197 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.494.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.494.219 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.494.220 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.576.835 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.02.576.836 I llama_perf_context_print:        load time =    1485.20 ms
0.02.576.837 I llama_perf_context_print: prompt eval time =      44.98 ms /     7 tokens (    6.43 ms per token,   155.61 tokens per second)
0.02.576.838 I llama_perf_context_print:        eval time =    1034.73 ms /    63 runs   (   16.42 ms per token,    60.89 tokens per second)
0.02.576.838 I llama_perf_context_print:       total time =    1083.57 ms /    70 tokens
0.02.580.882 I ggml_metal_free: deallocating

real	0m2.602s
user	0m0.107s
sys	0m0.293s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.684 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.201 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.210 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.217 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.790 I llama_model_loader: - type  f32:  194 tensors
0.00.026.790 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.791 I print_info: file format = GGUF V3 (latest)
0.00.026.792 I print_info: file type   = Q4_0
0.00.026.793 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.693 I load: special tokens cache size = 25
0.00.040.599 I load: token to piece cache size = 0.2984 MB
0.00.040.602 I print_info: arch             = gptneox
0.00.040.602 I print_info: vocab_only       = 0
0.00.040.602 I print_info: n_ctx_train      = 2048
0.00.040.603 I print_info: n_embd           = 2048
0.00.040.603 I print_info: n_layer          = 24
0.00.040.606 I print_info: n_head           = 16
0.00.040.607 I print_info: n_head_kv        = 16
0.00.040.607 I print_info: n_rot            = 32
0.00.040.607 I print_info: n_swa            = 0
0.00.040.607 I print_info: n_embd_head_k    = 128
0.00.040.607 I print_info: n_embd_head_v    = 128
0.00.040.608 I print_info: n_gqa            = 1
0.00.040.609 I print_info: n_embd_k_gqa     = 2048
0.00.040.610 I print_info: n_embd_v_gqa     = 2048
0.00.040.610 I print_info: f_norm_eps       = 1.0e-05
0.00.040.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.611 I print_info: f_logit_scale    = 0.0e+00
0.00.040.612 I print_info: n_ff             = 8192
0.00.040.612 I print_info: n_expert         = 0
0.00.040.612 I print_info: n_expert_used    = 0
0.00.040.612 I print_info: causal attn      = 1
0.00.040.612 I print_info: pooling type     = 0
0.00.040.613 I print_info: rope type        = 2
0.00.040.613 I print_info: rope scaling     = linear
0.00.040.613 I print_info: freq_base_train  = 10000.0
0.00.040.614 I print_info: freq_scale_train = 1
0.00.040.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.614 I print_info: rope_finetuned   = unknown
0.00.040.614 I print_info: ssm_d_conv       = 0
0.00.040.614 I print_info: ssm_d_inner      = 0
0.00.040.615 I print_info: ssm_d_state      = 0
0.00.040.615 I print_info: ssm_dt_rank      = 0
0.00.040.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.616 I print_info: model type       = 1.4B
0.00.040.618 I print_info: model params     = 1.41 B
0.00.040.618 I print_info: general.name     = 1.4B
0.00.040.619 I print_info: vocab type       = BPE
0.00.040.619 I print_info: n_vocab          = 50304
0.00.040.619 I print_info: n_merges         = 50009
0.00.040.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: LF token         = 187 'Ċ'
0.00.040.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.625 I print_info: max token length = 1024
0.00.040.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.549.186 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.201 I load_tensors: offloading output layer to GPU
0.00.549.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.234 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.549.236 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.550.857 I llama_context_unified: n_seq_max     = 1
0.00.550.860 I llama_context_unified: n_ctx         = 2048
0.00.550.860 I llama_context_unified: n_ctx_per_seq = 2048
0.00.550.861 I llama_context_unified: n_batch       = 2048
0.00.550.861 I llama_context_unified: n_ubatch      = 512
0.00.550.862 I llama_context_unified: flash_attn    = 0
0.00.550.864 I llama_context_unified: freq_base     = 10000.0
0.00.550.865 I llama_context_unified: freq_scale    = 1
0.00.550.867 I ggml_metal_init: allocating
0.00.550.936 I ggml_metal_init: found device: Apple M4
0.00.550.951 I ggml_metal_init: picking default device: Apple M4
0.00.552.748 I ggml_metal_init: using embedded metal library
0.00.559.162 I ggml_metal_init: GPU name:   Apple M4
0.00.559.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.170 I ggml_metal_init: simdgroup reduction   = true
0.00.559.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.171 I ggml_metal_init: has residency sets    = true
0.00.559.171 I ggml_metal_init: has bfloat            = true
0.00.559.172 I ggml_metal_init: use bfloat            = true
0.00.559.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.298 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.512 I init:      Metal KV buffer size =   384.00 MiB
0.00.635.521 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.635.547 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.639.911 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.639.913 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.639.913 I llama_context_unified: graph nodes  = 967
0.00.639.913 I llama_context_unified: graph splits = 2
0.00.639.919 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.640.035 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.171 I main: llama threadpool init, n_threads = 4
0.00.694.217 I 
0.00.694.242 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.243 I 
0.00.694.424 I sampler seed: 1234
0.00.694.428 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.439 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.441 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.441 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.372.171 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.372.171 I llama_perf_context_print:        load time =     682.79 ms
0.01.372.172 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.61 tokens per second)
0.01.372.174 I llama_perf_context_print:        eval time =     627.73 ms /    63 runs   (    9.96 ms per token,   100.36 tokens per second)
0.01.372.174 I llama_perf_context_print:       total time =     678.70 ms /    70 tokens
0.01.376.101 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.025 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.741 I llama_model_loader: - type  f32:  194 tensors
0.00.024.741 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.742 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.742 I print_info: file format = GGUF V3 (latest)
0.00.024.743 I print_info: file type   = Q4_1
0.00.024.744 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.424 I load: special tokens cache size = 25
0.00.038.415 I load: token to piece cache size = 0.2984 MB
0.00.038.418 I print_info: arch             = gptneox
0.00.038.418 I print_info: vocab_only       = 0
0.00.038.418 I print_info: n_ctx_train      = 2048
0.00.038.418 I print_info: n_embd           = 2048
0.00.038.419 I print_info: n_layer          = 24
0.00.038.421 I print_info: n_head           = 16
0.00.038.422 I print_info: n_head_kv        = 16
0.00.038.422 I print_info: n_rot            = 32
0.00.038.422 I print_info: n_swa            = 0
0.00.038.422 I print_info: n_embd_head_k    = 128
0.00.038.422 I print_info: n_embd_head_v    = 128
0.00.038.425 I print_info: n_gqa            = 1
0.00.038.426 I print_info: n_embd_k_gqa     = 2048
0.00.038.426 I print_info: n_embd_v_gqa     = 2048
0.00.038.432 I print_info: f_norm_eps       = 1.0e-05
0.00.038.435 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.439 I print_info: f_logit_scale    = 0.0e+00
0.00.038.441 I print_info: n_ff             = 8192
0.00.038.441 I print_info: n_expert         = 0
0.00.038.441 I print_info: n_expert_used    = 0
0.00.038.441 I print_info: causal attn      = 1
0.00.038.441 I print_info: pooling type     = 0
0.00.038.442 I print_info: rope type        = 2
0.00.038.442 I print_info: rope scaling     = linear
0.00.038.442 I print_info: freq_base_train  = 10000.0
0.00.038.442 I print_info: freq_scale_train = 1
0.00.038.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.443 I print_info: rope_finetuned   = unknown
0.00.038.443 I print_info: ssm_d_conv       = 0
0.00.038.443 I print_info: ssm_d_inner      = 0
0.00.038.443 I print_info: ssm_d_state      = 0
0.00.038.443 I print_info: ssm_dt_rank      = 0
0.00.038.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.445 I print_info: model type       = 1.4B
0.00.038.445 I print_info: model params     = 1.41 B
0.00.038.445 I print_info: general.name     = 1.4B
0.00.038.446 I print_info: vocab type       = BPE
0.00.038.447 I print_info: n_vocab          = 50304
0.00.038.448 I print_info: n_merges         = 50009
0.00.038.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.449 I print_info: LF token         = 187 'Ċ'
0.00.038.450 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.450 I print_info: max token length = 1024
0.00.038.451 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.210 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.225 I load_tensors: offloading output layer to GPU
0.00.617.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.259 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.617.261 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.618.963 I llama_context_unified: n_seq_max     = 1
0.00.618.966 I llama_context_unified: n_ctx         = 2048
0.00.618.966 I llama_context_unified: n_ctx_per_seq = 2048
0.00.618.967 I llama_context_unified: n_batch       = 2048
0.00.618.967 I llama_context_unified: n_ubatch      = 512
0.00.618.968 I llama_context_unified: flash_attn    = 0
0.00.618.969 I llama_context_unified: freq_base     = 10000.0
0.00.618.970 I llama_context_unified: freq_scale    = 1
0.00.618.972 I ggml_metal_init: allocating
0.00.619.051 I ggml_metal_init: found device: Apple M4
0.00.619.064 I ggml_metal_init: picking default device: Apple M4
0.00.621.022 I ggml_metal_init: using embedded metal library
0.00.627.838 I ggml_metal_init: GPU name:   Apple M4
0.00.627.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.845 I ggml_metal_init: simdgroup reduction   = true
0.00.627.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.846 I ggml_metal_init: has residency sets    = true
0.00.627.846 I ggml_metal_init: has bfloat            = true
0.00.627.846 I ggml_metal_init: use bfloat            = true
0.00.627.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.274 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.296 I init:      Metal KV buffer size =   384.00 MiB
0.00.700.305 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.329 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.705.341 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.705.344 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.705.344 I llama_context_unified: graph nodes  = 967
0.00.705.344 I llama_context_unified: graph splits = 2
0.00.705.349 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.705.476 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.705.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.729 I main: llama threadpool init, n_threads = 4
0.00.762.769 I 
0.00.762.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.791 I 
0.00.762.962 I sampler seed: 1234
0.00.762.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.002 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.004 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.486.711 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.486.711 I llama_perf_context_print:        load time =     753.41 ms
0.01.486.712 I llama_perf_context_print: prompt eval time =      49.28 ms /     7 tokens (    7.04 ms per token,   142.05 tokens per second)
0.01.486.712 I llama_perf_context_print:        eval time =     671.63 ms /    63 runs   (   10.66 ms per token,    93.80 tokens per second)
0.01.486.713 I llama_perf_context_print:       total time =     724.68 ms /    70 tokens
0.01.490.648 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.109s
sys	0m0.223s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.707 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.668 I llama_model_loader: - type  f32:  194 tensors
0.00.025.668 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.668 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.669 I print_info: file format = GGUF V3 (latest)
0.00.025.669 I print_info: file type   = Q5_0
0.00.025.672 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.518 I load: special tokens cache size = 25
0.00.039.468 I load: token to piece cache size = 0.2984 MB
0.00.039.470 I print_info: arch             = gptneox
0.00.039.471 I print_info: vocab_only       = 0
0.00.039.471 I print_info: n_ctx_train      = 2048
0.00.039.471 I print_info: n_embd           = 2048
0.00.039.471 I print_info: n_layer          = 24
0.00.039.474 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.475 I print_info: n_swa            = 0
0.00.039.475 I print_info: n_embd_head_k    = 128
0.00.039.477 I print_info: n_embd_head_v    = 128
0.00.039.478 I print_info: n_gqa            = 1
0.00.039.479 I print_info: n_embd_k_gqa     = 2048
0.00.039.479 I print_info: n_embd_v_gqa     = 2048
0.00.039.480 I print_info: f_norm_eps       = 1.0e-05
0.00.039.481 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.481 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.481 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.481 I print_info: f_logit_scale    = 0.0e+00
0.00.039.482 I print_info: n_ff             = 8192
0.00.039.482 I print_info: n_expert         = 0
0.00.039.482 I print_info: n_expert_used    = 0
0.00.039.482 I print_info: causal attn      = 1
0.00.039.482 I print_info: pooling type     = 0
0.00.039.483 I print_info: rope type        = 2
0.00.039.483 I print_info: rope scaling     = linear
0.00.039.483 I print_info: freq_base_train  = 10000.0
0.00.039.484 I print_info: freq_scale_train = 1
0.00.039.484 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.484 I print_info: rope_finetuned   = unknown
0.00.039.484 I print_info: ssm_d_conv       = 0
0.00.039.484 I print_info: ssm_d_inner      = 0
0.00.039.484 I print_info: ssm_d_state      = 0
0.00.039.484 I print_info: ssm_dt_rank      = 0
0.00.039.485 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.486 I print_info: model type       = 1.4B
0.00.039.487 I print_info: model params     = 1.41 B
0.00.039.487 I print_info: general.name     = 1.4B
0.00.039.487 I print_info: vocab type       = BPE
0.00.039.488 I print_info: n_vocab          = 50304
0.00.039.488 I print_info: n_merges         = 50009
0.00.039.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: LF token         = 187 'Ċ'
0.00.039.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: max token length = 1024
0.00.039.490 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.657.571 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.587 I load_tensors: offloading output layer to GPU
0.00.657.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.618 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.657.624 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.659.104 I llama_context_unified: n_seq_max     = 1
0.00.659.107 I llama_context_unified: n_ctx         = 2048
0.00.659.108 I llama_context_unified: n_ctx_per_seq = 2048
0.00.659.108 I llama_context_unified: n_batch       = 2048
0.00.659.108 I llama_context_unified: n_ubatch      = 512
0.00.659.108 I llama_context_unified: flash_attn    = 0
0.00.659.110 I llama_context_unified: freq_base     = 10000.0
0.00.659.111 I llama_context_unified: freq_scale    = 1
0.00.659.120 I ggml_metal_init: allocating
0.00.659.174 I ggml_metal_init: found device: Apple M4
0.00.659.188 I ggml_metal_init: picking default device: Apple M4
0.00.661.016 I ggml_metal_init: using embedded metal library
0.00.667.453 I ggml_metal_init: GPU name:   Apple M4
0.00.667.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.460 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.461 I ggml_metal_init: simdgroup reduction   = true
0.00.667.461 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.461 I ggml_metal_init: has residency sets    = true
0.00.667.462 I ggml_metal_init: has bfloat            = true
0.00.667.462 I ggml_metal_init: use bfloat            = true
0.00.667.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.481 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.015 I init:      Metal KV buffer size =   384.00 MiB
0.00.751.021 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.043 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.756.153 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.756.155 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.756.155 I llama_context_unified: graph nodes  = 967
0.00.756.155 I llama_context_unified: graph splits = 2
0.00.756.161 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.044 I main: llama threadpool init, n_threads = 4
0.00.816.083 I 
0.00.816.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.104 I 
0.00.816.257 I sampler seed: 1234
0.00.816.261 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.273 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.600.699 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47144.75 tokens per second)
0.01.600.699 I llama_perf_context_print:        load time =     805.60 ms
0.01.600.701 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.52 tokens per second)
0.01.600.702 I llama_perf_context_print:        eval time =     739.10 ms /    63 runs   (   11.73 ms per token,    85.24 tokens per second)
0.01.600.703 I llama_perf_context_print:       total time =     785.39 ms /    70 tokens
0.01.604.706 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.110s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.813 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.747 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.652 I llama_model_loader: - type  f32:  194 tensors
0.00.025.652 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.653 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.653 I print_info: file format = GGUF V3 (latest)
0.00.025.654 I print_info: file type   = Q5_1
0.00.025.655 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.788 I load: special tokens cache size = 25
0.00.039.913 I load: token to piece cache size = 0.2984 MB
0.00.039.916 I print_info: arch             = gptneox
0.00.039.916 I print_info: vocab_only       = 0
0.00.039.917 I print_info: n_ctx_train      = 2048
0.00.039.917 I print_info: n_embd           = 2048
0.00.039.917 I print_info: n_layer          = 24
0.00.039.920 I print_info: n_head           = 16
0.00.039.921 I print_info: n_head_kv        = 16
0.00.039.921 I print_info: n_rot            = 32
0.00.039.921 I print_info: n_swa            = 0
0.00.039.921 I print_info: n_embd_head_k    = 128
0.00.039.921 I print_info: n_embd_head_v    = 128
0.00.039.922 I print_info: n_gqa            = 1
0.00.039.923 I print_info: n_embd_k_gqa     = 2048
0.00.039.924 I print_info: n_embd_v_gqa     = 2048
0.00.039.924 I print_info: f_norm_eps       = 1.0e-05
0.00.039.925 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.927 I print_info: f_logit_scale    = 0.0e+00
0.00.039.928 I print_info: n_ff             = 8192
0.00.039.928 I print_info: n_expert         = 0
0.00.039.928 I print_info: n_expert_used    = 0
0.00.039.929 I print_info: causal attn      = 1
0.00.039.929 I print_info: pooling type     = 0
0.00.039.929 I print_info: rope type        = 2
0.00.039.929 I print_info: rope scaling     = linear
0.00.039.930 I print_info: freq_base_train  = 10000.0
0.00.039.930 I print_info: freq_scale_train = 1
0.00.039.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.930 I print_info: rope_finetuned   = unknown
0.00.039.931 I print_info: ssm_d_conv       = 0
0.00.039.931 I print_info: ssm_d_inner      = 0
0.00.039.931 I print_info: ssm_d_state      = 0
0.00.039.931 I print_info: ssm_dt_rank      = 0
0.00.039.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.931 I print_info: model type       = 1.4B
0.00.039.932 I print_info: model params     = 1.41 B
0.00.039.932 I print_info: general.name     = 1.4B
0.00.039.932 I print_info: vocab type       = BPE
0.00.039.932 I print_info: n_vocab          = 50304
0.00.039.933 I print_info: n_merges         = 50009
0.00.039.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.933 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.933 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.934 I print_info: LF token         = 187 'Ċ'
0.00.039.934 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.934 I print_info: max token length = 1024
0.00.039.934 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.014 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.018 I load_tensors: offloading output layer to GPU
0.00.664.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.043 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.664.047 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.665.302 I llama_context_unified: n_seq_max     = 1
0.00.665.305 I llama_context_unified: n_ctx         = 2048
0.00.665.305 I llama_context_unified: n_ctx_per_seq = 2048
0.00.665.306 I llama_context_unified: n_batch       = 2048
0.00.665.306 I llama_context_unified: n_ubatch      = 512
0.00.665.306 I llama_context_unified: flash_attn    = 0
0.00.665.307 I llama_context_unified: freq_base     = 10000.0
0.00.665.308 I llama_context_unified: freq_scale    = 1
0.00.665.310 I ggml_metal_init: allocating
0.00.665.325 I ggml_metal_init: found device: Apple M4
0.00.665.340 I ggml_metal_init: picking default device: Apple M4
0.00.666.789 I ggml_metal_init: using embedded metal library
0.00.672.763 I ggml_metal_init: GPU name:   Apple M4
0.00.672.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.769 I ggml_metal_init: simdgroup reduction   = true
0.00.672.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.769 I ggml_metal_init: has residency sets    = true
0.00.672.770 I ggml_metal_init: has bfloat            = true
0.00.672.770 I ggml_metal_init: use bfloat            = true
0.00.672.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.874 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.754 I init:      Metal KV buffer size =   384.00 MiB
0.00.740.761 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.794 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.745.124 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.745.126 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.745.126 I llama_context_unified: graph nodes  = 967
0.00.745.127 I llama_context_unified: graph splits = 2
0.00.745.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.745.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.683 I main: llama threadpool init, n_threads = 4
0.00.800.722 I 
0.00.800.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.742 I 
0.00.800.858 I sampler seed: 1234
0.00.800.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.880 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.650.204 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.650.205 I llama_perf_context_print:        load time =     790.98 ms
0.01.650.206 I llama_perf_context_print: prompt eval time =      52.14 ms /     7 tokens (    7.45 ms per token,   134.25 tokens per second)
0.01.650.206 I llama_perf_context_print:        eval time =     794.25 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.650.207 I llama_perf_context_print:       total time =     850.20 ms /    70 tokens
0.01.653.945 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.908 I llama_model_loader: - type  f32:  194 tensors
0.00.024.909 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.909 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.909 I print_info: file format = GGUF V3 (latest)
0.00.024.910 I print_info: file type   = Q2_K - Medium
0.00.024.910 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.624 I load: special tokens cache size = 25
0.00.038.546 I load: token to piece cache size = 0.2984 MB
0.00.038.549 I print_info: arch             = gptneox
0.00.038.549 I print_info: vocab_only       = 0
0.00.038.549 I print_info: n_ctx_train      = 2048
0.00.038.550 I print_info: n_embd           = 2048
0.00.038.550 I print_info: n_layer          = 24
0.00.038.553 I print_info: n_head           = 16
0.00.038.553 I print_info: n_head_kv        = 16
0.00.038.554 I print_info: n_rot            = 32
0.00.038.554 I print_info: n_swa            = 0
0.00.038.554 I print_info: n_embd_head_k    = 128
0.00.038.554 I print_info: n_embd_head_v    = 128
0.00.038.555 I print_info: n_gqa            = 1
0.00.038.556 I print_info: n_embd_k_gqa     = 2048
0.00.038.557 I print_info: n_embd_v_gqa     = 2048
0.00.038.558 I print_info: f_norm_eps       = 1.0e-05
0.00.038.558 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.561 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.561 I print_info: f_logit_scale    = 0.0e+00
0.00.038.561 I print_info: n_ff             = 8192
0.00.038.562 I print_info: n_expert         = 0
0.00.038.563 I print_info: n_expert_used    = 0
0.00.038.563 I print_info: causal attn      = 1
0.00.038.563 I print_info: pooling type     = 0
0.00.038.563 I print_info: rope type        = 2
0.00.038.564 I print_info: rope scaling     = linear
0.00.038.564 I print_info: freq_base_train  = 10000.0
0.00.038.564 I print_info: freq_scale_train = 1
0.00.038.565 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.565 I print_info: rope_finetuned   = unknown
0.00.038.565 I print_info: ssm_d_conv       = 0
0.00.038.567 I print_info: ssm_d_inner      = 0
0.00.038.567 I print_info: ssm_d_state      = 0
0.00.038.567 I print_info: ssm_dt_rank      = 0
0.00.038.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.567 I print_info: model type       = 1.4B
0.00.038.568 I print_info: model params     = 1.41 B
0.00.038.568 I print_info: general.name     = 1.4B
0.00.038.570 I print_info: vocab type       = BPE
0.00.038.570 I print_info: n_vocab          = 50304
0.00.038.570 I print_info: n_merges         = 50009
0.00.038.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.571 I print_info: LF token         = 187 'Ċ'
0.00.038.571 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.571 I print_info: max token length = 1024
0.00.038.572 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.775 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.790 I load_tensors: offloading output layer to GPU
0.00.366.791 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.820 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.822 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.368.489 I llama_context_unified: n_seq_max     = 1
0.00.368.495 I llama_context_unified: n_ctx         = 2048
0.00.368.496 I llama_context_unified: n_ctx_per_seq = 2048
0.00.368.497 I llama_context_unified: n_batch       = 2048
0.00.368.497 I llama_context_unified: n_ubatch      = 512
0.00.368.497 I llama_context_unified: flash_attn    = 0
0.00.368.499 I llama_context_unified: freq_base     = 10000.0
0.00.368.499 I llama_context_unified: freq_scale    = 1
0.00.368.501 I ggml_metal_init: allocating
0.00.368.596 I ggml_metal_init: found device: Apple M4
0.00.368.610 I ggml_metal_init: picking default device: Apple M4
0.00.370.412 I ggml_metal_init: using embedded metal library
0.00.375.953 I ggml_metal_init: GPU name:   Apple M4
0.00.375.966 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.967 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.968 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.968 I ggml_metal_init: simdgroup reduction   = true
0.00.375.969 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.969 I ggml_metal_init: has residency sets    = true
0.00.375.969 I ggml_metal_init: has bfloat            = true
0.00.375.969 I ggml_metal_init: use bfloat            = true
0.00.375.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.625 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.456.074 I init:      Metal KV buffer size =   384.00 MiB
0.00.456.083 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.456.108 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.460.440 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.460.442 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.460.443 I llama_context_unified: graph nodes  = 967
0.00.460.443 I llama_context_unified: graph splits = 2
0.00.460.448 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.460.571 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.460.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.820 I main: llama threadpool init, n_threads = 4
0.00.518.860 I 
0.00.518.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.888 I 
0.00.519.064 I sampler seed: 1234
0.00.519.068 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.519.079 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.519.080 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.519.080 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.192.289 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.192.290 I llama_perf_context_print:        load time =     508.23 ms
0.01.192.291 I llama_perf_context_print: prompt eval time =      35.43 ms /     7 tokens (    5.06 ms per token,   197.57 tokens per second)
0.01.192.292 I llama_perf_context_print:        eval time =     634.88 ms /    63 runs   (   10.08 ms per token,    99.23 tokens per second)
0.01.192.292 I llama_perf_context_print:       total time =     674.17 ms /    70 tokens
0.01.196.182 I ggml_metal_free: deallocating

real	0m1.213s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.567 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.970 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.980 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.981 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.577 I llama_model_loader: - type  f32:  194 tensors
0.00.024.577 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.577 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.577 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.578 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.578 I print_info: file format = GGUF V3 (latest)
0.00.024.579 I print_info: file type   = Q3_K - Medium
0.00.024.580 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.296 I load: special tokens cache size = 25
0.00.038.029 I load: token to piece cache size = 0.2984 MB
0.00.038.032 I print_info: arch             = gptneox
0.00.038.032 I print_info: vocab_only       = 0
0.00.038.032 I print_info: n_ctx_train      = 2048
0.00.038.032 I print_info: n_embd           = 2048
0.00.038.033 I print_info: n_layer          = 24
0.00.038.035 I print_info: n_head           = 16
0.00.038.036 I print_info: n_head_kv        = 16
0.00.038.036 I print_info: n_rot            = 32
0.00.038.036 I print_info: n_swa            = 0
0.00.038.038 I print_info: n_embd_head_k    = 128
0.00.038.038 I print_info: n_embd_head_v    = 128
0.00.038.039 I print_info: n_gqa            = 1
0.00.038.040 I print_info: n_embd_k_gqa     = 2048
0.00.038.040 I print_info: n_embd_v_gqa     = 2048
0.00.038.041 I print_info: f_norm_eps       = 1.0e-05
0.00.038.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.046 I print_info: f_logit_scale    = 0.0e+00
0.00.038.047 I print_info: n_ff             = 8192
0.00.038.047 I print_info: n_expert         = 0
0.00.038.047 I print_info: n_expert_used    = 0
0.00.038.047 I print_info: causal attn      = 1
0.00.038.049 I print_info: pooling type     = 0
0.00.038.049 I print_info: rope type        = 2
0.00.038.049 I print_info: rope scaling     = linear
0.00.038.049 I print_info: freq_base_train  = 10000.0
0.00.038.050 I print_info: freq_scale_train = 1
0.00.038.050 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.050 I print_info: rope_finetuned   = unknown
0.00.038.050 I print_info: ssm_d_conv       = 0
0.00.038.051 I print_info: ssm_d_inner      = 0
0.00.038.051 I print_info: ssm_d_state      = 0
0.00.038.051 I print_info: ssm_dt_rank      = 0
0.00.038.051 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.051 I print_info: model type       = 1.4B
0.00.038.052 I print_info: model params     = 1.41 B
0.00.038.052 I print_info: general.name     = 1.4B
0.00.038.052 I print_info: vocab type       = BPE
0.00.038.053 I print_info: n_vocab          = 50304
0.00.038.053 I print_info: n_merges         = 50009
0.00.038.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.054 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.054 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.055 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.055 I print_info: LF token         = 187 'Ċ'
0.00.038.055 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.055 I print_info: max token length = 1024
0.00.038.056 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.216 I load_tensors: offloading output layer to GPU
0.00.435.217 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.255 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.258 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.863 I llama_context_unified: n_seq_max     = 1
0.00.436.866 I llama_context_unified: n_ctx         = 2048
0.00.436.866 I llama_context_unified: n_ctx_per_seq = 2048
0.00.436.867 I llama_context_unified: n_batch       = 2048
0.00.436.867 I llama_context_unified: n_ubatch      = 512
0.00.436.868 I llama_context_unified: flash_attn    = 0
0.00.436.869 I llama_context_unified: freq_base     = 10000.0
0.00.436.870 I llama_context_unified: freq_scale    = 1
0.00.436.872 I ggml_metal_init: allocating
0.00.436.943 I ggml_metal_init: found device: Apple M4
0.00.436.955 I ggml_metal_init: picking default device: Apple M4
0.00.438.784 I ggml_metal_init: using embedded metal library
0.00.444.427 I ggml_metal_init: GPU name:   Apple M4
0.00.444.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.442 I ggml_metal_init: simdgroup reduction   = true
0.00.444.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.443 I ggml_metal_init: has residency sets    = true
0.00.444.443 I ggml_metal_init: has bfloat            = true
0.00.444.444 I ggml_metal_init: use bfloat            = true
0.00.444.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.920 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.519.141 I init:      Metal KV buffer size =   384.00 MiB
0.00.519.153 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.519.174 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.523.506 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.523.508 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.523.509 I llama_context_unified: graph nodes  = 967
0.00.523.509 I llama_context_unified: graph splits = 2
0.00.523.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.523.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.523.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.854 I main: llama threadpool init, n_threads = 4
0.00.581.904 I 
0.00.581.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.928 I 
0.00.582.079 I sampler seed: 1234
0.00.582.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.103 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.104 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.104 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.331.577 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48730.27 tokens per second)
0.01.331.577 I llama_perf_context_print:        load time =     572.58 ms
0.01.331.578 I llama_perf_context_print: prompt eval time =      48.62 ms /     7 tokens (    6.95 ms per token,   143.97 tokens per second)
0.01.331.579 I llama_perf_context_print:        eval time =     698.08 ms /    63 runs   (   11.08 ms per token,    90.25 tokens per second)
0.01.331.579 I llama_perf_context_print:       total time =     750.43 ms /    70 tokens
0.01.335.480 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.109s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.612 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.622 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.623 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.624 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.626 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.196 I llama_model_loader: - type  f32:  194 tensors
0.00.025.197 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.197 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.197 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.198 I print_info: file format = GGUF V3 (latest)
0.00.025.198 I print_info: file type   = Q4_K - Medium
0.00.025.203 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.939 I load: special tokens cache size = 25
0.00.039.052 I load: token to piece cache size = 0.2984 MB
0.00.039.055 I print_info: arch             = gptneox
0.00.039.055 I print_info: vocab_only       = 0
0.00.039.056 I print_info: n_ctx_train      = 2048
0.00.039.056 I print_info: n_embd           = 2048
0.00.039.056 I print_info: n_layer          = 24
0.00.039.059 I print_info: n_head           = 16
0.00.039.059 I print_info: n_head_kv        = 16
0.00.039.060 I print_info: n_rot            = 32
0.00.039.060 I print_info: n_swa            = 0
0.00.039.060 I print_info: n_embd_head_k    = 128
0.00.039.060 I print_info: n_embd_head_v    = 128
0.00.039.061 I print_info: n_gqa            = 1
0.00.039.062 I print_info: n_embd_k_gqa     = 2048
0.00.039.062 I print_info: n_embd_v_gqa     = 2048
0.00.039.063 I print_info: f_norm_eps       = 1.0e-05
0.00.039.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.064 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.065 I print_info: f_logit_scale    = 0.0e+00
0.00.039.066 I print_info: n_ff             = 8192
0.00.039.066 I print_info: n_expert         = 0
0.00.039.066 I print_info: n_expert_used    = 0
0.00.039.066 I print_info: causal attn      = 1
0.00.039.068 I print_info: pooling type     = 0
0.00.039.069 I print_info: rope type        = 2
0.00.039.070 I print_info: rope scaling     = linear
0.00.039.070 I print_info: freq_base_train  = 10000.0
0.00.039.070 I print_info: freq_scale_train = 1
0.00.039.070 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.071 I print_info: rope_finetuned   = unknown
0.00.039.071 I print_info: ssm_d_conv       = 0
0.00.039.071 I print_info: ssm_d_inner      = 0
0.00.039.071 I print_info: ssm_d_state      = 0
0.00.039.071 I print_info: ssm_dt_rank      = 0
0.00.039.071 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.072 I print_info: model type       = 1.4B
0.00.039.072 I print_info: model params     = 1.41 B
0.00.039.073 I print_info: general.name     = 1.4B
0.00.039.074 I print_info: vocab type       = BPE
0.00.039.074 I print_info: n_vocab          = 50304
0.00.039.074 I print_info: n_merges         = 50009
0.00.039.074 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.075 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.075 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.075 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.075 I print_info: LF token         = 187 'Ċ'
0.00.039.076 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.076 I print_info: max token length = 1024
0.00.039.076 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.525.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.525.860 I load_tensors: offloading output layer to GPU
0.00.525.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.525.907 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.525.910 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.527.624 I llama_context_unified: n_seq_max     = 1
0.00.527.627 I llama_context_unified: n_ctx         = 2048
0.00.527.627 I llama_context_unified: n_ctx_per_seq = 2048
0.00.527.628 I llama_context_unified: n_batch       = 2048
0.00.527.628 I llama_context_unified: n_ubatch      = 512
0.00.527.628 I llama_context_unified: flash_attn    = 0
0.00.527.630 I llama_context_unified: freq_base     = 10000.0
0.00.527.631 I llama_context_unified: freq_scale    = 1
0.00.527.635 I ggml_metal_init: allocating
0.00.527.718 I ggml_metal_init: found device: Apple M4
0.00.527.736 I ggml_metal_init: picking default device: Apple M4
0.00.529.614 I ggml_metal_init: using embedded metal library
0.00.536.042 I ggml_metal_init: GPU name:   Apple M4
0.00.536.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.047 I ggml_metal_init: simdgroup reduction   = true
0.00.536.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.048 I ggml_metal_init: has residency sets    = true
0.00.536.048 I ggml_metal_init: has bfloat            = true
0.00.536.048 I ggml_metal_init: use bfloat            = true
0.00.536.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.736 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.574 I init:      Metal KV buffer size =   384.00 MiB
0.00.609.579 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.609.649 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.613.761 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.613.762 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.613.763 I llama_context_unified: graph nodes  = 967
0.00.613.763 I llama_context_unified: graph splits = 2
0.00.613.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.922 I main: llama threadpool init, n_threads = 4
0.00.669.960 I 
0.00.669.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.983 I 
0.00.670.156 I sampler seed: 1234
0.00.670.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.171 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.171 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.171 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.419.888 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.419.889 I llama_perf_context_print:        load time =     660.31 ms
0.01.419.890 I llama_perf_context_print: prompt eval time =      46.72 ms /     7 tokens (    6.67 ms per token,   149.83 tokens per second)
0.01.419.891 I llama_perf_context_print:        eval time =     700.10 ms /    63 runs   (   11.11 ms per token,    89.99 tokens per second)
0.01.419.891 I llama_perf_context_print:       total time =     750.69 ms /    70 tokens
0.01.423.790 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.108s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.845 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.626 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.626 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.503 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.503 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.503 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.504 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.504 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.505 I llama_model_loader: - type  f32:  194 tensors
0.00.026.505 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.505 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.506 I print_info: file format = GGUF V3 (latest)
0.00.026.506 I print_info: file type   = Q5_K - Medium
0.00.026.507 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.580 I load: special tokens cache size = 25
0.00.040.630 I load: token to piece cache size = 0.2984 MB
0.00.040.632 I print_info: arch             = gptneox
0.00.040.633 I print_info: vocab_only       = 0
0.00.040.633 I print_info: n_ctx_train      = 2048
0.00.040.633 I print_info: n_embd           = 2048
0.00.040.633 I print_info: n_layer          = 24
0.00.040.636 I print_info: n_head           = 16
0.00.040.637 I print_info: n_head_kv        = 16
0.00.040.637 I print_info: n_rot            = 32
0.00.040.638 I print_info: n_swa            = 0
0.00.040.638 I print_info: n_embd_head_k    = 128
0.00.040.638 I print_info: n_embd_head_v    = 128
0.00.040.639 I print_info: n_gqa            = 1
0.00.040.640 I print_info: n_embd_k_gqa     = 2048
0.00.040.640 I print_info: n_embd_v_gqa     = 2048
0.00.040.641 I print_info: f_norm_eps       = 1.0e-05
0.00.040.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.642 I print_info: f_logit_scale    = 0.0e+00
0.00.040.643 I print_info: n_ff             = 8192
0.00.040.643 I print_info: n_expert         = 0
0.00.040.643 I print_info: n_expert_used    = 0
0.00.040.643 I print_info: causal attn      = 1
0.00.040.645 I print_info: pooling type     = 0
0.00.040.645 I print_info: rope type        = 2
0.00.040.645 I print_info: rope scaling     = linear
0.00.040.646 I print_info: freq_base_train  = 10000.0
0.00.040.646 I print_info: freq_scale_train = 1
0.00.040.646 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.646 I print_info: rope_finetuned   = unknown
0.00.040.646 I print_info: ssm_d_conv       = 0
0.00.040.647 I print_info: ssm_d_inner      = 0
0.00.040.648 I print_info: ssm_d_state      = 0
0.00.040.648 I print_info: ssm_dt_rank      = 0
0.00.040.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.649 I print_info: model type       = 1.4B
0.00.040.649 I print_info: model params     = 1.41 B
0.00.040.649 I print_info: general.name     = 1.4B
0.00.040.650 I print_info: vocab type       = BPE
0.00.040.650 I print_info: n_vocab          = 50304
0.00.040.650 I print_info: n_merges         = 50009
0.00.040.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.651 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.651 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.655 I print_info: LF token         = 187 'Ċ'
0.00.040.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.655 I print_info: max token length = 1024
0.00.040.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.116 I load_tensors: offloading output layer to GPU
0.00.632.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.149 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.632.151 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.633.242 I llama_context_unified: n_seq_max     = 1
0.00.633.245 I llama_context_unified: n_ctx         = 2048
0.00.633.246 I llama_context_unified: n_ctx_per_seq = 2048
0.00.633.246 I llama_context_unified: n_batch       = 2048
0.00.633.247 I llama_context_unified: n_ubatch      = 512
0.00.633.247 I llama_context_unified: flash_attn    = 0
0.00.633.249 I llama_context_unified: freq_base     = 10000.0
0.00.633.250 I llama_context_unified: freq_scale    = 1
0.00.633.252 I ggml_metal_init: allocating
0.00.633.324 I ggml_metal_init: found device: Apple M4
0.00.633.337 I ggml_metal_init: picking default device: Apple M4
0.00.635.183 I ggml_metal_init: using embedded metal library
0.00.641.568 I ggml_metal_init: GPU name:   Apple M4
0.00.641.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.574 I ggml_metal_init: simdgroup reduction   = true
0.00.641.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.575 I ggml_metal_init: has residency sets    = true
0.00.641.575 I ggml_metal_init: has bfloat            = true
0.00.641.575 I ggml_metal_init: use bfloat            = true
0.00.641.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.578 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.318 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.491 I init:      Metal KV buffer size =   384.00 MiB
0.00.713.498 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.521 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.717.666 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.717.668 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.717.668 I llama_context_unified: graph nodes  = 967
0.00.717.668 I llama_context_unified: graph splits = 2
0.00.717.671 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.492 I main: llama threadpool init, n_threads = 4
0.00.781.541 I 
0.00.781.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.563 I 
0.00.781.718 I sampler seed: 1234
0.00.781.722 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.733 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.733 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.621.245 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.621.246 I llama_perf_context_print:        load time =     770.94 ms
0.01.621.247 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.65 tokens per second)
0.01.621.248 I llama_perf_context_print:        eval time =     785.34 ms /    63 runs   (   12.47 ms per token,    80.22 tokens per second)
0.01.621.249 I llama_perf_context_print:       total time =     840.46 ms /    70 tokens
0.01.624.226 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.108s
sys	0m0.242s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.570 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.153 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.938 I llama_model_loader: - type  f32:  194 tensors
0.00.024.938 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.939 I print_info: file format = GGUF V3 (latest)
0.00.024.939 I print_info: file type   = Q6_K
0.00.024.941 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.067 I load: special tokens cache size = 25
0.00.039.100 I load: token to piece cache size = 0.2984 MB
0.00.039.102 I print_info: arch             = gptneox
0.00.039.103 I print_info: vocab_only       = 0
0.00.039.103 I print_info: n_ctx_train      = 2048
0.00.039.103 I print_info: n_embd           = 2048
0.00.039.103 I print_info: n_layer          = 24
0.00.039.106 I print_info: n_head           = 16
0.00.039.107 I print_info: n_head_kv        = 16
0.00.039.107 I print_info: n_rot            = 32
0.00.039.107 I print_info: n_swa            = 0
0.00.039.107 I print_info: n_embd_head_k    = 128
0.00.039.108 I print_info: n_embd_head_v    = 128
0.00.039.108 I print_info: n_gqa            = 1
0.00.039.109 I print_info: n_embd_k_gqa     = 2048
0.00.039.110 I print_info: n_embd_v_gqa     = 2048
0.00.039.111 I print_info: f_norm_eps       = 1.0e-05
0.00.039.111 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.111 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.112 I print_info: f_logit_scale    = 0.0e+00
0.00.039.112 I print_info: n_ff             = 8192
0.00.039.112 I print_info: n_expert         = 0
0.00.039.113 I print_info: n_expert_used    = 0
0.00.039.113 I print_info: causal attn      = 1
0.00.039.113 I print_info: pooling type     = 0
0.00.039.113 I print_info: rope type        = 2
0.00.039.114 I print_info: rope scaling     = linear
0.00.039.116 I print_info: freq_base_train  = 10000.0
0.00.039.116 I print_info: freq_scale_train = 1
0.00.039.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.117 I print_info: rope_finetuned   = unknown
0.00.039.117 I print_info: ssm_d_conv       = 0
0.00.039.117 I print_info: ssm_d_inner      = 0
0.00.039.117 I print_info: ssm_d_state      = 0
0.00.039.117 I print_info: ssm_dt_rank      = 0
0.00.039.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.118 I print_info: model type       = 1.4B
0.00.039.118 I print_info: model params     = 1.41 B
0.00.039.118 I print_info: general.name     = 1.4B
0.00.039.119 I print_info: vocab type       = BPE
0.00.039.119 I print_info: n_vocab          = 50304
0.00.039.119 I print_info: n_merges         = 50009
0.00.039.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: LF token         = 187 'Ċ'
0.00.039.124 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.124 I print_info: max token length = 1024
0.00.039.124 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.074 I load_tensors: offloading output layer to GPU
0.00.670.075 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.098 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.670.099 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.671.307 I llama_context_unified: n_seq_max     = 1
0.00.671.310 I llama_context_unified: n_ctx         = 2048
0.00.671.310 I llama_context_unified: n_ctx_per_seq = 2048
0.00.671.310 I llama_context_unified: n_batch       = 2048
0.00.671.311 I llama_context_unified: n_ubatch      = 512
0.00.671.311 I llama_context_unified: flash_attn    = 0
0.00.671.312 I llama_context_unified: freq_base     = 10000.0
0.00.671.313 I llama_context_unified: freq_scale    = 1
0.00.671.314 I ggml_metal_init: allocating
0.00.671.331 I ggml_metal_init: found device: Apple M4
0.00.671.340 I ggml_metal_init: picking default device: Apple M4
0.00.672.646 I ggml_metal_init: using embedded metal library
0.00.678.282 I ggml_metal_init: GPU name:   Apple M4
0.00.678.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.287 I ggml_metal_init: simdgroup reduction   = true
0.00.678.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.287 I ggml_metal_init: has residency sets    = true
0.00.678.287 I ggml_metal_init: has bfloat            = true
0.00.678.288 I ggml_metal_init: use bfloat            = true
0.00.678.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.694.145 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.099 I init:      Metal KV buffer size =   384.00 MiB
0.00.747.107 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.129 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.751.510 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.751.512 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.751.512 I llama_context_unified: graph nodes  = 967
0.00.751.512 I llama_context_unified: graph splits = 2
0.00.751.518 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.751.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.726 I main: llama threadpool init, n_threads = 4
0.00.815.771 I 
0.00.815.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.792 I 
0.00.815.943 I sampler seed: 1234
0.00.815.948 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.959 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.684.923 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.684.924 I llama_perf_context_print:        load time =     806.46 ms
0.01.684.924 I llama_perf_context_print: prompt eval time =      54.00 ms /     7 tokens (    7.71 ms per token,   129.62 tokens per second)
0.01.684.925 I llama_perf_context_print:        eval time =     811.95 ms /    63 runs   (   12.89 ms per token,    77.59 tokens per second)
0.01.684.925 I llama_perf_context_print:       total time =     869.89 ms /    70 tokens
0.01.688.893 I ggml_metal_free: deallocating

real	0m1.704s
user	0m0.105s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.736 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.972 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.319 I llama_model_loader: - type  f32:  194 tensors
0.00.056.320 I llama_model_loader: - type  f16:   98 tensors
0.00.056.321 I print_info: file format = GGUF V3 (latest)
0.00.056.321 I print_info: file type   = all F32 (guessed)
0.00.056.322 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.345 I load: special tokens cache size = 25
0.00.074.636 I load: token to piece cache size = 0.2984 MB
0.00.074.639 I print_info: arch             = gptneox
0.00.074.639 I print_info: vocab_only       = 0
0.00.074.639 I print_info: n_ctx_train      = 2048
0.00.074.640 I print_info: n_embd           = 2048
0.00.074.640 I print_info: n_layer          = 24
0.00.074.643 I print_info: n_head           = 16
0.00.074.644 I print_info: n_head_kv        = 16
0.00.074.644 I print_info: n_rot            = 32
0.00.074.644 I print_info: n_swa            = 0
0.00.074.644 I print_info: n_embd_head_k    = 128
0.00.074.644 I print_info: n_embd_head_v    = 128
0.00.074.645 I print_info: n_gqa            = 1
0.00.074.648 I print_info: n_embd_k_gqa     = 2048
0.00.074.649 I print_info: n_embd_v_gqa     = 2048
0.00.074.649 I print_info: f_norm_eps       = 1.0e-05
0.00.074.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.650 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.650 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.651 I print_info: f_logit_scale    = 0.0e+00
0.00.074.651 I print_info: n_ff             = 8192
0.00.074.652 I print_info: n_expert         = 0
0.00.074.652 I print_info: n_expert_used    = 0
0.00.074.652 I print_info: causal attn      = 1
0.00.074.653 I print_info: pooling type     = 0
0.00.074.653 I print_info: rope type        = 2
0.00.074.654 I print_info: rope scaling     = linear
0.00.074.654 I print_info: freq_base_train  = 10000.0
0.00.074.654 I print_info: freq_scale_train = 1
0.00.074.654 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.655 I print_info: rope_finetuned   = unknown
0.00.074.655 I print_info: ssm_d_conv       = 0
0.00.074.655 I print_info: ssm_d_inner      = 0
0.00.074.655 I print_info: ssm_d_state      = 0
0.00.074.655 I print_info: ssm_dt_rank      = 0
0.00.074.655 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.655 I print_info: model type       = 1.4B
0.00.074.659 I print_info: model params     = 1.41 B
0.00.074.659 I print_info: general.name     = 1.4B
0.00.074.660 I print_info: vocab type       = BPE
0.00.074.660 I print_info: n_vocab          = 50304
0.00.074.660 I print_info: n_merges         = 50009
0.00.074.661 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.661 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.661 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.661 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.661 I print_info: LF token         = 187 'Ċ'
0.00.074.662 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.662 I print_info: max token length = 1024
0.00.074.662 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.434.773 I load_tensors: offloading 24 repeating layers to GPU
0.01.434.780 I load_tensors: offloading output layer to GPU
0.01.434.780 I load_tensors: offloaded 25/25 layers to GPU
0.01.434.806 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.434.808 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.435.882 I llama_context_unified: n_seq_max     = 1
0.01.435.884 I llama_context_unified: n_ctx         = 128
0.01.435.884 I llama_context_unified: n_ctx_per_seq = 128
0.01.435.884 I llama_context_unified: n_batch       = 128
0.01.435.885 I llama_context_unified: n_ubatch      = 128
0.01.435.885 I llama_context_unified: flash_attn    = 0
0.01.435.886 I llama_context_unified: freq_base     = 10000.0
0.01.435.887 I llama_context_unified: freq_scale    = 1
0.01.435.887 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.435.889 I ggml_metal_init: allocating
0.01.435.948 I ggml_metal_init: found device: Apple M4
0.01.435.958 I ggml_metal_init: picking default device: Apple M4
0.01.437.150 I ggml_metal_init: using embedded metal library
0.01.442.401 I ggml_metal_init: GPU name:   Apple M4
0.01.442.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.442.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.442.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.442.406 I ggml_metal_init: simdgroup reduction   = true
0.01.442.406 I ggml_metal_init: simdgroup matrix mul. = true
0.01.442.407 I ggml_metal_init: has residency sets    = true
0.01.442.407 I ggml_metal_init: has bfloat            = true
0.01.442.407 I ggml_metal_init: use bfloat            = true
0.01.442.408 I ggml_metal_init: hasUnifiedMemory      = true
0.01.442.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.669 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.730 I init:      Metal KV buffer size =    24.00 MiB
0.01.459.734 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.753 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.461.673 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.461.674 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.461.675 I llama_context_unified: graph nodes  = 967
0.01.461.675 I llama_context_unified: graph splits = 2
0.01.461.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.495.835 I 
0.01.495.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.495.880 I perplexity: tokenizing the input ..
0.01.500.068 I perplexity: tokenization took 4.187 ms
0.01.500.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.619.035 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.621.632 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.621.646 I llama_perf_context_print:        load time =    1469.31 ms
0.01.621.647 I llama_perf_context_print: prompt eval time =     118.69 ms /   128 tokens (    0.93 ms per token,  1078.47 tokens per second)
0.01.621.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.621.648 I llama_perf_context_print:       total time =     125.81 ms /   129 tokens
0.01.622.207 I ggml_metal_free: deallocating

real	0m1.813s
user	0m0.103s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.439 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.253 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.940 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.810 I llama_model_loader: - type  f32:  194 tensors
0.00.029.811 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.811 I print_info: file format = GGUF V3 (latest)
0.00.029.812 I print_info: file type   = Q8_0
0.00.029.813 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.971 I load: special tokens cache size = 25
0.00.044.278 I load: token to piece cache size = 0.2984 MB
0.00.044.282 I print_info: arch             = gptneox
0.00.044.282 I print_info: vocab_only       = 0
0.00.044.283 I print_info: n_ctx_train      = 2048
0.00.044.283 I print_info: n_embd           = 2048
0.00.044.283 I print_info: n_layer          = 24
0.00.044.287 I print_info: n_head           = 16
0.00.044.287 I print_info: n_head_kv        = 16
0.00.044.288 I print_info: n_rot            = 32
0.00.044.288 I print_info: n_swa            = 0
0.00.044.288 I print_info: n_embd_head_k    = 128
0.00.044.288 I print_info: n_embd_head_v    = 128
0.00.044.289 I print_info: n_gqa            = 1
0.00.044.289 I print_info: n_embd_k_gqa     = 2048
0.00.044.290 I print_info: n_embd_v_gqa     = 2048
0.00.044.290 I print_info: f_norm_eps       = 1.0e-05
0.00.044.291 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.291 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.291 I print_info: f_logit_scale    = 0.0e+00
0.00.044.292 I print_info: n_ff             = 8192
0.00.044.292 I print_info: n_expert         = 0
0.00.044.292 I print_info: n_expert_used    = 0
0.00.044.292 I print_info: causal attn      = 1
0.00.044.292 I print_info: pooling type     = 0
0.00.044.293 I print_info: rope type        = 2
0.00.044.293 I print_info: rope scaling     = linear
0.00.044.294 I print_info: freq_base_train  = 10000.0
0.00.044.296 I print_info: freq_scale_train = 1
0.00.044.296 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.297 I print_info: rope_finetuned   = unknown
0.00.044.297 I print_info: ssm_d_conv       = 0
0.00.044.297 I print_info: ssm_d_inner      = 0
0.00.044.297 I print_info: ssm_d_state      = 0
0.00.044.297 I print_info: ssm_dt_rank      = 0
0.00.044.297 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.297 I print_info: model type       = 1.4B
0.00.044.298 I print_info: model params     = 1.41 B
0.00.044.298 I print_info: general.name     = 1.4B
0.00.044.298 I print_info: vocab type       = BPE
0.00.044.299 I print_info: n_vocab          = 50304
0.00.044.302 I print_info: n_merges         = 50009
0.00.044.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.304 I print_info: LF token         = 187 'Ċ'
0.00.044.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.304 I print_info: max token length = 1024
0.00.044.305 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.946.402 I load_tensors: offloading 24 repeating layers to GPU
0.00.946.412 I load_tensors: offloading output layer to GPU
0.00.946.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.946.441 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.946.444 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.947.528 I llama_context_unified: n_seq_max     = 1
0.00.947.530 I llama_context_unified: n_ctx         = 128
0.00.947.530 I llama_context_unified: n_ctx_per_seq = 128
0.00.947.530 I llama_context_unified: n_batch       = 128
0.00.947.530 I llama_context_unified: n_ubatch      = 128
0.00.947.531 I llama_context_unified: flash_attn    = 0
0.00.947.531 I llama_context_unified: freq_base     = 10000.0
0.00.947.532 I llama_context_unified: freq_scale    = 1
0.00.947.533 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.947.534 I ggml_metal_init: allocating
0.00.947.571 I ggml_metal_init: found device: Apple M4
0.00.947.580 I ggml_metal_init: picking default device: Apple M4
0.00.948.826 I ggml_metal_init: using embedded metal library
0.00.954.124 I ggml_metal_init: GPU name:   Apple M4
0.00.954.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.954.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.954.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.954.129 I ggml_metal_init: simdgroup reduction   = true
0.00.954.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.954.129 I ggml_metal_init: has residency sets    = true
0.00.954.130 I ggml_metal_init: has bfloat            = true
0.00.954.130 I ggml_metal_init: use bfloat            = true
0.00.954.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.954.133 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.969.188 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.972.579 I init:      Metal KV buffer size =    24.00 MiB
0.00.972.584 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.972.623 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.975.967 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.975.969 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.975.969 I llama_context_unified: graph nodes  = 967
0.00.975.970 I llama_context_unified: graph splits = 2
0.00.975.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.975.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.003.665 I 
0.01.003.748 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.003.767 I perplexity: tokenizing the input ..
0.01.011.358 I perplexity: tokenization took 7.588 ms
0.01.011.382 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.149.938 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.151.311 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.151.328 I llama_perf_context_print:        load time =     993.40 ms
0.01.151.330 I llama_perf_context_print: prompt eval time =     137.51 ms /   128 tokens (    1.07 ms per token,   930.83 tokens per second)
0.01.151.331 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.151.331 I llama_perf_context_print:       total time =     147.66 ms /   129 tokens
0.01.151.882 I ggml_metal_free: deallocating

real	0m1.169s
user	0m0.078s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.666 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.999 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.004 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.014 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.901 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.777 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.778 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.778 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.779 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.779 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.779 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.780 I llama_model_loader: - type  f32:  194 tensors
0.00.028.780 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.781 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.781 I print_info: file format = GGUF V3 (latest)
0.00.028.786 I print_info: file type   = Q4_0
0.00.028.787 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.981 I load: special tokens cache size = 25
0.00.043.093 I load: token to piece cache size = 0.2984 MB
0.00.043.097 I print_info: arch             = gptneox
0.00.043.098 I print_info: vocab_only       = 0
0.00.043.098 I print_info: n_ctx_train      = 2048
0.00.043.098 I print_info: n_embd           = 2048
0.00.043.098 I print_info: n_layer          = 24
0.00.043.102 I print_info: n_head           = 16
0.00.043.105 I print_info: n_head_kv        = 16
0.00.043.106 I print_info: n_rot            = 32
0.00.043.106 I print_info: n_swa            = 0
0.00.043.106 I print_info: n_embd_head_k    = 128
0.00.043.106 I print_info: n_embd_head_v    = 128
0.00.043.107 I print_info: n_gqa            = 1
0.00.043.108 I print_info: n_embd_k_gqa     = 2048
0.00.043.109 I print_info: n_embd_v_gqa     = 2048
0.00.043.110 I print_info: f_norm_eps       = 1.0e-05
0.00.043.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.113 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.113 I print_info: f_logit_scale    = 0.0e+00
0.00.043.114 I print_info: n_ff             = 8192
0.00.043.114 I print_info: n_expert         = 0
0.00.043.114 I print_info: n_expert_used    = 0
0.00.043.114 I print_info: causal attn      = 1
0.00.043.114 I print_info: pooling type     = 0
0.00.043.114 I print_info: rope type        = 2
0.00.043.115 I print_info: rope scaling     = linear
0.00.043.115 I print_info: freq_base_train  = 10000.0
0.00.043.115 I print_info: freq_scale_train = 1
0.00.043.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.116 I print_info: rope_finetuned   = unknown
0.00.043.116 I print_info: ssm_d_conv       = 0
0.00.043.116 I print_info: ssm_d_inner      = 0
0.00.043.116 I print_info: ssm_d_state      = 0
0.00.043.116 I print_info: ssm_dt_rank      = 0
0.00.043.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.121 I print_info: model type       = 1.4B
0.00.043.121 I print_info: model params     = 1.41 B
0.00.043.121 I print_info: general.name     = 1.4B
0.00.043.122 I print_info: vocab type       = BPE
0.00.043.122 I print_info: n_vocab          = 50304
0.00.043.123 I print_info: n_merges         = 50009
0.00.043.123 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.124 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.124 I print_info: LF token         = 187 'Ċ'
0.00.043.124 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.124 I print_info: max token length = 1024
0.00.043.125 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.782 I load_tensors: offloading output layer to GPU
0.00.591.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.799 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.591.800 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.592.734 I llama_context_unified: n_seq_max     = 1
0.00.592.736 I llama_context_unified: n_ctx         = 128
0.00.592.736 I llama_context_unified: n_ctx_per_seq = 128
0.00.592.737 I llama_context_unified: n_batch       = 128
0.00.592.737 I llama_context_unified: n_ubatch      = 128
0.00.592.737 I llama_context_unified: flash_attn    = 0
0.00.592.738 I llama_context_unified: freq_base     = 10000.0
0.00.592.739 I llama_context_unified: freq_scale    = 1
0.00.592.739 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.741 I ggml_metal_init: allocating
0.00.592.785 I ggml_metal_init: found device: Apple M4
0.00.592.796 I ggml_metal_init: picking default device: Apple M4
0.00.593.830 I ggml_metal_init: using embedded metal library
0.00.598.042 I ggml_metal_init: GPU name:   Apple M4
0.00.598.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.050 I ggml_metal_init: simdgroup reduction   = true
0.00.598.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.051 I ggml_metal_init: has residency sets    = true
0.00.598.051 I ggml_metal_init: has bfloat            = true
0.00.598.051 I ggml_metal_init: use bfloat            = true
0.00.598.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.803 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.308 I init:      Metal KV buffer size =    24.00 MiB
0.00.616.311 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.336 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.617.729 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.617.730 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.617.730 I llama_context_unified: graph nodes  = 967
0.00.617.731 I llama_context_unified: graph splits = 2
0.00.617.732 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.398 I 
0.00.641.421 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.428 I perplexity: tokenizing the input ..
0.00.645.011 I perplexity: tokenization took 3.581 ms
0.00.645.020 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.826 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.777.320 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.777.334 I llama_perf_context_print:        load time =     629.73 ms
0.00.777.337 I llama_perf_context_print: prompt eval time =     130.55 ms /   128 tokens (    1.02 ms per token,   980.44 tokens per second)
0.00.777.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.337 I llama_perf_context_print:       total time =     135.94 ms /   129 tokens
0.00.777.865 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.070s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.225 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.245 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.943 I llama_model_loader: - type  f32:  194 tensors
0.00.024.943 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.944 I print_info: file format = GGUF V3 (latest)
0.00.024.945 I print_info: file type   = Q4_1
0.00.024.946 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.266 I load: special tokens cache size = 25
0.00.039.537 I load: token to piece cache size = 0.2984 MB
0.00.039.542 I print_info: arch             = gptneox
0.00.039.543 I print_info: vocab_only       = 0
0.00.039.543 I print_info: n_ctx_train      = 2048
0.00.039.543 I print_info: n_embd           = 2048
0.00.039.543 I print_info: n_layer          = 24
0.00.039.547 I print_info: n_head           = 16
0.00.039.548 I print_info: n_head_kv        = 16
0.00.039.548 I print_info: n_rot            = 32
0.00.039.550 I print_info: n_swa            = 0
0.00.039.550 I print_info: n_embd_head_k    = 128
0.00.039.550 I print_info: n_embd_head_v    = 128
0.00.039.551 I print_info: n_gqa            = 1
0.00.039.552 I print_info: n_embd_k_gqa     = 2048
0.00.039.552 I print_info: n_embd_v_gqa     = 2048
0.00.039.553 I print_info: f_norm_eps       = 1.0e-05
0.00.039.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.554 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.554 I print_info: f_logit_scale    = 0.0e+00
0.00.039.555 I print_info: n_ff             = 8192
0.00.039.555 I print_info: n_expert         = 0
0.00.039.557 I print_info: n_expert_used    = 0
0.00.039.557 I print_info: causal attn      = 1
0.00.039.557 I print_info: pooling type     = 0
0.00.039.557 I print_info: rope type        = 2
0.00.039.557 I print_info: rope scaling     = linear
0.00.039.558 I print_info: freq_base_train  = 10000.0
0.00.039.558 I print_info: freq_scale_train = 1
0.00.039.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.559 I print_info: rope_finetuned   = unknown
0.00.039.560 I print_info: ssm_d_conv       = 0
0.00.039.560 I print_info: ssm_d_inner      = 0
0.00.039.560 I print_info: ssm_d_state      = 0
0.00.039.560 I print_info: ssm_dt_rank      = 0
0.00.039.560 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.560 I print_info: model type       = 1.4B
0.00.039.562 I print_info: model params     = 1.41 B
0.00.039.562 I print_info: general.name     = 1.4B
0.00.039.562 I print_info: vocab type       = BPE
0.00.039.563 I print_info: n_vocab          = 50304
0.00.039.563 I print_info: n_merges         = 50009
0.00.039.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.563 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.563 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.564 I print_info: LF token         = 187 'Ċ'
0.00.039.564 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.564 I print_info: max token length = 1024
0.00.039.564 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.134 I load_tensors: offloading output layer to GPU
0.00.590.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.168 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.590.169 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.591.661 I llama_context_unified: n_seq_max     = 1
0.00.591.663 I llama_context_unified: n_ctx         = 128
0.00.591.664 I llama_context_unified: n_ctx_per_seq = 128
0.00.591.665 I llama_context_unified: n_batch       = 128
0.00.591.665 I llama_context_unified: n_ubatch      = 128
0.00.591.665 I llama_context_unified: flash_attn    = 0
0.00.591.667 I llama_context_unified: freq_base     = 10000.0
0.00.591.668 I llama_context_unified: freq_scale    = 1
0.00.591.668 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.671 I ggml_metal_init: allocating
0.00.591.765 I ggml_metal_init: found device: Apple M4
0.00.591.780 I ggml_metal_init: picking default device: Apple M4
0.00.593.548 I ggml_metal_init: using embedded metal library
0.00.600.441 I ggml_metal_init: GPU name:   Apple M4
0.00.600.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.450 I ggml_metal_init: simdgroup reduction   = true
0.00.600.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.451 I ggml_metal_init: has residency sets    = true
0.00.600.451 I ggml_metal_init: has bfloat            = true
0.00.600.451 I ggml_metal_init: use bfloat            = true
0.00.600.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.420 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.943 I init:      Metal KV buffer size =    24.00 MiB
0.00.621.951 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.982 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.625.341 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.625.343 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.625.343 I llama_context_unified: graph nodes  = 967
0.00.625.343 I llama_context_unified: graph splits = 2
0.00.625.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.471 I 
0.00.653.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.567 I perplexity: tokenizing the input ..
0.00.660.779 I perplexity: tokenization took 7.208 ms
0.00.660.799 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.545 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.795.891 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.795.914 I llama_perf_context_print:        load time =     644.56 ms
0.00.795.917 I llama_perf_context_print: prompt eval time =     132.84 ms /   128 tokens (    1.04 ms per token,   963.57 tokens per second)
0.00.795.917 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.918 I llama_perf_context_print:       total time =     142.44 ms /   129 tokens
0.00.796.501 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.080s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.984 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.362 I llama_model_loader: - type  f32:  194 tensors
0.00.025.363 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.364 I print_info: file format = GGUF V3 (latest)
0.00.025.364 I print_info: file type   = Q5_0
0.00.025.365 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.655 I load: special tokens cache size = 25
0.00.039.816 I load: token to piece cache size = 0.2984 MB
0.00.039.821 I print_info: arch             = gptneox
0.00.039.821 I print_info: vocab_only       = 0
0.00.039.821 I print_info: n_ctx_train      = 2048
0.00.039.821 I print_info: n_embd           = 2048
0.00.039.822 I print_info: n_layer          = 24
0.00.039.826 I print_info: n_head           = 16
0.00.039.827 I print_info: n_head_kv        = 16
0.00.039.827 I print_info: n_rot            = 32
0.00.039.827 I print_info: n_swa            = 0
0.00.039.827 I print_info: n_embd_head_k    = 128
0.00.039.827 I print_info: n_embd_head_v    = 128
0.00.039.828 I print_info: n_gqa            = 1
0.00.039.829 I print_info: n_embd_k_gqa     = 2048
0.00.039.829 I print_info: n_embd_v_gqa     = 2048
0.00.039.830 I print_info: f_norm_eps       = 1.0e-05
0.00.039.831 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.831 I print_info: f_logit_scale    = 0.0e+00
0.00.039.832 I print_info: n_ff             = 8192
0.00.039.832 I print_info: n_expert         = 0
0.00.039.832 I print_info: n_expert_used    = 0
0.00.039.832 I print_info: causal attn      = 1
0.00.039.832 I print_info: pooling type     = 0
0.00.039.832 I print_info: rope type        = 2
0.00.039.833 I print_info: rope scaling     = linear
0.00.039.833 I print_info: freq_base_train  = 10000.0
0.00.039.833 I print_info: freq_scale_train = 1
0.00.039.833 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.834 I print_info: rope_finetuned   = unknown
0.00.039.834 I print_info: ssm_d_conv       = 0
0.00.039.834 I print_info: ssm_d_inner      = 0
0.00.039.834 I print_info: ssm_d_state      = 0
0.00.039.834 I print_info: ssm_dt_rank      = 0
0.00.039.836 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.836 I print_info: model type       = 1.4B
0.00.039.837 I print_info: model params     = 1.41 B
0.00.039.837 I print_info: general.name     = 1.4B
0.00.039.838 I print_info: vocab type       = BPE
0.00.039.838 I print_info: n_vocab          = 50304
0.00.039.838 I print_info: n_merges         = 50009
0.00.039.838 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.841 I print_info: LF token         = 187 'Ċ'
0.00.039.841 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.841 I print_info: max token length = 1024
0.00.039.842 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.944 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.961 I load_tensors: offloading output layer to GPU
0.00.652.962 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.995 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.652.997 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.654.639 I llama_context_unified: n_seq_max     = 1
0.00.654.642 I llama_context_unified: n_ctx         = 128
0.00.654.643 I llama_context_unified: n_ctx_per_seq = 128
0.00.654.643 I llama_context_unified: n_batch       = 128
0.00.654.644 I llama_context_unified: n_ubatch      = 128
0.00.654.644 I llama_context_unified: flash_attn    = 0
0.00.654.647 I llama_context_unified: freq_base     = 10000.0
0.00.654.647 I llama_context_unified: freq_scale    = 1
0.00.654.648 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.654.651 I ggml_metal_init: allocating
0.00.654.754 I ggml_metal_init: found device: Apple M4
0.00.654.787 I ggml_metal_init: picking default device: Apple M4
0.00.656.191 I ggml_metal_init: using embedded metal library
0.00.662.799 I ggml_metal_init: GPU name:   Apple M4
0.00.662.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.662.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.662.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.662.806 I ggml_metal_init: simdgroup reduction   = true
0.00.662.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.662.807 I ggml_metal_init: has residency sets    = true
0.00.662.807 I ggml_metal_init: has bfloat            = true
0.00.662.807 I ggml_metal_init: use bfloat            = true
0.00.662.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.662.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.451 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.022 I init:      Metal KV buffer size =    24.00 MiB
0.00.684.026 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.684.053 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.687.296 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.687.298 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.687.298 I llama_context_unified: graph nodes  = 967
0.00.687.299 I llama_context_unified: graph splits = 2
0.00.687.302 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.687.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.513 I 
0.00.713.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.621 I perplexity: tokenizing the input ..
0.00.721.194 I perplexity: tokenization took 7.569 ms
0.00.721.216 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.461 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.858.788 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.858.800 I llama_perf_context_print:        load time =     703.52 ms
0.00.858.801 I llama_perf_context_print: prompt eval time =     135.28 ms /   128 tokens (    1.06 ms per token,   946.15 tokens per second)
0.00.858.802 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.802 I llama_perf_context_print:       total time =     145.29 ms /   129 tokens
0.00.859.339 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.080s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.822 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.823 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.579 I llama_model_loader: - type  f32:  194 tensors
0.00.024.579 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.580 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.580 I print_info: file format = GGUF V3 (latest)
0.00.024.581 I print_info: file type   = Q5_1
0.00.024.582 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.422 I load: special tokens cache size = 25
0.00.038.539 I load: token to piece cache size = 0.2984 MB
0.00.038.543 I print_info: arch             = gptneox
0.00.038.543 I print_info: vocab_only       = 0
0.00.038.543 I print_info: n_ctx_train      = 2048
0.00.038.543 I print_info: n_embd           = 2048
0.00.038.543 I print_info: n_layer          = 24
0.00.038.547 I print_info: n_head           = 16
0.00.038.548 I print_info: n_head_kv        = 16
0.00.038.548 I print_info: n_rot            = 32
0.00.038.548 I print_info: n_swa            = 0
0.00.038.548 I print_info: n_embd_head_k    = 128
0.00.038.552 I print_info: n_embd_head_v    = 128
0.00.038.552 I print_info: n_gqa            = 1
0.00.038.553 I print_info: n_embd_k_gqa     = 2048
0.00.038.554 I print_info: n_embd_v_gqa     = 2048
0.00.038.554 I print_info: f_norm_eps       = 1.0e-05
0.00.038.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.555 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.555 I print_info: f_logit_scale    = 0.0e+00
0.00.038.556 I print_info: n_ff             = 8192
0.00.038.556 I print_info: n_expert         = 0
0.00.038.556 I print_info: n_expert_used    = 0
0.00.038.557 I print_info: causal attn      = 1
0.00.038.558 I print_info: pooling type     = 0
0.00.038.558 I print_info: rope type        = 2
0.00.038.558 I print_info: rope scaling     = linear
0.00.038.559 I print_info: freq_base_train  = 10000.0
0.00.038.559 I print_info: freq_scale_train = 1
0.00.038.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.560 I print_info: rope_finetuned   = unknown
0.00.038.560 I print_info: ssm_d_conv       = 0
0.00.038.560 I print_info: ssm_d_inner      = 0
0.00.038.560 I print_info: ssm_d_state      = 0
0.00.038.560 I print_info: ssm_dt_rank      = 0
0.00.038.560 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.562 I print_info: model type       = 1.4B
0.00.038.562 I print_info: model params     = 1.41 B
0.00.038.562 I print_info: general.name     = 1.4B
0.00.038.563 I print_info: vocab type       = BPE
0.00.038.563 I print_info: n_vocab          = 50304
0.00.038.563 I print_info: n_merges         = 50009
0.00.038.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.564 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.564 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.564 I print_info: LF token         = 187 'Ċ'
0.00.038.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.569 I print_info: max token length = 1024
0.00.038.569 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.469 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.477 I load_tensors: offloading output layer to GPU
0.00.683.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.502 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.683.503 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.684.858 I llama_context_unified: n_seq_max     = 1
0.00.684.861 I llama_context_unified: n_ctx         = 128
0.00.684.861 I llama_context_unified: n_ctx_per_seq = 128
0.00.684.861 I llama_context_unified: n_batch       = 128
0.00.684.862 I llama_context_unified: n_ubatch      = 128
0.00.684.862 I llama_context_unified: flash_attn    = 0
0.00.684.864 I llama_context_unified: freq_base     = 10000.0
0.00.684.864 I llama_context_unified: freq_scale    = 1
0.00.684.865 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.873 I ggml_metal_init: allocating
0.00.684.923 I ggml_metal_init: found device: Apple M4
0.00.684.935 I ggml_metal_init: picking default device: Apple M4
0.00.686.735 I ggml_metal_init: using embedded metal library
0.00.693.687 I ggml_metal_init: GPU name:   Apple M4
0.00.693.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.694 I ggml_metal_init: simdgroup reduction   = true
0.00.693.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.695 I ggml_metal_init: has residency sets    = true
0.00.693.695 I ggml_metal_init: has bfloat            = true
0.00.693.696 I ggml_metal_init: use bfloat            = true
0.00.693.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.771 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.266 I init:      Metal KV buffer size =    24.00 MiB
0.00.715.271 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.715.299 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.718.657 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.718.659 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.718.659 I llama_context_unified: graph nodes  = 967
0.00.718.660 I llama_context_unified: graph splits = 2
0.00.718.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.359 I 
0.00.750.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.465 I perplexity: tokenizing the input ..
0.00.757.452 I perplexity: tokenization took 6.984 ms
0.00.757.474 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.264 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.906.600 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.906.615 I llama_perf_context_print:        load time =     741.43 ms
0.00.906.616 I llama_perf_context_print: prompt eval time =     146.88 ms /   128 tokens (    1.15 ms per token,   871.45 tokens per second)
0.00.906.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.617 I llama_perf_context_print:       total time =     156.26 ms /   129 tokens
0.00.907.149 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.079s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.938 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.761 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.764 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.764 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.316 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.316 I llama_model_loader: - type  f32:  194 tensors
0.00.025.316 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.317 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.317 I print_info: file format = GGUF V3 (latest)
0.00.025.318 I print_info: file type   = Q2_K - Medium
0.00.025.319 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.355 I load: special tokens cache size = 25
0.00.039.508 I load: token to piece cache size = 0.2984 MB
0.00.039.511 I print_info: arch             = gptneox
0.00.039.511 I print_info: vocab_only       = 0
0.00.039.511 I print_info: n_ctx_train      = 2048
0.00.039.511 I print_info: n_embd           = 2048
0.00.039.511 I print_info: n_layer          = 24
0.00.039.515 I print_info: n_head           = 16
0.00.039.516 I print_info: n_head_kv        = 16
0.00.039.516 I print_info: n_rot            = 32
0.00.039.517 I print_info: n_swa            = 0
0.00.039.517 I print_info: n_embd_head_k    = 128
0.00.039.517 I print_info: n_embd_head_v    = 128
0.00.039.520 I print_info: n_gqa            = 1
0.00.039.521 I print_info: n_embd_k_gqa     = 2048
0.00.039.521 I print_info: n_embd_v_gqa     = 2048
0.00.039.522 I print_info: f_norm_eps       = 1.0e-05
0.00.039.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.529 I print_info: f_logit_scale    = 0.0e+00
0.00.039.536 I print_info: n_ff             = 8192
0.00.039.536 I print_info: n_expert         = 0
0.00.039.536 I print_info: n_expert_used    = 0
0.00.039.536 I print_info: causal attn      = 1
0.00.039.537 I print_info: pooling type     = 0
0.00.039.537 I print_info: rope type        = 2
0.00.039.537 I print_info: rope scaling     = linear
0.00.039.538 I print_info: freq_base_train  = 10000.0
0.00.039.538 I print_info: freq_scale_train = 1
0.00.039.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.539 I print_info: rope_finetuned   = unknown
0.00.039.539 I print_info: ssm_d_conv       = 0
0.00.039.539 I print_info: ssm_d_inner      = 0
0.00.039.539 I print_info: ssm_d_state      = 0
0.00.039.540 I print_info: ssm_dt_rank      = 0
0.00.039.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.540 I print_info: model type       = 1.4B
0.00.039.541 I print_info: model params     = 1.41 B
0.00.039.541 I print_info: general.name     = 1.4B
0.00.039.541 I print_info: vocab type       = BPE
0.00.039.542 I print_info: n_vocab          = 50304
0.00.039.542 I print_info: n_merges         = 50009
0.00.039.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.543 I print_info: LF token         = 187 'Ċ'
0.00.039.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: max token length = 1024
0.00.039.544 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.135 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.146 I load_tensors: offloading output layer to GPU
0.00.366.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.175 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.176 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.771 I llama_context_unified: n_seq_max     = 1
0.00.367.779 I llama_context_unified: n_ctx         = 128
0.00.367.780 I llama_context_unified: n_ctx_per_seq = 128
0.00.367.780 I llama_context_unified: n_batch       = 128
0.00.367.780 I llama_context_unified: n_ubatch      = 128
0.00.367.781 I llama_context_unified: flash_attn    = 0
0.00.367.782 I llama_context_unified: freq_base     = 10000.0
0.00.367.783 I llama_context_unified: freq_scale    = 1
0.00.367.784 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.367.787 I ggml_metal_init: allocating
0.00.367.840 I ggml_metal_init: found device: Apple M4
0.00.367.853 I ggml_metal_init: picking default device: Apple M4
0.00.369.564 I ggml_metal_init: using embedded metal library
0.00.375.208 I ggml_metal_init: GPU name:   Apple M4
0.00.375.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.224 I ggml_metal_init: simdgroup reduction   = true
0.00.375.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.225 I ggml_metal_init: has residency sets    = true
0.00.375.225 I ggml_metal_init: has bfloat            = true
0.00.375.225 I ggml_metal_init: use bfloat            = true
0.00.375.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.988 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.400.543 I init:      Metal KV buffer size =    24.00 MiB
0.00.400.548 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.400.573 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.403.836 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.403.838 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.403.838 I llama_context_unified: graph nodes  = 967
0.00.403.838 I llama_context_unified: graph splits = 2
0.00.403.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.403.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.775 I 
0.00.431.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.876 I perplexity: tokenizing the input ..
0.00.437.228 I perplexity: tokenization took 5.351 ms
0.00.437.240 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.559 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.569.876 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.569.889 I llama_perf_context_print:        load time =     421.83 ms
0.00.569.890 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.44 tokens per second)
0.00.569.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.569.891 I llama_perf_context_print:       total time =     138.12 ms /   129 tokens
0.00.570.447 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.939 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.570 I llama_model_loader: - type  f32:  194 tensors
0.00.024.570 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.570 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.570 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.571 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.571 I print_info: file format = GGUF V3 (latest)
0.00.024.573 I print_info: file type   = Q3_K - Medium
0.00.024.573 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.905 I load: special tokens cache size = 25
0.00.038.991 I load: token to piece cache size = 0.2984 MB
0.00.038.994 I print_info: arch             = gptneox
0.00.038.995 I print_info: vocab_only       = 0
0.00.038.995 I print_info: n_ctx_train      = 2048
0.00.038.995 I print_info: n_embd           = 2048
0.00.038.995 I print_info: n_layer          = 24
0.00.038.999 I print_info: n_head           = 16
0.00.039.000 I print_info: n_head_kv        = 16
0.00.039.000 I print_info: n_rot            = 32
0.00.039.000 I print_info: n_swa            = 0
0.00.039.003 I print_info: n_embd_head_k    = 128
0.00.039.004 I print_info: n_embd_head_v    = 128
0.00.039.004 I print_info: n_gqa            = 1
0.00.039.005 I print_info: n_embd_k_gqa     = 2048
0.00.039.006 I print_info: n_embd_v_gqa     = 2048
0.00.039.006 I print_info: f_norm_eps       = 1.0e-05
0.00.039.007 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.007 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.007 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.007 I print_info: f_logit_scale    = 0.0e+00
0.00.039.009 I print_info: n_ff             = 8192
0.00.039.010 I print_info: n_expert         = 0
0.00.039.010 I print_info: n_expert_used    = 0
0.00.039.010 I print_info: causal attn      = 1
0.00.039.010 I print_info: pooling type     = 0
0.00.039.010 I print_info: rope type        = 2
0.00.039.011 I print_info: rope scaling     = linear
0.00.039.011 I print_info: freq_base_train  = 10000.0
0.00.039.012 I print_info: freq_scale_train = 1
0.00.039.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.012 I print_info: rope_finetuned   = unknown
0.00.039.012 I print_info: ssm_d_conv       = 0
0.00.039.012 I print_info: ssm_d_inner      = 0
0.00.039.013 I print_info: ssm_d_state      = 0
0.00.039.013 I print_info: ssm_dt_rank      = 0
0.00.039.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.013 I print_info: model type       = 1.4B
0.00.039.013 I print_info: model params     = 1.41 B
0.00.039.013 I print_info: general.name     = 1.4B
0.00.039.014 I print_info: vocab type       = BPE
0.00.039.014 I print_info: n_vocab          = 50304
0.00.039.014 I print_info: n_merges         = 50009
0.00.039.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: LF token         = 187 'Ċ'
0.00.039.016 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.016 I print_info: max token length = 1024
0.00.039.016 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.016 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.024 I load_tensors: offloading output layer to GPU
0.00.435.025 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.064 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.066 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.705 I llama_context_unified: n_seq_max     = 1
0.00.436.707 I llama_context_unified: n_ctx         = 128
0.00.436.708 I llama_context_unified: n_ctx_per_seq = 128
0.00.436.708 I llama_context_unified: n_batch       = 128
0.00.436.709 I llama_context_unified: n_ubatch      = 128
0.00.436.709 I llama_context_unified: flash_attn    = 0
0.00.436.711 I llama_context_unified: freq_base     = 10000.0
0.00.436.711 I llama_context_unified: freq_scale    = 1
0.00.436.712 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.436.715 I ggml_metal_init: allocating
0.00.436.777 I ggml_metal_init: found device: Apple M4
0.00.436.791 I ggml_metal_init: picking default device: Apple M4
0.00.438.513 I ggml_metal_init: using embedded metal library
0.00.443.922 I ggml_metal_init: GPU name:   Apple M4
0.00.443.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.929 I ggml_metal_init: simdgroup reduction   = true
0.00.443.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.930 I ggml_metal_init: has residency sets    = true
0.00.443.930 I ggml_metal_init: has bfloat            = true
0.00.443.930 I ggml_metal_init: use bfloat            = true
0.00.443.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.796 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.467.489 I init:      Metal KV buffer size =    24.00 MiB
0.00.467.494 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.467.518 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.470.856 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.470.858 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.470.858 I llama_context_unified: graph nodes  = 967
0.00.470.859 I llama_context_unified: graph splits = 2
0.00.470.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.470.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.429 I 
0.00.496.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.535 I perplexity: tokenizing the input ..
0.00.503.711 I perplexity: tokenization took 7.172 ms
0.00.503.732 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.636.820 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.638.157 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.638.224 I llama_perf_context_print:        load time =     487.60 ms
0.00.638.225 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.28 tokens per second)
0.00.638.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.638.226 I llama_perf_context_print:       total time =     141.80 ms /   129 tokens
0.00.638.822 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.080s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.721 I llama_model_loader: - type  f32:  194 tensors
0.00.024.721 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.721 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.722 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.723 I print_info: file format = GGUF V3 (latest)
0.00.024.723 I print_info: file type   = Q4_K - Medium
0.00.024.724 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.049 I load: special tokens cache size = 25
0.00.039.098 I load: token to piece cache size = 0.2984 MB
0.00.039.102 I print_info: arch             = gptneox
0.00.039.103 I print_info: vocab_only       = 0
0.00.039.103 I print_info: n_ctx_train      = 2048
0.00.039.103 I print_info: n_embd           = 2048
0.00.039.103 I print_info: n_layer          = 24
0.00.039.107 I print_info: n_head           = 16
0.00.039.108 I print_info: n_head_kv        = 16
0.00.039.111 I print_info: n_rot            = 32
0.00.039.111 I print_info: n_swa            = 0
0.00.039.111 I print_info: n_embd_head_k    = 128
0.00.039.111 I print_info: n_embd_head_v    = 128
0.00.039.112 I print_info: n_gqa            = 1
0.00.039.113 I print_info: n_embd_k_gqa     = 2048
0.00.039.113 I print_info: n_embd_v_gqa     = 2048
0.00.039.114 I print_info: f_norm_eps       = 1.0e-05
0.00.039.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.114 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.115 I print_info: f_logit_scale    = 0.0e+00
0.00.039.115 I print_info: n_ff             = 8192
0.00.039.115 I print_info: n_expert         = 0
0.00.039.116 I print_info: n_expert_used    = 0
0.00.039.116 I print_info: causal attn      = 1
0.00.039.116 I print_info: pooling type     = 0
0.00.039.116 I print_info: rope type        = 2
0.00.039.117 I print_info: rope scaling     = linear
0.00.039.118 I print_info: freq_base_train  = 10000.0
0.00.039.118 I print_info: freq_scale_train = 1
0.00.039.118 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.118 I print_info: rope_finetuned   = unknown
0.00.039.119 I print_info: ssm_d_conv       = 0
0.00.039.119 I print_info: ssm_d_inner      = 0
0.00.039.119 I print_info: ssm_d_state      = 0
0.00.039.119 I print_info: ssm_dt_rank      = 0
0.00.039.119 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.119 I print_info: model type       = 1.4B
0.00.039.120 I print_info: model params     = 1.41 B
0.00.039.120 I print_info: general.name     = 1.4B
0.00.039.121 I print_info: vocab type       = BPE
0.00.039.121 I print_info: n_vocab          = 50304
0.00.039.121 I print_info: n_merges         = 50009
0.00.039.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.122 I print_info: LF token         = 187 'Ċ'
0.00.039.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.123 I print_info: max token length = 1024
0.00.039.123 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.540.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.380 I load_tensors: offloading output layer to GPU
0.00.540.381 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.412 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.540.414 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.542.195 I llama_context_unified: n_seq_max     = 1
0.00.542.199 I llama_context_unified: n_ctx         = 128
0.00.542.199 I llama_context_unified: n_ctx_per_seq = 128
0.00.542.199 I llama_context_unified: n_batch       = 128
0.00.542.200 I llama_context_unified: n_ubatch      = 128
0.00.542.200 I llama_context_unified: flash_attn    = 0
0.00.542.202 I llama_context_unified: freq_base     = 10000.0
0.00.542.203 I llama_context_unified: freq_scale    = 1
0.00.542.203 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.542.205 I ggml_metal_init: allocating
0.00.542.259 I ggml_metal_init: found device: Apple M4
0.00.542.271 I ggml_metal_init: picking default device: Apple M4
0.00.544.259 I ggml_metal_init: using embedded metal library
0.00.550.941 I ggml_metal_init: GPU name:   Apple M4
0.00.550.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.949 I ggml_metal_init: simdgroup reduction   = true
0.00.550.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.950 I ggml_metal_init: has residency sets    = true
0.00.550.950 I ggml_metal_init: has bfloat            = true
0.00.550.950 I ggml_metal_init: use bfloat            = true
0.00.550.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.569.012 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.572.533 I init:      Metal KV buffer size =    24.00 MiB
0.00.572.538 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.572.570 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.575.617 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.575.618 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.575.619 I llama_context_unified: graph nodes  = 967
0.00.575.619 I llama_context_unified: graph splits = 2
0.00.575.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.575.623 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.514 I 
0.00.605.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.603 I perplexity: tokenizing the input ..
0.00.611.025 I perplexity: tokenization took 5.421 ms
0.00.611.036 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.442 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.746.775 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.746.799 I llama_perf_context_print:        load time =     596.68 ms
0.00.746.799 I llama_perf_context_print: prompt eval time =     134.17 ms /   128 tokens (    1.05 ms per token,   954.03 tokens per second)
0.00.746.800 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.801 I llama_perf_context_print:       total time =     141.29 ms /   129 tokens
0.00.747.352 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.078s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.816 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.819 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.518 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.329 I llama_model_loader: - type  f32:  194 tensors
0.00.025.329 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.330 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.330 I print_info: file format = GGUF V3 (latest)
0.00.025.331 I print_info: file type   = Q5_K - Medium
0.00.025.333 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.224 I load: special tokens cache size = 25
0.00.039.181 I load: token to piece cache size = 0.2984 MB
0.00.039.185 I print_info: arch             = gptneox
0.00.039.185 I print_info: vocab_only       = 0
0.00.039.185 I print_info: n_ctx_train      = 2048
0.00.039.185 I print_info: n_embd           = 2048
0.00.039.186 I print_info: n_layer          = 24
0.00.039.189 I print_info: n_head           = 16
0.00.039.193 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.193 I print_info: n_swa            = 0
0.00.039.193 I print_info: n_embd_head_k    = 128
0.00.039.194 I print_info: n_embd_head_v    = 128
0.00.039.194 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.196 I print_info: n_embd_v_gqa     = 2048
0.00.039.196 I print_info: f_norm_eps       = 1.0e-05
0.00.039.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.197 I print_info: f_logit_scale    = 0.0e+00
0.00.039.198 I print_info: n_ff             = 8192
0.00.039.198 I print_info: n_expert         = 0
0.00.039.198 I print_info: n_expert_used    = 0
0.00.039.198 I print_info: causal attn      = 1
0.00.039.198 I print_info: pooling type     = 0
0.00.039.199 I print_info: rope type        = 2
0.00.039.199 I print_info: rope scaling     = linear
0.00.039.201 I print_info: freq_base_train  = 10000.0
0.00.039.201 I print_info: freq_scale_train = 1
0.00.039.202 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.202 I print_info: rope_finetuned   = unknown
0.00.039.202 I print_info: ssm_d_conv       = 0
0.00.039.202 I print_info: ssm_d_inner      = 0
0.00.039.202 I print_info: ssm_d_state      = 0
0.00.039.202 I print_info: ssm_dt_rank      = 0
0.00.039.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.203 I print_info: model type       = 1.4B
0.00.039.203 I print_info: model params     = 1.41 B
0.00.039.203 I print_info: general.name     = 1.4B
0.00.039.204 I print_info: vocab type       = BPE
0.00.039.204 I print_info: n_vocab          = 50304
0.00.039.204 I print_info: n_merges         = 50009
0.00.039.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.208 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.208 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.208 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.208 I print_info: LF token         = 187 'Ċ'
0.00.039.209 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.209 I print_info: max token length = 1024
0.00.039.209 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.069 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.084 I load_tensors: offloading output layer to GPU
0.00.613.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.123 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.613.125 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.614.815 I llama_context_unified: n_seq_max     = 1
0.00.614.818 I llama_context_unified: n_ctx         = 128
0.00.614.818 I llama_context_unified: n_ctx_per_seq = 128
0.00.614.819 I llama_context_unified: n_batch       = 128
0.00.614.819 I llama_context_unified: n_ubatch      = 128
0.00.614.819 I llama_context_unified: flash_attn    = 0
0.00.614.821 I llama_context_unified: freq_base     = 10000.0
0.00.614.822 I llama_context_unified: freq_scale    = 1
0.00.614.822 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.825 I ggml_metal_init: allocating
0.00.614.896 I ggml_metal_init: found device: Apple M4
0.00.614.910 I ggml_metal_init: picking default device: Apple M4
0.00.616.739 I ggml_metal_init: using embedded metal library
0.00.623.292 I ggml_metal_init: GPU name:   Apple M4
0.00.623.296 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.297 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.298 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.298 I ggml_metal_init: simdgroup reduction   = true
0.00.623.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.299 I ggml_metal_init: has residency sets    = true
0.00.623.299 I ggml_metal_init: has bfloat            = true
0.00.623.299 I ggml_metal_init: use bfloat            = true
0.00.623.300 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.193 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.643.626 I init:      Metal KV buffer size =    24.00 MiB
0.00.643.633 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.643.681 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.646.873 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.646.875 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.646.876 I llama_context_unified: graph nodes  = 967
0.00.646.876 I llama_context_unified: graph splits = 2
0.00.646.880 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.597 I 
0.00.680.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.700 I perplexity: tokenizing the input ..
0.00.688.021 I perplexity: tokenization took 7.316 ms
0.00.688.043 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.850 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.831.209 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.831.224 I llama_perf_context_print:        load time =     670.67 ms
0.00.831.225 I llama_perf_context_print: prompt eval time =     140.84 ms /   128 tokens (    1.10 ms per token,   908.81 tokens per second)
0.00.831.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.226 I llama_perf_context_print:       total time =     150.63 ms /   129 tokens
0.00.831.834 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.079s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.747 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.752 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.297 I llama_model_loader: - type  f32:  194 tensors
0.00.024.297 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.298 I print_info: file format = GGUF V3 (latest)
0.00.024.298 I print_info: file type   = Q6_K
0.00.024.299 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.076 I load: special tokens cache size = 25
0.00.038.226 I load: token to piece cache size = 0.2984 MB
0.00.038.228 I print_info: arch             = gptneox
0.00.038.229 I print_info: vocab_only       = 0
0.00.038.229 I print_info: n_ctx_train      = 2048
0.00.038.229 I print_info: n_embd           = 2048
0.00.038.229 I print_info: n_layer          = 24
0.00.038.233 I print_info: n_head           = 16
0.00.038.233 I print_info: n_head_kv        = 16
0.00.038.234 I print_info: n_rot            = 32
0.00.038.234 I print_info: n_swa            = 0
0.00.038.234 I print_info: n_embd_head_k    = 128
0.00.038.234 I print_info: n_embd_head_v    = 128
0.00.038.237 I print_info: n_gqa            = 1
0.00.038.238 I print_info: n_embd_k_gqa     = 2048
0.00.038.239 I print_info: n_embd_v_gqa     = 2048
0.00.038.239 I print_info: f_norm_eps       = 1.0e-05
0.00.038.240 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.242 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.242 I print_info: f_logit_scale    = 0.0e+00
0.00.038.243 I print_info: n_ff             = 8192
0.00.038.243 I print_info: n_expert         = 0
0.00.038.243 I print_info: n_expert_used    = 0
0.00.038.243 I print_info: causal attn      = 1
0.00.038.243 I print_info: pooling type     = 0
0.00.038.243 I print_info: rope type        = 2
0.00.038.244 I print_info: rope scaling     = linear
0.00.038.244 I print_info: freq_base_train  = 10000.0
0.00.038.244 I print_info: freq_scale_train = 1
0.00.038.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.245 I print_info: rope_finetuned   = unknown
0.00.038.245 I print_info: ssm_d_conv       = 0
0.00.038.245 I print_info: ssm_d_inner      = 0
0.00.038.249 I print_info: ssm_d_state      = 0
0.00.038.249 I print_info: ssm_dt_rank      = 0
0.00.038.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.254 I print_info: model type       = 1.4B
0.00.038.255 I print_info: model params     = 1.41 B
0.00.038.255 I print_info: general.name     = 1.4B
0.00.038.257 I print_info: vocab type       = BPE
0.00.038.257 I print_info: n_vocab          = 50304
0.00.038.257 I print_info: n_merges         = 50009
0.00.038.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.258 I print_info: LF token         = 187 'Ċ'
0.00.038.258 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.260 I print_info: max token length = 1024
0.00.038.260 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.417 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.423 I load_tensors: offloading output layer to GPU
0.00.608.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.446 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.608.447 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.609.807 I llama_context_unified: n_seq_max     = 1
0.00.609.809 I llama_context_unified: n_ctx         = 128
0.00.609.809 I llama_context_unified: n_ctx_per_seq = 128
0.00.609.810 I llama_context_unified: n_batch       = 128
0.00.609.810 I llama_context_unified: n_ubatch      = 128
0.00.609.810 I llama_context_unified: flash_attn    = 0
0.00.609.811 I llama_context_unified: freq_base     = 10000.0
0.00.609.812 I llama_context_unified: freq_scale    = 1
0.00.609.813 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.814 I ggml_metal_init: allocating
0.00.609.836 I ggml_metal_init: found device: Apple M4
0.00.609.845 I ggml_metal_init: picking default device: Apple M4
0.00.611.270 I ggml_metal_init: using embedded metal library
0.00.617.256 I ggml_metal_init: GPU name:   Apple M4
0.00.617.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.261 I ggml_metal_init: simdgroup reduction   = true
0.00.617.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.262 I ggml_metal_init: has residency sets    = true
0.00.617.262 I ggml_metal_init: has bfloat            = true
0.00.617.262 I ggml_metal_init: use bfloat            = true
0.00.617.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.311 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.782 I init:      Metal KV buffer size =    24.00 MiB
0.00.637.791 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.829 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.641.021 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.641.023 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.641.024 I llama_context_unified: graph nodes  = 967
0.00.641.024 I llama_context_unified: graph splits = 2
0.00.641.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.855 I 
0.00.671.938 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.957 I perplexity: tokenizing the input ..
0.00.679.615 I perplexity: tokenization took 7.654 ms
0.00.679.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.107 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.822.527 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.822.542 I llama_perf_context_print:        load time =     663.00 ms
0.00.822.543 I llama_perf_context_print: prompt eval time =     140.59 ms /   128 tokens (    1.10 ms per token,   910.47 tokens per second)
0.00.822.544 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.544 I llama_perf_context_print:       total time =     150.69 ms /   129 tokens
0.00.823.155 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.078s
sys	0m0.140s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.323 I build: 4717 (02ef4be9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.123 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.764 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.764 I llama_model_loader: - type  f32:  194 tensors
0.00.053.764 I llama_model_loader: - type  f16:   98 tensors
0.00.053.765 I print_info: file format = GGUF V3 (latest)
0.00.053.766 I print_info: file type   = all F32 (guessed)
0.00.053.767 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.851 I load: special tokens cache size = 25
0.00.071.993 I load: token to piece cache size = 0.2984 MB
0.00.071.996 I print_info: arch             = gptneox
0.00.071.996 I print_info: vocab_only       = 0
0.00.071.996 I print_info: n_ctx_train      = 2048
0.00.071.997 I print_info: n_embd           = 2048
0.00.071.997 I print_info: n_layer          = 24
0.00.071.999 I print_info: n_head           = 16
0.00.072.000 I print_info: n_head_kv        = 16
0.00.072.000 I print_info: n_rot            = 32
0.00.072.002 I print_info: n_swa            = 0
0.00.072.002 I print_info: n_embd_head_k    = 128
0.00.072.003 I print_info: n_embd_head_v    = 128
0.00.072.003 I print_info: n_gqa            = 1
0.00.072.004 I print_info: n_embd_k_gqa     = 2048
0.00.072.005 I print_info: n_embd_v_gqa     = 2048
0.00.072.009 I print_info: f_norm_eps       = 1.0e-05
0.00.072.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.010 I print_info: f_logit_scale    = 0.0e+00
0.00.072.011 I print_info: n_ff             = 8192
0.00.072.011 I print_info: n_expert         = 0
0.00.072.011 I print_info: n_expert_used    = 0
0.00.072.013 I print_info: causal attn      = 1
0.00.072.013 I print_info: pooling type     = 0
0.00.072.013 I print_info: rope type        = 2
0.00.072.013 I print_info: rope scaling     = linear
0.00.072.014 I print_info: freq_base_train  = 10000.0
0.00.072.014 I print_info: freq_scale_train = 1
0.00.072.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.014 I print_info: rope_finetuned   = unknown
0.00.072.014 I print_info: ssm_d_conv       = 0
0.00.072.016 I print_info: ssm_d_inner      = 0
0.00.072.016 I print_info: ssm_d_state      = 0
0.00.072.016 I print_info: ssm_dt_rank      = 0
0.00.072.016 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.016 I print_info: model type       = 1.4B
0.00.072.017 I print_info: model params     = 1.41 B
0.00.072.017 I print_info: general.name     = 1.4B
0.00.072.017 I print_info: vocab type       = BPE
0.00.072.018 I print_info: n_vocab          = 50304
0.00.072.018 I print_info: n_merges         = 50009
0.00.072.018 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.018 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.018 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.023 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.024 I print_info: LF token         = 187 'Ċ'
0.00.072.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.024 I print_info: max token length = 1024
0.00.072.025 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.342.389 I load_tensors: offloading 24 repeating layers to GPU
0.01.342.394 I load_tensors: offloading output layer to GPU
0.01.342.394 I load_tensors: offloaded 25/25 layers to GPU
0.01.342.414 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.342.416 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.343.214 I llama_context_unified: n_seq_max     = 1
0.01.343.215 I llama_context_unified: n_ctx         = 128
0.01.343.215 I llama_context_unified: n_ctx_per_seq = 128
0.01.343.215 I llama_context_unified: n_batch       = 128
0.01.343.216 I llama_context_unified: n_ubatch      = 128
0.01.343.216 I llama_context_unified: flash_attn    = 0
0.01.343.216 I llama_context_unified: freq_base     = 10000.0
0.01.343.217 I llama_context_unified: freq_scale    = 1
0.01.343.217 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.343.218 I ggml_metal_init: allocating
0.01.343.250 I ggml_metal_init: found device: Apple M4
0.01.343.256 I ggml_metal_init: picking default device: Apple M4
0.01.344.318 I ggml_metal_init: using embedded metal library
0.01.348.197 I ggml_metal_init: GPU name:   Apple M4
0.01.348.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.348.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.348.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.348.200 I ggml_metal_init: simdgroup reduction   = true
0.01.348.200 I ggml_metal_init: simdgroup matrix mul. = true
0.01.348.200 I ggml_metal_init: has residency sets    = true
0.01.348.201 I ggml_metal_init: has bfloat            = true
0.01.348.201 I ggml_metal_init: use bfloat            = true
0.01.348.201 I ggml_metal_init: hasUnifiedMemory      = true
0.01.348.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.358.621 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.360.435 I init:      Metal KV buffer size =    24.00 MiB
0.01.360.439 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.360.452 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.362.129 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.362.130 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.362.130 I llama_context_unified: graph nodes  = 967
0.01.362.131 I llama_context_unified: graph splits = 2
0.01.362.132 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.362.133 I 
0.01.362.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.362.170 I compute_imatrix: tokenizing the input ..
0.01.366.158 I compute_imatrix: tokenization took 3.988 ms
0.01.366.160 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.631.863 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.474 I llama_perf_context_print:        load time =    1612.00 ms
0.01.634.475 I llama_perf_context_print: prompt eval time =     263.96 ms /   128 tokens (    2.06 ms per token,   484.92 tokens per second)
0.01.634.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.476 I llama_perf_context_print:       total time =    1614.61 ms /   129 tokens
0.01.635.226 I ggml_metal_free: deallocating

real	0m1.832s
user	0m0.125s
sys	0m0.257s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4717 (02ef4be9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118607b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118608050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1186084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118608da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118609210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118609680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118609af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118609f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11860a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11860a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11860aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11860ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11860c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11860c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11860d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11860d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11860df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11860e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11860ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11860f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11860fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118610370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118610c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118611330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1186115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1186118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118611d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118612440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1186128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1186137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118613ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118613f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118614390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118614800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118614c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1186150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118615550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1186159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118615e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1186162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118616710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118616b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118616ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118617460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1186178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1186184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11861a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11861a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11861a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11861adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11861b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11861b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11861bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11861c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11861c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11861ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11861cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11861d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11861d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11861de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11861e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11861e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11861ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11861f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11861f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11861fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1186202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1186213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1186219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118621f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118622ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118623060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118623610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118624720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118624cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118626390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118626ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1186274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118627a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1186281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118629040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1186295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118629ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11862a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11862a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11862acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11862b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11862b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11862bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11862c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11862c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11862ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11862d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11862d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11862de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11862e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11862e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11862ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11862f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11862f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11862fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118630180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118630680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118630b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118631080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118631580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118631a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118631f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118632980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118633380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118633880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118633d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118634280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118634780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118634c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118635680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118636080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118636f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118637480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118637980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118637e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118638380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118638880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118638d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118639280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118639780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11863a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11863a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11863ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11863b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11863b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11863ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11863bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11863c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11863c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11863ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11863d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11863d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11863dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11863e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11863e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11863ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11863f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11863f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11863fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118640080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118640580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118641e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118642380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118642880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118642d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118643280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118643780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118643c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118644180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118645a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118645f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118646480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118647590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118648150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118648d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118649560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118649cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11864a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11864a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11864b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11864b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11864ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11864beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11864c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11864cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11864d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11864d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11864dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11864e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11864e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11864eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11864f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11864f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11864fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1186500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118650620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118650b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1186510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118651610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118651b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1186520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118652b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1186530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1186535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118653b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1186545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118654b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1186555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118655b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118656070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1186565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118656b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1186575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118657b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118658050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1186585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118659040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118659590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11865a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11865a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11865aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11865b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11865b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11865bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11865c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11865c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11865cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11865d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11865d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11865daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11865dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11865e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11865ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11865efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11865f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11865f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11865fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118660260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118660700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118660ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118661040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1186614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118661980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118661e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1186622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118662760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118662c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1186630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118663540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118663a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1186641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1186648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118664ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118665710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1186659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1186661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118666480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118666a90 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
0.00.697.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106f089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106f08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106f09280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106f096f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106f09b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106f09fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106f0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106f0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106f0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106f0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106f0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106f0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106f0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106f0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106f0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106f0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106f0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106f0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106f0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106f0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106f10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106f10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106f11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106f11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106f11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106f12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106f12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106f12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106f12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106f13260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106f13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106f13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106f140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106f143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106f14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106f14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106f151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106f156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106f15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106f160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106f16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106f16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106f174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106f179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106f17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106f182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106f18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106f18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106f19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106f19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106f198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106f19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106f1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106f1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106f1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106f1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106f1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106f1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106f1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106f1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106f1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106f1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106f1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106f1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106f1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106f1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106f1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106f1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106f1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106f1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106f1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106f204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106f20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106f20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106f214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106f21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106f21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106f224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106f22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106f22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106f234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106f23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106f23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106f244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106f249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106f24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106f25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106f259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106f25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106f26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106f269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106f26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106f27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106f279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106f27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106f28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106f289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106f29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106f29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106f2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106f2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106f2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106f2b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106f2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106f2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106f2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106f2c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106f2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106f2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106f2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106f2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106f2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106f2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106f2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106f2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106f2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106f2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106f30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106f306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106f30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106f31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106f314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106f31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106f31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106f322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106f32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106f33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106f33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106f339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106f33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106f34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106f34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106f350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106f35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106f35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106f35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106f36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106f36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106f36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106f37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106f375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106f37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106f37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106f383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106f38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106f38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106f391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106f39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106f39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106f3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106f3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106f3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106f3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106f3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106f3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106f3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106f3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106f3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106f3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106f3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106f3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106f3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106f3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106f3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106f3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106f3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106f3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106f3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106f3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106f400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106f40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106f409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106f40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106f41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106f417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106f41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106f42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106f425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106f42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106f42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106f43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106f43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106f43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106f44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106f44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106f44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106f450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106f45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106f45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106f45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106f46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106f47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106f47830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106f47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106f47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106f485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106f48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106f493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106f49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106f49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106f4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106f4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106f4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106f4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106f4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106f4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106f4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106f4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106f4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106f4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106f4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106f4de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106f4e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106f4e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106f4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106f4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106f4f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106f4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106f50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106f508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106f50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106f51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106f518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106f51e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106f52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106f528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106f52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106f53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106f538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106f53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106f54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106f54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106f54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106f55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106f55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106f55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106f56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106f56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106f56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106f57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106f57860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106f57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106f58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106f58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106f58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106f592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106f59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106f5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106f5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106f5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106f5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106f5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106f5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106f5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106f5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106f5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106f5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106f5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106f5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106f5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106f5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106f5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106f5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106f5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106f5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106f5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106f600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106f60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106f60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106f60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106f61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106f61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106f61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106f62480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106f62ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106f632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106f639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106f63ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106f64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106f64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106f64d60 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11861fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11861fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118627760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118624f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11862c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11862c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11862bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1186271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118626c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1186216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1186249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118623320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1186298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118646740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11862b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118626650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118621100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11862af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1186260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118620b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118623e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11862a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118625af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1186238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11862a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118666740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118647e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118648a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11864a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1186108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11860ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11861b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118665c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118619dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118627d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11864aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118649030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118666ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1186671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118667470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118667730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1186679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118667cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118667f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118668230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1186684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1186687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118668a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118668d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118668ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1186692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118669570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118669830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118669af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118669db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11866a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11866a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11866a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11866a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11866ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11866ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11866b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11866b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11866b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11866b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11866bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11866beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11866c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11866c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11866c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11866c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11866cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11866cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11866d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11866d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11866d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11866da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11866dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11866dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11866e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11866e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11866e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11866eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11866ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11866f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11866f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11866f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11866f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11866fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11866fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1186700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118670370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118670630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1186708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118670bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118670e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118671130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1186713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1186716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118671970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118671c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118671ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1186721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118672470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118672730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1186729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118672cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118672f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118673230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1186734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1186737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118673a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118673d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118673ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1186742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118674570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118674830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118674af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118674db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118675070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118675330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1186755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1186758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118675b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118675e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1186760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1186763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118676670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118676930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118676bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118676eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118677170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118677430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1186776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1186779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118677c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118677f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1186781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1186784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118678770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118678a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118678cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118678fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118679270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118679530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1186797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118679ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118679d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11867a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11867a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11867a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11867a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11867ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11867adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11867b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11867b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11867b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11867b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11867bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11867be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11867c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11867c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11867c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11867c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11867cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11867cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11867d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11867d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11867d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11867d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11867dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11867df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11867e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11867e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11867e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11867ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11867ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11867eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11867f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11867f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11867f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11867faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11867fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118680070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118680330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1186805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1186808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118680b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118680e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1186810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1186813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118681670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118681930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118681bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118681eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118682170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118682430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1186826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1186829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118682c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118682f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118683500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1186837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118683a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118683d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118684000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1186842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118684580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118684840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118684b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118684dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118685860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118685db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118686300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118686850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118686da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1186872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118687840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118687d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1186882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118688830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118688d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1186892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118689820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118689d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11868a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11868a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11868ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11868b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11868b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11868bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11868c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11868c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11868cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11868d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11868d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11868dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11868e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11868e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11868ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11868f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11868f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11868fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118690260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1186907b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118690d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118691250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1186917a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118691cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118692240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118692790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118692ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118693230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118693780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118693cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118694220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118694770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118694c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1186950b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118695550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1186959f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118695e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118696330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1186967d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118696c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118697110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1186975b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118697a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118697ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118698390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118698830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118698cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118699220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118699940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11869a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11869a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11869aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11869b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11869b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11869bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11869c220 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.747s
user	0m0.279s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4717 (02ef4be9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b109000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b109710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b109cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b10a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b10a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b10add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b10b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b10b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b10bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b10c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b10c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b10cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b10d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b10e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b10e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b10efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b10f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b10fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b110540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b110d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b111430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b111b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b112270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b112b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b113230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b1134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b113b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b114770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b114cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b114f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b115410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b1156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b115f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b1164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b116760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b116c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b1170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b117540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b1179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b117e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b118320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b1187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b118c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b119100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b1193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b1199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b119fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b11a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b11af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b11b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b11bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b11c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b11c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b11cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b11d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b11d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b11de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b11e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b11e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b11ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b11f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b11f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b11fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b11fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b120490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b120930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b120dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b121270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b121710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b121bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b122050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b1224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b122990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b122ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b123430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b123980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b123ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b124420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b124970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b124ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b125410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b125960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b125eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b126400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b126950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b126ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b1273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b127940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b127e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b1283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b128930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b128e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b1293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b129920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b129e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b12a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b12a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b11a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b12ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b12b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b12ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b12bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b12c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b12ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b12cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b12d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b12da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b12dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b12e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b12ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b12efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b12f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b12fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b12fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b130380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b130820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b130cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b131160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b131600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b131aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b131f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b1323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b132880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b132d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b1331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b133660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b133b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b133fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b134440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b1348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b134d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b135220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b1356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b135b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b136000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b1364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b136940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b136de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b137280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b137720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b137bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b138060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b138500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b1389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b138e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b1392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b139780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b139c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b13a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b13a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b13aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b13aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b13b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b13b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b13bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e05390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e05800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e05c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e060e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e06550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e069c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e06e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e072a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e07710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e0c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e0d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e11fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e12420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e12890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e13170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e13b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e14fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e15ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e16fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e19200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e1b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e1cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e1e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e25910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e25ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e26470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e27580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e27b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e2b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e2e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e2ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e30fe0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
0.00.098.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148e1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148e183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148e27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148e25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148e22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148e20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148e18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148e16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148e1b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148e1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148e21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148e1ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148e26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148e1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148e239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148e1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148e1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148e25620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148e20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148e17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148e1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148e25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148e1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148e222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148e23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148e18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148e26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148e24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148e200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148e1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148e28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148e17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148e28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148e16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148e27290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148e211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148e26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148e1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148e31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148e32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148e32550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148e32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148e32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148e33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148e337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148e33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148e34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148e344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148e34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148e34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148e35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148e356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148e35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148e36320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148e365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148e36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148e373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148e37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148e37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148e381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148e38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148e38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148e398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148e39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148e3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148e3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148e3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148e3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148e3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148e3c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148e3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148e3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148e3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148e3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148e3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148e3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148e3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148e3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148e3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148e3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148e40500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148e40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148e40fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148e414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148e41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148e41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148e424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148e42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148e42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148e43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148e43f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148e444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148e44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148e44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148e454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148e45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148e45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148e464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148e469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148e46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148e47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148e479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148e47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148e48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148e48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148e49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148e49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148e49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148e4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148e4a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148e4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148e4ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148e4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148e4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148e4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148e4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148e4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148e4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148e4ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148e4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148e4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148e4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148e4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148e4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148e4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148e4f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148e4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148e4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148e50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148e50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148e50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148e50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148e513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148e51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148e51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148e521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148e52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148e52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148e52fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148e53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148e538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148e53d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148e54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148e546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e55000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e55940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e56280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e56720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e56bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e57500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e5ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e5c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e5cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e5d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e5dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e5f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e61490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e63610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e63c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e64d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e66ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e68420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e68970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e69960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e69eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e6aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e6be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e6c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e6ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e6d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e6de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e6e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e6e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e6ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e6f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e6f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e6fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e703a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e70e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e71390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e71e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e72380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e728d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e72e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e73370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e738c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e73e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e74360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e748b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e74e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e75350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e758a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e76de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e77330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e78c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e79100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e795a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e79ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e7a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e7a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e7acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e7b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e7b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e7baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e7bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e7c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e7c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e7cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e7d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e7dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e7e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e7ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e7ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e7f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e7f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e7fdd0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b0046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b0058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b0065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b006fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b007440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b007ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b0085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b008d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b0095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b009cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b00a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b00ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b00b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b00b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b00c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b00c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b00cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b00d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b00dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b00e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b00e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b00e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b00ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b00f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b00f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b00fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b00fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b010130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b0105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b010e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b0112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b011760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b011bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b012040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b0124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b012920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b012d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b013200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b013670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b013ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b013f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b0143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b014830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b0159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b0162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b016d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b0171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b017620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b017a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b017f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b018370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b0187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b018c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b0190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b019530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b0199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b019e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b01a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b01a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b01ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b01afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b01b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b01b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b01bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b01c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b01c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b01ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b01cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b01d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b01d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b01dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b01e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b01e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b01e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b01edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b01f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b01f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b01fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b01ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b020420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b020890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b020d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b021170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b0215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b021a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b021ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b022330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b0227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b022c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b023080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b0234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b023d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b024040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b0244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b024920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b024d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b025200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b025670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b025ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b025f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b0263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b026830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b026ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b027110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b027580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b0279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b027e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b0282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b028740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b028bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b029020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b029490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b029900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b029d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b02a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b02a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b02aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b02af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b02b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b02b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b02bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b02c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b02c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b02c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b02ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b02d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b02d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b02db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b02e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b02e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b02e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b02ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b02f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b02f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b02faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b02ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b030380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b0307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b030c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b0310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b031540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b0319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b031e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b032290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b032700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b032b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b032fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b033450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b0338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b033d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b0341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b034610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b034a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b034ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b035360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b0357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b035c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b0360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b036520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b036990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b036e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b037270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b0376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b037b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b037fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b038430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b0388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b038d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b0395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b039a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b039ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b03a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b03a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b03ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b03b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b03b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b03b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b03bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b03c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b03c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b03cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b03cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b03d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b03d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b03dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b03e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b03e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b03ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b03eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b03f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b03f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b03fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b040070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b0404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b040950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b041db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b042070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b042330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b0427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b042c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b043080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b0434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b043960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b043dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b0446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b044b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b044f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b045400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b045870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b045ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b046150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b0465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b046a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b046ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b047310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b047780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b047bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b048060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b0484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b048940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b048db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b049220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b049690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b049b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b049f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b04a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b04a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b04acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b04b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b04b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b04ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b04be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b04c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b04c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b04cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b04d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b04d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b04d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b04dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b04e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b04e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b04eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b04ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b04f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b04f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b04fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b050110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b050580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b0509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b050e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b0512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b051740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b051bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b052020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b052490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b052900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b052d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b0531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b053650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b053ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b053f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b0543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b054c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b0550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b055560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b0559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b056440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b056b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b057280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b0579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b057c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b0580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b0586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b058ce0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.229s
sys	0m0.179s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
