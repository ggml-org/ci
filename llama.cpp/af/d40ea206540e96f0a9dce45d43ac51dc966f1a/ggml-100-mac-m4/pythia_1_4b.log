Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.524s
user	0m0.867s
sys	0m1.193s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target test-c
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-sampling
[ 51%] Linking CXX executable ../bin/test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gbnf-validator
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-lookup-merge
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-cli
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-perplexity
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-run
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.973s
user	0m5.963s
sys	0m9.398s

main: quantize time =  3480.37 ms
main:    total time =  3480.37 ms

main: quantize time =  1743.72 ms
main:    total time =  1743.72 ms

main: quantize time =  1667.98 ms
main:    total time =  1667.98 ms

main: quantize time =  2567.46 ms
main:    total time =  2567.46 ms

main: quantize time =  2830.59 ms
main:    total time =  2830.59 ms

main: quantize time =  5172.15 ms
main:    total time =  5172.15 ms

main: quantize time =  6308.69 ms
main:    total time =  6308.69 ms

main: quantize time =  6862.70 ms
main:    total time =  6862.70 ms

main: quantize time =  5816.69 ms
main:    total time =  5816.69 ms

main: quantize time =  4550.12 ms
main:    total time =  4550.12 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.117 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.193 I main: llama backend init
0.00.000.198 I main: load the model and apply lora adapter, if any
0.00.033.279 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.929 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.948 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.949 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.952 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.956 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.957 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.090 I llama_model_loader: - type  f32:  194 tensors
0.00.061.091 I llama_model_loader: - type  f16:   98 tensors
0.00.061.091 I print_info: file format = GGUF V3 (latest)
0.00.061.106 I print_info: file type   = all F32 (guessed)
0.00.061.109 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.296 I load: special tokens cache size = 25
0.00.087.522 I load: token to piece cache size = 0.2984 MB
0.00.087.527 I print_info: arch             = gptneox
0.00.087.528 I print_info: vocab_only       = 0
0.00.087.528 I print_info: n_ctx_train      = 2048
0.00.087.528 I print_info: n_embd           = 2048
0.00.087.528 I print_info: n_layer          = 24
0.00.087.533 I print_info: n_head           = 16
0.00.087.533 I print_info: n_head_kv        = 16
0.00.087.534 I print_info: n_rot            = 32
0.00.087.534 I print_info: n_swa            = 0
0.00.087.534 I print_info: n_embd_head_k    = 128
0.00.087.534 I print_info: n_embd_head_v    = 128
0.00.087.535 I print_info: n_gqa            = 1
0.00.087.536 I print_info: n_embd_k_gqa     = 2048
0.00.087.537 I print_info: n_embd_v_gqa     = 2048
0.00.087.537 I print_info: f_norm_eps       = 1.0e-05
0.00.087.538 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.538 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.538 I print_info: f_logit_scale    = 0.0e+00
0.00.087.539 I print_info: n_ff             = 8192
0.00.087.539 I print_info: n_expert         = 0
0.00.087.540 I print_info: n_expert_used    = 0
0.00.087.540 I print_info: causal attn      = 1
0.00.087.540 I print_info: pooling type     = 0
0.00.087.540 I print_info: rope type        = 2
0.00.087.540 I print_info: rope scaling     = linear
0.00.087.541 I print_info: freq_base_train  = 10000.0
0.00.087.541 I print_info: freq_scale_train = 1
0.00.087.541 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.541 I print_info: rope_finetuned   = unknown
0.00.087.541 I print_info: ssm_d_conv       = 0
0.00.087.542 I print_info: ssm_d_inner      = 0
0.00.087.542 I print_info: ssm_d_state      = 0
0.00.087.542 I print_info: ssm_dt_rank      = 0
0.00.087.544 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.544 I print_info: model type       = 1.4B
0.00.087.544 I print_info: model params     = 1.41 B
0.00.087.544 I print_info: general.name     = 1.4B
0.00.087.545 I print_info: vocab type       = BPE
0.00.087.545 I print_info: n_vocab          = 50304
0.00.087.545 I print_info: n_merges         = 50009
0.00.087.545 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.545 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.545 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.546 I print_info: LF token         = 128 'Ä'
0.00.087.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.547 I print_info: max token length = 1024
0.00.089.877 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.877 I load_tensors: offloading output layer to GPU
0.00.089.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.898 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.899 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.195 I llama_init_from_model: n_seq_max     = 1
0.00.090.196 I llama_init_from_model: n_ctx         = 2048
0.00.090.196 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.090.196 I llama_init_from_model: n_batch       = 2048
0.00.090.197 I llama_init_from_model: n_ubatch      = 512
0.00.090.197 I llama_init_from_model: flash_attn    = 0
0.00.090.197 I llama_init_from_model: freq_base     = 10000.0
0.00.090.198 I llama_init_from_model: freq_scale    = 1
0.00.090.198 I ggml_metal_init: allocating
0.00.090.202 I ggml_metal_init: found device: Apple M4
0.00.090.204 I ggml_metal_init: picking default device: Apple M4
0.00.090.870 I ggml_metal_init: using embedded metal library
0.00.129.412 I ggml_metal_init: GPU name:   Apple M4
0.00.129.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.129.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.129.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.129.417 I ggml_metal_init: simdgroup reduction   = true
0.00.129.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.129.417 I ggml_metal_init: has bfloat            = true
0.00.129.418 I ggml_metal_init: use bfloat            = true
0.00.129.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.129.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.212.199 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.236.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.236.860 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.236.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.237.761 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.237.763 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.237.763 I llama_init_from_model: graph nodes  = 967
0.00.237.763 I llama_init_from_model: graph splits = 2
0.00.237.766 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.237.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.237.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.328.727 I main: llama threadpool init, n_threads = 4
0.00.328.774 I 
0.00.328.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.328.802 I 
0.00.329.053 I sampler seed: 1234
0.00.329.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.329.084 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.329.086 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.329.086 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.179.555 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.02.179.556 I llama_perf_context_print:        load time =     295.44 ms
0.02.179.557 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.63 tokens per second)
0.02.179.557 I llama_perf_context_print:        eval time =    1803.84 ms /    63 runs   (   28.63 ms per token,    34.93 tokens per second)
0.02.179.558 I llama_perf_context_print:       total time =    1850.84 ms /    70 tokens
0.02.179.835 I ggml_metal_free: deallocating

real	0m2.499s
user	0m0.131s
sys	0m0.108s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.881 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.559 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.559 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.560 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.317 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.994 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.996 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.996 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.997 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.997 I llama_model_loader: - type  f32:  194 tensors
0.00.033.998 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.998 I print_info: file format = GGUF V3 (latest)
0.00.034.014 I print_info: file type   = Q8_0
0.00.034.016 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.057 I load: special tokens cache size = 25
0.00.060.294 I load: token to piece cache size = 0.2984 MB
0.00.060.299 I print_info: arch             = gptneox
0.00.060.299 I print_info: vocab_only       = 0
0.00.060.300 I print_info: n_ctx_train      = 2048
0.00.060.300 I print_info: n_embd           = 2048
0.00.060.300 I print_info: n_layer          = 24
0.00.060.306 I print_info: n_head           = 16
0.00.060.307 I print_info: n_head_kv        = 16
0.00.060.307 I print_info: n_rot            = 32
0.00.060.307 I print_info: n_swa            = 0
0.00.060.308 I print_info: n_embd_head_k    = 128
0.00.060.308 I print_info: n_embd_head_v    = 128
0.00.060.308 I print_info: n_gqa            = 1
0.00.060.309 I print_info: n_embd_k_gqa     = 2048
0.00.060.310 I print_info: n_embd_v_gqa     = 2048
0.00.060.310 I print_info: f_norm_eps       = 1.0e-05
0.00.060.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.311 I print_info: f_logit_scale    = 0.0e+00
0.00.060.314 I print_info: n_ff             = 8192
0.00.060.316 I print_info: n_expert         = 0
0.00.060.316 I print_info: n_expert_used    = 0
0.00.060.316 I print_info: causal attn      = 1
0.00.060.316 I print_info: pooling type     = 0
0.00.060.317 I print_info: rope type        = 2
0.00.060.317 I print_info: rope scaling     = linear
0.00.060.317 I print_info: freq_base_train  = 10000.0
0.00.060.318 I print_info: freq_scale_train = 1
0.00.060.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.318 I print_info: rope_finetuned   = unknown
0.00.060.318 I print_info: ssm_d_conv       = 0
0.00.060.318 I print_info: ssm_d_inner      = 0
0.00.060.320 I print_info: ssm_d_state      = 0
0.00.060.320 I print_info: ssm_dt_rank      = 0
0.00.060.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.320 I print_info: model type       = 1.4B
0.00.060.321 I print_info: model params     = 1.41 B
0.00.060.321 I print_info: general.name     = 1.4B
0.00.060.321 I print_info: vocab type       = BPE
0.00.060.322 I print_info: n_vocab          = 50304
0.00.060.322 I print_info: n_merges         = 50009
0.00.060.322 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.323 I print_info: LF token         = 128 'Ä'
0.00.060.323 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.323 I print_info: max token length = 1024
0.00.062.746 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.746 I load_tensors: offloading output layer to GPU
0.00.062.747 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.758 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.759 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.090 I llama_init_from_model: n_seq_max     = 1
0.00.063.091 I llama_init_from_model: n_ctx         = 2048
0.00.063.091 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.063.091 I llama_init_from_model: n_batch       = 2048
0.00.063.091 I llama_init_from_model: n_ubatch      = 512
0.00.063.092 I llama_init_from_model: flash_attn    = 0
0.00.063.092 I llama_init_from_model: freq_base     = 10000.0
0.00.063.092 I llama_init_from_model: freq_scale    = 1
0.00.063.092 I ggml_metal_init: allocating
0.00.063.096 I ggml_metal_init: found device: Apple M4
0.00.063.099 I ggml_metal_init: picking default device: Apple M4
0.00.063.834 I ggml_metal_init: using embedded metal library
0.00.066.413 I ggml_metal_init: GPU name:   Apple M4
0.00.066.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.416 I ggml_metal_init: simdgroup reduction   = true
0.00.066.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.416 I ggml_metal_init: has bfloat            = true
0.00.066.417 I ggml_metal_init: use bfloat            = true
0.00.066.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.869 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.690 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.008 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.011 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.011 I llama_init_from_model: graph nodes  = 967
0.00.104.011 I llama_init_from_model: graph splits = 2
0.00.104.016 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.124.787 I main: llama threadpool init, n_threads = 4
0.01.124.828 I 
0.01.124.851 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.124.851 I 
0.01.125.091 I sampler seed: 1234
0.01.125.095 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.125.106 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.125.106 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.125.106 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.218.818 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.02.218.819 I llama_perf_context_print:        load time =    1114.90 ms
0.02.218.820 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.83 tokens per second)
0.02.218.821 I llama_perf_context_print:        eval time =    1046.96 ms /    63 runs   (   16.62 ms per token,    60.17 tokens per second)
0.02.218.822 I llama_perf_context_print:       total time =    1094.03 ms /    70 tokens
0.02.219.046 I ggml_metal_free: deallocating

real	0m2.238s
user	0m0.113s
sys	0m0.210s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.017.923 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.964 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.969 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.975 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.976 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.139 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.139 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.140 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.141 I llama_model_loader: - type  f32:  194 tensors
0.00.043.141 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.141 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.142 I print_info: file format = GGUF V3 (latest)
0.00.043.155 I print_info: file type   = Q4_0
0.00.043.156 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.066.975 I load: special tokens cache size = 25
0.00.075.342 I load: token to piece cache size = 0.2984 MB
0.00.075.346 I print_info: arch             = gptneox
0.00.075.347 I print_info: vocab_only       = 0
0.00.075.347 I print_info: n_ctx_train      = 2048
0.00.075.347 I print_info: n_embd           = 2048
0.00.075.347 I print_info: n_layer          = 24
0.00.075.351 I print_info: n_head           = 16
0.00.075.352 I print_info: n_head_kv        = 16
0.00.075.355 I print_info: n_rot            = 32
0.00.075.355 I print_info: n_swa            = 0
0.00.075.355 I print_info: n_embd_head_k    = 128
0.00.075.356 I print_info: n_embd_head_v    = 128
0.00.075.356 I print_info: n_gqa            = 1
0.00.075.357 I print_info: n_embd_k_gqa     = 2048
0.00.075.358 I print_info: n_embd_v_gqa     = 2048
0.00.075.359 I print_info: f_norm_eps       = 1.0e-05
0.00.075.359 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.360 I print_info: f_logit_scale    = 0.0e+00
0.00.075.361 I print_info: n_ff             = 8192
0.00.075.361 I print_info: n_expert         = 0
0.00.075.362 I print_info: n_expert_used    = 0
0.00.075.362 I print_info: causal attn      = 1
0.00.075.362 I print_info: pooling type     = 0
0.00.075.362 I print_info: rope type        = 2
0.00.075.362 I print_info: rope scaling     = linear
0.00.075.363 I print_info: freq_base_train  = 10000.0
0.00.075.363 I print_info: freq_scale_train = 1
0.00.075.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.364 I print_info: rope_finetuned   = unknown
0.00.075.364 I print_info: ssm_d_conv       = 0
0.00.075.364 I print_info: ssm_d_inner      = 0
0.00.075.364 I print_info: ssm_d_state      = 0
0.00.075.365 I print_info: ssm_dt_rank      = 0
0.00.075.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.367 I print_info: model type       = 1.4B
0.00.075.367 I print_info: model params     = 1.41 B
0.00.075.367 I print_info: general.name     = 1.4B
0.00.075.368 I print_info: vocab type       = BPE
0.00.075.368 I print_info: n_vocab          = 50304
0.00.075.368 I print_info: n_merges         = 50009
0.00.075.370 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.370 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.371 I print_info: LF token         = 128 'Ä'
0.00.075.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.372 I print_info: max token length = 1024
0.00.077.873 I load_tensors: offloading 24 repeating layers to GPU
0.00.077.874 I load_tensors: offloading output layer to GPU
0.00.077.874 I load_tensors: offloaded 25/25 layers to GPU
0.00.077.885 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.077.886 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.078.295 I llama_init_from_model: n_seq_max     = 1
0.00.078.296 I llama_init_from_model: n_ctx         = 2048
0.00.078.297 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.078.297 I llama_init_from_model: n_batch       = 2048
0.00.078.297 I llama_init_from_model: n_ubatch      = 512
0.00.078.297 I llama_init_from_model: flash_attn    = 0
0.00.078.298 I llama_init_from_model: freq_base     = 10000.0
0.00.078.298 I llama_init_from_model: freq_scale    = 1
0.00.078.299 I ggml_metal_init: allocating
0.00.078.303 I ggml_metal_init: found device: Apple M4
0.00.078.305 I ggml_metal_init: picking default device: Apple M4
0.00.079.247 I ggml_metal_init: using embedded metal library
0.00.083.315 I ggml_metal_init: GPU name:   Apple M4
0.00.083.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.319 I ggml_metal_init: simdgroup reduction   = true
0.00.083.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.319 I ggml_metal_init: has bfloat            = true
0.00.083.319 I ggml_metal_init: use bfloat            = true
0.00.083.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.551 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.125.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.066 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.126.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.126.250 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.126.251 I llama_init_from_model: graph nodes  = 967
0.00.126.251 I llama_init_from_model: graph splits = 2
0.00.126.255 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.339 I main: llama threadpool init, n_threads = 4
0.00.652.378 I 
0.00.652.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.404 I 
0.00.652.652 I sampler seed: 1234
0.00.652.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.652.680 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.652.680 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.652.680 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.329.503 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.329.504 I llama_perf_context_print:        load time =     634.41 ms
0.01.329.504 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.47 tokens per second)
0.01.329.505 I llama_perf_context_print:        eval time =     630.22 ms /    63 runs   (   10.00 ms per token,    99.96 tokens per second)
0.01.329.507 I llama_perf_context_print:       total time =     677.17 ms /    70 tokens
0.01.329.743 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.127s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.565 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.798 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.806 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.808 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.809 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.247 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.248 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.248 I llama_model_loader: - type  f32:  194 tensors
0.00.026.248 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.249 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.249 I print_info: file format = GGUF V3 (latest)
0.00.026.261 I print_info: file type   = Q4_1
0.00.026.261 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.734 I load: special tokens cache size = 25
0.00.050.759 I load: token to piece cache size = 0.2984 MB
0.00.050.762 I print_info: arch             = gptneox
0.00.050.762 I print_info: vocab_only       = 0
0.00.050.762 I print_info: n_ctx_train      = 2048
0.00.050.763 I print_info: n_embd           = 2048
0.00.050.763 I print_info: n_layer          = 24
0.00.050.765 I print_info: n_head           = 16
0.00.050.766 I print_info: n_head_kv        = 16
0.00.050.766 I print_info: n_rot            = 32
0.00.050.766 I print_info: n_swa            = 0
0.00.050.766 I print_info: n_embd_head_k    = 128
0.00.050.768 I print_info: n_embd_head_v    = 128
0.00.050.769 I print_info: n_gqa            = 1
0.00.050.770 I print_info: n_embd_k_gqa     = 2048
0.00.050.771 I print_info: n_embd_v_gqa     = 2048
0.00.050.776 I print_info: f_norm_eps       = 1.0e-05
0.00.050.776 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.776 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.777 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.777 I print_info: f_logit_scale    = 0.0e+00
0.00.050.777 I print_info: n_ff             = 8192
0.00.050.778 I print_info: n_expert         = 0
0.00.050.778 I print_info: n_expert_used    = 0
0.00.050.780 I print_info: causal attn      = 1
0.00.050.780 I print_info: pooling type     = 0
0.00.050.780 I print_info: rope type        = 2
0.00.050.780 I print_info: rope scaling     = linear
0.00.050.780 I print_info: freq_base_train  = 10000.0
0.00.050.781 I print_info: freq_scale_train = 1
0.00.050.781 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.781 I print_info: rope_finetuned   = unknown
0.00.050.781 I print_info: ssm_d_conv       = 0
0.00.050.781 I print_info: ssm_d_inner      = 0
0.00.050.781 I print_info: ssm_d_state      = 0
0.00.050.782 I print_info: ssm_dt_rank      = 0
0.00.050.782 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.782 I print_info: model type       = 1.4B
0.00.050.784 I print_info: model params     = 1.41 B
0.00.050.784 I print_info: general.name     = 1.4B
0.00.050.784 I print_info: vocab type       = BPE
0.00.050.784 I print_info: n_vocab          = 50304
0.00.050.786 I print_info: n_merges         = 50009
0.00.050.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.786 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.786 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.786 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.787 I print_info: LF token         = 128 'Ä'
0.00.050.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.787 I print_info: max token length = 1024
0.00.052.733 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.733 I load_tensors: offloading output layer to GPU
0.00.052.733 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.744 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.745 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.020 I llama_init_from_model: n_seq_max     = 1
0.00.053.020 I llama_init_from_model: n_ctx         = 2048
0.00.053.020 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.021 I llama_init_from_model: n_batch       = 2048
0.00.053.021 I llama_init_from_model: n_ubatch      = 512
0.00.053.021 I llama_init_from_model: flash_attn    = 0
0.00.053.021 I llama_init_from_model: freq_base     = 10000.0
0.00.053.022 I llama_init_from_model: freq_scale    = 1
0.00.053.022 I ggml_metal_init: allocating
0.00.053.025 I ggml_metal_init: found device: Apple M4
0.00.053.027 I ggml_metal_init: picking default device: Apple M4
0.00.053.620 I ggml_metal_init: using embedded metal library
0.00.055.943 I ggml_metal_init: GPU name:   Apple M4
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.945 I ggml_metal_init: simdgroup reduction   = true
0.00.055.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.945 I ggml_metal_init: has bfloat            = true
0.00.055.945 I ggml_metal_init: use bfloat            = true
0.00.055.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.185 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.194 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.168 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.170 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.170 I llama_init_from_model: graph nodes  = 967
0.00.086.170 I llama_init_from_model: graph splits = 2
0.00.086.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.010 I main: llama threadpool init, n_threads = 4
0.00.720.051 I 
0.00.720.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.076 I 
0.00.720.316 I sampler seed: 1234
0.00.720.321 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.362 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.363 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.363 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.446.593 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66541.71 tokens per second)
0.01.446.594 I llama_perf_context_print:        load time =     711.44 ms
0.01.446.594 I llama_perf_context_print: prompt eval time =      46.30 ms /     7 tokens (    6.61 ms per token,   151.19 tokens per second)
0.01.446.595 I llama_perf_context_print:        eval time =     677.08 ms /    63 runs   (   10.75 ms per token,    93.05 tokens per second)
0.01.446.596 I llama_perf_context_print:       total time =     726.59 ms /    70 tokens
0.01.446.813 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.107s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.015.959 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.037.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.049.033 I llama_model_loader: - type  f32:  194 tensors
0.00.049.033 I llama_model_loader: - type q5_0:   97 tensors
0.00.049.034 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.034 I print_info: file format = GGUF V3 (latest)
0.00.049.047 I print_info: file type   = Q5_0
0.00.049.048 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.084.432 I load: special tokens cache size = 25
0.00.094.296 I load: token to piece cache size = 0.2984 MB
0.00.094.300 I print_info: arch             = gptneox
0.00.094.300 I print_info: vocab_only       = 0
0.00.094.301 I print_info: n_ctx_train      = 2048
0.00.094.301 I print_info: n_embd           = 2048
0.00.094.301 I print_info: n_layer          = 24
0.00.094.305 I print_info: n_head           = 16
0.00.094.306 I print_info: n_head_kv        = 16
0.00.094.306 I print_info: n_rot            = 32
0.00.094.306 I print_info: n_swa            = 0
0.00.094.307 I print_info: n_embd_head_k    = 128
0.00.094.307 I print_info: n_embd_head_v    = 128
0.00.094.308 I print_info: n_gqa            = 1
0.00.094.308 I print_info: n_embd_k_gqa     = 2048
0.00.094.310 I print_info: n_embd_v_gqa     = 2048
0.00.094.311 I print_info: f_norm_eps       = 1.0e-05
0.00.094.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.094.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.094.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.094.312 I print_info: f_logit_scale    = 0.0e+00
0.00.094.313 I print_info: n_ff             = 8192
0.00.094.315 I print_info: n_expert         = 0
0.00.094.316 I print_info: n_expert_used    = 0
0.00.094.317 I print_info: causal attn      = 1
0.00.094.318 I print_info: pooling type     = 0
0.00.094.318 I print_info: rope type        = 2
0.00.094.318 I print_info: rope scaling     = linear
0.00.094.318 I print_info: freq_base_train  = 10000.0
0.00.094.319 I print_info: freq_scale_train = 1
0.00.094.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.094.319 I print_info: rope_finetuned   = unknown
0.00.094.319 I print_info: ssm_d_conv       = 0
0.00.094.320 I print_info: ssm_d_inner      = 0
0.00.094.320 I print_info: ssm_d_state      = 0
0.00.094.320 I print_info: ssm_dt_rank      = 0
0.00.094.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.320 I print_info: model type       = 1.4B
0.00.094.321 I print_info: model params     = 1.41 B
0.00.094.321 I print_info: general.name     = 1.4B
0.00.094.321 I print_info: vocab type       = BPE
0.00.094.322 I print_info: n_vocab          = 50304
0.00.094.322 I print_info: n_merges         = 50009
0.00.094.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.327 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.327 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.327 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.330 I print_info: LF token         = 128 'Ä'
0.00.094.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.330 I print_info: max token length = 1024
0.00.097.049 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.050 I load_tensors: offloading output layer to GPU
0.00.097.050 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.061 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.097.063 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.097.603 I llama_init_from_model: n_seq_max     = 1
0.00.097.605 I llama_init_from_model: n_ctx         = 2048
0.00.097.605 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.097.605 I llama_init_from_model: n_batch       = 2048
0.00.097.605 I llama_init_from_model: n_ubatch      = 512
0.00.097.606 I llama_init_from_model: flash_attn    = 0
0.00.097.606 I llama_init_from_model: freq_base     = 10000.0
0.00.097.606 I llama_init_from_model: freq_scale    = 1
0.00.097.607 I ggml_metal_init: allocating
0.00.097.611 I ggml_metal_init: found device: Apple M4
0.00.097.614 I ggml_metal_init: picking default device: Apple M4
0.00.098.464 I ggml_metal_init: using embedded metal library
0.00.101.967 I ggml_metal_init: GPU name:   Apple M4
0.00.101.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.971 I ggml_metal_init: simdgroup reduction   = true
0.00.101.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.971 I ggml_metal_init: has bfloat            = true
0.00.101.971 I ggml_metal_init: use bfloat            = true
0.00.101.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.143 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.064 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.092 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.136.091 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.136.093 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.136.093 I llama_init_from_model: graph nodes  = 967
0.00.136.093 I llama_init_from_model: graph splits = 2
0.00.136.099 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.136.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.009.345 I main: llama threadpool init, n_threads = 4
0.01.009.413 I 
0.01.009.472 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.009.475 I 
0.01.010.047 I sampler seed: 1234
0.01.010.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.010.146 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.010.153 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.010.153 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.831.688 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.831.689 I llama_perf_context_print:        load time =     993.38 ms
0.01.831.694 I llama_perf_context_print: prompt eval time =      52.27 ms /     7 tokens (    7.47 ms per token,   133.91 tokens per second)
0.01.831.694 I llama_perf_context_print:        eval time =     766.22 ms /    63 runs   (   12.16 ms per token,    82.22 tokens per second)
0.01.831.695 I llama_perf_context_print:       total time =     822.35 ms /    70 tokens
0.01.832.013 I ggml_metal_free: deallocating

real	0m1.862s
user	0m0.148s
sys	0m0.197s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.626 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.028.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.352 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.355 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.360 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.138 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.864 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.865 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.867 I llama_model_loader: - type  f32:  194 tensors
0.00.036.867 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.868 I print_info: file format = GGUF V3 (latest)
0.00.036.880 I print_info: file type   = Q5_1
0.00.036.881 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.057.444 I load: special tokens cache size = 25
0.00.063.630 I load: token to piece cache size = 0.2984 MB
0.00.063.633 I print_info: arch             = gptneox
0.00.063.633 I print_info: vocab_only       = 0
0.00.063.633 I print_info: n_ctx_train      = 2048
0.00.063.633 I print_info: n_embd           = 2048
0.00.063.634 I print_info: n_layer          = 24
0.00.063.637 I print_info: n_head           = 16
0.00.063.638 I print_info: n_head_kv        = 16
0.00.063.638 I print_info: n_rot            = 32
0.00.063.638 I print_info: n_swa            = 0
0.00.063.638 I print_info: n_embd_head_k    = 128
0.00.063.640 I print_info: n_embd_head_v    = 128
0.00.063.641 I print_info: n_gqa            = 1
0.00.063.642 I print_info: n_embd_k_gqa     = 2048
0.00.063.644 I print_info: n_embd_v_gqa     = 2048
0.00.063.645 I print_info: f_norm_eps       = 1.0e-05
0.00.063.646 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.648 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.648 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.648 I print_info: f_logit_scale    = 0.0e+00
0.00.063.649 I print_info: n_ff             = 8192
0.00.063.649 I print_info: n_expert         = 0
0.00.063.649 I print_info: n_expert_used    = 0
0.00.063.649 I print_info: causal attn      = 1
0.00.063.649 I print_info: pooling type     = 0
0.00.063.649 I print_info: rope type        = 2
0.00.063.650 I print_info: rope scaling     = linear
0.00.063.650 I print_info: freq_base_train  = 10000.0
0.00.063.650 I print_info: freq_scale_train = 1
0.00.063.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.650 I print_info: rope_finetuned   = unknown
0.00.063.651 I print_info: ssm_d_conv       = 0
0.00.063.651 I print_info: ssm_d_inner      = 0
0.00.063.654 I print_info: ssm_d_state      = 0
0.00.063.654 I print_info: ssm_dt_rank      = 0
0.00.063.654 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.654 I print_info: model type       = 1.4B
0.00.063.655 I print_info: model params     = 1.41 B
0.00.063.656 I print_info: general.name     = 1.4B
0.00.063.657 I print_info: vocab type       = BPE
0.00.063.657 I print_info: n_vocab          = 50304
0.00.063.657 I print_info: n_merges         = 50009
0.00.063.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.657 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.657 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.657 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.658 I print_info: LF token         = 128 'Ä'
0.00.063.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.658 I print_info: max token length = 1024
0.00.065.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.353 I load_tensors: offloading output layer to GPU
0.00.065.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.364 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.065.365 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.065.665 I llama_init_from_model: n_seq_max     = 1
0.00.065.666 I llama_init_from_model: n_ctx         = 2048
0.00.065.666 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.666 I llama_init_from_model: n_batch       = 2048
0.00.065.666 I llama_init_from_model: n_ubatch      = 512
0.00.065.666 I llama_init_from_model: flash_attn    = 0
0.00.065.667 I llama_init_from_model: freq_base     = 10000.0
0.00.065.667 I llama_init_from_model: freq_scale    = 1
0.00.065.667 I ggml_metal_init: allocating
0.00.065.670 I ggml_metal_init: found device: Apple M4
0.00.065.673 I ggml_metal_init: picking default device: Apple M4
0.00.066.271 I ggml_metal_init: using embedded metal library
0.00.068.616 I ggml_metal_init: GPU name:   Apple M4
0.00.068.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.619 I ggml_metal_init: simdgroup reduction   = true
0.00.068.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.619 I ggml_metal_init: has bfloat            = true
0.00.068.620 I ggml_metal_init: use bfloat            = true
0.00.068.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.621 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.890 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.523 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.532 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.557 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.487 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.489 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.489 I llama_init_from_model: graph nodes  = 967
0.00.099.489 I llama_init_from_model: graph splits = 2
0.00.099.493 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.850.741 I main: llama threadpool init, n_threads = 4
0.00.850.784 I 
0.00.850.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.850.807 I 
0.00.850.970 I sampler seed: 1234
0.00.850.975 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.851.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.851.014 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.851.014 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.732.115 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.732.116 I llama_perf_context_print:        load time =     841.11 ms
0.01.732.117 I llama_perf_context_print: prompt eval time =      42.34 ms /     7 tokens (    6.05 ms per token,   165.31 tokens per second)
0.01.732.117 I llama_perf_context_print:        eval time =     835.78 ms /    63 runs   (   13.27 ms per token,    75.38 tokens per second)
0.01.732.118 I llama_perf_context_print:       total time =     881.38 ms /    70 tokens
0.01.732.325 I ggml_metal_free: deallocating

real	0m1.749s
user	0m0.112s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.016.636 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.016 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.016 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.017 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.018 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.018 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.019 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.019 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.020 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.022 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.022 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.855 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.032.731 I llama_model_loader: - type  f32:  194 tensors
0.00.032.731 I llama_model_loader: - type q2_K:   49 tensors
0.00.032.732 I llama_model_loader: - type q3_K:   48 tensors
0.00.032.732 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.732 I print_info: file format = GGUF V3 (latest)
0.00.032.744 I print_info: file type   = Q2_K - Medium
0.00.032.745 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.054.376 I load: special tokens cache size = 25
0.00.060.020 I load: token to piece cache size = 0.2984 MB
0.00.060.023 I print_info: arch             = gptneox
0.00.060.023 I print_info: vocab_only       = 0
0.00.060.023 I print_info: n_ctx_train      = 2048
0.00.060.024 I print_info: n_embd           = 2048
0.00.060.024 I print_info: n_layer          = 24
0.00.060.027 I print_info: n_head           = 16
0.00.060.027 I print_info: n_head_kv        = 16
0.00.060.027 I print_info: n_rot            = 32
0.00.060.028 I print_info: n_swa            = 0
0.00.060.029 I print_info: n_embd_head_k    = 128
0.00.060.029 I print_info: n_embd_head_v    = 128
0.00.060.030 I print_info: n_gqa            = 1
0.00.060.031 I print_info: n_embd_k_gqa     = 2048
0.00.060.032 I print_info: n_embd_v_gqa     = 2048
0.00.060.032 I print_info: f_norm_eps       = 1.0e-05
0.00.060.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.035 I print_info: f_logit_scale    = 0.0e+00
0.00.060.035 I print_info: n_ff             = 8192
0.00.060.036 I print_info: n_expert         = 0
0.00.060.036 I print_info: n_expert_used    = 0
0.00.060.037 I print_info: causal attn      = 1
0.00.060.037 I print_info: pooling type     = 0
0.00.060.037 I print_info: rope type        = 2
0.00.060.038 I print_info: rope scaling     = linear
0.00.060.038 I print_info: freq_base_train  = 10000.0
0.00.060.040 I print_info: freq_scale_train = 1
0.00.060.040 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.040 I print_info: rope_finetuned   = unknown
0.00.060.040 I print_info: ssm_d_conv       = 0
0.00.060.040 I print_info: ssm_d_inner      = 0
0.00.060.041 I print_info: ssm_d_state      = 0
0.00.060.041 I print_info: ssm_dt_rank      = 0
0.00.060.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.042 I print_info: model type       = 1.4B
0.00.060.043 I print_info: model params     = 1.41 B
0.00.060.043 I print_info: general.name     = 1.4B
0.00.060.044 I print_info: vocab type       = BPE
0.00.060.044 I print_info: n_vocab          = 50304
0.00.060.044 I print_info: n_merges         = 50009
0.00.060.044 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.044 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.045 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.045 I print_info: LF token         = 128 'Ä'
0.00.060.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.046 I print_info: max token length = 1024
0.00.061.662 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.662 I load_tensors: offloading output layer to GPU
0.00.061.662 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.673 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.061.674 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.061.933 I llama_init_from_model: n_seq_max     = 1
0.00.061.934 I llama_init_from_model: n_ctx         = 2048
0.00.061.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.934 I llama_init_from_model: n_batch       = 2048
0.00.061.934 I llama_init_from_model: n_ubatch      = 512
0.00.061.934 I llama_init_from_model: flash_attn    = 0
0.00.061.935 I llama_init_from_model: freq_base     = 10000.0
0.00.061.935 I llama_init_from_model: freq_scale    = 1
0.00.061.936 I ggml_metal_init: allocating
0.00.061.939 I ggml_metal_init: found device: Apple M4
0.00.061.941 I ggml_metal_init: picking default device: Apple M4
0.00.062.548 I ggml_metal_init: using embedded metal library
0.00.065.049 I ggml_metal_init: GPU name:   Apple M4
0.00.065.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.051 I ggml_metal_init: simdgroup reduction   = true
0.00.065.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.052 I ggml_metal_init: has bfloat            = true
0.00.065.052 I ggml_metal_init: use bfloat            = true
0.00.065.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.450 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.459 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.483 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.638 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.640 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.640 I llama_init_from_model: graph nodes  = 967
0.00.097.641 I llama_init_from_model: graph splits = 2
0.00.097.644 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.485.931 I main: llama threadpool init, n_threads = 4
0.00.485.979 I 
0.00.486.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.018 I 
0.00.486.186 I sampler seed: 1234
0.00.486.191 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.486.222 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.486.224 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.486.224 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.177.501 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.177.503 I llama_perf_context_print:        load time =     469.29 ms
0.01.177.503 I llama_perf_context_print: prompt eval time =      35.92 ms /     7 tokens (    5.13 ms per token,   194.89 tokens per second)
0.01.177.504 I llama_perf_context_print:        eval time =     652.25 ms /    63 runs   (   10.35 ms per token,    96.59 tokens per second)
0.01.177.504 I llama_perf_context_print:       total time =     691.58 ms /    70 tokens
0.01.177.699 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.112s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.320 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.191 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.027.195 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.197 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.197 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.198 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.198 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.199 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.036.270 I llama_model_loader: - type  f32:  194 tensors
0.00.036.270 I llama_model_loader: - type q3_K:   25 tensors
0.00.036.271 I llama_model_loader: - type q4_K:   71 tensors
0.00.036.271 I llama_model_loader: - type q5_K:    1 tensors
0.00.036.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.272 I print_info: file format = GGUF V3 (latest)
0.00.036.283 I print_info: file type   = Q3_K - Medium
0.00.036.284 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.058.582 I load: special tokens cache size = 25
0.00.065.460 I load: token to piece cache size = 0.2984 MB
0.00.065.462 I print_info: arch             = gptneox
0.00.065.463 I print_info: vocab_only       = 0
0.00.065.463 I print_info: n_ctx_train      = 2048
0.00.065.463 I print_info: n_embd           = 2048
0.00.065.463 I print_info: n_layer          = 24
0.00.065.466 I print_info: n_head           = 16
0.00.065.467 I print_info: n_head_kv        = 16
0.00.065.467 I print_info: n_rot            = 32
0.00.065.467 I print_info: n_swa            = 0
0.00.065.467 I print_info: n_embd_head_k    = 128
0.00.065.467 I print_info: n_embd_head_v    = 128
0.00.065.468 I print_info: n_gqa            = 1
0.00.065.468 I print_info: n_embd_k_gqa     = 2048
0.00.065.469 I print_info: n_embd_v_gqa     = 2048
0.00.065.469 I print_info: f_norm_eps       = 1.0e-05
0.00.065.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.470 I print_info: f_logit_scale    = 0.0e+00
0.00.065.471 I print_info: n_ff             = 8192
0.00.065.472 I print_info: n_expert         = 0
0.00.065.473 I print_info: n_expert_used    = 0
0.00.065.473 I print_info: causal attn      = 1
0.00.065.473 I print_info: pooling type     = 0
0.00.065.474 I print_info: rope type        = 2
0.00.065.474 I print_info: rope scaling     = linear
0.00.065.474 I print_info: freq_base_train  = 10000.0
0.00.065.474 I print_info: freq_scale_train = 1
0.00.065.475 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.475 I print_info: rope_finetuned   = unknown
0.00.065.475 I print_info: ssm_d_conv       = 0
0.00.065.475 I print_info: ssm_d_inner      = 0
0.00.065.475 I print_info: ssm_d_state      = 0
0.00.065.475 I print_info: ssm_dt_rank      = 0
0.00.065.477 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.477 I print_info: model type       = 1.4B
0.00.065.477 I print_info: model params     = 1.41 B
0.00.065.478 I print_info: general.name     = 1.4B
0.00.065.478 I print_info: vocab type       = BPE
0.00.065.478 I print_info: n_vocab          = 50304
0.00.065.478 I print_info: n_merges         = 50009
0.00.065.479 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.479 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.479 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.479 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.480 I print_info: LF token         = 128 'Ä'
0.00.065.480 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.481 I print_info: max token length = 1024
0.00.067.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.170 I load_tensors: offloading output layer to GPU
0.00.067.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.180 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.067.181 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.067.463 I llama_init_from_model: n_seq_max     = 1
0.00.067.464 I llama_init_from_model: n_ctx         = 2048
0.00.067.464 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.464 I llama_init_from_model: n_batch       = 2048
0.00.067.465 I llama_init_from_model: n_ubatch      = 512
0.00.067.465 I llama_init_from_model: flash_attn    = 0
0.00.067.465 I llama_init_from_model: freq_base     = 10000.0
0.00.067.465 I llama_init_from_model: freq_scale    = 1
0.00.067.466 I ggml_metal_init: allocating
0.00.067.468 I ggml_metal_init: found device: Apple M4
0.00.067.470 I ggml_metal_init: picking default device: Apple M4
0.00.068.086 I ggml_metal_init: using embedded metal library
0.00.070.653 I ggml_metal_init: GPU name:   Apple M4
0.00.070.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.655 I ggml_metal_init: simdgroup reduction   = true
0.00.070.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.655 I ggml_metal_init: has bfloat            = true
0.00.070.655 I ggml_metal_init: use bfloat            = true
0.00.070.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.500 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.456 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.462 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.482 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.551 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.553 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.553 I llama_init_from_model: graph nodes  = 967
0.00.102.553 I llama_init_from_model: graph splits = 2
0.00.102.556 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.681 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.469 I main: llama threadpool init, n_threads = 4
0.00.570.511 I 
0.00.570.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.536 I 
0.00.570.774 I sampler seed: 1234
0.00.570.778 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.570.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.570.816 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.570.816 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.316.091 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.316.092 I llama_perf_context_print:        load time =     559.14 ms
0.01.316.093 I llama_perf_context_print: prompt eval time =      44.55 ms /     7 tokens (    6.36 ms per token,   157.13 tokens per second)
0.01.316.094 I llama_perf_context_print:        eval time =     697.62 ms /    63 runs   (   11.07 ms per token,    90.31 tokens per second)
0.01.316.095 I llama_perf_context_print:       total time =     745.62 ms /    70 tokens
0.01.316.316 I ggml_metal_free: deallocating

real	0m1.332s
user	0m0.114s
sys	0m0.135s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.662 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.125 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.126 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.126 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.127 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.577 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.578 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.579 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.579 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.580 I llama_model_loader: - type  f32:  194 tensors
0.00.024.580 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.580 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.580 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.581 I print_info: file format = GGUF V3 (latest)
0.00.024.587 I print_info: file type   = Q4_K - Medium
0.00.024.588 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.179 I load: special tokens cache size = 25
0.00.049.188 I load: token to piece cache size = 0.2984 MB
0.00.049.192 I print_info: arch             = gptneox
0.00.049.192 I print_info: vocab_only       = 0
0.00.049.192 I print_info: n_ctx_train      = 2048
0.00.049.192 I print_info: n_embd           = 2048
0.00.049.192 I print_info: n_layer          = 24
0.00.049.195 I print_info: n_head           = 16
0.00.049.196 I print_info: n_head_kv        = 16
0.00.049.196 I print_info: n_rot            = 32
0.00.049.197 I print_info: n_swa            = 0
0.00.049.197 I print_info: n_embd_head_k    = 128
0.00.049.197 I print_info: n_embd_head_v    = 128
0.00.049.198 I print_info: n_gqa            = 1
0.00.049.198 I print_info: n_embd_k_gqa     = 2048
0.00.049.202 I print_info: n_embd_v_gqa     = 2048
0.00.049.202 I print_info: f_norm_eps       = 1.0e-05
0.00.049.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.203 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.203 I print_info: f_logit_scale    = 0.0e+00
0.00.049.204 I print_info: n_ff             = 8192
0.00.049.204 I print_info: n_expert         = 0
0.00.049.206 I print_info: n_expert_used    = 0
0.00.049.207 I print_info: causal attn      = 1
0.00.049.207 I print_info: pooling type     = 0
0.00.049.207 I print_info: rope type        = 2
0.00.049.207 I print_info: rope scaling     = linear
0.00.049.208 I print_info: freq_base_train  = 10000.0
0.00.049.208 I print_info: freq_scale_train = 1
0.00.049.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.209 I print_info: rope_finetuned   = unknown
0.00.049.209 I print_info: ssm_d_conv       = 0
0.00.049.209 I print_info: ssm_d_inner      = 0
0.00.049.209 I print_info: ssm_d_state      = 0
0.00.049.209 I print_info: ssm_dt_rank      = 0
0.00.049.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.209 I print_info: model type       = 1.4B
0.00.049.214 I print_info: model params     = 1.41 B
0.00.049.215 I print_info: general.name     = 1.4B
0.00.049.216 I print_info: vocab type       = BPE
0.00.049.216 I print_info: n_vocab          = 50304
0.00.049.216 I print_info: n_merges         = 50009
0.00.049.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.219 I print_info: LF token         = 128 'Ä'
0.00.049.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.219 I print_info: max token length = 1024
0.00.051.112 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.112 I load_tensors: offloading output layer to GPU
0.00.051.112 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.123 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.124 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.402 I llama_init_from_model: n_seq_max     = 1
0.00.051.402 I llama_init_from_model: n_ctx         = 2048
0.00.051.403 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.403 I llama_init_from_model: n_batch       = 2048
0.00.051.403 I llama_init_from_model: n_ubatch      = 512
0.00.051.403 I llama_init_from_model: flash_attn    = 0
0.00.051.403 I llama_init_from_model: freq_base     = 10000.0
0.00.051.404 I llama_init_from_model: freq_scale    = 1
0.00.051.404 I ggml_metal_init: allocating
0.00.051.407 I ggml_metal_init: found device: Apple M4
0.00.051.409 I ggml_metal_init: picking default device: Apple M4
0.00.051.976 I ggml_metal_init: using embedded metal library
0.00.054.280 I ggml_metal_init: GPU name:   Apple M4
0.00.054.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.282 I ggml_metal_init: simdgroup reduction   = true
0.00.054.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.283 I ggml_metal_init: has bfloat            = true
0.00.054.283 I ggml_metal_init: use bfloat            = true
0.00.054.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.779 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.039 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.997 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.999 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.999 I llama_init_from_model: graph nodes  = 967
0.00.084.999 I llama_init_from_model: graph splits = 2
0.00.085.002 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.399 I main: llama threadpool init, n_threads = 4
0.00.617.441 I 
0.00.617.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.465 I 
0.00.617.694 I sampler seed: 1234
0.00.617.698 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.740 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.742 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.372.930 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.372.931 I llama_perf_context_print:        load time =     608.73 ms
0.01.372.931 I llama_perf_context_print: prompt eval time =      51.02 ms /     7 tokens (    7.29 ms per token,   137.19 tokens per second)
0.01.372.932 I llama_perf_context_print:        eval time =     701.15 ms /    63 runs   (   11.13 ms per token,    89.85 tokens per second)
0.01.372.933 I llama_perf_context_print:       total time =     755.54 ms /    70 tokens
0.01.373.152 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.107s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.834 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.520 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.532 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.534 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.535 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.535 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.538 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.329 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.080 I llama_model_loader: - type  f32:  194 tensors
0.00.026.081 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.081 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.081 I print_info: file format = GGUF V3 (latest)
0.00.026.086 I print_info: file type   = Q5_K - Medium
0.00.026.087 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.516 I load: special tokens cache size = 25
0.00.050.521 I load: token to piece cache size = 0.2984 MB
0.00.050.524 I print_info: arch             = gptneox
0.00.050.524 I print_info: vocab_only       = 0
0.00.050.524 I print_info: n_ctx_train      = 2048
0.00.050.525 I print_info: n_embd           = 2048
0.00.050.525 I print_info: n_layer          = 24
0.00.050.527 I print_info: n_head           = 16
0.00.050.528 I print_info: n_head_kv        = 16
0.00.050.528 I print_info: n_rot            = 32
0.00.050.528 I print_info: n_swa            = 0
0.00.050.528 I print_info: n_embd_head_k    = 128
0.00.050.537 I print_info: n_embd_head_v    = 128
0.00.050.540 I print_info: n_gqa            = 1
0.00.050.541 I print_info: n_embd_k_gqa     = 2048
0.00.050.542 I print_info: n_embd_v_gqa     = 2048
0.00.050.542 I print_info: f_norm_eps       = 1.0e-05
0.00.050.543 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.545 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.545 I print_info: f_logit_scale    = 0.0e+00
0.00.050.546 I print_info: n_ff             = 8192
0.00.050.546 I print_info: n_expert         = 0
0.00.050.547 I print_info: n_expert_used    = 0
0.00.050.549 I print_info: causal attn      = 1
0.00.050.550 I print_info: pooling type     = 0
0.00.050.550 I print_info: rope type        = 2
0.00.050.550 I print_info: rope scaling     = linear
0.00.050.551 I print_info: freq_base_train  = 10000.0
0.00.050.551 I print_info: freq_scale_train = 1
0.00.050.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.551 I print_info: rope_finetuned   = unknown
0.00.050.552 I print_info: ssm_d_conv       = 0
0.00.050.552 I print_info: ssm_d_inner      = 0
0.00.050.552 I print_info: ssm_d_state      = 0
0.00.050.552 I print_info: ssm_dt_rank      = 0
0.00.050.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.553 I print_info: model type       = 1.4B
0.00.050.553 I print_info: model params     = 1.41 B
0.00.050.554 I print_info: general.name     = 1.4B
0.00.050.555 I print_info: vocab type       = BPE
0.00.050.555 I print_info: n_vocab          = 50304
0.00.050.555 I print_info: n_merges         = 50009
0.00.050.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: LF token         = 128 'Ä'
0.00.050.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.559 I print_info: max token length = 1024
0.00.052.507 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.507 I load_tensors: offloading output layer to GPU
0.00.052.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.518 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.519 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.796 I llama_init_from_model: n_seq_max     = 1
0.00.052.797 I llama_init_from_model: n_ctx         = 2048
0.00.052.797 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.797 I llama_init_from_model: n_batch       = 2048
0.00.052.798 I llama_init_from_model: n_ubatch      = 512
0.00.052.798 I llama_init_from_model: flash_attn    = 0
0.00.052.798 I llama_init_from_model: freq_base     = 10000.0
0.00.052.798 I llama_init_from_model: freq_scale    = 1
0.00.052.799 I ggml_metal_init: allocating
0.00.052.802 I ggml_metal_init: found device: Apple M4
0.00.052.803 I ggml_metal_init: picking default device: Apple M4
0.00.053.382 I ggml_metal_init: using embedded metal library
0.00.055.709 I ggml_metal_init: GPU name:   Apple M4
0.00.055.711 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.712 I ggml_metal_init: simdgroup reduction   = true
0.00.055.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.712 I ggml_metal_init: has bfloat            = true
0.00.055.712 I ggml_metal_init: use bfloat            = true
0.00.055.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.237 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.445 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.452 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.539 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.541 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.541 I llama_init_from_model: graph nodes  = 967
0.00.086.541 I llama_init_from_model: graph splits = 2
0.00.086.545 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.680 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.066 I main: llama threadpool init, n_threads = 4
0.00.697.107 I 
0.00.697.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.132 I 
0.00.697.352 I sampler seed: 1234
0.00.697.356 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.391 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.395 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.395 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.549.870 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.549.870 I llama_perf_context_print:        load time =     687.23 ms
0.01.549.871 I llama_perf_context_print: prompt eval time =      55.51 ms /     7 tokens (    7.93 ms per token,   126.10 tokens per second)
0.01.549.872 I llama_perf_context_print:        eval time =     793.90 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.549.872 I llama_perf_context_print:       total time =     852.81 ms /    70 tokens
0.01.550.076 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.723 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.198 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.694 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.696 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.697 I llama_model_loader: - type  f32:  194 tensors
0.00.024.697 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.698 I print_info: file format = GGUF V3 (latest)
0.00.024.703 I print_info: file type   = Q6_K
0.00.024.704 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.230 I load: special tokens cache size = 25
0.00.049.148 I load: token to piece cache size = 0.2984 MB
0.00.049.151 I print_info: arch             = gptneox
0.00.049.151 I print_info: vocab_only       = 0
0.00.049.151 I print_info: n_ctx_train      = 2048
0.00.049.151 I print_info: n_embd           = 2048
0.00.049.151 I print_info: n_layer          = 24
0.00.049.154 I print_info: n_head           = 16
0.00.049.155 I print_info: n_head_kv        = 16
0.00.049.155 I print_info: n_rot            = 32
0.00.049.158 I print_info: n_swa            = 0
0.00.049.158 I print_info: n_embd_head_k    = 128
0.00.049.158 I print_info: n_embd_head_v    = 128
0.00.049.159 I print_info: n_gqa            = 1
0.00.049.159 I print_info: n_embd_k_gqa     = 2048
0.00.049.160 I print_info: n_embd_v_gqa     = 2048
0.00.049.161 I print_info: f_norm_eps       = 1.0e-05
0.00.049.161 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.162 I print_info: f_logit_scale    = 0.0e+00
0.00.049.162 I print_info: n_ff             = 8192
0.00.049.162 I print_info: n_expert         = 0
0.00.049.163 I print_info: n_expert_used    = 0
0.00.049.163 I print_info: causal attn      = 1
0.00.049.164 I print_info: pooling type     = 0
0.00.049.168 I print_info: rope type        = 2
0.00.049.168 I print_info: rope scaling     = linear
0.00.049.168 I print_info: freq_base_train  = 10000.0
0.00.049.169 I print_info: freq_scale_train = 1
0.00.049.169 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.169 I print_info: rope_finetuned   = unknown
0.00.049.169 I print_info: ssm_d_conv       = 0
0.00.049.170 I print_info: ssm_d_inner      = 0
0.00.049.170 I print_info: ssm_d_state      = 0
0.00.049.170 I print_info: ssm_dt_rank      = 0
0.00.049.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.170 I print_info: model type       = 1.4B
0.00.049.171 I print_info: model params     = 1.41 B
0.00.049.171 I print_info: general.name     = 1.4B
0.00.049.171 I print_info: vocab type       = BPE
0.00.049.172 I print_info: n_vocab          = 50304
0.00.049.172 I print_info: n_merges         = 50009
0.00.049.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.178 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.180 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.180 I print_info: LF token         = 128 'Ä'
0.00.049.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.180 I print_info: max token length = 1024
0.00.051.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.164 I load_tensors: offloading output layer to GPU
0.00.051.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.175 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.176 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.468 I llama_init_from_model: n_seq_max     = 1
0.00.051.468 I llama_init_from_model: n_ctx         = 2048
0.00.051.469 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.469 I llama_init_from_model: n_batch       = 2048
0.00.051.469 I llama_init_from_model: n_ubatch      = 512
0.00.051.469 I llama_init_from_model: flash_attn    = 0
0.00.051.469 I llama_init_from_model: freq_base     = 10000.0
0.00.051.470 I llama_init_from_model: freq_scale    = 1
0.00.051.470 I ggml_metal_init: allocating
0.00.051.473 I ggml_metal_init: found device: Apple M4
0.00.051.475 I ggml_metal_init: picking default device: Apple M4
0.00.052.038 I ggml_metal_init: using embedded metal library
0.00.054.361 I ggml_metal_init: GPU name:   Apple M4
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.363 I ggml_metal_init: simdgroup reduction   = true
0.00.054.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.363 I ggml_metal_init: has bfloat            = true
0.00.054.363 I ggml_metal_init: use bfloat            = true
0.00.054.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.821 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.655 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.681 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.702 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.704 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.704 I llama_init_from_model: graph nodes  = 967
0.00.084.704 I llama_init_from_model: graph splits = 2
0.00.084.707 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.493 I main: llama threadpool init, n_threads = 4
0.00.739.537 I 
0.00.739.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.583 I 
0.00.739.823 I sampler seed: 1234
0.00.739.829 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.862 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.863 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.864 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.621.333 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.621.334 I llama_perf_context_print:        load time =     730.76 ms
0.01.621.335 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.52 tokens per second)
0.01.621.336 I llama_perf_context_print:        eval time =     823.98 ms /    63 runs   (   13.08 ms per token,    76.46 tokens per second)
0.01.621.336 I llama_perf_context_print:       total time =     881.85 ms /    70 tokens
0.01.621.545 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.561 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.211 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.492 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.967 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.971 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.971 I llama_model_loader: - type  f32:  194 tensors
0.00.054.972 I llama_model_loader: - type  f16:   98 tensors
0.00.054.973 I print_info: file format = GGUF V3 (latest)
0.00.054.989 I print_info: file type   = all F32 (guessed)
0.00.054.993 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.186 I load: special tokens cache size = 25
0.00.089.948 I load: token to piece cache size = 0.2984 MB
0.00.089.952 I print_info: arch             = gptneox
0.00.089.952 I print_info: vocab_only       = 0
0.00.089.952 I print_info: n_ctx_train      = 2048
0.00.089.952 I print_info: n_embd           = 2048
0.00.089.952 I print_info: n_layer          = 24
0.00.089.956 I print_info: n_head           = 16
0.00.089.957 I print_info: n_head_kv        = 16
0.00.089.957 I print_info: n_rot            = 32
0.00.089.959 I print_info: n_swa            = 0
0.00.089.959 I print_info: n_embd_head_k    = 128
0.00.089.959 I print_info: n_embd_head_v    = 128
0.00.089.960 I print_info: n_gqa            = 1
0.00.089.961 I print_info: n_embd_k_gqa     = 2048
0.00.089.961 I print_info: n_embd_v_gqa     = 2048
0.00.089.963 I print_info: f_norm_eps       = 1.0e-05
0.00.089.963 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.964 I print_info: f_logit_scale    = 0.0e+00
0.00.089.964 I print_info: n_ff             = 8192
0.00.089.965 I print_info: n_expert         = 0
0.00.089.965 I print_info: n_expert_used    = 0
0.00.089.965 I print_info: causal attn      = 1
0.00.089.965 I print_info: pooling type     = 0
0.00.089.965 I print_info: rope type        = 2
0.00.089.965 I print_info: rope scaling     = linear
0.00.089.966 I print_info: freq_base_train  = 10000.0
0.00.089.966 I print_info: freq_scale_train = 1
0.00.089.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.966 I print_info: rope_finetuned   = unknown
0.00.089.970 I print_info: ssm_d_conv       = 0
0.00.089.970 I print_info: ssm_d_inner      = 0
0.00.089.970 I print_info: ssm_d_state      = 0
0.00.089.970 I print_info: ssm_dt_rank      = 0
0.00.089.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.971 I print_info: model type       = 1.4B
0.00.089.971 I print_info: model params     = 1.41 B
0.00.089.971 I print_info: general.name     = 1.4B
0.00.089.972 I print_info: vocab type       = BPE
0.00.089.972 I print_info: n_vocab          = 50304
0.00.089.972 I print_info: n_merges         = 50009
0.00.089.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.973 I print_info: LF token         = 128 'Ä'
0.00.089.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.973 I print_info: max token length = 1024
0.00.092.539 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.539 I load_tensors: offloading output layer to GPU
0.00.092.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.550 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.551 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.835 I llama_init_from_model: n_seq_max     = 1
0.00.092.836 I llama_init_from_model: n_ctx         = 128
0.00.092.836 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.836 I llama_init_from_model: n_batch       = 128
0.00.092.836 I llama_init_from_model: n_ubatch      = 128
0.00.092.837 I llama_init_from_model: flash_attn    = 0
0.00.092.837 I llama_init_from_model: freq_base     = 10000.0
0.00.092.837 I llama_init_from_model: freq_scale    = 1
0.00.092.838 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.838 I ggml_metal_init: allocating
0.00.092.841 I ggml_metal_init: found device: Apple M4
0.00.092.843 I ggml_metal_init: picking default device: Apple M4
0.00.093.448 I ggml_metal_init: using embedded metal library
0.00.096.045 I ggml_metal_init: GPU name:   Apple M4
0.00.096.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.047 I ggml_metal_init: simdgroup reduction   = true
0.00.096.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.048 I ggml_metal_init: has bfloat            = true
0.00.096.048 I ggml_metal_init: use bfloat            = true
0.00.096.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.491 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.764 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.767 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.780 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.590 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.591 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.592 I llama_init_from_model: graph nodes  = 967
0.00.107.592 I llama_init_from_model: graph splits = 2
0.00.107.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.453.026 I 
0.01.453.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.453.085 I perplexity: tokenizing the input ..
0.01.463.380 I perplexity: tokenization took 10.293 ms
0.01.463.387 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.594.177 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.595.584 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.595.625 I llama_perf_context_print:        load time =    1428.80 ms
0.01.595.626 I llama_perf_context_print: prompt eval time =     130.48 ms /   128 tokens (    1.02 ms per token,   981.02 tokens per second)
0.01.595.627 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.595.627 I llama_perf_context_print:       total time =     142.60 ms /   129 tokens
0.01.595.994 I ggml_metal_free: deallocating

real	0m1.782s
user	0m0.113s
sys	0m0.254s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.884 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.399 I llama_model_loader: - type  f32:  194 tensors
0.00.029.399 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.400 I print_info: file format = GGUF V3 (latest)
0.00.029.414 I print_info: file type   = Q8_0
0.00.029.415 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.590 I load: special tokens cache size = 25
0.00.054.541 I load: token to piece cache size = 0.2984 MB
0.00.054.546 I print_info: arch             = gptneox
0.00.054.546 I print_info: vocab_only       = 0
0.00.054.546 I print_info: n_ctx_train      = 2048
0.00.054.546 I print_info: n_embd           = 2048
0.00.054.546 I print_info: n_layer          = 24
0.00.054.550 I print_info: n_head           = 16
0.00.054.551 I print_info: n_head_kv        = 16
0.00.054.551 I print_info: n_rot            = 32
0.00.054.551 I print_info: n_swa            = 0
0.00.054.551 I print_info: n_embd_head_k    = 128
0.00.054.551 I print_info: n_embd_head_v    = 128
0.00.054.552 I print_info: n_gqa            = 1
0.00.054.553 I print_info: n_embd_k_gqa     = 2048
0.00.054.553 I print_info: n_embd_v_gqa     = 2048
0.00.054.554 I print_info: f_norm_eps       = 1.0e-05
0.00.054.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.558 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.558 I print_info: f_logit_scale    = 0.0e+00
0.00.054.558 I print_info: n_ff             = 8192
0.00.054.559 I print_info: n_expert         = 0
0.00.054.559 I print_info: n_expert_used    = 0
0.00.054.559 I print_info: causal attn      = 1
0.00.054.559 I print_info: pooling type     = 0
0.00.054.559 I print_info: rope type        = 2
0.00.054.559 I print_info: rope scaling     = linear
0.00.054.560 I print_info: freq_base_train  = 10000.0
0.00.054.560 I print_info: freq_scale_train = 1
0.00.054.560 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.560 I print_info: rope_finetuned   = unknown
0.00.054.560 I print_info: ssm_d_conv       = 0
0.00.054.560 I print_info: ssm_d_inner      = 0
0.00.054.562 I print_info: ssm_d_state      = 0
0.00.054.562 I print_info: ssm_dt_rank      = 0
0.00.054.562 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.562 I print_info: model type       = 1.4B
0.00.054.563 I print_info: model params     = 1.41 B
0.00.054.563 I print_info: general.name     = 1.4B
0.00.054.563 I print_info: vocab type       = BPE
0.00.054.564 I print_info: n_vocab          = 50304
0.00.054.564 I print_info: n_merges         = 50009
0.00.054.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.564 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.564 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.565 I print_info: LF token         = 128 'Ä'
0.00.054.566 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.566 I print_info: max token length = 1024
0.00.056.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.667 I load_tensors: offloading output layer to GPU
0.00.056.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.679 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.680 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.057.017 I llama_init_from_model: n_seq_max     = 1
0.00.057.018 I llama_init_from_model: n_ctx         = 128
0.00.057.018 I llama_init_from_model: n_ctx_per_seq = 128
0.00.057.018 I llama_init_from_model: n_batch       = 128
0.00.057.018 I llama_init_from_model: n_ubatch      = 128
0.00.057.019 I llama_init_from_model: flash_attn    = 0
0.00.057.019 I llama_init_from_model: freq_base     = 10000.0
0.00.057.019 I llama_init_from_model: freq_scale    = 1
0.00.057.020 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.020 I ggml_metal_init: allocating
0.00.057.024 I ggml_metal_init: found device: Apple M4
0.00.057.026 I ggml_metal_init: picking default device: Apple M4
0.00.057.609 I ggml_metal_init: using embedded metal library
0.00.059.981 I ggml_metal_init: GPU name:   Apple M4
0.00.059.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.983 I ggml_metal_init: simdgroup reduction   = true
0.00.059.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.984 I ggml_metal_init: has bfloat            = true
0.00.059.984 I ggml_metal_init: use bfloat            = true
0.00.059.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.585 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.809 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.811 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.826 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.651 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.652 I llama_init_from_model: graph nodes  = 967
0.00.072.652 I llama_init_from_model: graph splits = 2
0.00.072.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.654 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.060 I 
0.00.822.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.106 I perplexity: tokenizing the input ..
0.00.830.320 I perplexity: tokenization took 8.212 ms
0.00.830.324 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.954.872 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.956.162 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.956.186 I llama_perf_context_print:        load time =     811.17 ms
0.00.956.188 I llama_perf_context_print: prompt eval time =     124.31 ms /   128 tokens (    0.97 ms per token,  1029.73 tokens per second)
0.00.956.189 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.956.189 I llama_perf_context_print:       total time =     134.13 ms /   129 tokens
0.00.956.670 I ggml_metal_free: deallocating

real	0m0.973s
user	0m0.080s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.850 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.814 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.648 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.650 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.650 I llama_model_loader: - type  f32:  194 tensors
0.00.035.651 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.651 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.651 I print_info: file format = GGUF V3 (latest)
0.00.035.663 I print_info: file type   = Q4_0
0.00.035.664 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.040 I load: special tokens cache size = 25
0.00.063.200 I load: token to piece cache size = 0.2984 MB
0.00.063.203 I print_info: arch             = gptneox
0.00.063.204 I print_info: vocab_only       = 0
0.00.063.204 I print_info: n_ctx_train      = 2048
0.00.063.204 I print_info: n_embd           = 2048
0.00.063.204 I print_info: n_layer          = 24
0.00.063.206 I print_info: n_head           = 16
0.00.063.208 I print_info: n_head_kv        = 16
0.00.063.208 I print_info: n_rot            = 32
0.00.063.208 I print_info: n_swa            = 0
0.00.063.208 I print_info: n_embd_head_k    = 128
0.00.063.208 I print_info: n_embd_head_v    = 128
0.00.063.209 I print_info: n_gqa            = 1
0.00.063.210 I print_info: n_embd_k_gqa     = 2048
0.00.063.212 I print_info: n_embd_v_gqa     = 2048
0.00.063.212 I print_info: f_norm_eps       = 1.0e-05
0.00.063.213 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.213 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.213 I print_info: f_logit_scale    = 0.0e+00
0.00.063.214 I print_info: n_ff             = 8192
0.00.063.214 I print_info: n_expert         = 0
0.00.063.214 I print_info: n_expert_used    = 0
0.00.063.214 I print_info: causal attn      = 1
0.00.063.214 I print_info: pooling type     = 0
0.00.063.214 I print_info: rope type        = 2
0.00.063.214 I print_info: rope scaling     = linear
0.00.063.220 I print_info: freq_base_train  = 10000.0
0.00.063.221 I print_info: freq_scale_train = 1
0.00.063.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.221 I print_info: rope_finetuned   = unknown
0.00.063.221 I print_info: ssm_d_conv       = 0
0.00.063.223 I print_info: ssm_d_inner      = 0
0.00.063.223 I print_info: ssm_d_state      = 0
0.00.063.223 I print_info: ssm_dt_rank      = 0
0.00.063.223 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.223 I print_info: model type       = 1.4B
0.00.063.224 I print_info: model params     = 1.41 B
0.00.063.225 I print_info: general.name     = 1.4B
0.00.063.225 I print_info: vocab type       = BPE
0.00.063.225 I print_info: n_vocab          = 50304
0.00.063.226 I print_info: n_merges         = 50009
0.00.063.227 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.227 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.227 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.227 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.228 I print_info: LF token         = 128 'Ä'
0.00.063.228 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.229 I print_info: max token length = 1024
0.00.065.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.277 I load_tensors: offloading output layer to GPU
0.00.065.277 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.288 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.065.289 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.065.575 I llama_init_from_model: n_seq_max     = 1
0.00.065.576 I llama_init_from_model: n_ctx         = 128
0.00.065.576 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.576 I llama_init_from_model: n_batch       = 128
0.00.065.576 I llama_init_from_model: n_ubatch      = 128
0.00.065.576 I llama_init_from_model: flash_attn    = 0
0.00.065.577 I llama_init_from_model: freq_base     = 10000.0
0.00.065.577 I llama_init_from_model: freq_scale    = 1
0.00.065.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.578 I ggml_metal_init: allocating
0.00.065.581 I ggml_metal_init: found device: Apple M4
0.00.065.583 I ggml_metal_init: picking default device: Apple M4
0.00.066.170 I ggml_metal_init: using embedded metal library
0.00.068.742 I ggml_metal_init: GPU name:   Apple M4
0.00.068.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.745 I ggml_metal_init: simdgroup reduction   = true
0.00.068.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.745 I ggml_metal_init: has bfloat            = true
0.00.068.745 I ggml_metal_init: use bfloat            = true
0.00.068.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.393 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.407 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.324 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.082.325 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.082.325 I llama_init_from_model: graph nodes  = 967
0.00.082.326 I llama_init_from_model: graph splits = 2
0.00.082.327 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.873 I 
0.00.632.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.919 I perplexity: tokenizing the input ..
0.00.640.837 I perplexity: tokenization took 7.915 ms
0.00.640.841 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.490 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.764.628 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.764.651 I llama_perf_context_print:        load time =     622.02 ms
0.00.764.652 I llama_perf_context_print: prompt eval time =     122.42 ms /   128 tokens (    0.96 ms per token,  1045.56 tokens per second)
0.00.764.653 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.653 I llama_perf_context_print:       total time =     131.78 ms /   129 tokens
0.00.765.146 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.864 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.759 I llama_model_loader: - type  f32:  194 tensors
0.00.023.759 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.760 I print_info: file format = GGUF V3 (latest)
0.00.023.772 I print_info: file type   = Q4_1
0.00.023.773 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.075 I load: special tokens cache size = 25
0.00.048.084 I load: token to piece cache size = 0.2984 MB
0.00.048.087 I print_info: arch             = gptneox
0.00.048.087 I print_info: vocab_only       = 0
0.00.048.088 I print_info: n_ctx_train      = 2048
0.00.048.088 I print_info: n_embd           = 2048
0.00.048.088 I print_info: n_layer          = 24
0.00.048.091 I print_info: n_head           = 16
0.00.048.092 I print_info: n_head_kv        = 16
0.00.048.092 I print_info: n_rot            = 32
0.00.048.093 I print_info: n_swa            = 0
0.00.048.093 I print_info: n_embd_head_k    = 128
0.00.048.093 I print_info: n_embd_head_v    = 128
0.00.048.094 I print_info: n_gqa            = 1
0.00.048.094 I print_info: n_embd_k_gqa     = 2048
0.00.048.096 I print_info: n_embd_v_gqa     = 2048
0.00.048.097 I print_info: f_norm_eps       = 1.0e-05
0.00.048.097 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.097 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.099 I print_info: f_logit_scale    = 0.0e+00
0.00.048.100 I print_info: n_ff             = 8192
0.00.048.100 I print_info: n_expert         = 0
0.00.048.100 I print_info: n_expert_used    = 0
0.00.048.100 I print_info: causal attn      = 1
0.00.048.100 I print_info: pooling type     = 0
0.00.048.100 I print_info: rope type        = 2
0.00.048.101 I print_info: rope scaling     = linear
0.00.048.102 I print_info: freq_base_train  = 10000.0
0.00.048.103 I print_info: freq_scale_train = 1
0.00.048.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.103 I print_info: rope_finetuned   = unknown
0.00.048.103 I print_info: ssm_d_conv       = 0
0.00.048.103 I print_info: ssm_d_inner      = 0
0.00.048.103 I print_info: ssm_d_state      = 0
0.00.048.103 I print_info: ssm_dt_rank      = 0
0.00.048.104 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.104 I print_info: model type       = 1.4B
0.00.048.104 I print_info: model params     = 1.41 B
0.00.048.104 I print_info: general.name     = 1.4B
0.00.048.109 I print_info: vocab type       = BPE
0.00.048.109 I print_info: n_vocab          = 50304
0.00.048.109 I print_info: n_merges         = 50009
0.00.048.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.111 I print_info: LF token         = 128 'Ä'
0.00.048.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.111 I print_info: max token length = 1024
0.00.050.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.012 I load_tensors: offloading output layer to GPU
0.00.050.013 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.023 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.024 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.050.293 I llama_init_from_model: n_seq_max     = 1
0.00.050.293 I llama_init_from_model: n_ctx         = 128
0.00.050.294 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.294 I llama_init_from_model: n_batch       = 128
0.00.050.294 I llama_init_from_model: n_ubatch      = 128
0.00.050.294 I llama_init_from_model: flash_attn    = 0
0.00.050.294 I llama_init_from_model: freq_base     = 10000.0
0.00.050.295 I llama_init_from_model: freq_scale    = 1
0.00.050.295 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.295 I ggml_metal_init: allocating
0.00.050.297 I ggml_metal_init: found device: Apple M4
0.00.050.299 I ggml_metal_init: picking default device: Apple M4
0.00.050.858 I ggml_metal_init: using embedded metal library
0.00.053.195 I ggml_metal_init: GPU name:   Apple M4
0.00.053.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.197 I ggml_metal_init: simdgroup reduction   = true
0.00.053.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.197 I ggml_metal_init: has bfloat            = true
0.00.053.197 I ggml_metal_init: use bfloat            = true
0.00.053.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.572 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.820 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.822 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.836 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.763 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.764 I llama_init_from_model: graph nodes  = 967
0.00.064.765 I llama_init_from_model: graph splits = 2
0.00.064.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.563 I 
0.00.684.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.614 I perplexity: tokenizing the input ..
0.00.692.742 I perplexity: tokenization took 8.126 ms
0.00.692.745 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.814 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.817.031 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.817.065 I llama_perf_context_print:        load time =     675.69 ms
0.00.817.066 I llama_perf_context_print: prompt eval time =     122.83 ms /   128 tokens (    0.96 ms per token,  1042.06 tokens per second)
0.00.817.067 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.067 I llama_perf_context_print:       total time =     132.50 ms /   129 tokens
0.00.817.620 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.679 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.649 I llama_model_loader: - type  f32:  194 tensors
0.00.029.649 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.649 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.650 I print_info: file format = GGUF V3 (latest)
0.00.029.662 I print_info: file type   = Q5_0
0.00.029.663 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.697 I load: special tokens cache size = 25
0.00.054.505 I load: token to piece cache size = 0.2984 MB
0.00.054.508 I print_info: arch             = gptneox
0.00.054.508 I print_info: vocab_only       = 0
0.00.054.508 I print_info: n_ctx_train      = 2048
0.00.054.509 I print_info: n_embd           = 2048
0.00.054.509 I print_info: n_layer          = 24
0.00.054.512 I print_info: n_head           = 16
0.00.054.512 I print_info: n_head_kv        = 16
0.00.054.512 I print_info: n_rot            = 32
0.00.054.513 I print_info: n_swa            = 0
0.00.054.513 I print_info: n_embd_head_k    = 128
0.00.054.513 I print_info: n_embd_head_v    = 128
0.00.054.514 I print_info: n_gqa            = 1
0.00.054.514 I print_info: n_embd_k_gqa     = 2048
0.00.054.515 I print_info: n_embd_v_gqa     = 2048
0.00.054.518 I print_info: f_norm_eps       = 1.0e-05
0.00.054.518 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.518 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.518 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.518 I print_info: f_logit_scale    = 0.0e+00
0.00.054.519 I print_info: n_ff             = 8192
0.00.054.519 I print_info: n_expert         = 0
0.00.054.519 I print_info: n_expert_used    = 0
0.00.054.520 I print_info: causal attn      = 1
0.00.054.520 I print_info: pooling type     = 0
0.00.054.520 I print_info: rope type        = 2
0.00.054.520 I print_info: rope scaling     = linear
0.00.054.520 I print_info: freq_base_train  = 10000.0
0.00.054.521 I print_info: freq_scale_train = 1
0.00.054.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.521 I print_info: rope_finetuned   = unknown
0.00.054.521 I print_info: ssm_d_conv       = 0
0.00.054.521 I print_info: ssm_d_inner      = 0
0.00.054.522 I print_info: ssm_d_state      = 0
0.00.054.522 I print_info: ssm_dt_rank      = 0
0.00.054.522 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.522 I print_info: model type       = 1.4B
0.00.054.523 I print_info: model params     = 1.41 B
0.00.054.523 I print_info: general.name     = 1.4B
0.00.054.523 I print_info: vocab type       = BPE
0.00.054.523 I print_info: n_vocab          = 50304
0.00.054.524 I print_info: n_merges         = 50009
0.00.054.524 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.524 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.524 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: LF token         = 128 'Ä'
0.00.054.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: max token length = 1024
0.00.056.458 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.458 I load_tensors: offloading output layer to GPU
0.00.056.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.468 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.470 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.739 I llama_init_from_model: n_seq_max     = 1
0.00.056.740 I llama_init_from_model: n_ctx         = 128
0.00.056.740 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.740 I llama_init_from_model: n_batch       = 128
0.00.056.740 I llama_init_from_model: n_ubatch      = 128
0.00.056.740 I llama_init_from_model: flash_attn    = 0
0.00.056.741 I llama_init_from_model: freq_base     = 10000.0
0.00.056.741 I llama_init_from_model: freq_scale    = 1
0.00.056.741 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.742 I ggml_metal_init: allocating
0.00.056.745 I ggml_metal_init: found device: Apple M4
0.00.056.747 I ggml_metal_init: picking default device: Apple M4
0.00.057.298 I ggml_metal_init: using embedded metal library
0.00.059.643 I ggml_metal_init: GPU name:   Apple M4
0.00.059.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.646 I ggml_metal_init: simdgroup reduction   = true
0.00.059.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.646 I ggml_metal_init: has bfloat            = true
0.00.059.646 I ggml_metal_init: use bfloat            = true
0.00.059.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.647 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.068 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.304 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.323 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.202 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.203 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.204 I llama_init_from_model: graph nodes  = 967
0.00.071.204 I llama_init_from_model: graph splits = 2
0.00.071.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.024 I 
0.00.682.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.065 I perplexity: tokenizing the input ..
0.00.690.194 I perplexity: tokenization took 8.127 ms
0.00.690.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.093 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.826.243 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.826.267 I llama_perf_context_print:        load time =     671.34 ms
0.00.826.268 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.49 tokens per second)
0.00.826.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.270 I llama_perf_context_print:       total time =     144.24 ms /   129 tokens
0.00.826.673 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.077s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.774 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.268 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.823 I llama_model_loader: - type  f32:  194 tensors
0.00.028.823 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.824 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.824 I print_info: file format = GGUF V3 (latest)
0.00.028.831 I print_info: file type   = Q5_1
0.00.028.832 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.047.075 I load: special tokens cache size = 25
0.00.053.109 I load: token to piece cache size = 0.2984 MB
0.00.053.111 I print_info: arch             = gptneox
0.00.053.112 I print_info: vocab_only       = 0
0.00.053.112 I print_info: n_ctx_train      = 2048
0.00.053.112 I print_info: n_embd           = 2048
0.00.053.112 I print_info: n_layer          = 24
0.00.053.115 I print_info: n_head           = 16
0.00.053.116 I print_info: n_head_kv        = 16
0.00.053.116 I print_info: n_rot            = 32
0.00.053.116 I print_info: n_swa            = 0
0.00.053.118 I print_info: n_embd_head_k    = 128
0.00.053.118 I print_info: n_embd_head_v    = 128
0.00.053.119 I print_info: n_gqa            = 1
0.00.053.120 I print_info: n_embd_k_gqa     = 2048
0.00.053.120 I print_info: n_embd_v_gqa     = 2048
0.00.053.121 I print_info: f_norm_eps       = 1.0e-05
0.00.053.121 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.121 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.122 I print_info: f_logit_scale    = 0.0e+00
0.00.053.122 I print_info: n_ff             = 8192
0.00.053.123 I print_info: n_expert         = 0
0.00.053.123 I print_info: n_expert_used    = 0
0.00.053.123 I print_info: causal attn      = 1
0.00.053.123 I print_info: pooling type     = 0
0.00.053.123 I print_info: rope type        = 2
0.00.053.123 I print_info: rope scaling     = linear
0.00.053.124 I print_info: freq_base_train  = 10000.0
0.00.053.125 I print_info: freq_scale_train = 1
0.00.053.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.125 I print_info: rope_finetuned   = unknown
0.00.053.125 I print_info: ssm_d_conv       = 0
0.00.053.125 I print_info: ssm_d_inner      = 0
0.00.053.125 I print_info: ssm_d_state      = 0
0.00.053.126 I print_info: ssm_dt_rank      = 0
0.00.053.126 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.127 I print_info: model type       = 1.4B
0.00.053.128 I print_info: model params     = 1.41 B
0.00.053.128 I print_info: general.name     = 1.4B
0.00.053.128 I print_info: vocab type       = BPE
0.00.053.129 I print_info: n_vocab          = 50304
0.00.053.129 I print_info: n_merges         = 50009
0.00.053.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.130 I print_info: LF token         = 128 'Ä'
0.00.053.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.130 I print_info: max token length = 1024
0.00.055.048 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.049 I load_tensors: offloading output layer to GPU
0.00.055.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.060 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.061 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.055.326 I llama_init_from_model: n_seq_max     = 1
0.00.055.327 I llama_init_from_model: n_ctx         = 128
0.00.055.327 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.327 I llama_init_from_model: n_batch       = 128
0.00.055.327 I llama_init_from_model: n_ubatch      = 128
0.00.055.327 I llama_init_from_model: flash_attn    = 0
0.00.055.328 I llama_init_from_model: freq_base     = 10000.0
0.00.055.328 I llama_init_from_model: freq_scale    = 1
0.00.055.328 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.329 I ggml_metal_init: allocating
0.00.055.331 I ggml_metal_init: found device: Apple M4
0.00.055.332 I ggml_metal_init: picking default device: Apple M4
0.00.055.891 I ggml_metal_init: using embedded metal library
0.00.058.203 I ggml_metal_init: GPU name:   Apple M4
0.00.058.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.206 I ggml_metal_init: simdgroup reduction   = true
0.00.058.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.206 I ggml_metal_init: has bfloat            = true
0.00.058.206 I ggml_metal_init: use bfloat            = true
0.00.058.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.798 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.092 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.106 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.012 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.013 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.013 I llama_init_from_model: graph nodes  = 967
0.00.069.013 I llama_init_from_model: graph splits = 2
0.00.069.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.297 I 
0.00.786.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.338 I perplexity: tokenizing the input ..
0.00.794.084 I perplexity: tokenization took 7.745 ms
0.00.794.088 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.929.140 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.930.300 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.930.327 I llama_perf_context_print:        load time =     777.52 ms
0.00.930.328 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.36 tokens per second)
0.00.930.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.930.329 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.00.930.790 I ggml_metal_free: deallocating

real	0m0.946s
user	0m0.075s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.537 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.027.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.558 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.560 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.764 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.764 I llama_model_loader: - type  f32:  194 tensors
0.00.036.764 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.765 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.765 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.765 I print_info: file format = GGUF V3 (latest)
0.00.036.777 I print_info: file type   = Q2_K - Medium
0.00.036.778 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.060.474 I load: special tokens cache size = 25
0.00.069.367 I load: token to piece cache size = 0.2984 MB
0.00.069.371 I print_info: arch             = gptneox
0.00.069.371 I print_info: vocab_only       = 0
0.00.069.371 I print_info: n_ctx_train      = 2048
0.00.069.371 I print_info: n_embd           = 2048
0.00.069.372 I print_info: n_layer          = 24
0.00.069.375 I print_info: n_head           = 16
0.00.069.376 I print_info: n_head_kv        = 16
0.00.069.376 I print_info: n_rot            = 32
0.00.069.376 I print_info: n_swa            = 0
0.00.069.377 I print_info: n_embd_head_k    = 128
0.00.069.377 I print_info: n_embd_head_v    = 128
0.00.069.378 I print_info: n_gqa            = 1
0.00.069.379 I print_info: n_embd_k_gqa     = 2048
0.00.069.380 I print_info: n_embd_v_gqa     = 2048
0.00.069.380 I print_info: f_norm_eps       = 1.0e-05
0.00.069.381 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.381 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.381 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.381 I print_info: f_logit_scale    = 0.0e+00
0.00.069.382 I print_info: n_ff             = 8192
0.00.069.382 I print_info: n_expert         = 0
0.00.069.382 I print_info: n_expert_used    = 0
0.00.069.383 I print_info: causal attn      = 1
0.00.069.383 I print_info: pooling type     = 0
0.00.069.383 I print_info: rope type        = 2
0.00.069.383 I print_info: rope scaling     = linear
0.00.069.384 I print_info: freq_base_train  = 10000.0
0.00.069.384 I print_info: freq_scale_train = 1
0.00.069.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.386 I print_info: rope_finetuned   = unknown
0.00.069.386 I print_info: ssm_d_conv       = 0
0.00.069.386 I print_info: ssm_d_inner      = 0
0.00.069.386 I print_info: ssm_d_state      = 0
0.00.069.387 I print_info: ssm_dt_rank      = 0
0.00.069.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.387 I print_info: model type       = 1.4B
0.00.069.388 I print_info: model params     = 1.41 B
0.00.069.388 I print_info: general.name     = 1.4B
0.00.069.388 I print_info: vocab type       = BPE
0.00.069.389 I print_info: n_vocab          = 50304
0.00.069.389 I print_info: n_merges         = 50009
0.00.069.389 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.390 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.390 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.391 I print_info: LF token         = 128 'Ä'
0.00.069.391 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.391 I print_info: max token length = 1024
0.00.071.905 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.905 I load_tensors: offloading output layer to GPU
0.00.071.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.917 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.071.918 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.072.310 I llama_init_from_model: n_seq_max     = 1
0.00.072.312 I llama_init_from_model: n_ctx         = 128
0.00.072.312 I llama_init_from_model: n_ctx_per_seq = 128
0.00.072.312 I llama_init_from_model: n_batch       = 128
0.00.072.312 I llama_init_from_model: n_ubatch      = 128
0.00.072.313 I llama_init_from_model: flash_attn    = 0
0.00.072.313 I llama_init_from_model: freq_base     = 10000.0
0.00.072.314 I llama_init_from_model: freq_scale    = 1
0.00.072.314 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.314 I ggml_metal_init: allocating
0.00.072.319 I ggml_metal_init: found device: Apple M4
0.00.072.321 I ggml_metal_init: picking default device: Apple M4
0.00.073.095 I ggml_metal_init: using embedded metal library
0.00.076.415 I ggml_metal_init: GPU name:   Apple M4
0.00.076.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.418 I ggml_metal_init: simdgroup reduction   = true
0.00.076.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.419 I ggml_metal_init: has bfloat            = true
0.00.076.419 I ggml_metal_init: use bfloat            = true
0.00.076.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.872 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.090.874 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.891 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.111 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.092.112 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.092.113 I llama_init_from_model: graph nodes  = 967
0.00.092.113 I llama_init_from_model: graph splits = 2
0.00.092.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.092.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.719 I 
0.00.438.750 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.765 I perplexity: tokenizing the input ..
0.00.447.078 I perplexity: tokenization took 8.312 ms
0.00.447.082 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.666 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.831 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.849 I llama_perf_context_print:        load time =     420.18 ms
0.00.580.854 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.07 tokens per second)
0.00.580.856 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.857 I llama_perf_context_print:       total time =     142.13 ms /   129 tokens
0.00.581.160 I ggml_metal_free: deallocating

real	0m0.598s
user	0m0.091s
sys	0m0.070s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.807 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.751 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.284 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.284 I llama_model_loader: - type  f32:  194 tensors
0.00.031.285 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.285 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.285 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.285 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.286 I print_info: file format = GGUF V3 (latest)
0.00.031.298 I print_info: file type   = Q3_K - Medium
0.00.031.299 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.051.588 I load: special tokens cache size = 25
0.00.057.607 I load: token to piece cache size = 0.2984 MB
0.00.057.610 I print_info: arch             = gptneox
0.00.057.610 I print_info: vocab_only       = 0
0.00.057.610 I print_info: n_ctx_train      = 2048
0.00.057.610 I print_info: n_embd           = 2048
0.00.057.610 I print_info: n_layer          = 24
0.00.057.613 I print_info: n_head           = 16
0.00.057.614 I print_info: n_head_kv        = 16
0.00.057.614 I print_info: n_rot            = 32
0.00.057.615 I print_info: n_swa            = 0
0.00.057.615 I print_info: n_embd_head_k    = 128
0.00.057.617 I print_info: n_embd_head_v    = 128
0.00.057.618 I print_info: n_gqa            = 1
0.00.057.618 I print_info: n_embd_k_gqa     = 2048
0.00.057.619 I print_info: n_embd_v_gqa     = 2048
0.00.057.620 I print_info: f_norm_eps       = 1.0e-05
0.00.057.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.621 I print_info: f_logit_scale    = 0.0e+00
0.00.057.622 I print_info: n_ff             = 8192
0.00.057.622 I print_info: n_expert         = 0
0.00.057.622 I print_info: n_expert_used    = 0
0.00.057.622 I print_info: causal attn      = 1
0.00.057.622 I print_info: pooling type     = 0
0.00.057.622 I print_info: rope type        = 2
0.00.057.623 I print_info: rope scaling     = linear
0.00.057.623 I print_info: freq_base_train  = 10000.0
0.00.057.623 I print_info: freq_scale_train = 1
0.00.057.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.624 I print_info: rope_finetuned   = unknown
0.00.057.626 I print_info: ssm_d_conv       = 0
0.00.057.626 I print_info: ssm_d_inner      = 0
0.00.057.626 I print_info: ssm_d_state      = 0
0.00.057.626 I print_info: ssm_dt_rank      = 0
0.00.057.626 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.626 I print_info: model type       = 1.4B
0.00.057.627 I print_info: model params     = 1.41 B
0.00.057.627 I print_info: general.name     = 1.4B
0.00.057.627 I print_info: vocab type       = BPE
0.00.057.628 I print_info: n_vocab          = 50304
0.00.057.628 I print_info: n_merges         = 50009
0.00.057.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.635 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.635 I print_info: LF token         = 128 'Ä'
0.00.057.636 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.636 I print_info: max token length = 1024
0.00.059.506 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.507 I load_tensors: offloading output layer to GPU
0.00.059.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.517 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.059.518 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.059.792 I llama_init_from_model: n_seq_max     = 1
0.00.059.793 I llama_init_from_model: n_ctx         = 128
0.00.059.793 I llama_init_from_model: n_ctx_per_seq = 128
0.00.059.793 I llama_init_from_model: n_batch       = 128
0.00.059.793 I llama_init_from_model: n_ubatch      = 128
0.00.059.793 I llama_init_from_model: flash_attn    = 0
0.00.059.794 I llama_init_from_model: freq_base     = 10000.0
0.00.059.794 I llama_init_from_model: freq_scale    = 1
0.00.059.794 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.795 I ggml_metal_init: allocating
0.00.059.797 I ggml_metal_init: found device: Apple M4
0.00.059.799 I ggml_metal_init: picking default device: Apple M4
0.00.060.333 I ggml_metal_init: using embedded metal library
0.00.062.689 I ggml_metal_init: GPU name:   Apple M4
0.00.062.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.691 I ggml_metal_init: simdgroup reduction   = true
0.00.062.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.691 I ggml_metal_init: has bfloat            = true
0.00.062.691 I ggml_metal_init: use bfloat            = true
0.00.062.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.931 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.073.201 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.205 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.221 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.074.079 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.074.080 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.074.080 I llama_init_from_model: graph nodes  = 967
0.00.074.080 I llama_init_from_model: graph splits = 2
0.00.074.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.074.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.562.642 I 
0.00.562.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.692 I perplexity: tokenizing the input ..
0.00.570.672 I perplexity: tokenization took 7.977 ms
0.00.570.679 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.785 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.703.971 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.703.998 I llama_perf_context_print:        load time =     553.83 ms
0.00.703.998 I llama_perf_context_print: prompt eval time =     131.87 ms /   128 tokens (    1.03 ms per token,   970.65 tokens per second)
0.00.703.999 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.704.002 I llama_perf_context_print:       total time =     141.36 ms /   129 tokens
0.00.704.521 I ggml_metal_free: deallocating

real	0m0.718s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.750 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.020.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.029.023 I llama_model_loader: - type  f32:  194 tensors
0.00.029.023 I llama_model_loader: - type q4_K:   61 tensors
0.00.029.023 I llama_model_loader: - type q5_K:   24 tensors
0.00.029.023 I llama_model_loader: - type q6_K:   13 tensors
0.00.029.024 I print_info: file format = GGUF V3 (latest)
0.00.029.036 I print_info: file type   = Q4_K - Medium
0.00.029.037 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.048.179 I load: special tokens cache size = 25
0.00.054.194 I load: token to piece cache size = 0.2984 MB
0.00.054.197 I print_info: arch             = gptneox
0.00.054.197 I print_info: vocab_only       = 0
0.00.054.197 I print_info: n_ctx_train      = 2048
0.00.054.198 I print_info: n_embd           = 2048
0.00.054.198 I print_info: n_layer          = 24
0.00.054.201 I print_info: n_head           = 16
0.00.054.202 I print_info: n_head_kv        = 16
0.00.054.202 I print_info: n_rot            = 32
0.00.054.202 I print_info: n_swa            = 0
0.00.054.202 I print_info: n_embd_head_k    = 128
0.00.054.202 I print_info: n_embd_head_v    = 128
0.00.054.203 I print_info: n_gqa            = 1
0.00.054.204 I print_info: n_embd_k_gqa     = 2048
0.00.054.205 I print_info: n_embd_v_gqa     = 2048
0.00.054.205 I print_info: f_norm_eps       = 1.0e-05
0.00.054.206 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.206 I print_info: f_logit_scale    = 0.0e+00
0.00.054.207 I print_info: n_ff             = 8192
0.00.054.207 I print_info: n_expert         = 0
0.00.054.207 I print_info: n_expert_used    = 0
0.00.054.207 I print_info: causal attn      = 1
0.00.054.208 I print_info: pooling type     = 0
0.00.054.208 I print_info: rope type        = 2
0.00.054.208 I print_info: rope scaling     = linear
0.00.054.208 I print_info: freq_base_train  = 10000.0
0.00.054.209 I print_info: freq_scale_train = 1
0.00.054.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.211 I print_info: rope_finetuned   = unknown
0.00.054.211 I print_info: ssm_d_conv       = 0
0.00.054.211 I print_info: ssm_d_inner      = 0
0.00.054.212 I print_info: ssm_d_state      = 0
0.00.054.212 I print_info: ssm_dt_rank      = 0
0.00.054.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.212 I print_info: model type       = 1.4B
0.00.054.212 I print_info: model params     = 1.41 B
0.00.054.213 I print_info: general.name     = 1.4B
0.00.054.213 I print_info: vocab type       = BPE
0.00.054.213 I print_info: n_vocab          = 50304
0.00.054.213 I print_info: n_merges         = 50009
0.00.054.217 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.218 I print_info: LF token         = 128 'Ä'
0.00.054.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.219 I print_info: max token length = 1024
0.00.056.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.176 I load_tensors: offloading output layer to GPU
0.00.056.176 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.187 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.188 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.490 I llama_init_from_model: n_seq_max     = 1
0.00.056.491 I llama_init_from_model: n_ctx         = 128
0.00.056.491 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.491 I llama_init_from_model: n_batch       = 128
0.00.056.491 I llama_init_from_model: n_ubatch      = 128
0.00.056.491 I llama_init_from_model: flash_attn    = 0
0.00.056.492 I llama_init_from_model: freq_base     = 10000.0
0.00.056.492 I llama_init_from_model: freq_scale    = 1
0.00.056.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.492 I ggml_metal_init: allocating
0.00.056.496 I ggml_metal_init: found device: Apple M4
0.00.056.497 I ggml_metal_init: picking default device: Apple M4
0.00.057.051 I ggml_metal_init: using embedded metal library
0.00.059.418 I ggml_metal_init: GPU name:   Apple M4
0.00.059.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.420 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.420 I ggml_metal_init: simdgroup reduction   = true
0.00.059.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.421 I ggml_metal_init: has bfloat            = true
0.00.059.421 I ggml_metal_init: use bfloat            = true
0.00.059.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.055 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.287 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.289 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.302 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.218 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.219 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.219 I llama_init_from_model: graph nodes  = 967
0.00.071.219 I llama_init_from_model: graph splits = 2
0.00.071.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.919 I 
0.00.587.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.951 I perplexity: tokenizing the input ..
0.00.595.883 I perplexity: tokenization took 7.931 ms
0.00.595.887 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.141 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.389 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.416 I llama_perf_context_print:        load time =     579.16 ms
0.00.731.417 I llama_perf_context_print: prompt eval time =     134.00 ms /   128 tokens (    1.05 ms per token,   955.24 tokens per second)
0.00.731.418 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.418 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.731.749 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.077s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.258 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.757 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.139 I llama_model_loader: - type  f32:  194 tensors
0.00.025.140 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.140 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.140 I print_info: file format = GGUF V3 (latest)
0.00.025.147 I print_info: file type   = Q5_K - Medium
0.00.025.148 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.373 I load: special tokens cache size = 25
0.00.049.295 I load: token to piece cache size = 0.2984 MB
0.00.049.298 I print_info: arch             = gptneox
0.00.049.299 I print_info: vocab_only       = 0
0.00.049.299 I print_info: n_ctx_train      = 2048
0.00.049.299 I print_info: n_embd           = 2048
0.00.049.299 I print_info: n_layer          = 24
0.00.049.302 I print_info: n_head           = 16
0.00.049.303 I print_info: n_head_kv        = 16
0.00.049.303 I print_info: n_rot            = 32
0.00.049.303 I print_info: n_swa            = 0
0.00.049.303 I print_info: n_embd_head_k    = 128
0.00.049.303 I print_info: n_embd_head_v    = 128
0.00.049.304 I print_info: n_gqa            = 1
0.00.049.305 I print_info: n_embd_k_gqa     = 2048
0.00.049.306 I print_info: n_embd_v_gqa     = 2048
0.00.049.306 I print_info: f_norm_eps       = 1.0e-05
0.00.049.306 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.307 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.307 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.307 I print_info: f_logit_scale    = 0.0e+00
0.00.049.308 I print_info: n_ff             = 8192
0.00.049.308 I print_info: n_expert         = 0
0.00.049.308 I print_info: n_expert_used    = 0
0.00.049.308 I print_info: causal attn      = 1
0.00.049.308 I print_info: pooling type     = 0
0.00.049.308 I print_info: rope type        = 2
0.00.049.309 I print_info: rope scaling     = linear
0.00.049.310 I print_info: freq_base_train  = 10000.0
0.00.049.313 I print_info: freq_scale_train = 1
0.00.049.313 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.313 I print_info: rope_finetuned   = unknown
0.00.049.313 I print_info: ssm_d_conv       = 0
0.00.049.313 I print_info: ssm_d_inner      = 0
0.00.049.314 I print_info: ssm_d_state      = 0
0.00.049.314 I print_info: ssm_dt_rank      = 0
0.00.049.314 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.314 I print_info: model type       = 1.4B
0.00.049.314 I print_info: model params     = 1.41 B
0.00.049.315 I print_info: general.name     = 1.4B
0.00.049.315 I print_info: vocab type       = BPE
0.00.049.315 I print_info: n_vocab          = 50304
0.00.049.315 I print_info: n_merges         = 50009
0.00.049.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: LF token         = 128 'Ä'
0.00.049.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: max token length = 1024
0.00.051.355 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.355 I load_tensors: offloading output layer to GPU
0.00.051.355 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.366 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.367 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.753 I llama_init_from_model: n_seq_max     = 1
0.00.051.754 I llama_init_from_model: n_ctx         = 128
0.00.051.754 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.754 I llama_init_from_model: n_batch       = 128
0.00.051.754 I llama_init_from_model: n_ubatch      = 128
0.00.051.754 I llama_init_from_model: flash_attn    = 0
0.00.051.755 I llama_init_from_model: freq_base     = 10000.0
0.00.051.755 I llama_init_from_model: freq_scale    = 1
0.00.051.755 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.756 I ggml_metal_init: allocating
0.00.051.758 I ggml_metal_init: found device: Apple M4
0.00.051.760 I ggml_metal_init: picking default device: Apple M4
0.00.052.331 I ggml_metal_init: using embedded metal library
0.00.054.673 I ggml_metal_init: GPU name:   Apple M4
0.00.054.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.676 I ggml_metal_init: simdgroup reduction   = true
0.00.054.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.676 I ggml_metal_init: has bfloat            = true
0.00.054.676 I ggml_metal_init: use bfloat            = true
0.00.054.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.089 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.336 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.338 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.353 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.293 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.295 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.295 I llama_init_from_model: graph nodes  = 967
0.00.066.295 I llama_init_from_model: graph splits = 2
0.00.066.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.547 I 
0.00.630.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.627 I perplexity: tokenizing the input ..
0.00.638.549 I perplexity: tokenization took 7.921 ms
0.00.638.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.178 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.335 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.367 I llama_perf_context_print:        load time =     620.28 ms
0.00.780.368 I llama_perf_context_print: prompt eval time =     140.40 ms /   128 tokens (    1.10 ms per token,   911.71 tokens per second)
0.00.780.369 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.370 I llama_perf_context_print:       total time =     149.82 ms /   129 tokens
0.00.780.908 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.076s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.987 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.784 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.784 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.787 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.098 I llama_model_loader: - type  f32:  194 tensors
0.00.024.098 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.099 I print_info: file format = GGUF V3 (latest)
0.00.024.106 I print_info: file type   = Q6_K
0.00.024.107 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.403 I load: special tokens cache size = 25
0.00.048.399 I load: token to piece cache size = 0.2984 MB
0.00.048.403 I print_info: arch             = gptneox
0.00.048.403 I print_info: vocab_only       = 0
0.00.048.403 I print_info: n_ctx_train      = 2048
0.00.048.403 I print_info: n_embd           = 2048
0.00.048.403 I print_info: n_layer          = 24
0.00.048.406 I print_info: n_head           = 16
0.00.048.407 I print_info: n_head_kv        = 16
0.00.048.407 I print_info: n_rot            = 32
0.00.048.407 I print_info: n_swa            = 0
0.00.048.408 I print_info: n_embd_head_k    = 128
0.00.048.409 I print_info: n_embd_head_v    = 128
0.00.048.409 I print_info: n_gqa            = 1
0.00.048.410 I print_info: n_embd_k_gqa     = 2048
0.00.048.411 I print_info: n_embd_v_gqa     = 2048
0.00.048.412 I print_info: f_norm_eps       = 1.0e-05
0.00.048.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.413 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.413 I print_info: f_logit_scale    = 0.0e+00
0.00.048.414 I print_info: n_ff             = 8192
0.00.048.414 I print_info: n_expert         = 0
0.00.048.414 I print_info: n_expert_used    = 0
0.00.048.414 I print_info: causal attn      = 1
0.00.048.414 I print_info: pooling type     = 0
0.00.048.414 I print_info: rope type        = 2
0.00.048.415 I print_info: rope scaling     = linear
0.00.048.415 I print_info: freq_base_train  = 10000.0
0.00.048.417 I print_info: freq_scale_train = 1
0.00.048.417 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.417 I print_info: rope_finetuned   = unknown
0.00.048.418 I print_info: ssm_d_conv       = 0
0.00.048.418 I print_info: ssm_d_inner      = 0
0.00.048.418 I print_info: ssm_d_state      = 0
0.00.048.418 I print_info: ssm_dt_rank      = 0
0.00.048.418 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.418 I print_info: model type       = 1.4B
0.00.048.419 I print_info: model params     = 1.41 B
0.00.048.419 I print_info: general.name     = 1.4B
0.00.048.419 I print_info: vocab type       = BPE
0.00.048.419 I print_info: n_vocab          = 50304
0.00.048.420 I print_info: n_merges         = 50009
0.00.048.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.420 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.424 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.425 I print_info: LF token         = 128 'Ä'
0.00.048.425 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.425 I print_info: max token length = 1024
0.00.050.365 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.365 I load_tensors: offloading output layer to GPU
0.00.050.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.376 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.377 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.645 I llama_init_from_model: n_seq_max     = 1
0.00.050.646 I llama_init_from_model: n_ctx         = 128
0.00.050.646 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.646 I llama_init_from_model: n_batch       = 128
0.00.050.646 I llama_init_from_model: n_ubatch      = 128
0.00.050.646 I llama_init_from_model: flash_attn    = 0
0.00.050.647 I llama_init_from_model: freq_base     = 10000.0
0.00.050.647 I llama_init_from_model: freq_scale    = 1
0.00.050.647 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.648 I ggml_metal_init: allocating
0.00.050.650 I ggml_metal_init: found device: Apple M4
0.00.050.652 I ggml_metal_init: picking default device: Apple M4
0.00.051.201 I ggml_metal_init: using embedded metal library
0.00.053.556 I ggml_metal_init: GPU name:   Apple M4
0.00.053.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.558 I ggml_metal_init: simdgroup reduction   = true
0.00.053.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.558 I ggml_metal_init: has bfloat            = true
0.00.053.558 I ggml_metal_init: use bfloat            = true
0.00.053.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.479 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.836 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.838 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.852 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.774 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.775 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.776 I llama_init_from_model: graph nodes  = 967
0.00.064.776 I llama_init_from_model: graph splits = 2
0.00.064.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.253 I 
0.00.635.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.290 I perplexity: tokenizing the input ..
0.00.643.084 I perplexity: tokenization took 7.792 ms
0.00.643.093 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.758 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.785.018 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.785.043 I llama_perf_context_print:        load time =     626.26 ms
0.00.785.044 I llama_perf_context_print: prompt eval time =     140.44 ms /   128 tokens (    1.10 ms per token,   911.45 tokens per second)
0.00.785.045 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.045 I llama_perf_context_print:       total time =     149.79 ms /   129 tokens
0.00.785.480 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.075s
sys	0m0.116s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.253 I build: 4478 (afd40ea2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.702 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.595 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.610 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.623 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.626 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.044 I llama_model_loader: - type  f32:  194 tensors
0.00.057.045 I llama_model_loader: - type  f16:   98 tensors
0.00.057.046 I print_info: file format = GGUF V3 (latest)
0.00.057.061 I print_info: file type   = all F32 (guessed)
0.00.057.063 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.463 I load: special tokens cache size = 25
0.00.090.165 I load: token to piece cache size = 0.2984 MB
0.00.090.168 I print_info: arch             = gptneox
0.00.090.168 I print_info: vocab_only       = 0
0.00.090.169 I print_info: n_ctx_train      = 2048
0.00.090.169 I print_info: n_embd           = 2048
0.00.090.169 I print_info: n_layer          = 24
0.00.090.172 I print_info: n_head           = 16
0.00.090.172 I print_info: n_head_kv        = 16
0.00.090.173 I print_info: n_rot            = 32
0.00.090.174 I print_info: n_swa            = 0
0.00.090.174 I print_info: n_embd_head_k    = 128
0.00.090.175 I print_info: n_embd_head_v    = 128
0.00.090.175 I print_info: n_gqa            = 1
0.00.090.176 I print_info: n_embd_k_gqa     = 2048
0.00.090.177 I print_info: n_embd_v_gqa     = 2048
0.00.090.177 I print_info: f_norm_eps       = 1.0e-05
0.00.090.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.179 I print_info: f_logit_scale    = 0.0e+00
0.00.090.180 I print_info: n_ff             = 8192
0.00.090.180 I print_info: n_expert         = 0
0.00.090.180 I print_info: n_expert_used    = 0
0.00.090.180 I print_info: causal attn      = 1
0.00.090.180 I print_info: pooling type     = 0
0.00.090.188 I print_info: rope type        = 2
0.00.090.190 I print_info: rope scaling     = linear
0.00.090.190 I print_info: freq_base_train  = 10000.0
0.00.090.191 I print_info: freq_scale_train = 1
0.00.090.191 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.191 I print_info: rope_finetuned   = unknown
0.00.090.191 I print_info: ssm_d_conv       = 0
0.00.090.191 I print_info: ssm_d_inner      = 0
0.00.090.191 I print_info: ssm_d_state      = 0
0.00.090.192 I print_info: ssm_dt_rank      = 0
0.00.090.192 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.192 I print_info: model type       = 1.4B
0.00.090.192 I print_info: model params     = 1.41 B
0.00.090.192 I print_info: general.name     = 1.4B
0.00.090.194 I print_info: vocab type       = BPE
0.00.090.194 I print_info: n_vocab          = 50304
0.00.090.194 I print_info: n_merges         = 50009
0.00.090.195 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.195 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.195 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.195 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.195 I print_info: LF token         = 128 'Ä'
0.00.090.197 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.197 I print_info: max token length = 1024
0.00.092.788 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.788 I load_tensors: offloading output layer to GPU
0.00.092.788 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.799 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.800 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.123 I llama_init_from_model: n_seq_max     = 1
0.00.093.124 I llama_init_from_model: n_ctx         = 128
0.00.093.124 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.124 I llama_init_from_model: n_batch       = 128
0.00.093.124 I llama_init_from_model: n_ubatch      = 128
0.00.093.125 I llama_init_from_model: flash_attn    = 0
0.00.093.125 I llama_init_from_model: freq_base     = 10000.0
0.00.093.125 I llama_init_from_model: freq_scale    = 1
0.00.093.126 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.126 I ggml_metal_init: allocating
0.00.093.129 I ggml_metal_init: found device: Apple M4
0.00.093.131 I ggml_metal_init: picking default device: Apple M4
0.00.093.749 I ggml_metal_init: using embedded metal library
0.00.096.299 I ggml_metal_init: GPU name:   Apple M4
0.00.096.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.302 I ggml_metal_init: simdgroup reduction   = true
0.00.096.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.302 I ggml_metal_init: has bfloat            = true
0.00.096.302 I ggml_metal_init: use bfloat            = true
0.00.096.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.303 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.611 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.907 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.909 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.925 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.761 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.762 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.762 I llama_init_from_model: graph nodes  = 967
0.00.107.762 I llama_init_from_model: graph splits = 2
0.00.107.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.764 I 
0.00.107.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.107.792 I compute_imatrix: tokenizing the input ..
0.00.114.367 I compute_imatrix: tokenization took 6.575 ms
0.00.114.369 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.555.925 I compute_imatrix: 1.44 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.559.196 I llama_perf_context_print:        load time =    1531.22 ms
0.01.559.197 I llama_perf_context_print: prompt eval time =    1440.94 ms /   128 tokens (   11.26 ms per token,    88.83 tokens per second)
0.01.559.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.559.199 I llama_perf_context_print:       total time =    1534.48 ms /   129 tokens
0.01.560.035 I ggml_metal_free: deallocating

real	0m1.747s
user	0m0.173s
sys	0m0.239s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4478 (afd40ea2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13870a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13870aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13870aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13870b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13870bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13870c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13870c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13870cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13870d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13870d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13870dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13870e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13870ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13870f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13870fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138710310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138711150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138711870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138712040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138712e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1387135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138713e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138714820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138715aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1387162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138716a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138717290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1387177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138717a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1387183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138718d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1387191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138719f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13871a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13871a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13871ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13871b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13871bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13871c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13871c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13871ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13871d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13871da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13871e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13871e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13871ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13871f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13871f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13871fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138720280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138720540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1387209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138720e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138721320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1387217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138721c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138722100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1387225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138722ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138723380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138723cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138724210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138725200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138725750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138725ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1387261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138726740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138726c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1387271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138727730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138727c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1387281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138728c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1387291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138729710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13872a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13872a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13872ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13872b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13872b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13872bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13871b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13872c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13872c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13872cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13872d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13872d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13872dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13872e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13872e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13872ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13872f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13872f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13872fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1387302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138730820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138730d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138731210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1387316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138731ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138732490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138732930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138732dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138733270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138733710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138733bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138734050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1387344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138734990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138734e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1387352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138735770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138735c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1387360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138736550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1387369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138737330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1387377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1387385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138738a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138738ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138739390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138739830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138739cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13873a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13873a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13873aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13873af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13873b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13873b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13873bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13873c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13873c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13873cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13873cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13873d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13873d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13873dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13873e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13873e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13873eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13873f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13873f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13873f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13873fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138740290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138740bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138741070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138741510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1387419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1387422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138742790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138742c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1387430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138743570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138743a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138743eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138744350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1387447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138744c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138745130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1387455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138745a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138745f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1387463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138746850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138746cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138747190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138747630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138747ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138747f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1387484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138748f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1387494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138749770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138749d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13874a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13874a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13874b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13874b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13874b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13874bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13874c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13874cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13874d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13874d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13874dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13874e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13874e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13874ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13874f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13874f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13874fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138750270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1387507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138750d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138751260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1387517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138751d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138752250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1387527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138752cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138753240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138753790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138753ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138754230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138754780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138754cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138755220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138755770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138755cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138756210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138756760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138756cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138757200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138757750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1387581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138758740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138758c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1387591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138759730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138759c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13875a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13875a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13875ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13875b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13875b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13875bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13875c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13875c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13875cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13875d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13875d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13875dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13875e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13875e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13875ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13875f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13875f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13875fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138760170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1387606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138760c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1387610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138761550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1387619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138761e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138762330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1387627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138763110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1387635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138763a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138763ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138764390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138764830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138764cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138765170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1387656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138765de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138766500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138766c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138767340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1387680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1387686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13874bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138749a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13874a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13871d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13871d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13871f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13874c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138714ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13871b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13871bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13871c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13871a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13871cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13871fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13872c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1387678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138716cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138716f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13874c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13874ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1387150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1387153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138715670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138768b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138768de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1387690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138769620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1387698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138769ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138769e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13876a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13876a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13876a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13876a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13876ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13876aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13876b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13876b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13876b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13876b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13876bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13876bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13876c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13876c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13876c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13876ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13876cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13876cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13876d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13876d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13876d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13876dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13876dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13876e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13876e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13876e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13876e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13876eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13876ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13876f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13876f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13876f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13876f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13876fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13876fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12bb04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12bb046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12bb04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12bb04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12bb05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12bb058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12bb05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12bb06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12bb06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12bb06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12bb06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12bb07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12bb077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12bb07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12bb080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12bb08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12bb08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12bb08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12bb09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12bb096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12bb09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12bb09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12bb0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12bb0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12bb0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12bb0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12bb0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12bb0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12bb0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12bb0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12bb0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12bb0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12bb0d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12bb0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12bb0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12bb0ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12bb0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12bb0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12bb0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12bb0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12bb0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12bb0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12bb0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12bb10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12bb105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12bb10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12bb10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12bb11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12bb11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12bb11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12bb12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12bb124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12bb12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12bb12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12bb13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12bb13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12bb13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12bb13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12bb143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12bb14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12bb14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12bb15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12bb155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12bb15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12bb15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12bb162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12bb16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12bb16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12bb17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12bb174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12bb17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12bb17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12bb18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12bb18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12bb18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12bb18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12bb193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12bb19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12bb19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12bb1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12bb1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12bb1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12bb1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12bb1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12bb1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12bb1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12bb1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12bb1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12bb1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12bb1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12bb1d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12bb1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12bb1dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12bb1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12bb1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12bb1e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12bb1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12bb1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12bb1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12bb1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12bb202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12bb20710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12bb20b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12bb20ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12bb21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12bb218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12bb21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12bb221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12bb22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12bb22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12bb22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12bb23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12bb237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12bb23c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12bb240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12bb24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12bb249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12bb24e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12bb25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12bb256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12bb25b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12bb25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12bb26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12bb268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12bb26d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12bb27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12bb27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12bb27a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12bb27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12bb28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12bb287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12bb28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12bb290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12bb29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12bb29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12bb29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12bb2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12bb2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12bb2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12bb2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12bb2b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12bb2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12bb2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12bb2c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12bb2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12bb2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12bb2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12bb2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12bb2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12bb2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12bb2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12bb2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12bb2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12bb30320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12bb308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12bb30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12bb31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12bb31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12bb31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12bb325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12bb32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12bb33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12bb336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12bb33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12bb34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12bb34820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12bb34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12bb353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12bb35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12bb35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12bb364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12bb36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12bb37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12bb37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12bb37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12bb381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12bb38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12bb38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12bb392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12bb398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12bb39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12bb3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12bb3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12bb3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12bb3b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12bb3bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12bb3c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12bb3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12bb3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12bb3d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12bb3d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12bb3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12bb3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12bb3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12bb3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12bb3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12bb3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12bb40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12bb405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12bb40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12bb410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12bb415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12bb41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12bb41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12bb424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12bb429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12bb42ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12bb433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12bb438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12bb43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12bb442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12bb447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12bb44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12bb451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12bb456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12bb460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12bb467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12bb46ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12bb47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12bb478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12bb480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12bb48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12bb48990 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138770160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138770420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1387707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138770a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138770d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138771000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1387712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138771580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138771840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138771b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138771dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138772080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138772650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138772c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138773250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138773510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1387737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138773a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138773d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138774010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1387742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138774590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138774850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138774b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138774dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138775090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138775350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138775610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1387758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138775b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138775e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138776110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1387763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138776690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138776950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138776c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138776ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138777190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138777450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138777710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1387779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138777c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138777f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138778210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1387784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138778790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138778a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138778d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138778fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138779290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138779550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138779810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138779ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138779d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13877a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13877a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13877a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13877a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13877ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13877ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13877b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13877b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13877b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13877b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13877bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13877be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13877c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13877c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13877c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13877c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13877cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13877cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13877d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13877d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13877d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13877da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13877dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13877df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13877e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13877e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13877e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13877ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13877ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13877f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13877f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13877f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13877f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13877fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13877fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138780090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138780350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138780610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1387808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138780b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138780e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138781110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1387813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138781690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138781950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138781c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138781ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138782190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138782450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138782710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1387829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138782c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138782f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138783210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1387834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138783790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138783a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138783d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138783fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138784290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138784550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138784810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138784ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138784d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138785050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138785310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1387855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138785890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138785b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138785e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1387860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138786390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138786650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138786910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138786bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138786e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138787150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138787410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1387876d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138787990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138787c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138787f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1387881d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138788490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138788750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138788a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138788cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138788f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138789250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138789510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1387897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138789a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138789d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13878a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13878a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13878a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13878a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13878ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13878add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13878b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13878b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13878b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13878b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13878bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13878be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13878c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13878c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13878c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13878c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13878cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13878ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13878d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13878d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13878d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13878d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13878dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13878df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13878e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13878e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13878e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13878ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13878ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13878efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13878f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13878f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13878f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13878fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13878fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138790050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138790310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1387905d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138790890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138790b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138790e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1387910d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138791390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138791650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138791910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138791bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138791e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138792150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138792410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1387926d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138792c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138793150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138793690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138793bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138793e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138794290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138794550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138794810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138794c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1387950f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138795600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138795b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138796010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138796b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138796e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138797400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1387979c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138797f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138798540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138798b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1387990c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138799680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138799c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13879a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13879a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13879ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13879b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13879b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13879bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13879c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13879ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13879d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13879d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13879db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13879e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13879e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13879ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13879f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13879f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13879fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1387a03c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1387a0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1387a0f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1387a1500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1387a1ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1387a2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1387a2640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1387a2c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1387a31c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1387a3780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1387a3d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1387a4300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1387a48c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1387a4e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1387a5440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1387a5a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1387a5fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1387a6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1387a6b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1387a7100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1387a76c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1387a7c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1387a8240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1387a8800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1387a8dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1387a9380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1387a9940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1387a9f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1387aa4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1387aaa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1387ab040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1387ab540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1387aba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1387abf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1387ac440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1387ac940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1387ace40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1387ad340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1387ad840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1387add40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1387ae240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1387ae740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1387aec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1387af140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1387af640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1387afb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1387b0550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1387b0c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1387b1390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1387b1ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1387b1d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1387b2560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1387b2820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1387b2e30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.799s
user	0m0.297s
sys	0m0.309s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4478 (afd40ea2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e0c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e0cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e0d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e0f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e15ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e1bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e1c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134e25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134e260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134e26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134e26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134e270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134e27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134e27b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134e280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134e28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134e28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134e290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134e29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134e29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134e2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134e2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134e2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134e2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134e2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134e2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134e2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134e2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134e2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134e2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134e2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134e1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134e2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134e2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134e2e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134e2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134e2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134e2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134e2fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134e301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134e30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134e311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134e31700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134e31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134e321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134e326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e33970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e35530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e36310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e39150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e39f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e3ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e3c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e42550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e45830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e46610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e46f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e47890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134e4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134e4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134e4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e53bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e54bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e5d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e5d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e5e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e5e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e5f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e5f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e5fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e61af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e62590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134e62a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134e62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e63810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e64150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e64f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e65870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e661b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e66af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134e67040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134e67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134e67e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134e685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134e68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134e68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134e69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134e69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134e6a040 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136004d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1360051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136005660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136005ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136005f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1360063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136006820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136006c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136007100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136007570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1360079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1360080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1360093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13600a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13600a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13600b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13600b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13600bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13600c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13600cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13600d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13600dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13600e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13600e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13600e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13600ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13600f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13600f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13600fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13600ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1360103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1360106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136010b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136010f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1360113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136011860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136011cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136012140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1360125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136012a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136012e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136013300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136013770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136013be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136014050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1360144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136014930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136014da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136015210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136015680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136015af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136015f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1360163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136016db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1360172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136017b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136018000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136018470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134f09860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134f09cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134f0a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134f0a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134f0ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134f0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134f0b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134f0bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134f0c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134f0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134f0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134f0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134f0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134f0d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134f0dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134f0e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134f0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134f0e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134f0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134f0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134f0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134f0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134f0ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134f103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134f10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134f10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134f11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134f115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134f11a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134f11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134f12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134f12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134f12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134f13050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134f134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134f13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134f13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134f14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134f14680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134f14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134f14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134f153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134f15840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134f15cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134f16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134f16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134f16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134f16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134f172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134f17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134f17bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134f18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134f184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134f18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134f18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134f191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134f19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134f19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134f19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134f1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134f1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134f1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134f1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134f1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134f1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134f1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134f1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134f1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134f1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134f1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134f1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134f1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134f1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134f1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134f1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134f1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134f1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134f1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134f1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134f1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134f200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134f20550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134f209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134f20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134f212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134f21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134f21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134f21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134f22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134f228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134f22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134f231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134f23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134f23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134f23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134f24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134f247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134f24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134f250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134f259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134f25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134f26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134f269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134f26eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134f273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134f278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134f27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134f287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134f28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134f291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134f296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134f29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134f2a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134f2a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134f2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134f2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134f2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134f2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134f2beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134f2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134f2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134f2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134f2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134f2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134f2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134f2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134f2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134f2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134f2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134f301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134f30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134f30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134f31390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134f319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134f32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134f32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134f328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134f32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134f33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134f33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134f341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134f34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134f34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134f35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134f357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134f36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134f367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134f37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134f377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134f37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134f38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134f387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134f38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134f39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134f397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134f39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134f3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134f3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134f3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134f3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134f3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134f3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134f3c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134f3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134f3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134f3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134f3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134f3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134f3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134f3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134f3eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134f3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134f3fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134f401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134f40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134f411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134f41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134f41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134f421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134f42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134f42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134f431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134f43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134f441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134f446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134f44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134f45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134f456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134f45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134f46180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134f466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134f46c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134f47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134f476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134f47c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134f480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134f48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134f489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134f48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134f49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134f497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134f49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134f4a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134f4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134f4aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134f4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134f4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134f4b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134f4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134f4c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134f4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134f4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134f4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134f4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134f4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134f4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134f4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134f4f6c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e69cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e1d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e1e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e15460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e18900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e4c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e6a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e6a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e6aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e6ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e6afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e6b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e6b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e6b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e6baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e6bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e6c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e6c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e6c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e6c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e6cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e6cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e6d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e6d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e6d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e6d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e6dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e6de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e6e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e6e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e6e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e6e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e6ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e6eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e6f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e6f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e6f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e6f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e6fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e6ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e70220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e704e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e707a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e70a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e70d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e70fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e712a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e71560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e71820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e71ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e71da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e72060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e72320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e725e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134e728a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134e72b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134e72e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134e730e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134e733a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134e73660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134e73920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134e73be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134e73ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134e74160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134e74420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134e746e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134e749a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134e74c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134e74f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134e751e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134e754a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134e75760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134e75a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134e75ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134e75fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134e76260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134e76520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134e767e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134e76aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134e76d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134e77020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134e772e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134e775a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134e77860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134e77b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134e77de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134e780a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134e78360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134e78620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134e788e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134e78ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134e78e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134e79120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134e793e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e796a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e79960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e79c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e79ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e7a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e7a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e7a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e7a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e7aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e7af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e7b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e7b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e7b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e7ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e7bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e7bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e7c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e7c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e7c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e7cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e7cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e7d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e7d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e7d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e7d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e7db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e7de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e7e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e7e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e7e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e7e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e7ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e7eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e7f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e7f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e7f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e7f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e7fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e7ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e801e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e804a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e80760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e80a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e80ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e80fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e81260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e81520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e817e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e81aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e81d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e82020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e822e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e825a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e82860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e82b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e82de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e830a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e83360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e83620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e838e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e83ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e83e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e84120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e843e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e846a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e84960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e84c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e84ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e851a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e85460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e85720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e859e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e85ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e85f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e86220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e864e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e867a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e86a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e86d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e86fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e872a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e87560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e87820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e87ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e87da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e88060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e88320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134e888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134e88b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e88e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e890e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134e893a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e89660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e89920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e89be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e89ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e8a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e8a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e8a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e8af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e8b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e8b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e8bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e8c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e8c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e8cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e8d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e8d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e8df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e8e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e8e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e8ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e8f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e8f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e8fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e90440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e90990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e90ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e91430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e91980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e91ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e92420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e92970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e92ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e93410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e93960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e93eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e94400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e94950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e94ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e953f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e95940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e95e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e963e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e96930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e96e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e973d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e97920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e97e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e983c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e98910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e98e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e993b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e99900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e99e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e9a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e9a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e9ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e9b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e9b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e9be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e9c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e9c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134e9cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134e9ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e9d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e9d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e9d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e9de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e9e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e9e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e9ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e9f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e9f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e9f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e9fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134ea01e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134ea0650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134ea0ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134ea0f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134ea1c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134ea2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134ea2a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134ea2d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134ea3190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134ea3790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134ea3da0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.920s
user	0m0.243s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
