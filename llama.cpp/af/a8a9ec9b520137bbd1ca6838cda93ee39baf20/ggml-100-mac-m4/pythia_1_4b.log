Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.621s
user	0m0.859s
sys	0m1.241s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Built target llava
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking C executable ../bin/test-c
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-sampling
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-log
[ 50%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-gguf
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-barrier
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-backend-ops
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-gguf
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-autorelease
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-rope
[ 64%] Built target test-chat-template
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-infill
[ 73%] Built target llama-bench
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-lookahead
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Built target llama-passkey
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 83%] Built target llama-quantize
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-tts
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.096s
user	0m5.944s
sys	0m10.424s

main: quantize time =  8745.55 ms
main:    total time =  8745.55 ms

main: quantize time =  1495.89 ms
main:    total time =  1495.89 ms

main: quantize time =  1310.45 ms
main:    total time =  1310.45 ms

main: quantize time =  2239.07 ms
main:    total time =  2239.07 ms

main: quantize time =  1775.09 ms
main:    total time =  1775.09 ms

main: quantize time =  4957.04 ms
main:    total time =  4957.04 ms

main: quantize time =  5671.01 ms
main:    total time =  5671.01 ms

main: quantize time =  7194.72 ms
main:    total time =  7194.72 ms

main: quantize time =  5900.57 ms
main:    total time =  5900.57 ms

main: quantize time =  4587.10 ms
main:    total time =  4587.10 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.180 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.285 I main: llama backend init
0.00.000.291 I main: load the model and apply lora adapter, if any
0.00.037.402 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.815 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.851 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.851 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.852 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.858 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.906 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.338 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.069.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.069.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.069.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.069.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.069.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.069.833 I llama_model_loader: - type  f32:  194 tensors
0.00.069.833 I llama_model_loader: - type  f16:   98 tensors
0.00.069.834 I print_info: file format = GGUF V3 (latest)
0.00.069.836 I print_info: file type   = all F32 (guessed)
0.00.069.838 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.116 I load: special tokens cache size = 25
0.00.104.956 I load: token to piece cache size = 0.2984 MB
0.00.104.960 I print_info: arch             = gptneox
0.00.104.960 I print_info: vocab_only       = 0
0.00.104.960 I print_info: n_ctx_train      = 2048
0.00.104.960 I print_info: n_embd           = 2048
0.00.104.961 I print_info: n_layer          = 24
0.00.104.964 I print_info: n_head           = 16
0.00.104.965 I print_info: n_head_kv        = 16
0.00.104.965 I print_info: n_rot            = 32
0.00.104.965 I print_info: n_swa            = 0
0.00.104.968 I print_info: n_embd_head_k    = 128
0.00.104.968 I print_info: n_embd_head_v    = 128
0.00.104.968 I print_info: n_gqa            = 1
0.00.104.969 I print_info: n_embd_k_gqa     = 2048
0.00.104.970 I print_info: n_embd_v_gqa     = 2048
0.00.104.970 I print_info: f_norm_eps       = 1.0e-05
0.00.104.971 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.971 I print_info: f_logit_scale    = 0.0e+00
0.00.104.972 I print_info: n_ff             = 8192
0.00.104.972 I print_info: n_expert         = 0
0.00.104.973 I print_info: n_expert_used    = 0
0.00.104.973 I print_info: causal attn      = 1
0.00.104.973 I print_info: pooling type     = 0
0.00.104.974 I print_info: rope type        = 2
0.00.104.974 I print_info: rope scaling     = linear
0.00.104.974 I print_info: freq_base_train  = 10000.0
0.00.104.975 I print_info: freq_scale_train = 1
0.00.104.975 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.975 I print_info: rope_finetuned   = unknown
0.00.104.984 I print_info: ssm_d_conv       = 0
0.00.104.987 I print_info: ssm_d_inner      = 0
0.00.104.987 I print_info: ssm_d_state      = 0
0.00.104.987 I print_info: ssm_dt_rank      = 0
0.00.104.987 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.988 I print_info: model type       = 1.4B
0.00.104.988 I print_info: model params     = 1.41 B
0.00.104.988 I print_info: general.name     = 1.4B
0.00.104.989 I print_info: vocab type       = BPE
0.00.104.989 I print_info: n_vocab          = 50304
0.00.104.989 I print_info: n_merges         = 50009
0.00.104.990 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.990 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.991 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.991 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.991 I print_info: LF token         = 128 'Ä'
0.00.104.992 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.992 I print_info: max token length = 1024
0.00.107.503 I load_tensors: offloading 24 repeating layers to GPU
0.00.107.503 I load_tensors: offloading output layer to GPU
0.00.107.504 I load_tensors: offloaded 25/25 layers to GPU
0.00.107.522 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.523 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.107.823 I llama_init_from_model: n_seq_max     = 1
0.00.107.824 I llama_init_from_model: n_ctx         = 2048
0.00.107.824 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.107.825 I llama_init_from_model: n_batch       = 2048
0.00.107.825 I llama_init_from_model: n_ubatch      = 512
0.00.107.825 I llama_init_from_model: flash_attn    = 0
0.00.107.825 I llama_init_from_model: freq_base     = 10000.0
0.00.107.826 I llama_init_from_model: freq_scale    = 1
0.00.107.826 I ggml_metal_init: allocating
0.00.107.829 I ggml_metal_init: found device: Apple M4
0.00.107.831 I ggml_metal_init: picking default device: Apple M4
0.00.108.494 I ggml_metal_init: using embedded metal library
0.00.126.768 I ggml_metal_init: GPU name:   Apple M4
0.00.126.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.126.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.126.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.126.771 I ggml_metal_init: simdgroup reduction   = true
0.00.126.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.126.771 I ggml_metal_init: has bfloat            = true
0.00.126.771 I ggml_metal_init: use bfloat            = true
0.00.126.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.126.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.184.622 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.206.553 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.206.559 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.206.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.207.557 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.207.559 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.207.559 I llama_init_from_model: graph nodes  = 967
0.00.207.559 I llama_init_from_model: graph splits = 2
0.00.207.563 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.207.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.207.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.291.790 I main: llama threadpool init, n_threads = 4
0.00.291.832 I 
0.00.291.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.291.854 I 
0.00.291.927 I sampler seed: 1234
0.00.291.932 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.291.957 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.291.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.291.959 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.125.251 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.125.252 I llama_perf_context_print:        load time =     254.38 ms
0.02.125.253 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.08 tokens per second)
0.02.125.254 I llama_perf_context_print:        eval time =    1786.70 ms /    63 runs   (   28.36 ms per token,    35.26 tokens per second)
0.02.125.254 I llama_perf_context_print:       total time =    1833.47 ms /    70 tokens
0.02.125.515 I ggml_metal_free: deallocating

real	0m2.436s
user	0m0.146s
sys	0m0.108s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.892 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.344 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.346 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.281 I llama_model_loader: - type  f32:  194 tensors
0.00.034.282 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.283 I print_info: file format = GGUF V3 (latest)
0.00.034.283 I print_info: file type   = Q8_0
0.00.034.284 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.015 I load: special tokens cache size = 25
0.00.060.752 I load: token to piece cache size = 0.2984 MB
0.00.060.756 I print_info: arch             = gptneox
0.00.060.757 I print_info: vocab_only       = 0
0.00.060.758 I print_info: n_ctx_train      = 2048
0.00.060.759 I print_info: n_embd           = 2048
0.00.060.759 I print_info: n_layer          = 24
0.00.060.765 I print_info: n_head           = 16
0.00.060.765 I print_info: n_head_kv        = 16
0.00.060.766 I print_info: n_rot            = 32
0.00.060.766 I print_info: n_swa            = 0
0.00.060.766 I print_info: n_embd_head_k    = 128
0.00.060.766 I print_info: n_embd_head_v    = 128
0.00.060.767 I print_info: n_gqa            = 1
0.00.060.768 I print_info: n_embd_k_gqa     = 2048
0.00.060.770 I print_info: n_embd_v_gqa     = 2048
0.00.060.771 I print_info: f_norm_eps       = 1.0e-05
0.00.060.771 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.772 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.772 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.772 I print_info: f_logit_scale    = 0.0e+00
0.00.060.773 I print_info: n_ff             = 8192
0.00.060.773 I print_info: n_expert         = 0
0.00.060.773 I print_info: n_expert_used    = 0
0.00.060.773 I print_info: causal attn      = 1
0.00.060.773 I print_info: pooling type     = 0
0.00.060.774 I print_info: rope type        = 2
0.00.060.774 I print_info: rope scaling     = linear
0.00.060.775 I print_info: freq_base_train  = 10000.0
0.00.060.775 I print_info: freq_scale_train = 1
0.00.060.775 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.775 I print_info: rope_finetuned   = unknown
0.00.060.776 I print_info: ssm_d_conv       = 0
0.00.060.776 I print_info: ssm_d_inner      = 0
0.00.060.776 I print_info: ssm_d_state      = 0
0.00.060.776 I print_info: ssm_dt_rank      = 0
0.00.060.776 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.777 I print_info: model type       = 1.4B
0.00.060.777 I print_info: model params     = 1.41 B
0.00.060.777 I print_info: general.name     = 1.4B
0.00.060.778 I print_info: vocab type       = BPE
0.00.060.778 I print_info: n_vocab          = 50304
0.00.060.778 I print_info: n_merges         = 50009
0.00.060.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.779 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.779 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.779 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.779 I print_info: LF token         = 128 'Ä'
0.00.060.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.781 I print_info: max token length = 1024
0.00.063.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.212 I load_tensors: offloading output layer to GPU
0.00.063.212 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.223 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.225 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.600 I llama_init_from_model: n_seq_max     = 1
0.00.063.600 I llama_init_from_model: n_ctx         = 2048
0.00.063.601 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.063.601 I llama_init_from_model: n_batch       = 2048
0.00.063.601 I llama_init_from_model: n_ubatch      = 512
0.00.063.601 I llama_init_from_model: flash_attn    = 0
0.00.063.601 I llama_init_from_model: freq_base     = 10000.0
0.00.063.602 I llama_init_from_model: freq_scale    = 1
0.00.063.602 I ggml_metal_init: allocating
0.00.063.605 I ggml_metal_init: found device: Apple M4
0.00.063.608 I ggml_metal_init: picking default device: Apple M4
0.00.064.355 I ggml_metal_init: using embedded metal library
0.00.066.939 I ggml_metal_init: GPU name:   Apple M4
0.00.066.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.942 I ggml_metal_init: simdgroup reduction   = true
0.00.066.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.942 I ggml_metal_init: has bfloat            = true
0.00.066.942 I ggml_metal_init: use bfloat            = true
0.00.066.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.944 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.519 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.575 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.589 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.773 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.775 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.775 I llama_init_from_model: graph nodes  = 967
0.00.104.775 I llama_init_from_model: graph splits = 2
0.00.104.783 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.916 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.313.511 I main: llama threadpool init, n_threads = 4
0.01.313.589 I 
0.01.313.641 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.313.643 I 
0.01.314.219 I sampler seed: 1234
0.01.314.225 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.314.273 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.314.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.314.273 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.410.092 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.02.410.093 I llama_perf_context_print:        load time =    1303.61 ms
0.02.410.094 I llama_perf_context_print: prompt eval time =      50.50 ms /     7 tokens (    7.21 ms per token,   138.61 tokens per second)
0.02.410.094 I llama_perf_context_print:        eval time =    1042.39 ms /    63 runs   (   16.55 ms per token,    60.44 tokens per second)
0.02.410.095 I llama_perf_context_print:       total time =    1096.59 ms /    70 tokens
0.02.410.307 I ggml_metal_free: deallocating

real	0m2.429s
user	0m0.121s
sys	0m0.246s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.017.635 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.335 I llama_model_loader: - type  f32:  194 tensors
0.00.036.336 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.337 I print_info: file format = GGUF V3 (latest)
0.00.036.337 I print_info: file type   = Q4_0
0.00.036.338 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.109 I load: special tokens cache size = 25
0.00.070.979 I load: token to piece cache size = 0.2984 MB
0.00.070.984 I print_info: arch             = gptneox
0.00.070.984 I print_info: vocab_only       = 0
0.00.070.984 I print_info: n_ctx_train      = 2048
0.00.070.985 I print_info: n_embd           = 2048
0.00.070.985 I print_info: n_layer          = 24
0.00.070.990 I print_info: n_head           = 16
0.00.070.991 I print_info: n_head_kv        = 16
0.00.070.991 I print_info: n_rot            = 32
0.00.070.991 I print_info: n_swa            = 0
0.00.070.992 I print_info: n_embd_head_k    = 128
0.00.070.992 I print_info: n_embd_head_v    = 128
0.00.070.993 I print_info: n_gqa            = 1
0.00.070.994 I print_info: n_embd_k_gqa     = 2048
0.00.070.995 I print_info: n_embd_v_gqa     = 2048
0.00.070.996 I print_info: f_norm_eps       = 1.0e-05
0.00.070.996 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.996 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.997 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.997 I print_info: f_logit_scale    = 0.0e+00
0.00.071.000 I print_info: n_ff             = 8192
0.00.071.000 I print_info: n_expert         = 0
0.00.071.001 I print_info: n_expert_used    = 0
0.00.071.001 I print_info: causal attn      = 1
0.00.071.001 I print_info: pooling type     = 0
0.00.071.001 I print_info: rope type        = 2
0.00.071.001 I print_info: rope scaling     = linear
0.00.071.002 I print_info: freq_base_train  = 10000.0
0.00.071.002 I print_info: freq_scale_train = 1
0.00.071.003 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.003 I print_info: rope_finetuned   = unknown
0.00.071.003 I print_info: ssm_d_conv       = 0
0.00.071.003 I print_info: ssm_d_inner      = 0
0.00.071.003 I print_info: ssm_d_state      = 0
0.00.071.003 I print_info: ssm_dt_rank      = 0
0.00.071.004 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.004 I print_info: model type       = 1.4B
0.00.071.004 I print_info: model params     = 1.41 B
0.00.071.004 I print_info: general.name     = 1.4B
0.00.071.005 I print_info: vocab type       = BPE
0.00.071.006 I print_info: n_vocab          = 50304
0.00.071.006 I print_info: n_merges         = 50009
0.00.071.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.011 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.011 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.012 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.012 I print_info: LF token         = 128 'Ä'
0.00.071.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.013 I print_info: max token length = 1024
0.00.073.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.073.964 I load_tensors: offloading output layer to GPU
0.00.073.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.073.978 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.073.979 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.074.416 I llama_init_from_model: n_seq_max     = 1
0.00.074.417 I llama_init_from_model: n_ctx         = 2048
0.00.074.417 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.074.417 I llama_init_from_model: n_batch       = 2048
0.00.074.418 I llama_init_from_model: n_ubatch      = 512
0.00.074.418 I llama_init_from_model: flash_attn    = 0
0.00.074.418 I llama_init_from_model: freq_base     = 10000.0
0.00.074.419 I llama_init_from_model: freq_scale    = 1
0.00.074.419 I ggml_metal_init: allocating
0.00.074.424 I ggml_metal_init: found device: Apple M4
0.00.074.426 I ggml_metal_init: picking default device: Apple M4
0.00.075.352 I ggml_metal_init: using embedded metal library
0.00.079.503 I ggml_metal_init: GPU name:   Apple M4
0.00.079.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.507 I ggml_metal_init: simdgroup reduction   = true
0.00.079.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.507 I ggml_metal_init: has bfloat            = true
0.00.079.507 I ggml_metal_init: use bfloat            = true
0.00.079.508 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.673 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.223 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.231 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.255 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.118.376 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.118.378 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.118.379 I llama_init_from_model: graph nodes  = 967
0.00.118.379 I llama_init_from_model: graph splits = 2
0.00.118.384 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.513 I main: llama threadpool init, n_threads = 4
0.00.822.559 I 
0.00.822.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.588 I 
0.00.822.833 I sampler seed: 1234
0.00.822.839 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.887 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.889 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.497.695 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.497.695 I llama_perf_context_print:        load time =     804.87 ms
0.01.497.696 I llama_perf_context_print: prompt eval time =      43.27 ms /     7 tokens (    6.18 ms per token,   161.79 tokens per second)
0.01.497.697 I llama_perf_context_print:        eval time =     628.54 ms /    63 runs   (    9.98 ms per token,   100.23 tokens per second)
0.01.497.698 I llama_perf_context_print:       total time =     675.19 ms /    70 tokens
0.01.497.933 I ggml_metal_free: deallocating

real	0m1.524s
user	0m0.128s
sys	0m0.175s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.022 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.552 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.524 I llama_model_loader: - type  f32:  194 tensors
0.00.034.524 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.524 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.525 I print_info: file format = GGUF V3 (latest)
0.00.034.525 I print_info: file type   = Q4_1
0.00.034.526 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.918 I load: special tokens cache size = 25
0.00.064.407 I load: token to piece cache size = 0.2984 MB
0.00.064.411 I print_info: arch             = gptneox
0.00.064.411 I print_info: vocab_only       = 0
0.00.064.411 I print_info: n_ctx_train      = 2048
0.00.064.411 I print_info: n_embd           = 2048
0.00.064.412 I print_info: n_layer          = 24
0.00.064.414 I print_info: n_head           = 16
0.00.064.415 I print_info: n_head_kv        = 16
0.00.064.415 I print_info: n_rot            = 32
0.00.064.415 I print_info: n_swa            = 0
0.00.064.416 I print_info: n_embd_head_k    = 128
0.00.064.416 I print_info: n_embd_head_v    = 128
0.00.064.416 I print_info: n_gqa            = 1
0.00.064.419 I print_info: n_embd_k_gqa     = 2048
0.00.064.420 I print_info: n_embd_v_gqa     = 2048
0.00.064.421 I print_info: f_norm_eps       = 1.0e-05
0.00.064.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.422 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.422 I print_info: f_logit_scale    = 0.0e+00
0.00.064.424 I print_info: n_ff             = 8192
0.00.064.424 I print_info: n_expert         = 0
0.00.064.424 I print_info: n_expert_used    = 0
0.00.064.424 I print_info: causal attn      = 1
0.00.064.424 I print_info: pooling type     = 0
0.00.064.424 I print_info: rope type        = 2
0.00.064.431 I print_info: rope scaling     = linear
0.00.064.432 I print_info: freq_base_train  = 10000.0
0.00.064.433 I print_info: freq_scale_train = 1
0.00.064.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.433 I print_info: rope_finetuned   = unknown
0.00.064.433 I print_info: ssm_d_conv       = 0
0.00.064.434 I print_info: ssm_d_inner      = 0
0.00.064.434 I print_info: ssm_d_state      = 0
0.00.064.435 I print_info: ssm_dt_rank      = 0
0.00.064.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.436 I print_info: model type       = 1.4B
0.00.064.436 I print_info: model params     = 1.41 B
0.00.064.437 I print_info: general.name     = 1.4B
0.00.064.437 I print_info: vocab type       = BPE
0.00.064.437 I print_info: n_vocab          = 50304
0.00.064.438 I print_info: n_merges         = 50009
0.00.064.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.439 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.439 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.439 I print_info: LF token         = 128 'Ä'
0.00.064.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.441 I print_info: max token length = 1024
0.00.066.681 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.681 I load_tensors: offloading output layer to GPU
0.00.066.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.692 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.066.694 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.067.022 I llama_init_from_model: n_seq_max     = 1
0.00.067.023 I llama_init_from_model: n_ctx         = 2048
0.00.067.023 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.023 I llama_init_from_model: n_batch       = 2048
0.00.067.023 I llama_init_from_model: n_ubatch      = 512
0.00.067.024 I llama_init_from_model: flash_attn    = 0
0.00.067.024 I llama_init_from_model: freq_base     = 10000.0
0.00.067.024 I llama_init_from_model: freq_scale    = 1
0.00.067.025 I ggml_metal_init: allocating
0.00.067.028 I ggml_metal_init: found device: Apple M4
0.00.067.030 I ggml_metal_init: picking default device: Apple M4
0.00.067.680 I ggml_metal_init: using embedded metal library
0.00.070.589 I ggml_metal_init: GPU name:   Apple M4
0.00.070.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.592 I ggml_metal_init: simdgroup reduction   = true
0.00.070.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.592 I ggml_metal_init: has bfloat            = true
0.00.070.592 I ggml_metal_init: use bfloat            = true
0.00.070.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.911 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.594 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.599 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.619 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.645 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.646 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.646 I llama_init_from_model: graph nodes  = 967
0.00.104.647 I llama_init_from_model: graph splits = 2
0.00.104.649 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.829 I main: llama threadpool init, n_threads = 4
0.00.820.874 I 
0.00.820.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.902 I 
0.00.821.134 I sampler seed: 1234
0.00.821.141 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.179 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.196 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.196 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.544.431 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.544.431 I llama_perf_context_print:        load time =     811.80 ms
0.01.544.432 I llama_perf_context_print: prompt eval time =      46.18 ms /     7 tokens (    6.60 ms per token,   151.57 tokens per second)
0.01.544.433 I llama_perf_context_print:        eval time =     674.03 ms /    63 runs   (   10.70 ms per token,    93.47 tokens per second)
0.01.544.433 I llama_perf_context_print:       total time =     723.61 ms /    70 tokens
0.01.544.656 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.116s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.495 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.050 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.826 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.479 I llama_model_loader: - type  f32:  194 tensors
0.00.027.479 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.480 I print_info: file format = GGUF V3 (latest)
0.00.027.480 I print_info: file type   = Q5_0
0.00.027.481 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.640 I load: special tokens cache size = 25
0.00.052.417 I load: token to piece cache size = 0.2984 MB
0.00.052.420 I print_info: arch             = gptneox
0.00.052.420 I print_info: vocab_only       = 0
0.00.052.421 I print_info: n_ctx_train      = 2048
0.00.052.421 I print_info: n_embd           = 2048
0.00.052.421 I print_info: n_layer          = 24
0.00.052.424 I print_info: n_head           = 16
0.00.052.424 I print_info: n_head_kv        = 16
0.00.052.424 I print_info: n_rot            = 32
0.00.052.425 I print_info: n_swa            = 0
0.00.052.425 I print_info: n_embd_head_k    = 128
0.00.052.425 I print_info: n_embd_head_v    = 128
0.00.052.427 I print_info: n_gqa            = 1
0.00.052.427 I print_info: n_embd_k_gqa     = 2048
0.00.052.430 I print_info: n_embd_v_gqa     = 2048
0.00.052.430 I print_info: f_norm_eps       = 1.0e-05
0.00.052.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.431 I print_info: f_logit_scale    = 0.0e+00
0.00.052.432 I print_info: n_ff             = 8192
0.00.052.434 I print_info: n_expert         = 0
0.00.052.434 I print_info: n_expert_used    = 0
0.00.052.434 I print_info: causal attn      = 1
0.00.052.434 I print_info: pooling type     = 0
0.00.052.434 I print_info: rope type        = 2
0.00.052.434 I print_info: rope scaling     = linear
0.00.052.435 I print_info: freq_base_train  = 10000.0
0.00.052.435 I print_info: freq_scale_train = 1
0.00.052.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.435 I print_info: rope_finetuned   = unknown
0.00.052.436 I print_info: ssm_d_conv       = 0
0.00.052.436 I print_info: ssm_d_inner      = 0
0.00.052.436 I print_info: ssm_d_state      = 0
0.00.052.436 I print_info: ssm_dt_rank      = 0
0.00.052.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.436 I print_info: model type       = 1.4B
0.00.052.437 I print_info: model params     = 1.41 B
0.00.052.437 I print_info: general.name     = 1.4B
0.00.052.437 I print_info: vocab type       = BPE
0.00.052.438 I print_info: n_vocab          = 50304
0.00.052.438 I print_info: n_merges         = 50009
0.00.052.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.442 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.442 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.443 I print_info: LF token         = 128 'Ä'
0.00.052.443 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.443 I print_info: max token length = 1024
0.00.054.202 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.202 I load_tensors: offloading output layer to GPU
0.00.054.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.208 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.210 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.782 I llama_init_from_model: n_seq_max     = 1
0.00.054.783 I llama_init_from_model: n_ctx         = 2048
0.00.054.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.783 I llama_init_from_model: n_batch       = 2048
0.00.054.783 I llama_init_from_model: n_ubatch      = 512
0.00.054.784 I llama_init_from_model: flash_attn    = 0
0.00.054.784 I llama_init_from_model: freq_base     = 10000.0
0.00.054.784 I llama_init_from_model: freq_scale    = 1
0.00.054.785 I ggml_metal_init: allocating
0.00.054.788 I ggml_metal_init: found device: Apple M4
0.00.054.790 I ggml_metal_init: picking default device: Apple M4
0.00.055.353 I ggml_metal_init: using embedded metal library
0.00.057.701 I ggml_metal_init: GPU name:   Apple M4
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.704 I ggml_metal_init: simdgroup reduction   = true
0.00.057.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.704 I ggml_metal_init: has bfloat            = true
0.00.057.704 I ggml_metal_init: use bfloat            = true
0.00.057.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.991 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.103 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.107 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.132 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.132 I llama_init_from_model: graph nodes  = 967
0.00.087.133 I llama_init_from_model: graph splits = 2
0.00.087.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.327 I main: llama threadpool init, n_threads = 4
0.00.776.365 I 
0.00.776.390 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.392 I 
0.00.776.609 I sampler seed: 1234
0.00.776.614 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.657 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.560.718 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.560.719 I llama_perf_context_print:        load time =     765.82 ms
0.01.560.719 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.47 tokens per second)
0.01.560.720 I llama_perf_context_print:        eval time =     738.07 ms /    63 runs   (   11.72 ms per token,    85.36 tokens per second)
0.01.560.721 I llama_perf_context_print:       total time =     784.40 ms /    70 tokens
0.01.560.962 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.879 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.643 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.643 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.644 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.067 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.070 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.070 I llama_model_loader: - type  f32:  194 tensors
0.00.025.071 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.071 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.071 I print_info: file format = GGUF V3 (latest)
0.00.025.072 I print_info: file type   = Q5_1
0.00.025.073 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.355 I load: special tokens cache size = 25
0.00.050.447 I load: token to piece cache size = 0.2984 MB
0.00.050.451 I print_info: arch             = gptneox
0.00.050.451 I print_info: vocab_only       = 0
0.00.050.451 I print_info: n_ctx_train      = 2048
0.00.050.451 I print_info: n_embd           = 2048
0.00.050.451 I print_info: n_layer          = 24
0.00.050.454 I print_info: n_head           = 16
0.00.050.455 I print_info: n_head_kv        = 16
0.00.050.455 I print_info: n_rot            = 32
0.00.050.456 I print_info: n_swa            = 0
0.00.050.456 I print_info: n_embd_head_k    = 128
0.00.050.456 I print_info: n_embd_head_v    = 128
0.00.050.457 I print_info: n_gqa            = 1
0.00.050.459 I print_info: n_embd_k_gqa     = 2048
0.00.050.460 I print_info: n_embd_v_gqa     = 2048
0.00.050.461 I print_info: f_norm_eps       = 1.0e-05
0.00.050.461 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.461 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.461 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.462 I print_info: f_logit_scale    = 0.0e+00
0.00.050.462 I print_info: n_ff             = 8192
0.00.050.463 I print_info: n_expert         = 0
0.00.050.463 I print_info: n_expert_used    = 0
0.00.050.464 I print_info: causal attn      = 1
0.00.050.466 I print_info: pooling type     = 0
0.00.050.466 I print_info: rope type        = 2
0.00.050.466 I print_info: rope scaling     = linear
0.00.050.467 I print_info: freq_base_train  = 10000.0
0.00.050.467 I print_info: freq_scale_train = 1
0.00.050.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.468 I print_info: rope_finetuned   = unknown
0.00.050.468 I print_info: ssm_d_conv       = 0
0.00.050.468 I print_info: ssm_d_inner      = 0
0.00.050.468 I print_info: ssm_d_state      = 0
0.00.050.468 I print_info: ssm_dt_rank      = 0
0.00.050.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.472 I print_info: model type       = 1.4B
0.00.050.472 I print_info: model params     = 1.41 B
0.00.050.473 I print_info: general.name     = 1.4B
0.00.050.473 I print_info: vocab type       = BPE
0.00.050.473 I print_info: n_vocab          = 50304
0.00.050.473 I print_info: n_merges         = 50009
0.00.050.474 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.474 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.475 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.476 I print_info: LF token         = 128 'Ä'
0.00.050.476 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.476 I print_info: max token length = 1024
0.00.052.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.510 I load_tensors: offloading output layer to GPU
0.00.052.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.520 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.521 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.806 I llama_init_from_model: n_seq_max     = 1
0.00.052.807 I llama_init_from_model: n_ctx         = 2048
0.00.052.807 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.807 I llama_init_from_model: n_batch       = 2048
0.00.052.808 I llama_init_from_model: n_ubatch      = 512
0.00.052.808 I llama_init_from_model: flash_attn    = 0
0.00.052.808 I llama_init_from_model: freq_base     = 10000.0
0.00.052.808 I llama_init_from_model: freq_scale    = 1
0.00.052.809 I ggml_metal_init: allocating
0.00.052.812 I ggml_metal_init: found device: Apple M4
0.00.052.814 I ggml_metal_init: picking default device: Apple M4
0.00.053.431 I ggml_metal_init: using embedded metal library
0.00.055.772 I ggml_metal_init: GPU name:   Apple M4
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.774 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.774 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.774 I ggml_metal_init: simdgroup reduction   = true
0.00.055.775 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.775 I ggml_metal_init: has bfloat            = true
0.00.055.775 I ggml_metal_init: use bfloat            = true
0.00.055.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.513 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.522 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.543 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.428 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.430 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.430 I llama_init_from_model: graph nodes  = 967
0.00.085.430 I llama_init_from_model: graph splits = 2
0.00.085.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.516 I main: llama threadpool init, n_threads = 4
0.00.696.557 I 
0.00.696.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.580 I 
0.00.696.821 I sampler seed: 1234
0.00.696.827 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.900 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.900 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.546.303 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47811.45 tokens per second)
0.01.546.303 I llama_perf_context_print:        load time =     687.63 ms
0.01.546.305 I llama_perf_context_print: prompt eval time =      46.80 ms /     7 tokens (    6.69 ms per token,   149.58 tokens per second)
0.01.546.306 I llama_perf_context_print:        eval time =     799.97 ms /    63 runs   (   12.70 ms per token,    78.75 tokens per second)
0.01.546.306 I llama_perf_context_print:       total time =     849.79 ms /    70 tokens
0.01.546.624 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.110s
sys	0m0.132s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.072 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.151 I llama_model_loader: - type  f32:  194 tensors
0.00.025.152 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.152 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.152 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.153 I print_info: file format = GGUF V3 (latest)
0.00.025.153 I print_info: file type   = Q2_K - Medium
0.00.025.155 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.733 I load: special tokens cache size = 25
0.00.049.582 I load: token to piece cache size = 0.2984 MB
0.00.049.585 I print_info: arch             = gptneox
0.00.049.585 I print_info: vocab_only       = 0
0.00.049.585 I print_info: n_ctx_train      = 2048
0.00.049.586 I print_info: n_embd           = 2048
0.00.049.586 I print_info: n_layer          = 24
0.00.049.589 I print_info: n_head           = 16
0.00.049.590 I print_info: n_head_kv        = 16
0.00.049.590 I print_info: n_rot            = 32
0.00.049.590 I print_info: n_swa            = 0
0.00.049.590 I print_info: n_embd_head_k    = 128
0.00.049.590 I print_info: n_embd_head_v    = 128
0.00.049.591 I print_info: n_gqa            = 1
0.00.049.592 I print_info: n_embd_k_gqa     = 2048
0.00.049.593 I print_info: n_embd_v_gqa     = 2048
0.00.049.593 I print_info: f_norm_eps       = 1.0e-05
0.00.049.594 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.594 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.594 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.594 I print_info: f_logit_scale    = 0.0e+00
0.00.049.596 I print_info: n_ff             = 8192
0.00.049.596 I print_info: n_expert         = 0
0.00.049.596 I print_info: n_expert_used    = 0
0.00.049.596 I print_info: causal attn      = 1
0.00.049.596 I print_info: pooling type     = 0
0.00.049.596 I print_info: rope type        = 2
0.00.049.597 I print_info: rope scaling     = linear
0.00.049.597 I print_info: freq_base_train  = 10000.0
0.00.049.597 I print_info: freq_scale_train = 1
0.00.049.598 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.598 I print_info: rope_finetuned   = unknown
0.00.049.598 I print_info: ssm_d_conv       = 0
0.00.049.598 I print_info: ssm_d_inner      = 0
0.00.049.598 I print_info: ssm_d_state      = 0
0.00.049.598 I print_info: ssm_dt_rank      = 0
0.00.049.599 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.599 I print_info: model type       = 1.4B
0.00.049.599 I print_info: model params     = 1.41 B
0.00.049.599 I print_info: general.name     = 1.4B
0.00.049.600 I print_info: vocab type       = BPE
0.00.049.602 I print_info: n_vocab          = 50304
0.00.049.602 I print_info: n_merges         = 50009
0.00.049.602 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.602 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.603 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.603 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.603 I print_info: LF token         = 128 'Ä'
0.00.049.603 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.605 I print_info: max token length = 1024
0.00.051.444 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.445 I load_tensors: offloading output layer to GPU
0.00.051.445 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.455 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.457 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.725 I llama_init_from_model: n_seq_max     = 1
0.00.051.726 I llama_init_from_model: n_ctx         = 2048
0.00.051.726 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.726 I llama_init_from_model: n_batch       = 2048
0.00.051.726 I llama_init_from_model: n_ubatch      = 512
0.00.051.727 I llama_init_from_model: flash_attn    = 0
0.00.051.727 I llama_init_from_model: freq_base     = 10000.0
0.00.051.727 I llama_init_from_model: freq_scale    = 1
0.00.051.728 I ggml_metal_init: allocating
0.00.051.731 I ggml_metal_init: found device: Apple M4
0.00.051.732 I ggml_metal_init: picking default device: Apple M4
0.00.052.303 I ggml_metal_init: using embedded metal library
0.00.054.635 I ggml_metal_init: GPU name:   Apple M4
0.00.054.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.637 I ggml_metal_init: simdgroup reduction   = true
0.00.054.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.638 I ggml_metal_init: has bfloat            = true
0.00.054.638 I ggml_metal_init: use bfloat            = true
0.00.054.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.918 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.162 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.242 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.245 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.245 I llama_init_from_model: graph nodes  = 967
0.00.085.245 I llama_init_from_model: graph splits = 2
0.00.085.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.408 I main: llama threadpool init, n_threads = 4
0.00.437.453 I 
0.00.437.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.480 I 
0.00.437.720 I sampler seed: 1234
0.00.437.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.748 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.749 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.749 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.115.932 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.115.932 I llama_perf_context_print:        load time =     427.33 ms
0.01.115.933 I llama_perf_context_print: prompt eval time =      36.00 ms /     7 tokens (    5.14 ms per token,   194.43 tokens per second)
0.01.115.934 I llama_perf_context_print:        eval time =     639.20 ms /    63 runs   (   10.15 ms per token,    98.56 tokens per second)
0.01.115.934 I llama_perf_context_print:       total time =     678.53 ms /    70 tokens
0.01.116.179 I ggml_metal_free: deallocating

real	0m1.134s
user	0m0.110s
sys	0m0.108s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.372 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.620 I llama_model_loader: - type  f32:  194 tensors
0.00.025.621 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.621 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.621 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.621 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.622 I print_info: file format = GGUF V3 (latest)
0.00.025.623 I print_info: file type   = Q3_K - Medium
0.00.025.624 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.094 I load: special tokens cache size = 25
0.00.049.847 I load: token to piece cache size = 0.2984 MB
0.00.049.851 I print_info: arch             = gptneox
0.00.049.851 I print_info: vocab_only       = 0
0.00.049.851 I print_info: n_ctx_train      = 2048
0.00.049.851 I print_info: n_embd           = 2048
0.00.049.852 I print_info: n_layer          = 24
0.00.049.854 I print_info: n_head           = 16
0.00.049.855 I print_info: n_head_kv        = 16
0.00.049.856 I print_info: n_rot            = 32
0.00.049.856 I print_info: n_swa            = 0
0.00.049.856 I print_info: n_embd_head_k    = 128
0.00.049.858 I print_info: n_embd_head_v    = 128
0.00.049.859 I print_info: n_gqa            = 1
0.00.049.860 I print_info: n_embd_k_gqa     = 2048
0.00.049.861 I print_info: n_embd_v_gqa     = 2048
0.00.049.861 I print_info: f_norm_eps       = 1.0e-05
0.00.049.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.863 I print_info: f_logit_scale    = 0.0e+00
0.00.049.864 I print_info: n_ff             = 8192
0.00.049.866 I print_info: n_expert         = 0
0.00.049.868 I print_info: n_expert_used    = 0
0.00.049.868 I print_info: causal attn      = 1
0.00.049.868 I print_info: pooling type     = 0
0.00.049.868 I print_info: rope type        = 2
0.00.049.868 I print_info: rope scaling     = linear
0.00.049.869 I print_info: freq_base_train  = 10000.0
0.00.049.869 I print_info: freq_scale_train = 1
0.00.049.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.869 I print_info: rope_finetuned   = unknown
0.00.049.869 I print_info: ssm_d_conv       = 0
0.00.049.870 I print_info: ssm_d_inner      = 0
0.00.049.870 I print_info: ssm_d_state      = 0
0.00.049.871 I print_info: ssm_dt_rank      = 0
0.00.049.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.874 I print_info: model type       = 1.4B
0.00.049.876 I print_info: model params     = 1.41 B
0.00.049.876 I print_info: general.name     = 1.4B
0.00.049.877 I print_info: vocab type       = BPE
0.00.049.877 I print_info: n_vocab          = 50304
0.00.049.877 I print_info: n_merges         = 50009
0.00.049.877 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.877 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.878 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.878 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.878 I print_info: LF token         = 128 'Ä'
0.00.049.878 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.878 I print_info: max token length = 1024
0.00.051.755 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.755 I load_tensors: offloading output layer to GPU
0.00.051.755 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.765 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.767 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.053 I llama_init_from_model: n_seq_max     = 1
0.00.052.054 I llama_init_from_model: n_ctx         = 2048
0.00.052.054 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.054 I llama_init_from_model: n_batch       = 2048
0.00.052.054 I llama_init_from_model: n_ubatch      = 512
0.00.052.055 I llama_init_from_model: flash_attn    = 0
0.00.052.055 I llama_init_from_model: freq_base     = 10000.0
0.00.052.055 I llama_init_from_model: freq_scale    = 1
0.00.052.056 I ggml_metal_init: allocating
0.00.052.059 I ggml_metal_init: found device: Apple M4
0.00.052.061 I ggml_metal_init: picking default device: Apple M4
0.00.052.642 I ggml_metal_init: using embedded metal library
0.00.054.953 I ggml_metal_init: GPU name:   Apple M4
0.00.054.955 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.955 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.955 I ggml_metal_init: simdgroup reduction   = true
0.00.054.956 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.956 I ggml_metal_init: has bfloat            = true
0.00.054.956 I ggml_metal_init: use bfloat            = true
0.00.054.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.467 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.326 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.334 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.368 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.436 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.437 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.438 I llama_init_from_model: graph nodes  = 967
0.00.085.438 I llama_init_from_model: graph splits = 2
0.00.085.441 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.698 I main: llama threadpool init, n_threads = 4
0.00.540.735 I 
0.00.540.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.771 I 
0.00.541.012 I sampler seed: 1234
0.00.541.016 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.541.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.541.068 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.541.068 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.280.416 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.280.417 I llama_perf_context_print:        load time =     531.32 ms
0.01.280.417 I llama_perf_context_print: prompt eval time =      40.48 ms /     7 tokens (    5.78 ms per token,   172.94 tokens per second)
0.01.280.418 I llama_perf_context_print:        eval time =     695.77 ms /    63 runs   (   11.04 ms per token,    90.55 tokens per second)
0.01.280.418 I llama_perf_context_print:       total time =     739.72 ms /    70 tokens
0.01.280.620 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.107s
sys	0m0.130s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.119 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.718 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.392 I llama_model_loader: - type  f32:  194 tensors
0.00.026.392 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.393 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.393 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.393 I print_info: file format = GGUF V3 (latest)
0.00.026.394 I print_info: file type   = Q4_K - Medium
0.00.026.395 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.668 I load: special tokens cache size = 25
0.00.051.484 I load: token to piece cache size = 0.2984 MB
0.00.051.487 I print_info: arch             = gptneox
0.00.051.488 I print_info: vocab_only       = 0
0.00.051.488 I print_info: n_ctx_train      = 2048
0.00.051.488 I print_info: n_embd           = 2048
0.00.051.488 I print_info: n_layer          = 24
0.00.051.491 I print_info: n_head           = 16
0.00.051.492 I print_info: n_head_kv        = 16
0.00.051.492 I print_info: n_rot            = 32
0.00.051.492 I print_info: n_swa            = 0
0.00.051.495 I print_info: n_embd_head_k    = 128
0.00.051.495 I print_info: n_embd_head_v    = 128
0.00.051.495 I print_info: n_gqa            = 1
0.00.051.496 I print_info: n_embd_k_gqa     = 2048
0.00.051.497 I print_info: n_embd_v_gqa     = 2048
0.00.051.497 I print_info: f_norm_eps       = 1.0e-05
0.00.051.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.498 I print_info: f_logit_scale    = 0.0e+00
0.00.051.499 I print_info: n_ff             = 8192
0.00.051.499 I print_info: n_expert         = 0
0.00.051.499 I print_info: n_expert_used    = 0
0.00.051.500 I print_info: causal attn      = 1
0.00.051.500 I print_info: pooling type     = 0
0.00.051.500 I print_info: rope type        = 2
0.00.051.500 I print_info: rope scaling     = linear
0.00.051.501 I print_info: freq_base_train  = 10000.0
0.00.051.501 I print_info: freq_scale_train = 1
0.00.051.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.506 I print_info: rope_finetuned   = unknown
0.00.051.506 I print_info: ssm_d_conv       = 0
0.00.051.506 I print_info: ssm_d_inner      = 0
0.00.051.506 I print_info: ssm_d_state      = 0
0.00.051.506 I print_info: ssm_dt_rank      = 0
0.00.051.506 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.507 I print_info: model type       = 1.4B
0.00.051.507 I print_info: model params     = 1.41 B
0.00.051.507 I print_info: general.name     = 1.4B
0.00.051.508 I print_info: vocab type       = BPE
0.00.051.508 I print_info: n_vocab          = 50304
0.00.051.508 I print_info: n_merges         = 50009
0.00.051.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.509 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.511 I print_info: LF token         = 128 'Ä'
0.00.051.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.512 I print_info: max token length = 1024
0.00.053.529 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.529 I load_tensors: offloading output layer to GPU
0.00.053.529 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.539 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.541 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.832 I llama_init_from_model: n_seq_max     = 1
0.00.053.833 I llama_init_from_model: n_ctx         = 2048
0.00.053.833 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.833 I llama_init_from_model: n_batch       = 2048
0.00.053.833 I llama_init_from_model: n_ubatch      = 512
0.00.053.833 I llama_init_from_model: flash_attn    = 0
0.00.053.834 I llama_init_from_model: freq_base     = 10000.0
0.00.053.834 I llama_init_from_model: freq_scale    = 1
0.00.053.834 I ggml_metal_init: allocating
0.00.053.837 I ggml_metal_init: found device: Apple M4
0.00.053.839 I ggml_metal_init: picking default device: Apple M4
0.00.054.425 I ggml_metal_init: using embedded metal library
0.00.056.758 I ggml_metal_init: GPU name:   Apple M4
0.00.056.760 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.761 I ggml_metal_init: simdgroup reduction   = true
0.00.056.761 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.761 I ggml_metal_init: has bfloat            = true
0.00.056.761 I ggml_metal_init: use bfloat            = true
0.00.056.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.762 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.449 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.948 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.953 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.971 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.042 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.043 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.043 I llama_init_from_model: graph nodes  = 967
0.00.088.044 I llama_init_from_model: graph splits = 2
0.00.088.046 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.069 I main: llama threadpool init, n_threads = 4
0.00.631.104 I 
0.00.631.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.128 I 
0.00.631.349 I sampler seed: 1234
0.00.631.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.631.365 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.631.366 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.631.366 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.386.096 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.386.097 I llama_perf_context_print:        load time =     620.95 ms
0.01.386.098 I llama_perf_context_print: prompt eval time =      50.99 ms /     7 tokens (    7.28 ms per token,   137.27 tokens per second)
0.01.386.098 I llama_perf_context_print:        eval time =     700.74 ms /    63 runs   (   11.12 ms per token,    89.90 tokens per second)
0.01.386.099 I llama_perf_context_print:       total time =     755.03 ms /    70 tokens
0.01.386.330 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.051 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.971 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.971 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.485 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.486 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.486 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.486 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.487 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.487 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.487 I llama_model_loader: - type  f32:  194 tensors
0.00.025.488 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.488 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.488 I print_info: file format = GGUF V3 (latest)
0.00.025.489 I print_info: file type   = Q5_K - Medium
0.00.025.489 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.919 I load: special tokens cache size = 25
0.00.049.758 I load: token to piece cache size = 0.2984 MB
0.00.049.760 I print_info: arch             = gptneox
0.00.049.761 I print_info: vocab_only       = 0
0.00.049.761 I print_info: n_ctx_train      = 2048
0.00.049.761 I print_info: n_embd           = 2048
0.00.049.761 I print_info: n_layer          = 24
0.00.049.764 I print_info: n_head           = 16
0.00.049.764 I print_info: n_head_kv        = 16
0.00.049.765 I print_info: n_rot            = 32
0.00.049.765 I print_info: n_swa            = 0
0.00.049.765 I print_info: n_embd_head_k    = 128
0.00.049.765 I print_info: n_embd_head_v    = 128
0.00.049.766 I print_info: n_gqa            = 1
0.00.049.767 I print_info: n_embd_k_gqa     = 2048
0.00.049.768 I print_info: n_embd_v_gqa     = 2048
0.00.049.768 I print_info: f_norm_eps       = 1.0e-05
0.00.049.770 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.770 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.770 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.770 I print_info: f_logit_scale    = 0.0e+00
0.00.049.771 I print_info: n_ff             = 8192
0.00.049.771 I print_info: n_expert         = 0
0.00.049.771 I print_info: n_expert_used    = 0
0.00.049.771 I print_info: causal attn      = 1
0.00.049.772 I print_info: pooling type     = 0
0.00.049.772 I print_info: rope type        = 2
0.00.049.772 I print_info: rope scaling     = linear
0.00.049.773 I print_info: freq_base_train  = 10000.0
0.00.049.773 I print_info: freq_scale_train = 1
0.00.049.773 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.774 I print_info: rope_finetuned   = unknown
0.00.049.774 I print_info: ssm_d_conv       = 0
0.00.049.774 I print_info: ssm_d_inner      = 0
0.00.049.775 I print_info: ssm_d_state      = 0
0.00.049.775 I print_info: ssm_dt_rank      = 0
0.00.049.776 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.776 I print_info: model type       = 1.4B
0.00.049.776 I print_info: model params     = 1.41 B
0.00.049.776 I print_info: general.name     = 1.4B
0.00.049.777 I print_info: vocab type       = BPE
0.00.049.777 I print_info: n_vocab          = 50304
0.00.049.777 I print_info: n_merges         = 50009
0.00.049.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.782 I print_info: LF token         = 128 'Ä'
0.00.049.783 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.783 I print_info: max token length = 1024
0.00.051.557 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.557 I load_tensors: offloading output layer to GPU
0.00.051.557 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.562 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.563 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.922 I llama_init_from_model: n_seq_max     = 1
0.00.051.923 I llama_init_from_model: n_ctx         = 2048
0.00.051.923 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.923 I llama_init_from_model: n_batch       = 2048
0.00.051.923 I llama_init_from_model: n_ubatch      = 512
0.00.051.924 I llama_init_from_model: flash_attn    = 0
0.00.051.924 I llama_init_from_model: freq_base     = 10000.0
0.00.051.924 I llama_init_from_model: freq_scale    = 1
0.00.051.925 I ggml_metal_init: allocating
0.00.051.927 I ggml_metal_init: found device: Apple M4
0.00.051.929 I ggml_metal_init: picking default device: Apple M4
0.00.052.516 I ggml_metal_init: using embedded metal library
0.00.054.865 I ggml_metal_init: GPU name:   Apple M4
0.00.054.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.867 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.867 I ggml_metal_init: simdgroup reduction   = true
0.00.054.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.867 I ggml_metal_init: has bfloat            = true
0.00.054.868 I ggml_metal_init: use bfloat            = true
0.00.054.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.483 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.413 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.418 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.436 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.444 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.445 I llama_init_from_model: graph nodes  = 967
0.00.085.446 I llama_init_from_model: graph splits = 2
0.00.085.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.531 I main: llama threadpool init, n_threads = 4
0.00.688.569 I 
0.00.688.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.608 I 
0.00.688.829 I sampler seed: 1234
0.00.688.833 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.874 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.874 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.536.274 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.536.275 I llama_perf_context_print:        load time =     679.47 ms
0.01.536.276 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.67 tokens per second)
0.01.536.276 I llama_perf_context_print:        eval time =     792.89 ms /    63 runs   (   12.59 ms per token,    79.46 tokens per second)
0.01.536.276 I llama_perf_context_print:       total time =     847.75 ms /    70 tokens
0.01.536.508 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.107s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.797 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.056 I llama_model_loader: - type  f32:  194 tensors
0.00.026.057 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.057 I print_info: file format = GGUF V3 (latest)
0.00.026.058 I print_info: file type   = Q6_K
0.00.026.058 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.523 I load: special tokens cache size = 25
0.00.050.252 I load: token to piece cache size = 0.2984 MB
0.00.050.256 I print_info: arch             = gptneox
0.00.050.256 I print_info: vocab_only       = 0
0.00.050.256 I print_info: n_ctx_train      = 2048
0.00.050.256 I print_info: n_embd           = 2048
0.00.050.257 I print_info: n_layer          = 24
0.00.050.259 I print_info: n_head           = 16
0.00.050.260 I print_info: n_head_kv        = 16
0.00.050.260 I print_info: n_rot            = 32
0.00.050.261 I print_info: n_swa            = 0
0.00.050.261 I print_info: n_embd_head_k    = 128
0.00.050.263 I print_info: n_embd_head_v    = 128
0.00.050.263 I print_info: n_gqa            = 1
0.00.050.264 I print_info: n_embd_k_gqa     = 2048
0.00.050.265 I print_info: n_embd_v_gqa     = 2048
0.00.050.265 I print_info: f_norm_eps       = 1.0e-05
0.00.050.266 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.266 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.266 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.266 I print_info: f_logit_scale    = 0.0e+00
0.00.050.267 I print_info: n_ff             = 8192
0.00.050.267 I print_info: n_expert         = 0
0.00.050.267 I print_info: n_expert_used    = 0
0.00.050.267 I print_info: causal attn      = 1
0.00.050.269 I print_info: pooling type     = 0
0.00.050.270 I print_info: rope type        = 2
0.00.050.271 I print_info: rope scaling     = linear
0.00.050.271 I print_info: freq_base_train  = 10000.0
0.00.050.271 I print_info: freq_scale_train = 1
0.00.050.272 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.273 I print_info: rope_finetuned   = unknown
0.00.050.273 I print_info: ssm_d_conv       = 0
0.00.050.273 I print_info: ssm_d_inner      = 0
0.00.050.273 I print_info: ssm_d_state      = 0
0.00.050.273 I print_info: ssm_dt_rank      = 0
0.00.050.274 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.274 I print_info: model type       = 1.4B
0.00.050.275 I print_info: model params     = 1.41 B
0.00.050.275 I print_info: general.name     = 1.4B
0.00.050.276 I print_info: vocab type       = BPE
0.00.050.276 I print_info: n_vocab          = 50304
0.00.050.276 I print_info: n_merges         = 50009
0.00.050.276 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.277 I print_info: LF token         = 128 'Ä'
0.00.050.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.278 I print_info: max token length = 1024
0.00.052.260 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.260 I load_tensors: offloading output layer to GPU
0.00.052.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.270 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.272 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.552 I llama_init_from_model: n_seq_max     = 1
0.00.052.553 I llama_init_from_model: n_ctx         = 2048
0.00.052.553 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.553 I llama_init_from_model: n_batch       = 2048
0.00.052.553 I llama_init_from_model: n_ubatch      = 512
0.00.052.554 I llama_init_from_model: flash_attn    = 0
0.00.052.554 I llama_init_from_model: freq_base     = 10000.0
0.00.052.554 I llama_init_from_model: freq_scale    = 1
0.00.052.555 I ggml_metal_init: allocating
0.00.052.558 I ggml_metal_init: found device: Apple M4
0.00.052.559 I ggml_metal_init: picking default device: Apple M4
0.00.053.147 I ggml_metal_init: using embedded metal library
0.00.055.450 I ggml_metal_init: GPU name:   Apple M4
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.453 I ggml_metal_init: simdgroup reduction   = true
0.00.055.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.453 I ggml_metal_init: has bfloat            = true
0.00.055.453 I ggml_metal_init: use bfloat            = true
0.00.055.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.953 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.648 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.654 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.673 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.721 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.722 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.723 I llama_init_from_model: graph nodes  = 967
0.00.084.723 I llama_init_from_model: graph splits = 2
0.00.084.726 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.197 I main: llama threadpool init, n_threads = 4
0.00.749.262 I 
0.00.749.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.285 I 
0.00.749.510 I sampler seed: 1234
0.00.749.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.585 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.585 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.630.928 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.630.928 I llama_perf_context_print:        load time =     739.40 ms
0.01.630.929 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.630.930 I llama_perf_context_print:        eval time =     824.10 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.630.930 I llama_perf_context_print:       total time =     881.73 ms /    70 tokens
0.01.631.166 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.107s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.542 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.602 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.351 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.398 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.778 I llama_model_loader: - type  f32:  194 tensors
0.00.055.779 I llama_model_loader: - type  f16:   98 tensors
0.00.055.780 I print_info: file format = GGUF V3 (latest)
0.00.055.781 I print_info: file type   = all F32 (guessed)
0.00.055.783 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.485 I load: special tokens cache size = 25
0.00.092.541 I load: token to piece cache size = 0.2984 MB
0.00.092.544 I print_info: arch             = gptneox
0.00.092.544 I print_info: vocab_only       = 0
0.00.092.544 I print_info: n_ctx_train      = 2048
0.00.092.544 I print_info: n_embd           = 2048
0.00.092.545 I print_info: n_layer          = 24
0.00.092.548 I print_info: n_head           = 16
0.00.092.549 I print_info: n_head_kv        = 16
0.00.092.549 I print_info: n_rot            = 32
0.00.092.549 I print_info: n_swa            = 0
0.00.092.549 I print_info: n_embd_head_k    = 128
0.00.092.550 I print_info: n_embd_head_v    = 128
0.00.092.550 I print_info: n_gqa            = 1
0.00.092.551 I print_info: n_embd_k_gqa     = 2048
0.00.092.551 I print_info: n_embd_v_gqa     = 2048
0.00.092.552 I print_info: f_norm_eps       = 1.0e-05
0.00.092.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.553 I print_info: f_logit_scale    = 0.0e+00
0.00.092.553 I print_info: n_ff             = 8192
0.00.092.554 I print_info: n_expert         = 0
0.00.092.554 I print_info: n_expert_used    = 0
0.00.092.554 I print_info: causal attn      = 1
0.00.092.557 I print_info: pooling type     = 0
0.00.092.557 I print_info: rope type        = 2
0.00.092.557 I print_info: rope scaling     = linear
0.00.092.557 I print_info: freq_base_train  = 10000.0
0.00.092.558 I print_info: freq_scale_train = 1
0.00.092.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.558 I print_info: rope_finetuned   = unknown
0.00.092.558 I print_info: ssm_d_conv       = 0
0.00.092.558 I print_info: ssm_d_inner      = 0
0.00.092.558 I print_info: ssm_d_state      = 0
0.00.092.558 I print_info: ssm_dt_rank      = 0
0.00.092.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.559 I print_info: model type       = 1.4B
0.00.092.563 I print_info: model params     = 1.41 B
0.00.092.563 I print_info: general.name     = 1.4B
0.00.092.564 I print_info: vocab type       = BPE
0.00.092.564 I print_info: n_vocab          = 50304
0.00.092.564 I print_info: n_merges         = 50009
0.00.092.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.565 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.565 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.565 I print_info: LF token         = 128 'Ä'
0.00.092.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.566 I print_info: max token length = 1024
0.00.095.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.251 I load_tensors: offloading output layer to GPU
0.00.095.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.262 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.263 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.095.571 I llama_init_from_model: n_seq_max     = 1
0.00.095.571 I llama_init_from_model: n_ctx         = 128
0.00.095.572 I llama_init_from_model: n_ctx_per_seq = 128
0.00.095.572 I llama_init_from_model: n_batch       = 128
0.00.095.572 I llama_init_from_model: n_ubatch      = 128
0.00.095.572 I llama_init_from_model: flash_attn    = 0
0.00.095.573 I llama_init_from_model: freq_base     = 10000.0
0.00.095.573 I llama_init_from_model: freq_scale    = 1
0.00.095.573 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.574 I ggml_metal_init: allocating
0.00.095.576 I ggml_metal_init: found device: Apple M4
0.00.095.579 I ggml_metal_init: picking default device: Apple M4
0.00.096.197 I ggml_metal_init: using embedded metal library
0.00.098.948 I ggml_metal_init: GPU name:   Apple M4
0.00.098.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.951 I ggml_metal_init: simdgroup reduction   = true
0.00.098.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.951 I ggml_metal_init: has bfloat            = true
0.00.098.951 I ggml_metal_init: use bfloat            = true
0.00.098.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.881 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.386 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.263 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.264 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.265 I llama_init_from_model: graph nodes  = 967
0.00.110.265 I llama_init_from_model: graph splits = 2
0.00.110.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.144.582 I 
0.01.144.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.144.677 I perplexity: tokenizing the input ..
0.01.155.403 I perplexity: tokenization took 10.723 ms
0.01.155.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.286.858 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.290.084 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.290.136 I llama_perf_context_print:        load time =    1119.96 ms
0.01.290.137 I llama_perf_context_print: prompt eval time =     131.21 ms /   128 tokens (    1.03 ms per token,   975.54 tokens per second)
0.01.290.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.290.139 I llama_perf_context_print:       total time =     145.56 ms /   129 tokens
0.01.290.869 I ggml_metal_free: deallocating

real	0m1.487s
user	0m0.129s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.407 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.330 I llama_model_loader: - type  f32:  194 tensors
0.00.025.330 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.331 I print_info: file format = GGUF V3 (latest)
0.00.025.331 I print_info: file type   = Q8_0
0.00.025.332 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.459 I load: special tokens cache size = 25
0.00.050.365 I load: token to piece cache size = 0.2984 MB
0.00.050.370 I print_info: arch             = gptneox
0.00.050.370 I print_info: vocab_only       = 0
0.00.050.370 I print_info: n_ctx_train      = 2048
0.00.050.371 I print_info: n_embd           = 2048
0.00.050.371 I print_info: n_layer          = 24
0.00.050.375 I print_info: n_head           = 16
0.00.050.375 I print_info: n_head_kv        = 16
0.00.050.379 I print_info: n_rot            = 32
0.00.050.379 I print_info: n_swa            = 0
0.00.050.379 I print_info: n_embd_head_k    = 128
0.00.050.379 I print_info: n_embd_head_v    = 128
0.00.050.380 I print_info: n_gqa            = 1
0.00.050.381 I print_info: n_embd_k_gqa     = 2048
0.00.050.381 I print_info: n_embd_v_gqa     = 2048
0.00.050.382 I print_info: f_norm_eps       = 1.0e-05
0.00.050.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.382 I print_info: f_logit_scale    = 0.0e+00
0.00.050.383 I print_info: n_ff             = 8192
0.00.050.383 I print_info: n_expert         = 0
0.00.050.383 I print_info: n_expert_used    = 0
0.00.050.383 I print_info: causal attn      = 1
0.00.050.384 I print_info: pooling type     = 0
0.00.050.384 I print_info: rope type        = 2
0.00.050.384 I print_info: rope scaling     = linear
0.00.050.384 I print_info: freq_base_train  = 10000.0
0.00.050.385 I print_info: freq_scale_train = 1
0.00.050.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.385 I print_info: rope_finetuned   = unknown
0.00.050.385 I print_info: ssm_d_conv       = 0
0.00.050.385 I print_info: ssm_d_inner      = 0
0.00.050.385 I print_info: ssm_d_state      = 0
0.00.050.386 I print_info: ssm_dt_rank      = 0
0.00.050.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.386 I print_info: model type       = 1.4B
0.00.050.386 I print_info: model params     = 1.41 B
0.00.050.387 I print_info: general.name     = 1.4B
0.00.050.387 I print_info: vocab type       = BPE
0.00.050.387 I print_info: n_vocab          = 50304
0.00.050.387 I print_info: n_merges         = 50009
0.00.050.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.388 I print_info: LF token         = 128 'Ä'
0.00.050.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.389 I print_info: max token length = 1024
0.00.052.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.499 I load_tensors: offloading output layer to GPU
0.00.052.499 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.510 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.052.511 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.052.869 I llama_init_from_model: n_seq_max     = 1
0.00.052.870 I llama_init_from_model: n_ctx         = 128
0.00.052.870 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.870 I llama_init_from_model: n_batch       = 128
0.00.052.871 I llama_init_from_model: n_ubatch      = 128
0.00.052.871 I llama_init_from_model: flash_attn    = 0
0.00.052.871 I llama_init_from_model: freq_base     = 10000.0
0.00.052.871 I llama_init_from_model: freq_scale    = 1
0.00.052.872 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.872 I ggml_metal_init: allocating
0.00.052.875 I ggml_metal_init: found device: Apple M4
0.00.052.877 I ggml_metal_init: picking default device: Apple M4
0.00.053.486 I ggml_metal_init: using embedded metal library
0.00.055.876 I ggml_metal_init: GPU name:   Apple M4
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.879 I ggml_metal_init: simdgroup reduction   = true
0.00.055.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.879 I ggml_metal_init: has bfloat            = true
0.00.055.880 I ggml_metal_init: use bfloat            = true
0.00.055.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.172 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.178 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.085 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.086 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.086 I llama_init_from_model: graph nodes  = 967
0.00.067.086 I llama_init_from_model: graph splits = 2
0.00.067.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.922.636 I 
0.00.922.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.922.672 I perplexity: tokenizing the input ..
0.00.931.770 I perplexity: tokenization took 9.096 ms
0.00.931.778 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.057.110 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.058.359 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.058.388 I llama_perf_context_print:        load time =     913.23 ms
0.01.058.389 I llama_perf_context_print: prompt eval time =     125.11 ms /   128 tokens (    0.98 ms per token,  1023.11 tokens per second)
0.01.058.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.058.390 I llama_perf_context_print:       total time =     135.75 ms /   129 tokens
0.01.058.861 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.081s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.548 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.245 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.917 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.917 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.918 I llama_model_loader: - type  f32:  194 tensors
0.00.026.919 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.919 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.919 I print_info: file format = GGUF V3 (latest)
0.00.026.920 I print_info: file type   = Q4_0
0.00.026.921 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.413 I load: special tokens cache size = 25
0.00.051.205 I load: token to piece cache size = 0.2984 MB
0.00.051.208 I print_info: arch             = gptneox
0.00.051.208 I print_info: vocab_only       = 0
0.00.051.208 I print_info: n_ctx_train      = 2048
0.00.051.208 I print_info: n_embd           = 2048
0.00.051.208 I print_info: n_layer          = 24
0.00.051.211 I print_info: n_head           = 16
0.00.051.212 I print_info: n_head_kv        = 16
0.00.051.212 I print_info: n_rot            = 32
0.00.051.212 I print_info: n_swa            = 0
0.00.051.212 I print_info: n_embd_head_k    = 128
0.00.051.213 I print_info: n_embd_head_v    = 128
0.00.051.213 I print_info: n_gqa            = 1
0.00.051.214 I print_info: n_embd_k_gqa     = 2048
0.00.051.215 I print_info: n_embd_v_gqa     = 2048
0.00.051.215 I print_info: f_norm_eps       = 1.0e-05
0.00.051.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.217 I print_info: f_logit_scale    = 0.0e+00
0.00.051.218 I print_info: n_ff             = 8192
0.00.051.218 I print_info: n_expert         = 0
0.00.051.218 I print_info: n_expert_used    = 0
0.00.051.218 I print_info: causal attn      = 1
0.00.051.218 I print_info: pooling type     = 0
0.00.051.219 I print_info: rope type        = 2
0.00.051.219 I print_info: rope scaling     = linear
0.00.051.219 I print_info: freq_base_train  = 10000.0
0.00.051.220 I print_info: freq_scale_train = 1
0.00.051.220 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.220 I print_info: rope_finetuned   = unknown
0.00.051.221 I print_info: ssm_d_conv       = 0
0.00.051.221 I print_info: ssm_d_inner      = 0
0.00.051.221 I print_info: ssm_d_state      = 0
0.00.051.221 I print_info: ssm_dt_rank      = 0
0.00.051.221 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.221 I print_info: model type       = 1.4B
0.00.051.222 I print_info: model params     = 1.41 B
0.00.051.222 I print_info: general.name     = 1.4B
0.00.051.223 I print_info: vocab type       = BPE
0.00.051.225 I print_info: n_vocab          = 50304
0.00.051.225 I print_info: n_merges         = 50009
0.00.051.225 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.226 I print_info: LF token         = 128 'Ä'
0.00.051.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.227 I print_info: max token length = 1024
0.00.053.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.065 I load_tensors: offloading output layer to GPU
0.00.053.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.075 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.076 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.408 I llama_init_from_model: n_seq_max     = 1
0.00.053.409 I llama_init_from_model: n_ctx         = 128
0.00.053.409 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.409 I llama_init_from_model: n_batch       = 128
0.00.053.409 I llama_init_from_model: n_ubatch      = 128
0.00.053.410 I llama_init_from_model: flash_attn    = 0
0.00.053.410 I llama_init_from_model: freq_base     = 10000.0
0.00.053.410 I llama_init_from_model: freq_scale    = 1
0.00.053.410 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.411 I ggml_metal_init: allocating
0.00.053.414 I ggml_metal_init: found device: Apple M4
0.00.053.416 I ggml_metal_init: picking default device: Apple M4
0.00.054.024 I ggml_metal_init: using embedded metal library
0.00.056.385 I ggml_metal_init: GPU name:   Apple M4
0.00.056.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.387 I ggml_metal_init: simdgroup reduction   = true
0.00.056.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.387 I ggml_metal_init: has bfloat            = true
0.00.056.388 I ggml_metal_init: use bfloat            = true
0.00.056.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.136 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.350 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.366 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.318 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.319 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.319 I llama_init_from_model: graph nodes  = 967
0.00.068.320 I llama_init_from_model: graph splits = 2
0.00.068.321 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.302 I 
0.00.611.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.412 I perplexity: tokenizing the input ..
0.00.619.693 I perplexity: tokenization took 8.28 ms
0.00.619.697 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.602 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.765 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.790 I llama_perf_context_print:        load time =     599.74 ms
0.00.743.791 I llama_perf_context_print: prompt eval time =     122.68 ms /   128 tokens (    0.96 ms per token,  1043.38 tokens per second)
0.00.743.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.792 I llama_perf_context_print:       total time =     132.49 ms /   129 tokens
0.00.744.278 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.077s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.224 I llama_model_loader: - type  f32:  194 tensors
0.00.024.224 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.225 I print_info: file format = GGUF V3 (latest)
0.00.024.226 I print_info: file type   = Q4_1
0.00.024.226 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.430 I load: special tokens cache size = 25
0.00.049.174 I load: token to piece cache size = 0.2984 MB
0.00.049.177 I print_info: arch             = gptneox
0.00.049.177 I print_info: vocab_only       = 0
0.00.049.178 I print_info: n_ctx_train      = 2048
0.00.049.178 I print_info: n_embd           = 2048
0.00.049.178 I print_info: n_layer          = 24
0.00.049.181 I print_info: n_head           = 16
0.00.049.182 I print_info: n_head_kv        = 16
0.00.049.182 I print_info: n_rot            = 32
0.00.049.182 I print_info: n_swa            = 0
0.00.049.182 I print_info: n_embd_head_k    = 128
0.00.049.183 I print_info: n_embd_head_v    = 128
0.00.049.186 I print_info: n_gqa            = 1
0.00.049.187 I print_info: n_embd_k_gqa     = 2048
0.00.049.187 I print_info: n_embd_v_gqa     = 2048
0.00.049.188 I print_info: f_norm_eps       = 1.0e-05
0.00.049.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.189 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.189 I print_info: f_logit_scale    = 0.0e+00
0.00.049.190 I print_info: n_ff             = 8192
0.00.049.190 I print_info: n_expert         = 0
0.00.049.190 I print_info: n_expert_used    = 0
0.00.049.190 I print_info: causal attn      = 1
0.00.049.190 I print_info: pooling type     = 0
0.00.049.190 I print_info: rope type        = 2
0.00.049.192 I print_info: rope scaling     = linear
0.00.049.193 I print_info: freq_base_train  = 10000.0
0.00.049.193 I print_info: freq_scale_train = 1
0.00.049.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.193 I print_info: rope_finetuned   = unknown
0.00.049.193 I print_info: ssm_d_conv       = 0
0.00.049.193 I print_info: ssm_d_inner      = 0
0.00.049.194 I print_info: ssm_d_state      = 0
0.00.049.194 I print_info: ssm_dt_rank      = 0
0.00.049.194 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.194 I print_info: model type       = 1.4B
0.00.049.195 I print_info: model params     = 1.41 B
0.00.049.195 I print_info: general.name     = 1.4B
0.00.049.196 I print_info: vocab type       = BPE
0.00.049.196 I print_info: n_vocab          = 50304
0.00.049.196 I print_info: n_merges         = 50009
0.00.049.196 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.196 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.196 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.197 I print_info: LF token         = 128 'Ä'
0.00.049.197 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.197 I print_info: max token length = 1024
0.00.051.194 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.194 I load_tensors: offloading output layer to GPU
0.00.051.194 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.205 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.206 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.482 I llama_init_from_model: n_seq_max     = 1
0.00.051.483 I llama_init_from_model: n_ctx         = 128
0.00.051.483 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.483 I llama_init_from_model: n_batch       = 128
0.00.051.483 I llama_init_from_model: n_ubatch      = 128
0.00.051.483 I llama_init_from_model: flash_attn    = 0
0.00.051.484 I llama_init_from_model: freq_base     = 10000.0
0.00.051.484 I llama_init_from_model: freq_scale    = 1
0.00.051.484 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.485 I ggml_metal_init: allocating
0.00.051.488 I ggml_metal_init: found device: Apple M4
0.00.051.490 I ggml_metal_init: picking default device: Apple M4
0.00.052.052 I ggml_metal_init: using embedded metal library
0.00.054.424 I ggml_metal_init: GPU name:   Apple M4
0.00.054.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.427 I ggml_metal_init: simdgroup reduction   = true
0.00.054.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.427 I ggml_metal_init: has bfloat            = true
0.00.054.427 I ggml_metal_init: use bfloat            = true
0.00.054.427 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.978 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.323 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.337 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.243 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.244 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.244 I llama_init_from_model: graph nodes  = 967
0.00.066.244 I llama_init_from_model: graph splits = 2
0.00.066.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.246 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.422 I 
0.00.649.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.546 I perplexity: tokenizing the input ..
0.00.657.223 I perplexity: tokenization took 7.676 ms
0.00.657.227 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.375 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.781.615 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.781.643 I llama_perf_context_print:        load time =     640.59 ms
0.00.781.644 I llama_perf_context_print: prompt eval time =     122.92 ms /   128 tokens (    0.96 ms per token,  1041.32 tokens per second)
0.00.781.645 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.645 I llama_perf_context_print:       total time =     132.23 ms /   129 tokens
0.00.782.115 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.076s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.956 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.090 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.091 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.094 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.095 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.095 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.095 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.096 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.880 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.564 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.564 I llama_model_loader: - type  f32:  194 tensors
0.00.025.564 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.565 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.565 I print_info: file format = GGUF V3 (latest)
0.00.025.566 I print_info: file type   = Q5_0
0.00.025.567 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.622 I load: special tokens cache size = 25
0.00.050.675 I load: token to piece cache size = 0.2984 MB
0.00.050.678 I print_info: arch             = gptneox
0.00.050.678 I print_info: vocab_only       = 0
0.00.050.679 I print_info: n_ctx_train      = 2048
0.00.050.679 I print_info: n_embd           = 2048
0.00.050.679 I print_info: n_layer          = 24
0.00.050.682 I print_info: n_head           = 16
0.00.050.683 I print_info: n_head_kv        = 16
0.00.050.683 I print_info: n_rot            = 32
0.00.050.683 I print_info: n_swa            = 0
0.00.050.684 I print_info: n_embd_head_k    = 128
0.00.050.684 I print_info: n_embd_head_v    = 128
0.00.050.685 I print_info: n_gqa            = 1
0.00.050.685 I print_info: n_embd_k_gqa     = 2048
0.00.050.687 I print_info: n_embd_v_gqa     = 2048
0.00.050.688 I print_info: f_norm_eps       = 1.0e-05
0.00.050.688 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.690 I print_info: f_logit_scale    = 0.0e+00
0.00.050.691 I print_info: n_ff             = 8192
0.00.050.691 I print_info: n_expert         = 0
0.00.050.691 I print_info: n_expert_used    = 0
0.00.050.691 I print_info: causal attn      = 1
0.00.050.691 I print_info: pooling type     = 0
0.00.050.691 I print_info: rope type        = 2
0.00.050.692 I print_info: rope scaling     = linear
0.00.050.694 I print_info: freq_base_train  = 10000.0
0.00.050.695 I print_info: freq_scale_train = 1
0.00.050.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.695 I print_info: rope_finetuned   = unknown
0.00.050.696 I print_info: ssm_d_conv       = 0
0.00.050.696 I print_info: ssm_d_inner      = 0
0.00.050.696 I print_info: ssm_d_state      = 0
0.00.050.696 I print_info: ssm_dt_rank      = 0
0.00.050.696 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.696 I print_info: model type       = 1.4B
0.00.050.697 I print_info: model params     = 1.41 B
0.00.050.697 I print_info: general.name     = 1.4B
0.00.050.697 I print_info: vocab type       = BPE
0.00.050.698 I print_info: n_vocab          = 50304
0.00.050.698 I print_info: n_merges         = 50009
0.00.050.698 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.698 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: LF token         = 128 'Ä'
0.00.050.699 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.699 I print_info: max token length = 1024
0.00.052.707 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.707 I load_tensors: offloading output layer to GPU
0.00.052.707 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.717 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.719 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.044 I llama_init_from_model: n_seq_max     = 1
0.00.053.045 I llama_init_from_model: n_ctx         = 128
0.00.053.045 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.045 I llama_init_from_model: n_batch       = 128
0.00.053.045 I llama_init_from_model: n_ubatch      = 128
0.00.053.045 I llama_init_from_model: flash_attn    = 0
0.00.053.046 I llama_init_from_model: freq_base     = 10000.0
0.00.053.046 I llama_init_from_model: freq_scale    = 1
0.00.053.046 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.047 I ggml_metal_init: allocating
0.00.053.050 I ggml_metal_init: found device: Apple M4
0.00.053.052 I ggml_metal_init: picking default device: Apple M4
0.00.053.628 I ggml_metal_init: using embedded metal library
0.00.055.975 I ggml_metal_init: GPU name:   Apple M4
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.978 I ggml_metal_init: simdgroup reduction   = true
0.00.055.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.978 I ggml_metal_init: has bfloat            = true
0.00.055.978 I ggml_metal_init: use bfloat            = true
0.00.055.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.588 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.849 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.866 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.833 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.834 I llama_init_from_model: graph nodes  = 967
0.00.067.834 I llama_init_from_model: graph splits = 2
0.00.067.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.910 I 
0.00.706.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.000 I perplexity: tokenizing the input ..
0.00.715.221 I perplexity: tokenization took 8.219 ms
0.00.715.231 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.517 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.545 I llama_perf_context_print:        load time =     696.95 ms
0.00.851.546 I llama_perf_context_print: prompt eval time =     134.89 ms /   128 tokens (    1.05 ms per token,   948.91 tokens per second)
0.00.851.547 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.547 I llama_perf_context_print:       total time =     144.64 ms /   129 tokens
0.00.851.994 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.887 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.888 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.888 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.891 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.620 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.394 I llama_model_loader: - type  f32:  194 tensors
0.00.024.394 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.395 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.395 I print_info: file format = GGUF V3 (latest)
0.00.024.396 I print_info: file type   = Q5_1
0.00.024.397 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.608 I load: special tokens cache size = 25
0.00.049.420 I load: token to piece cache size = 0.2984 MB
0.00.049.423 I print_info: arch             = gptneox
0.00.049.423 I print_info: vocab_only       = 0
0.00.049.424 I print_info: n_ctx_train      = 2048
0.00.049.424 I print_info: n_embd           = 2048
0.00.049.424 I print_info: n_layer          = 24
0.00.049.427 I print_info: n_head           = 16
0.00.049.428 I print_info: n_head_kv        = 16
0.00.049.428 I print_info: n_rot            = 32
0.00.049.428 I print_info: n_swa            = 0
0.00.049.428 I print_info: n_embd_head_k    = 128
0.00.049.428 I print_info: n_embd_head_v    = 128
0.00.049.429 I print_info: n_gqa            = 1
0.00.049.430 I print_info: n_embd_k_gqa     = 2048
0.00.049.430 I print_info: n_embd_v_gqa     = 2048
0.00.049.431 I print_info: f_norm_eps       = 1.0e-05
0.00.049.432 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.432 I print_info: f_logit_scale    = 0.0e+00
0.00.049.433 I print_info: n_ff             = 8192
0.00.049.433 I print_info: n_expert         = 0
0.00.049.433 I print_info: n_expert_used    = 0
0.00.049.433 I print_info: causal attn      = 1
0.00.049.433 I print_info: pooling type     = 0
0.00.049.435 I print_info: rope type        = 2
0.00.049.436 I print_info: rope scaling     = linear
0.00.049.436 I print_info: freq_base_train  = 10000.0
0.00.049.436 I print_info: freq_scale_train = 1
0.00.049.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.437 I print_info: rope_finetuned   = unknown
0.00.049.437 I print_info: ssm_d_conv       = 0
0.00.049.437 I print_info: ssm_d_inner      = 0
0.00.049.437 I print_info: ssm_d_state      = 0
0.00.049.437 I print_info: ssm_dt_rank      = 0
0.00.049.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.438 I print_info: model type       = 1.4B
0.00.049.438 I print_info: model params     = 1.41 B
0.00.049.438 I print_info: general.name     = 1.4B
0.00.049.439 I print_info: vocab type       = BPE
0.00.049.439 I print_info: n_vocab          = 50304
0.00.049.439 I print_info: n_merges         = 50009
0.00.049.439 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.445 I print_info: LF token         = 128 'Ä'
0.00.049.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.445 I print_info: max token length = 1024
0.00.051.421 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.421 I load_tensors: offloading output layer to GPU
0.00.051.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.432 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.433 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.713 I llama_init_from_model: n_seq_max     = 1
0.00.051.714 I llama_init_from_model: n_ctx         = 128
0.00.051.714 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.714 I llama_init_from_model: n_batch       = 128
0.00.051.714 I llama_init_from_model: n_ubatch      = 128
0.00.051.715 I llama_init_from_model: flash_attn    = 0
0.00.051.715 I llama_init_from_model: freq_base     = 10000.0
0.00.051.715 I llama_init_from_model: freq_scale    = 1
0.00.051.716 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.716 I ggml_metal_init: allocating
0.00.051.719 I ggml_metal_init: found device: Apple M4
0.00.051.721 I ggml_metal_init: picking default device: Apple M4
0.00.052.288 I ggml_metal_init: using embedded metal library
0.00.054.633 I ggml_metal_init: GPU name:   Apple M4
0.00.054.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.635 I ggml_metal_init: simdgroup reduction   = true
0.00.054.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.635 I ggml_metal_init: has bfloat            = true
0.00.054.635 I ggml_metal_init: use bfloat            = true
0.00.054.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.271 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.612 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.628 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.464 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.465 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.465 I llama_init_from_model: graph nodes  = 967
0.00.066.465 I llama_init_from_model: graph splits = 2
0.00.066.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.885 I 
0.00.655.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.924 I perplexity: tokenizing the input ..
0.00.663.766 I perplexity: tokenization took 7.84 ms
0.00.663.770 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.829 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.799.988 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.800.015 I llama_perf_context_print:        load time =     646.99 ms
0.00.800.016 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.42 tokens per second)
0.00.800.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.017 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.800.550 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.929 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.599 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.022 I llama_model_loader: - type  f32:  194 tensors
0.00.025.023 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.023 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.024 I print_info: file format = GGUF V3 (latest)
0.00.025.024 I print_info: file type   = Q2_K - Medium
0.00.025.025 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.199 I load: special tokens cache size = 25
0.00.049.031 I load: token to piece cache size = 0.2984 MB
0.00.049.034 I print_info: arch             = gptneox
0.00.049.034 I print_info: vocab_only       = 0
0.00.049.034 I print_info: n_ctx_train      = 2048
0.00.049.035 I print_info: n_embd           = 2048
0.00.049.035 I print_info: n_layer          = 24
0.00.049.037 I print_info: n_head           = 16
0.00.049.038 I print_info: n_head_kv        = 16
0.00.049.038 I print_info: n_rot            = 32
0.00.049.039 I print_info: n_swa            = 0
0.00.049.039 I print_info: n_embd_head_k    = 128
0.00.049.039 I print_info: n_embd_head_v    = 128
0.00.049.040 I print_info: n_gqa            = 1
0.00.049.040 I print_info: n_embd_k_gqa     = 2048
0.00.049.041 I print_info: n_embd_v_gqa     = 2048
0.00.049.042 I print_info: f_norm_eps       = 1.0e-05
0.00.049.042 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.042 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.043 I print_info: f_logit_scale    = 0.0e+00
0.00.049.046 I print_info: n_ff             = 8192
0.00.049.046 I print_info: n_expert         = 0
0.00.049.046 I print_info: n_expert_used    = 0
0.00.049.046 I print_info: causal attn      = 1
0.00.049.047 I print_info: pooling type     = 0
0.00.049.051 I print_info: rope type        = 2
0.00.049.052 I print_info: rope scaling     = linear
0.00.049.052 I print_info: freq_base_train  = 10000.0
0.00.049.053 I print_info: freq_scale_train = 1
0.00.049.053 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.053 I print_info: rope_finetuned   = unknown
0.00.049.053 I print_info: ssm_d_conv       = 0
0.00.049.053 I print_info: ssm_d_inner      = 0
0.00.049.055 I print_info: ssm_d_state      = 0
0.00.049.055 I print_info: ssm_dt_rank      = 0
0.00.049.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.056 I print_info: model type       = 1.4B
0.00.049.056 I print_info: model params     = 1.41 B
0.00.049.056 I print_info: general.name     = 1.4B
0.00.049.056 I print_info: vocab type       = BPE
0.00.049.057 I print_info: n_vocab          = 50304
0.00.049.058 I print_info: n_merges         = 50009
0.00.049.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.058 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: LF token         = 128 'Ä'
0.00.049.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.059 I print_info: max token length = 1024
0.00.050.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.942 I load_tensors: offloading output layer to GPU
0.00.050.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.953 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.954 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.236 I llama_init_from_model: n_seq_max     = 1
0.00.051.237 I llama_init_from_model: n_ctx         = 128
0.00.051.237 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.237 I llama_init_from_model: n_batch       = 128
0.00.051.238 I llama_init_from_model: n_ubatch      = 128
0.00.051.238 I llama_init_from_model: flash_attn    = 0
0.00.051.238 I llama_init_from_model: freq_base     = 10000.0
0.00.051.238 I llama_init_from_model: freq_scale    = 1
0.00.051.239 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.239 I ggml_metal_init: allocating
0.00.051.242 I ggml_metal_init: found device: Apple M4
0.00.051.244 I ggml_metal_init: picking default device: Apple M4
0.00.051.797 I ggml_metal_init: using embedded metal library
0.00.054.122 I ggml_metal_init: GPU name:   Apple M4
0.00.054.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.125 I ggml_metal_init: simdgroup reduction   = true
0.00.054.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.125 I ggml_metal_init: has bfloat            = true
0.00.054.125 I ggml_metal_init: use bfloat            = true
0.00.054.125 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.494 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.798 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.800 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.701 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.703 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.703 I llama_init_from_model: graph nodes  = 967
0.00.065.703 I llama_init_from_model: graph splits = 2
0.00.065.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.573 I 
0.00.373.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.611 I perplexity: tokenizing the input ..
0.00.381.014 I perplexity: tokenization took 7.402 ms
0.00.381.018 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.513.718 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.514.878 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.514.915 I llama_perf_context_print:        load time =     363.64 ms
0.00.514.916 I llama_perf_context_print: prompt eval time =     132.46 ms /   128 tokens (    1.03 ms per token,   966.30 tokens per second)
0.00.514.917 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.514.918 I llama_perf_context_print:       total time =     141.34 ms /   129 tokens
0.00.515.431 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.076s
sys	0m0.065s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.779 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.784 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.119 I llama_model_loader: - type  f32:  194 tensors
0.00.024.119 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.120 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.120 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.120 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.121 I print_info: file format = GGUF V3 (latest)
0.00.024.121 I print_info: file type   = Q3_K - Medium
0.00.024.122 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.326 I load: special tokens cache size = 25
0.00.049.077 I load: token to piece cache size = 0.2984 MB
0.00.049.080 I print_info: arch             = gptneox
0.00.049.080 I print_info: vocab_only       = 0
0.00.049.081 I print_info: n_ctx_train      = 2048
0.00.049.081 I print_info: n_embd           = 2048
0.00.049.081 I print_info: n_layer          = 24
0.00.049.084 I print_info: n_head           = 16
0.00.049.085 I print_info: n_head_kv        = 16
0.00.049.085 I print_info: n_rot            = 32
0.00.049.085 I print_info: n_swa            = 0
0.00.049.086 I print_info: n_embd_head_k    = 128
0.00.049.086 I print_info: n_embd_head_v    = 128
0.00.049.086 I print_info: n_gqa            = 1
0.00.049.087 I print_info: n_embd_k_gqa     = 2048
0.00.049.088 I print_info: n_embd_v_gqa     = 2048
0.00.049.089 I print_info: f_norm_eps       = 1.0e-05
0.00.049.089 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.089 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.089 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.089 I print_info: f_logit_scale    = 0.0e+00
0.00.049.090 I print_info: n_ff             = 8192
0.00.049.090 I print_info: n_expert         = 0
0.00.049.091 I print_info: n_expert_used    = 0
0.00.049.091 I print_info: causal attn      = 1
0.00.049.091 I print_info: pooling type     = 0
0.00.049.094 I print_info: rope type        = 2
0.00.049.096 I print_info: rope scaling     = linear
0.00.049.096 I print_info: freq_base_train  = 10000.0
0.00.049.097 I print_info: freq_scale_train = 1
0.00.049.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.097 I print_info: rope_finetuned   = unknown
0.00.049.098 I print_info: ssm_d_conv       = 0
0.00.049.100 I print_info: ssm_d_inner      = 0
0.00.049.100 I print_info: ssm_d_state      = 0
0.00.049.100 I print_info: ssm_dt_rank      = 0
0.00.049.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.101 I print_info: model type       = 1.4B
0.00.049.101 I print_info: model params     = 1.41 B
0.00.049.101 I print_info: general.name     = 1.4B
0.00.049.102 I print_info: vocab type       = BPE
0.00.049.102 I print_info: n_vocab          = 50304
0.00.049.102 I print_info: n_merges         = 50009
0.00.049.102 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.102 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.102 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.103 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.103 I print_info: LF token         = 128 'Ä'
0.00.049.103 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.103 I print_info: max token length = 1024
0.00.051.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.040 I load_tensors: offloading output layer to GPU
0.00.051.040 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.051 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.052 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.342 I llama_init_from_model: n_seq_max     = 1
0.00.051.343 I llama_init_from_model: n_ctx         = 128
0.00.051.343 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.343 I llama_init_from_model: n_batch       = 128
0.00.051.343 I llama_init_from_model: n_ubatch      = 128
0.00.051.343 I llama_init_from_model: flash_attn    = 0
0.00.051.344 I llama_init_from_model: freq_base     = 10000.0
0.00.051.344 I llama_init_from_model: freq_scale    = 1
0.00.051.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.345 I ggml_metal_init: allocating
0.00.051.347 I ggml_metal_init: found device: Apple M4
0.00.051.349 I ggml_metal_init: picking default device: Apple M4
0.00.051.929 I ggml_metal_init: using embedded metal library
0.00.054.317 I ggml_metal_init: GPU name:   Apple M4
0.00.054.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.319 I ggml_metal_init: simdgroup reduction   = true
0.00.054.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.320 I ggml_metal_init: has bfloat            = true
0.00.054.320 I ggml_metal_init: use bfloat            = true
0.00.054.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.934 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.198 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.214 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.063 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.064 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.064 I llama_init_from_model: graph nodes  = 967
0.00.066.064 I llama_init_from_model: graph splits = 2
0.00.066.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.131 I 
0.00.477.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.165 I perplexity: tokenizing the input ..
0.00.485.119 I perplexity: tokenization took 7.953 ms
0.00.485.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.676 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.854 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.885 I llama_perf_context_print:        load time =     468.33 ms
0.00.618.886 I llama_perf_context_print: prompt eval time =     132.30 ms /   128 tokens (    1.03 ms per token,   967.47 tokens per second)
0.00.618.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.887 I llama_perf_context_print:       total time =     141.75 ms /   129 tokens
0.00.619.434 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.077s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.861 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.113 I llama_model_loader: - type  f32:  194 tensors
0.00.024.113 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.113 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.113 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.114 I print_info: file format = GGUF V3 (latest)
0.00.024.115 I print_info: file type   = Q4_K - Medium
0.00.024.115 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.417 I load: special tokens cache size = 25
0.00.048.235 I load: token to piece cache size = 0.2984 MB
0.00.048.238 I print_info: arch             = gptneox
0.00.048.238 I print_info: vocab_only       = 0
0.00.048.238 I print_info: n_ctx_train      = 2048
0.00.048.239 I print_info: n_embd           = 2048
0.00.048.239 I print_info: n_layer          = 24
0.00.048.241 I print_info: n_head           = 16
0.00.048.242 I print_info: n_head_kv        = 16
0.00.048.242 I print_info: n_rot            = 32
0.00.048.245 I print_info: n_swa            = 0
0.00.048.245 I print_info: n_embd_head_k    = 128
0.00.048.245 I print_info: n_embd_head_v    = 128
0.00.048.246 I print_info: n_gqa            = 1
0.00.048.246 I print_info: n_embd_k_gqa     = 2048
0.00.048.247 I print_info: n_embd_v_gqa     = 2048
0.00.048.248 I print_info: f_norm_eps       = 1.0e-05
0.00.048.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.249 I print_info: f_logit_scale    = 0.0e+00
0.00.048.249 I print_info: n_ff             = 8192
0.00.048.250 I print_info: n_expert         = 0
0.00.048.250 I print_info: n_expert_used    = 0
0.00.048.250 I print_info: causal attn      = 1
0.00.048.250 I print_info: pooling type     = 0
0.00.048.250 I print_info: rope type        = 2
0.00.048.250 I print_info: rope scaling     = linear
0.00.048.251 I print_info: freq_base_train  = 10000.0
0.00.048.251 I print_info: freq_scale_train = 1
0.00.048.251 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.252 I print_info: rope_finetuned   = unknown
0.00.048.252 I print_info: ssm_d_conv       = 0
0.00.048.252 I print_info: ssm_d_inner      = 0
0.00.048.252 I print_info: ssm_d_state      = 0
0.00.048.253 I print_info: ssm_dt_rank      = 0
0.00.048.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.253 I print_info: model type       = 1.4B
0.00.048.255 I print_info: model params     = 1.41 B
0.00.048.255 I print_info: general.name     = 1.4B
0.00.048.256 I print_info: vocab type       = BPE
0.00.048.256 I print_info: n_vocab          = 50304
0.00.048.256 I print_info: n_merges         = 50009
0.00.048.260 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.260 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.261 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.261 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.262 I print_info: LF token         = 128 'Ä'
0.00.048.262 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.262 I print_info: max token length = 1024
0.00.050.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.200 I load_tensors: offloading output layer to GPU
0.00.050.200 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.210 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.211 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.529 I llama_init_from_model: n_seq_max     = 1
0.00.050.530 I llama_init_from_model: n_ctx         = 128
0.00.050.530 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.530 I llama_init_from_model: n_batch       = 128
0.00.050.531 I llama_init_from_model: n_ubatch      = 128
0.00.050.531 I llama_init_from_model: flash_attn    = 0
0.00.050.531 I llama_init_from_model: freq_base     = 10000.0
0.00.050.531 I llama_init_from_model: freq_scale    = 1
0.00.050.532 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.532 I ggml_metal_init: allocating
0.00.050.534 I ggml_metal_init: found device: Apple M4
0.00.050.536 I ggml_metal_init: picking default device: Apple M4
0.00.051.085 I ggml_metal_init: using embedded metal library
0.00.053.414 I ggml_metal_init: GPU name:   Apple M4
0.00.053.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.417 I ggml_metal_init: simdgroup reduction   = true
0.00.053.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.417 I ggml_metal_init: has bfloat            = true
0.00.053.417 I ggml_metal_init: use bfloat            = true
0.00.053.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.035 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.299 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.304 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.241 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.242 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.242 I llama_init_from_model: graph nodes  = 967
0.00.064.243 I llama_init_from_model: graph splits = 2
0.00.064.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.391 I 
0.00.545.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.452 I perplexity: tokenizing the input ..
0.00.553.645 I perplexity: tokenization took 8.191 ms
0.00.553.649 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.026 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.207 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.236 I llama_perf_context_print:        load time =     536.53 ms
0.00.689.237 I llama_perf_context_print: prompt eval time =     134.15 ms /   128 tokens (    1.05 ms per token,   954.16 tokens per second)
0.00.689.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.238 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.689.789 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.076s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.793 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.613 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.060 I llama_model_loader: - type  f32:  194 tensors
0.00.026.060 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.060 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.061 I print_info: file format = GGUF V3 (latest)
0.00.026.062 I print_info: file type   = Q5_K - Medium
0.00.026.062 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.285 I load: special tokens cache size = 25
0.00.050.018 I load: token to piece cache size = 0.2984 MB
0.00.050.021 I print_info: arch             = gptneox
0.00.050.021 I print_info: vocab_only       = 0
0.00.050.022 I print_info: n_ctx_train      = 2048
0.00.050.022 I print_info: n_embd           = 2048
0.00.050.022 I print_info: n_layer          = 24
0.00.050.025 I print_info: n_head           = 16
0.00.050.026 I print_info: n_head_kv        = 16
0.00.050.026 I print_info: n_rot            = 32
0.00.050.026 I print_info: n_swa            = 0
0.00.050.026 I print_info: n_embd_head_k    = 128
0.00.050.027 I print_info: n_embd_head_v    = 128
0.00.050.029 I print_info: n_gqa            = 1
0.00.050.030 I print_info: n_embd_k_gqa     = 2048
0.00.050.031 I print_info: n_embd_v_gqa     = 2048
0.00.050.031 I print_info: f_norm_eps       = 1.0e-05
0.00.050.031 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.032 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.032 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.032 I print_info: f_logit_scale    = 0.0e+00
0.00.050.033 I print_info: n_ff             = 8192
0.00.050.033 I print_info: n_expert         = 0
0.00.050.033 I print_info: n_expert_used    = 0
0.00.050.033 I print_info: causal attn      = 1
0.00.050.033 I print_info: pooling type     = 0
0.00.050.034 I print_info: rope type        = 2
0.00.050.034 I print_info: rope scaling     = linear
0.00.050.040 I print_info: freq_base_train  = 10000.0
0.00.050.041 I print_info: freq_scale_train = 1
0.00.050.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.043 I print_info: rope_finetuned   = unknown
0.00.050.043 I print_info: ssm_d_conv       = 0
0.00.050.043 I print_info: ssm_d_inner      = 0
0.00.050.043 I print_info: ssm_d_state      = 0
0.00.050.044 I print_info: ssm_dt_rank      = 0
0.00.050.044 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.045 I print_info: model type       = 1.4B
0.00.050.046 I print_info: model params     = 1.41 B
0.00.050.046 I print_info: general.name     = 1.4B
0.00.050.046 I print_info: vocab type       = BPE
0.00.050.047 I print_info: n_vocab          = 50304
0.00.050.047 I print_info: n_merges         = 50009
0.00.050.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.047 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.047 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.048 I print_info: LF token         = 128 'Ä'
0.00.050.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.048 I print_info: max token length = 1024
0.00.052.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.064 I load_tensors: offloading output layer to GPU
0.00.052.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.075 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.076 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.347 I llama_init_from_model: n_seq_max     = 1
0.00.052.348 I llama_init_from_model: n_ctx         = 128
0.00.052.348 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.349 I llama_init_from_model: n_batch       = 128
0.00.052.349 I llama_init_from_model: n_ubatch      = 128
0.00.052.349 I llama_init_from_model: flash_attn    = 0
0.00.052.349 I llama_init_from_model: freq_base     = 10000.0
0.00.052.350 I llama_init_from_model: freq_scale    = 1
0.00.052.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.350 I ggml_metal_init: allocating
0.00.052.353 I ggml_metal_init: found device: Apple M4
0.00.052.355 I ggml_metal_init: picking default device: Apple M4
0.00.052.922 I ggml_metal_init: using embedded metal library
0.00.055.317 I ggml_metal_init: GPU name:   Apple M4
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.319 I ggml_metal_init: simdgroup reduction   = true
0.00.055.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.319 I ggml_metal_init: has bfloat            = true
0.00.055.319 I ggml_metal_init: use bfloat            = true
0.00.055.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.320 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.761 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.997 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.001 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.876 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.877 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.877 I llama_init_from_model: graph nodes  = 967
0.00.065.877 I llama_init_from_model: graph splits = 2
0.00.065.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.237 I 
0.00.645.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.301 I perplexity: tokenizing the input ..
0.00.653.552 I perplexity: tokenization took 8.249 ms
0.00.653.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.416 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.571 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.595 I llama_perf_context_print:        load time =     634.44 ms
0.00.795.596 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.36 tokens per second)
0.00.795.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.597 I llama_perf_context_print:       total time =     150.36 ms /   129 tokens
0.00.796.126 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.075s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.056 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.854 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.868 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.876 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.876 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.327 I llama_model_loader: - type  f32:  194 tensors
0.00.024.327 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.328 I print_info: file format = GGUF V3 (latest)
0.00.024.328 I print_info: file type   = Q6_K
0.00.024.329 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.570 I load: special tokens cache size = 25
0.00.049.340 I load: token to piece cache size = 0.2984 MB
0.00.049.343 I print_info: arch             = gptneox
0.00.049.344 I print_info: vocab_only       = 0
0.00.049.344 I print_info: n_ctx_train      = 2048
0.00.049.344 I print_info: n_embd           = 2048
0.00.049.344 I print_info: n_layer          = 24
0.00.049.347 I print_info: n_head           = 16
0.00.049.348 I print_info: n_head_kv        = 16
0.00.049.348 I print_info: n_rot            = 32
0.00.049.348 I print_info: n_swa            = 0
0.00.049.348 I print_info: n_embd_head_k    = 128
0.00.049.348 I print_info: n_embd_head_v    = 128
0.00.049.349 I print_info: n_gqa            = 1
0.00.049.350 I print_info: n_embd_k_gqa     = 2048
0.00.049.351 I print_info: n_embd_v_gqa     = 2048
0.00.049.351 I print_info: f_norm_eps       = 1.0e-05
0.00.049.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.352 I print_info: f_logit_scale    = 0.0e+00
0.00.049.353 I print_info: n_ff             = 8192
0.00.049.353 I print_info: n_expert         = 0
0.00.049.353 I print_info: n_expert_used    = 0
0.00.049.353 I print_info: causal attn      = 1
0.00.049.353 I print_info: pooling type     = 0
0.00.049.354 I print_info: rope type        = 2
0.00.049.361 I print_info: rope scaling     = linear
0.00.049.363 I print_info: freq_base_train  = 10000.0
0.00.049.364 I print_info: freq_scale_train = 1
0.00.049.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.364 I print_info: rope_finetuned   = unknown
0.00.049.364 I print_info: ssm_d_conv       = 0
0.00.049.365 I print_info: ssm_d_inner      = 0
0.00.049.365 I print_info: ssm_d_state      = 0
0.00.049.365 I print_info: ssm_dt_rank      = 0
0.00.049.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.365 I print_info: model type       = 1.4B
0.00.049.367 I print_info: model params     = 1.41 B
0.00.049.367 I print_info: general.name     = 1.4B
0.00.049.367 I print_info: vocab type       = BPE
0.00.049.368 I print_info: n_vocab          = 50304
0.00.049.368 I print_info: n_merges         = 50009
0.00.049.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.368 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: LF token         = 128 'Ä'
0.00.049.370 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: max token length = 1024
0.00.051.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.364 I load_tensors: offloading output layer to GPU
0.00.051.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.374 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.375 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.651 I llama_init_from_model: n_seq_max     = 1
0.00.051.652 I llama_init_from_model: n_ctx         = 128
0.00.051.652 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.652 I llama_init_from_model: n_batch       = 128
0.00.051.652 I llama_init_from_model: n_ubatch      = 128
0.00.051.652 I llama_init_from_model: flash_attn    = 0
0.00.051.653 I llama_init_from_model: freq_base     = 10000.0
0.00.051.653 I llama_init_from_model: freq_scale    = 1
0.00.051.653 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.654 I ggml_metal_init: allocating
0.00.051.657 I ggml_metal_init: found device: Apple M4
0.00.051.658 I ggml_metal_init: picking default device: Apple M4
0.00.052.234 I ggml_metal_init: using embedded metal library
0.00.054.556 I ggml_metal_init: GPU name:   Apple M4
0.00.054.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.558 I ggml_metal_init: simdgroup reduction   = true
0.00.054.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.558 I ggml_metal_init: has bfloat            = true
0.00.054.558 I ggml_metal_init: use bfloat            = true
0.00.054.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.902 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.199 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.068 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.069 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.070 I llama_init_from_model: graph nodes  = 967
0.00.066.070 I llama_init_from_model: graph splits = 2
0.00.066.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.735 I 
0.00.429.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.776 I perplexity: tokenizing the input ..
0.00.437.452 I perplexity: tokenization took 7.675 ms
0.00.437.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.541 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.578.712 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.578.737 I llama_perf_context_print:        load time =     420.67 ms
0.00.578.740 I llama_perf_context_print: prompt eval time =     139.85 ms /   128 tokens (    1.09 ms per token,   915.25 tokens per second)
0.00.578.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.745 I llama_perf_context_print:       total time =     149.01 ms /   129 tokens
0.00.579.236 I ggml_metal_free: deallocating

real	0m0.593s
user	0m0.077s
sys	0m0.087s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4463 (afa8a9ec) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.922 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.389 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.402 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.243 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.247 I llama_model_loader: - type  f32:  194 tensors
0.00.055.248 I llama_model_loader: - type  f16:   98 tensors
0.00.055.249 I print_info: file format = GGUF V3 (latest)
0.00.055.250 I print_info: file type   = all F32 (guessed)
0.00.055.251 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.692 I load: special tokens cache size = 25
0.00.088.387 I load: token to piece cache size = 0.2984 MB
0.00.088.390 I print_info: arch             = gptneox
0.00.088.391 I print_info: vocab_only       = 0
0.00.088.391 I print_info: n_ctx_train      = 2048
0.00.088.391 I print_info: n_embd           = 2048
0.00.088.391 I print_info: n_layer          = 24
0.00.088.395 I print_info: n_head           = 16
0.00.088.395 I print_info: n_head_kv        = 16
0.00.088.396 I print_info: n_rot            = 32
0.00.088.396 I print_info: n_swa            = 0
0.00.088.396 I print_info: n_embd_head_k    = 128
0.00.088.396 I print_info: n_embd_head_v    = 128
0.00.088.397 I print_info: n_gqa            = 1
0.00.088.400 I print_info: n_embd_k_gqa     = 2048
0.00.088.400 I print_info: n_embd_v_gqa     = 2048
0.00.088.401 I print_info: f_norm_eps       = 1.0e-05
0.00.088.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.401 I print_info: f_logit_scale    = 0.0e+00
0.00.088.403 I print_info: n_ff             = 8192
0.00.088.403 I print_info: n_expert         = 0
0.00.088.403 I print_info: n_expert_used    = 0
0.00.088.403 I print_info: causal attn      = 1
0.00.088.404 I print_info: pooling type     = 0
0.00.088.404 I print_info: rope type        = 2
0.00.088.404 I print_info: rope scaling     = linear
0.00.088.404 I print_info: freq_base_train  = 10000.0
0.00.088.404 I print_info: freq_scale_train = 1
0.00.088.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.405 I print_info: rope_finetuned   = unknown
0.00.088.405 I print_info: ssm_d_conv       = 0
0.00.088.405 I print_info: ssm_d_inner      = 0
0.00.088.405 I print_info: ssm_d_state      = 0
0.00.088.405 I print_info: ssm_dt_rank      = 0
0.00.088.405 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.406 I print_info: model type       = 1.4B
0.00.088.406 I print_info: model params     = 1.41 B
0.00.088.406 I print_info: general.name     = 1.4B
0.00.088.407 I print_info: vocab type       = BPE
0.00.088.407 I print_info: n_vocab          = 50304
0.00.088.407 I print_info: n_merges         = 50009
0.00.088.407 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.407 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.407 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.408 I print_info: LF token         = 128 'Ä'
0.00.088.408 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.412 I print_info: max token length = 1024
0.00.090.221 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.221 I load_tensors: offloading output layer to GPU
0.00.090.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.231 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.232 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.528 I llama_init_from_model: n_seq_max     = 1
0.00.090.529 I llama_init_from_model: n_ctx         = 128
0.00.090.529 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.529 I llama_init_from_model: n_batch       = 128
0.00.090.529 I llama_init_from_model: n_ubatch      = 128
0.00.090.529 I llama_init_from_model: flash_attn    = 0
0.00.090.530 I llama_init_from_model: freq_base     = 10000.0
0.00.090.530 I llama_init_from_model: freq_scale    = 1
0.00.090.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.531 I ggml_metal_init: allocating
0.00.090.534 I ggml_metal_init: found device: Apple M4
0.00.090.536 I ggml_metal_init: picking default device: Apple M4
0.00.091.213 I ggml_metal_init: using embedded metal library
0.00.093.842 I ggml_metal_init: GPU name:   Apple M4
0.00.093.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.845 I ggml_metal_init: simdgroup reduction   = true
0.00.093.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.845 I ggml_metal_init: has bfloat            = true
0.00.093.845 I ggml_metal_init: use bfloat            = true
0.00.093.846 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.525 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.529 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.547 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.411 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.412 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.412 I llama_init_from_model: graph nodes  = 967
0.00.105.413 I llama_init_from_model: graph splits = 2
0.00.105.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.414 I 
0.00.105.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.441 I compute_imatrix: tokenizing the input ..
0.00.111.913 I compute_imatrix: tokenization took 6.472 ms
0.00.111.915 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.647.785 I compute_imatrix: 1.54 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.650.228 I llama_perf_context_print:        load time =    1623.86 ms
0.01.650.230 I llama_perf_context_print: prompt eval time =    1535.23 ms /   128 tokens (   11.99 ms per token,    83.38 tokens per second)
0.01.650.231 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.650.231 I llama_perf_context_print:       total time =    1626.29 ms /   129 tokens
0.01.650.833 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.168s
sys	0m0.232s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4463 (afa8a9ec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e60a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e60aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e60aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e60b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e60bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e60c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e60c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e60cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e60d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e60d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e60dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e60e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e60f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e60fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e6135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e6162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e6183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e6191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e61a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e61ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e61bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e61c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e61ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e61d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e61da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e61e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e61ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e61f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e61f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e61fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e6209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e6217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e6225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e6261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e6281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e6291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e62a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e62a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e62ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e62b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e62b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e62bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e61b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e62c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e62c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e62cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e62d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e62d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e62dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e62e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e62e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e62ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e62f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e62f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e62fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e6302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e6316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e6344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e6352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e6360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e6369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e6377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e6385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e63a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e63a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e63aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e63af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e63b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e63b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e63bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e63c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e63cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e63cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e63d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e63d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e63dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e63e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e63e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e63eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e63f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e63f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e63f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e63fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e6419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e6422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e6430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e6447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e6455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e6463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e6484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e6494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e64a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e64a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e64b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e64b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e64b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e64c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e64cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e64d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e64d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e64dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e64e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e64e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e64ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e64f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e64f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e64fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e6507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e6517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e6527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e6581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e6591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e65a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e65a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e65ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e65b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e65b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e65bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e65c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e65c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e65cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e65d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e65d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e65dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e65e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e65e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e65ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e65f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e65f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e65fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e6606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e6610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e6619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e6627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e6635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e6656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e6680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e6686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.152.034 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.152.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e6085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e608a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e608ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e609310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e609780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e609bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e60a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e60a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e60cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e60d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e60dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e610650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e611e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e6125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e612a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e6148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e615d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e616700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e617a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e618350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e6187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e6190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e619510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e619980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e61a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e61aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e61aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e61b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e61b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e61bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e61c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e61c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e61cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e61d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e61d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e61db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e61dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e61e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e61e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e61edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e61f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e61f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e61fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e620650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e620ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e6210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e6220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e622630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e622b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e6230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e6240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e624b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e6250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e6260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e6265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e626b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e6275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e627b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e6285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e628b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e629070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e6295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e629b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e62a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e62a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e62ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e62b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e62b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e62baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e62c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e62c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e62cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e62d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e62d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e62d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e62de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e62e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e62ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e62f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e62f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e62f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e62fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e630310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e6307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e6351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e6368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e63a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e63ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e63b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e63bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e63c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e63c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e63c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e63ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e63d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e63d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e63e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e63e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e63ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e63eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e63f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e63f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e6421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e644780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e644cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e645220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e645770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e645a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e646650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e6478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e6481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e6487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e648fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e649900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e649da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e64a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e64aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e64aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e64b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e64ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e64bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e64c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e64ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e64cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e64da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e64dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e64e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e64ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e64efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e64f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e64fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e64ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e6504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e650a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e650f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e6514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e651a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e651f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e6524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e652a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e652f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e6534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e653a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e653f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e6544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e654f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e6554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e6559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e655f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e656490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e6569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e656f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e657480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e6579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e657f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e658470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e6589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e658f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e659460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e6599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e65a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e65a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e65aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e65b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e65b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e65bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e65c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e65c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e65ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e65d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e65d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e65dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e65e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e65e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e65ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e65ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e65f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e65f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e65fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e6601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e660650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e661980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e6620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e6627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e662ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e663600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e6638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e6640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e664370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e664980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e664630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e645cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e61b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e648480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e60bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e608160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e663b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e61a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e648a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e6650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e665720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e6659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e665ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e665f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e666220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e6664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e6667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e666a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e666d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e666fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e6672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e667560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e667820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e667ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e668060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e668320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e6685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e6688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e668b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e668e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e6690e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e6693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e669660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e669920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e669be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e669ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e66a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e66a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e66a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e66a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e66ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e66af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e66b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e66b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e66b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e66ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e66bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e66bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e66c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e66c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e66c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e66caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e66cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e66d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e66d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e66d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e66d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e66db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e66dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e66e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e66e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e66e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e66e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e66eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e66ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e66f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e66f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e66f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e66f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e66fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e66fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e6701a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e670460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e670720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e6709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e670ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e670f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e671220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e6714e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e6717a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e671a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e671d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e671fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e6722a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e672560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e672820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e672ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e672da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e673060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e673320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e6735e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e6738a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e673b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e673e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e6740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e6743a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e674660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e674920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e674be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e674ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e675160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e675420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e6756e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e6759a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e675c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e675f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e6761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e6764a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e676760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e676a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e676ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e676fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e677260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e677520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e6777e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e677aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e677d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e678020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e6782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e6785a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e678860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e678b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e678de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e6790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e679360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e679620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e6798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e679ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e679e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e67a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e67a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e67a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e67a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e67ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e67aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e67b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e67b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e67b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e67b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e67bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e67bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e67c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e67c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e67c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e67ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e67cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e67cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e67d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e67d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e67d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e67dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e67dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e67e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e67e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e67e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e67e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e67eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e67ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e67f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e67f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e67f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e67f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e67fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e67fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e680160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e680420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e6806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e6809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e680c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e680f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e6811e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e6814a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e681760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e681a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e681ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e681fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e682260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e682520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e6827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e682aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e682d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e683020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e6832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e6835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e683860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e683b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e683de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e6840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e684360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e684620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e6848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e684ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e684e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e685120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e6853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e6856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e685960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e685c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e685ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e6861a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e686460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e686720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e6869e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e686ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e686f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e687530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e6877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e687ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e687d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e688030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e6882f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e6885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e688870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e688b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e688df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e6890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e689370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e689630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e6898f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e689bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e689e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e68a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e68a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e68a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e68a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e68ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e68aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e68b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e68b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e68b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e68b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e68bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e68bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e68c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e68c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e68c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e68ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e68cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e68cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e68d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e68d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e68d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e68daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e68ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e68e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e68e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e68e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e68e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e68eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e68ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e68f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e68f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e68f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e68f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e68fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e6903d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e690920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e690e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e6913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e691910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e691e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e6923b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e692670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e692930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e692e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e693330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e693830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e693d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e694230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e694730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e694c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e695130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e695630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e695b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e696030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e696530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e696a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e696f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e697940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e698060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e698780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e698ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e699160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e699950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e699c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e69a220 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.836s
user	0m0.293s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4463 (afa8a9ec)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157f0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157f0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157f0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157f0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157f0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157f0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157f0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157f0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157f10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157f10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157f10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157f11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157f11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157f125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157f12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157f14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157f14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157f15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157f15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157f16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157f167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157f17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157f17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157f17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157f18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157f18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157f191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157f194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157f19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157f1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157f1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157f1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157f1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157f1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157f1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157f1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157f1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157f1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157f1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157f1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157f1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157f1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157f1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157f1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157f1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157f1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157f20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157f20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157f21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157f21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157f21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157f223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157f22680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157f22c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157f23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157f23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157f23be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157f24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157f24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157f24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157f25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157f257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157f25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157f260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157f26580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157f26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157f26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157f27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157f27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157f27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157f28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157f28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157f28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157f293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157f29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157f2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157f2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157f2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157f2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157f2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157f2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157f2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157f2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157f2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157f2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157f2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157f2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157f2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157f2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157f1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157f2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157f2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157f30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157f30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157f30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157f314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157f31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157f31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157f324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157f32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157f32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157f334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157f33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157f33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157f34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157f348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157f34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157f351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157f35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157f35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157f35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157f36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157f36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157f36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157f37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157f376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157f37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157f38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157f384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157f38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157f38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157f392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157f39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157f39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157f3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157f3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157f3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157f3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157f3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157f3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157f3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157f3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157f3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157f3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157f3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157f3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157f3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157f3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157f3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157f3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157f3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157f3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157f3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157f3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157f401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157f40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157f40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157f40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157f41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157f418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157f41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157f42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157f426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157f42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157f42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157f43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157f43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157f43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157f44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157f44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157f44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157f45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157f454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157f45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157f45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157f462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157f46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157f46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157f470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157f47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157f479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157f47e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157f48330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157f487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157f48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157f49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157f495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157f49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157f49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157f4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157f4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157f4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157f4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157f4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157f4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157f4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157f4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157f4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157f4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157f4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157f4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157f4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157f4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157f4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157f4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157f4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157f4ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157f503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157f50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157f50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157f51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157f519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157f51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157f52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157f529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157f52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157f53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157f539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157f53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157f54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157f549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157f54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157f55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157f559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157f55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157f56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157f56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157f56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157f57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157f57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157f57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157f58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157f58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157f58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157f59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157f59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157f59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157f5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157f5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157f5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157f5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157f5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157f5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157f5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157f5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157f5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157f5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157f5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157f5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157f5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157f5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157f5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157f5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157f5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157f5fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157f603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157f608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157f60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157f61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157f618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157f61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157f62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157f628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157f62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157f63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157f638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157f63e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157f642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157f64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157f64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157f65090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157f65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157f659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157f65e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157f66310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157f667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157f66c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157f670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157f67590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157f67a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157f67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157f68370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157f688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157f68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157f69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157f69e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157f6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157f6a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157f6aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157f6b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157f6b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157f6b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157f4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157f4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157f4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157f20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157f20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157f22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157f4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157f1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157f1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157f1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157f1dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157f1fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157f16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157f22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157f2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157f6aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157f19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157f1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157f4f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157f4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157f182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157f185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157f18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157f6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157f6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157f6c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157f6c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157f6c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157f6cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157f6cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157f6d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157f6d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157f6d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157f6d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157f6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157f6de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157f6e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157f6e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157f6e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157f6e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157f6ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157f6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157f6f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157f6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157f6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157f6f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157f6fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157f6ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157f701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157f704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157f70760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157f70a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157f70ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157f70fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157f71260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157f71520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157f717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157f71aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157f71d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157f72020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157f722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157f725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157f72860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157f72b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157f72de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157f730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157f73360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157f73620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157f738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157f73ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157f73e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157f74120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157f743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157f746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157f74960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157f74c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157f74ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157f751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157f75460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157f75720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157f759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157f75ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157f75f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157f76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157f764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157f767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157f76a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157f76d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157f76fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157f772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157f77560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157f77820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157f77ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157f77da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157f78060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157f78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157f785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157f788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157f78b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157f78e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157f790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157f793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157f79660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157f79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157f79be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157f79ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157f7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157f7a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157f7a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157f7a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157f7ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157f7af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157f7b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157f7b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157f7b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157f7ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157f7bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157f7bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157f7c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157f7c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157f7c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157f7caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157f7cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157f7d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157f7d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157f7d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157f7d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157f7db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157f7dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157f7e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157f7e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157f7e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157f7e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157f7eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157f7ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157f7f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157f7f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157f7f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157f7f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157f7fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157f7fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157f801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157f80460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157f80720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157f809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157f80ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157f80f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157f81220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157f814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157f817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157f81a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157f81d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157f81fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157f822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157f82560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157f82820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157f82ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157f82da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157f83060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157f83320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157f835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157f838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157f83b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157f83e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157f840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157f843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157f84660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157f84920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157f84be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157f84ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157f85160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157f85420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157f856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157f859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157f85c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157f85f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157f861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157f864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157f86760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157f86a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157f86ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157f86fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157f87260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157f87520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157f877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157f87aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157f87d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157f88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157f882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157f885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157f88860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157f88b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157f88de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157f890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157f89360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157f89620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157f898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157f89ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157f89e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157f8a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157f8a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157f8a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157f8a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157f8ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157f8aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157f8b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157f8b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157f8b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157f8bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157f8bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157f8c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157f8c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157f8cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157f8cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157f8d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157f8d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157f8dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157f8e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157f8e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157f8ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157f8eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157f8f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157f8f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157f8fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157f90090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157f90500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157f90970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157f90de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157f91250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157f916c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157f91b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157f91fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157f92410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157f92880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157f92cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157f93160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157f935d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157f93a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157f93eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157f94320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157f94790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157f94c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157f95070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157f954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157f95950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157f95dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157f96230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157f966a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157f96b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157f96f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157f973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157f97860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157f97cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157f98140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157f985b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157f98a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157f98e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157f99300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157f99770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157f99be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157f9a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157f9a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157f9a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157f9ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157f9b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157f9b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157f9baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157f9bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157f9c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157f9c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157f9ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157f9d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157f9d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157f9da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157f9de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157f9e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157f9e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157f9ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157f9f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157f9f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157f9f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157fa0380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157fa0aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157fa11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157fa18e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157fa1ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157fa2390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157fa2650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157fa2c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157e04f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157e05380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157e057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157e05c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157e060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157e06540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157e069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157e06e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157e07290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157e077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157e07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157e082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157e08dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157e09580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157e09d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157e0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157e0abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157e0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157e0ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157e0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157e0c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157e0d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157e0d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157e0de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157e0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157e0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157e0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157e0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157e0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157e0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157e0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157e101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157e10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157e10920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157e10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157e11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157e11670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157e11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157e11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157e123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157e12830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157e12ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157e13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157e13580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157e139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157e13e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157e142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157e14740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157e14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157e15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157e15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157e15900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157e15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157e161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157e16650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157e16ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157e17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157e17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157e179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157e17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157e186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157e18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157e18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157e19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157e198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157e19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157e1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157e1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157e1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157e1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157e1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157e1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157e1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157e1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157e1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157e1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157e1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157e1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157e1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157e1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157e1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157e1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157e1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157e1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157e1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157e1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157e1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157e20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157e207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157e20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157e21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157e214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157e21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157e21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157e22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157e226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157e22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157e22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157e23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157e23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157e23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157e24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157e24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157e24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157e25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157e259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157e25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157e27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157e27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157e27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157e27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157e281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157e28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157e28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157e28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157e29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157e29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157e2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157e2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157e2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157e2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157e2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157e2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157e2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157e2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157e30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157e30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157e31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157e318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157e321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157e32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157e32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157e33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157e337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157e33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157e340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157e34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157e35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157e3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157e3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157e3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157e3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157e3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157e3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157e3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157e3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157e403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157e40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157e40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157e41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157e415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157e41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157e425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157e42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157e42b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157e42f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157e43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157e43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157e43ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157e44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157e445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157e44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157e44ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157e45310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157e45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157e45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157e46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157e464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157e46940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157e46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157e47220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157e47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157e47b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157e47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157e483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157e48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157e48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157e49130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157e495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157e49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157e49e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157e4a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157e4a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157e4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157e4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157e4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157e4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157e4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157e4c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157e4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157e4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157e4cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157e4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157e4d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157e4dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157e4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157e4e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157e4e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157e4ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157e4f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157e4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157e4fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157e50020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157e50490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157e50900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157e50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157e511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157e51650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157e51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157e51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157e523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157e52810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157e52c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157e530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157e53560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157e539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157e53e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157e542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157e54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157e54b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157e55000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157e55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157e558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157e55d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157e561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157e56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157e57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157e57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157e58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157e58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157e588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157e58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157e594d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.915s
user	0m0.242s
sys	0m0.137s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
