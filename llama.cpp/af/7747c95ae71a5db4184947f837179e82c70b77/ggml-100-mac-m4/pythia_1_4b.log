Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.047s
user	0m1.038s
sys	0m1.432s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Built target test-arg-parser
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-perf
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target test-quantize-fns
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-parallel
[ 80%] Built target llama-cli
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 81%] Built target llama-passkey
[ 81%] Built target llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-quantize
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-tokenize
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.245s
user	0m6.435s
sys	0m9.818s

main: quantize time =  3708.03 ms
main:    total time =  3708.03 ms

main: quantize time =  2321.51 ms
main:    total time =  2321.51 ms

main: quantize time =  2019.58 ms
main:    total time =  2019.58 ms

main: quantize time =  1877.94 ms
main:    total time =  1877.94 ms

main: quantize time =  1704.94 ms
main:    total time =  1704.94 ms

main: quantize time =  5544.68 ms
main:    total time =  5544.68 ms

main: quantize time =  5778.77 ms
main:    total time =  5778.77 ms

main: quantize time =  6953.83 ms
main:    total time =  6953.83 ms

main: quantize time =  5861.47 ms
main:    total time =  5861.47 ms

main: quantize time =  4341.51 ms
main:    total time =  4341.51 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.193 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.372 I main: llama backend init
0.00.000.378 I main: load the model and apply lora adapter, if any
0.00.046.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.059.594 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.613 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.793 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.793 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.794 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.794 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.795 I llama_model_loader: - type  f32:  194 tensors
0.00.077.795 I llama_model_loader: - type  f16:   98 tensors
0.00.077.796 I print_info: file format = GGUF V3 (latest)
0.00.077.798 I print_info: file type   = all F32 (guessed)
0.00.077.800 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.988 I load: special tokens cache size = 25
0.00.100.820 I load: token to piece cache size = 0.2984 MB
0.00.100.824 I print_info: arch             = gptneox
0.00.100.824 I print_info: vocab_only       = 0
0.00.100.825 I print_info: n_ctx_train      = 2048
0.00.100.825 I print_info: n_embd           = 2048
0.00.100.825 I print_info: n_layer          = 24
0.00.100.829 I print_info: n_head           = 16
0.00.100.830 I print_info: n_head_kv        = 16
0.00.100.830 I print_info: n_rot            = 32
0.00.100.830 I print_info: n_swa            = 0
0.00.100.830 I print_info: n_embd_head_k    = 128
0.00.100.831 I print_info: n_embd_head_v    = 128
0.00.100.833 I print_info: n_gqa            = 1
0.00.100.834 I print_info: n_embd_k_gqa     = 2048
0.00.100.837 I print_info: n_embd_v_gqa     = 2048
0.00.100.837 I print_info: f_norm_eps       = 1.0e-05
0.00.100.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.100.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.100.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.100.839 I print_info: f_logit_scale    = 0.0e+00
0.00.100.839 I print_info: n_ff             = 8192
0.00.100.841 I print_info: n_expert         = 0
0.00.100.841 I print_info: n_expert_used    = 0
0.00.100.841 I print_info: causal attn      = 1
0.00.100.841 I print_info: pooling type     = 0
0.00.100.842 I print_info: rope type        = 2
0.00.100.842 I print_info: rope scaling     = linear
0.00.100.842 I print_info: freq_base_train  = 10000.0
0.00.100.843 I print_info: freq_scale_train = 1
0.00.100.843 I print_info: n_ctx_orig_yarn  = 2048
0.00.100.843 I print_info: rope_finetuned   = unknown
0.00.100.844 I print_info: ssm_d_conv       = 0
0.00.100.844 I print_info: ssm_d_inner      = 0
0.00.100.844 I print_info: ssm_d_state      = 0
0.00.100.844 I print_info: ssm_dt_rank      = 0
0.00.100.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.100.845 I print_info: model type       = 1.4B
0.00.100.846 I print_info: model params     = 1.41 B
0.00.100.846 I print_info: general.name     = 1.4B
0.00.100.847 I print_info: vocab type       = BPE
0.00.100.847 I print_info: n_vocab          = 50304
0.00.100.847 I print_info: n_merges         = 50009
0.00.100.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.100.848 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.100.848 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.100.848 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.100.848 I print_info: LF token         = 187 'Ċ'
0.00.100.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.100.849 I print_info: max token length = 1024
0.00.100.849 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.141.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.141.997 I load_tensors: offloading output layer to GPU
0.00.141.997 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.023 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.024 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.142.460 I llama_init_from_model: n_seq_max     = 1
0.00.142.461 I llama_init_from_model: n_ctx         = 2048
0.00.142.461 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.142.461 I llama_init_from_model: n_batch       = 2048
0.00.142.462 I llama_init_from_model: n_ubatch      = 512
0.00.142.462 I llama_init_from_model: flash_attn    = 0
0.00.142.462 I llama_init_from_model: freq_base     = 10000.0
0.00.142.463 I llama_init_from_model: freq_scale    = 1
0.00.142.463 I ggml_metal_init: allocating
0.00.142.502 I ggml_metal_init: found device: Apple M4
0.00.142.510 I ggml_metal_init: picking default device: Apple M4
0.00.143.162 I ggml_metal_init: using embedded metal library
0.00.494.635 I ggml_metal_init: GPU name:   Apple M4
0.00.494.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.494.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.494.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.494.654 I ggml_metal_init: simdgroup reduction   = true
0.00.494.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.494.654 I ggml_metal_init: has residency sets    = true
0.00.494.655 I ggml_metal_init: has bfloat            = true
0.00.494.655 I ggml_metal_init: use bfloat            = true
0.00.494.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.494.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.998 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.574.967 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.574.981 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.575.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.579.974 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.579.977 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.579.977 I llama_init_from_model: graph nodes  = 967
0.00.579.977 I llama_init_from_model: graph splits = 2
0.00.579.982 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.580.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.580.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.534 I main: llama threadpool init, n_threads = 4
0.00.649.571 I 
0.00.649.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.603 I 
0.00.649.783 I sampler seed: 1234
0.00.649.788 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.649.815 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.649.817 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.649.817 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.481.166 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.02.481.166 I llama_perf_context_print:        load time =     601.45 ms
0.02.481.167 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.89 tokens per second)
0.02.481.168 I llama_perf_context_print:        eval time =    1784.70 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.481.168 I llama_perf_context_print:       total time =    1832.74 ms /    70 tokens
0.02.481.383 I ggml_metal_free: deallocating

real	0m2.799s
user	0m0.146s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.561 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.264 I llama_model_loader: - type  f32:  194 tensors
0.00.035.264 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.265 I print_info: file format = GGUF V3 (latest)
0.00.035.266 I print_info: file type   = Q8_0
0.00.035.267 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.064 I load: special tokens cache size = 25
0.00.050.969 I load: token to piece cache size = 0.2984 MB
0.00.050.974 I print_info: arch             = gptneox
0.00.050.975 I print_info: vocab_only       = 0
0.00.050.975 I print_info: n_ctx_train      = 2048
0.00.050.975 I print_info: n_embd           = 2048
0.00.050.975 I print_info: n_layer          = 24
0.00.050.982 I print_info: n_head           = 16
0.00.050.982 I print_info: n_head_kv        = 16
0.00.050.983 I print_info: n_rot            = 32
0.00.050.983 I print_info: n_swa            = 0
0.00.050.983 I print_info: n_embd_head_k    = 128
0.00.050.983 I print_info: n_embd_head_v    = 128
0.00.050.984 I print_info: n_gqa            = 1
0.00.050.985 I print_info: n_embd_k_gqa     = 2048
0.00.050.985 I print_info: n_embd_v_gqa     = 2048
0.00.050.986 I print_info: f_norm_eps       = 1.0e-05
0.00.050.987 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.987 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.987 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.987 I print_info: f_logit_scale    = 0.0e+00
0.00.050.988 I print_info: n_ff             = 8192
0.00.050.988 I print_info: n_expert         = 0
0.00.050.988 I print_info: n_expert_used    = 0
0.00.050.988 I print_info: causal attn      = 1
0.00.050.989 I print_info: pooling type     = 0
0.00.050.989 I print_info: rope type        = 2
0.00.050.992 I print_info: rope scaling     = linear
0.00.050.993 I print_info: freq_base_train  = 10000.0
0.00.050.993 I print_info: freq_scale_train = 1
0.00.050.993 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.994 I print_info: rope_finetuned   = unknown
0.00.050.994 I print_info: ssm_d_conv       = 0
0.00.050.994 I print_info: ssm_d_inner      = 0
0.00.050.994 I print_info: ssm_d_state      = 0
0.00.050.994 I print_info: ssm_dt_rank      = 0
0.00.050.994 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.995 I print_info: model type       = 1.4B
0.00.050.995 I print_info: model params     = 1.41 B
0.00.050.996 I print_info: general.name     = 1.4B
0.00.050.997 I print_info: vocab type       = BPE
0.00.050.997 I print_info: n_vocab          = 50304
0.00.050.997 I print_info: n_merges         = 50009
0.00.050.997 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.998 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.998 I print_info: LF token         = 187 'Ċ'
0.00.050.998 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.999 I print_info: max token length = 1024
0.00.050.999 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.227.855 I load_tensors: offloading 24 repeating layers to GPU
0.01.227.865 I load_tensors: offloading output layer to GPU
0.01.227.866 I load_tensors: offloaded 25/25 layers to GPU
0.01.227.897 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.227.900 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.228.809 I llama_init_from_model: n_seq_max     = 1
0.01.228.812 I llama_init_from_model: n_ctx         = 2048
0.01.228.813 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.228.813 I llama_init_from_model: n_batch       = 2048
0.01.228.813 I llama_init_from_model: n_ubatch      = 512
0.01.228.814 I llama_init_from_model: flash_attn    = 0
0.01.228.815 I llama_init_from_model: freq_base     = 10000.0
0.01.228.815 I llama_init_from_model: freq_scale    = 1
0.01.228.818 I ggml_metal_init: allocating
0.01.228.870 I ggml_metal_init: found device: Apple M4
0.01.228.881 I ggml_metal_init: picking default device: Apple M4
0.01.230.284 I ggml_metal_init: using embedded metal library
0.01.236.348 I ggml_metal_init: GPU name:   Apple M4
0.01.236.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.236.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.236.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.236.354 I ggml_metal_init: simdgroup reduction   = true
0.01.236.354 I ggml_metal_init: simdgroup matrix mul. = true
0.01.236.355 I ggml_metal_init: has residency sets    = true
0.01.236.355 I ggml_metal_init: has bfloat            = true
0.01.236.355 I ggml_metal_init: use bfloat            = true
0.01.236.356 I ggml_metal_init: hasUnifiedMemory      = true
0.01.236.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.252.189 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.305.624 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.305.633 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.305.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.310.474 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.310.476 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.310.476 I llama_init_from_model: graph nodes  = 967
0.01.310.477 I llama_init_from_model: graph splits = 2
0.01.310.481 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.310.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.310.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.364.676 I main: llama threadpool init, n_threads = 4
0.01.364.772 I 
0.01.364.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.364.794 I 
0.01.364.949 I sampler seed: 1234
0.01.364.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.364.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.364.992 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.364.992 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.468.320 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48563.61 tokens per second)
0.02.468.321 I llama_perf_context_print:        load time =    1353.98 ms
0.02.468.322 I llama_perf_context_print: prompt eval time =      49.22 ms /     7 tokens (    7.03 ms per token,   142.23 tokens per second)
0.02.468.322 I llama_perf_context_print:        eval time =    1051.65 ms /    63 runs   (   16.69 ms per token,    59.91 tokens per second)
0.02.468.323 I llama_perf_context_print:       total time =    1104.37 ms /    70 tokens
0.02.468.626 I ggml_metal_free: deallocating

real	0m2.487s
user	0m0.110s
sys	0m0.261s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.104 I main: llama backend init
0.00.000.106 I main: load the model and apply lora adapter, if any
0.00.015.474 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.318 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.324 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.336 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.004 I llama_model_loader: - type  f32:  194 tensors
0.00.040.004 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.005 I print_info: file format = GGUF V3 (latest)
0.00.040.006 I print_info: file type   = Q4_0
0.00.040.007 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.035 I load: special tokens cache size = 25
0.00.054.290 I load: token to piece cache size = 0.2984 MB
0.00.054.294 I print_info: arch             = gptneox
0.00.054.294 I print_info: vocab_only       = 0
0.00.054.295 I print_info: n_ctx_train      = 2048
0.00.054.295 I print_info: n_embd           = 2048
0.00.054.295 I print_info: n_layer          = 24
0.00.054.299 I print_info: n_head           = 16
0.00.054.300 I print_info: n_head_kv        = 16
0.00.054.300 I print_info: n_rot            = 32
0.00.054.301 I print_info: n_swa            = 0
0.00.054.301 I print_info: n_embd_head_k    = 128
0.00.054.301 I print_info: n_embd_head_v    = 128
0.00.054.302 I print_info: n_gqa            = 1
0.00.054.303 I print_info: n_embd_k_gqa     = 2048
0.00.054.303 I print_info: n_embd_v_gqa     = 2048
0.00.054.304 I print_info: f_norm_eps       = 1.0e-05
0.00.054.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.304 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.306 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.308 I print_info: f_logit_scale    = 0.0e+00
0.00.054.309 I print_info: n_ff             = 8192
0.00.054.309 I print_info: n_expert         = 0
0.00.054.309 I print_info: n_expert_used    = 0
0.00.054.309 I print_info: causal attn      = 1
0.00.054.309 I print_info: pooling type     = 0
0.00.054.311 I print_info: rope type        = 2
0.00.054.311 I print_info: rope scaling     = linear
0.00.054.311 I print_info: freq_base_train  = 10000.0
0.00.054.312 I print_info: freq_scale_train = 1
0.00.054.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.312 I print_info: rope_finetuned   = unknown
0.00.054.312 I print_info: ssm_d_conv       = 0
0.00.054.312 I print_info: ssm_d_inner      = 0
0.00.054.312 I print_info: ssm_d_state      = 0
0.00.054.312 I print_info: ssm_dt_rank      = 0
0.00.054.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.313 I print_info: model type       = 1.4B
0.00.054.313 I print_info: model params     = 1.41 B
0.00.054.313 I print_info: general.name     = 1.4B
0.00.054.314 I print_info: vocab type       = BPE
0.00.054.315 I print_info: n_vocab          = 50304
0.00.054.315 I print_info: n_merges         = 50009
0.00.054.315 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.316 I print_info: LF token         = 187 'Ċ'
0.00.054.316 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.316 I print_info: max token length = 1024
0.00.054.317 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.883.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.883.552 I load_tensors: offloading output layer to GPU
0.00.883.552 I load_tensors: offloaded 25/25 layers to GPU
0.00.883.570 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.883.571 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.884.521 I llama_init_from_model: n_seq_max     = 1
0.00.884.523 I llama_init_from_model: n_ctx         = 2048
0.00.884.524 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.884.524 I llama_init_from_model: n_batch       = 2048
0.00.884.525 I llama_init_from_model: n_ubatch      = 512
0.00.884.525 I llama_init_from_model: flash_attn    = 0
0.00.884.526 I llama_init_from_model: freq_base     = 10000.0
0.00.884.526 I llama_init_from_model: freq_scale    = 1
0.00.884.528 I ggml_metal_init: allocating
0.00.884.554 I ggml_metal_init: found device: Apple M4
0.00.884.564 I ggml_metal_init: picking default device: Apple M4
0.00.886.266 I ggml_metal_init: using embedded metal library
0.00.891.627 I ggml_metal_init: GPU name:   Apple M4
0.00.891.633 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.891.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.891.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.891.635 I ggml_metal_init: simdgroup reduction   = true
0.00.891.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.891.635 I ggml_metal_init: has residency sets    = true
0.00.891.636 I ggml_metal_init: has bfloat            = true
0.00.891.636 I ggml_metal_init: use bfloat            = true
0.00.891.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.891.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.909.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.942.355 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.942.361 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.942.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.946.771 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.946.773 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.946.774 I llama_init_from_model: graph nodes  = 967
0.00.946.774 I llama_init_from_model: graph splits = 2
0.00.946.783 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.946.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.946.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.006.717 I main: llama threadpool init, n_threads = 4
0.01.006.755 I 
0.01.006.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.006.778 I 
0.01.006.921 I sampler seed: 1234
0.01.006.926 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.006.970 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.006.973 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.006.973 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.688.643 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47619.05 tokens per second)
0.01.688.644 I llama_perf_context_print:        load time =     990.52 ms
0.01.688.644 I llama_perf_context_print: prompt eval time =      49.54 ms /     7 tokens (    7.08 ms per token,   141.30 tokens per second)
0.01.688.645 I llama_perf_context_print:        eval time =     629.62 ms /    63 runs   (    9.99 ms per token,   100.06 tokens per second)
0.01.688.645 I llama_perf_context_print:       total time =     682.64 ms /    70 tokens
0.01.688.889 I ggml_metal_free: deallocating

real	0m1.719s
user	0m0.108s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.868 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.666 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.668 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.669 I llama_model_loader: - type  f32:  194 tensors
0.00.025.669 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.670 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.670 I print_info: file format = GGUF V3 (latest)
0.00.025.671 I print_info: file type   = Q4_1
0.00.025.672 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.845 I load: special tokens cache size = 25
0.00.039.964 I load: token to piece cache size = 0.2984 MB
0.00.039.967 I print_info: arch             = gptneox
0.00.039.967 I print_info: vocab_only       = 0
0.00.039.967 I print_info: n_ctx_train      = 2048
0.00.039.968 I print_info: n_embd           = 2048
0.00.039.968 I print_info: n_layer          = 24
0.00.039.971 I print_info: n_head           = 16
0.00.039.972 I print_info: n_head_kv        = 16
0.00.039.972 I print_info: n_rot            = 32
0.00.039.972 I print_info: n_swa            = 0
0.00.039.972 I print_info: n_embd_head_k    = 128
0.00.039.972 I print_info: n_embd_head_v    = 128
0.00.039.973 I print_info: n_gqa            = 1
0.00.039.974 I print_info: n_embd_k_gqa     = 2048
0.00.039.975 I print_info: n_embd_v_gqa     = 2048
0.00.039.975 I print_info: f_norm_eps       = 1.0e-05
0.00.039.976 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.976 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.976 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.976 I print_info: f_logit_scale    = 0.0e+00
0.00.039.977 I print_info: n_ff             = 8192
0.00.039.977 I print_info: n_expert         = 0
0.00.039.977 I print_info: n_expert_used    = 0
0.00.039.978 I print_info: causal attn      = 1
0.00.039.978 I print_info: pooling type     = 0
0.00.039.978 I print_info: rope type        = 2
0.00.039.978 I print_info: rope scaling     = linear
0.00.039.979 I print_info: freq_base_train  = 10000.0
0.00.039.979 I print_info: freq_scale_train = 1
0.00.039.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.979 I print_info: rope_finetuned   = unknown
0.00.039.979 I print_info: ssm_d_conv       = 0
0.00.039.979 I print_info: ssm_d_inner      = 0
0.00.039.980 I print_info: ssm_d_state      = 0
0.00.039.980 I print_info: ssm_dt_rank      = 0
0.00.039.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.980 I print_info: model type       = 1.4B
0.00.039.981 I print_info: model params     = 1.41 B
0.00.039.981 I print_info: general.name     = 1.4B
0.00.039.981 I print_info: vocab type       = BPE
0.00.039.981 I print_info: n_vocab          = 50304
0.00.039.982 I print_info: n_merges         = 50009
0.00.039.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.983 I print_info: LF token         = 187 'Ċ'
0.00.039.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.983 I print_info: max token length = 1024
0.00.039.983 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.715.711 I load_tensors: offloading 24 repeating layers to GPU
0.00.715.718 I load_tensors: offloading output layer to GPU
0.00.715.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.715.754 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.715.756 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.716.995 I llama_init_from_model: n_seq_max     = 1
0.00.716.997 I llama_init_from_model: n_ctx         = 2048
0.00.716.998 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.716.998 I llama_init_from_model: n_batch       = 2048
0.00.716.999 I llama_init_from_model: n_ubatch      = 512
0.00.716.999 I llama_init_from_model: flash_attn    = 0
0.00.717.002 I llama_init_from_model: freq_base     = 10000.0
0.00.717.002 I llama_init_from_model: freq_scale    = 1
0.00.717.005 I ggml_metal_init: allocating
0.00.717.131 I ggml_metal_init: found device: Apple M4
0.00.717.147 I ggml_metal_init: picking default device: Apple M4
0.00.719.393 I ggml_metal_init: using embedded metal library
0.00.724.762 I ggml_metal_init: GPU name:   Apple M4
0.00.724.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.724.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.724.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.724.770 I ggml_metal_init: simdgroup reduction   = true
0.00.724.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.724.771 I ggml_metal_init: has residency sets    = true
0.00.724.771 I ggml_metal_init: has bfloat            = true
0.00.724.771 I ggml_metal_init: use bfloat            = true
0.00.724.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.724.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.739.617 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.782.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.782.846 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.782.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.787.480 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.787.482 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.787.482 I llama_init_from_model: graph nodes  = 967
0.00.787.482 I llama_init_from_model: graph splits = 2
0.00.787.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.787.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.787.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.543 I main: llama threadpool init, n_threads = 4
0.00.841.588 I 
0.00.841.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.610 I 
0.00.841.776 I sampler seed: 1234
0.00.841.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.841.823 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.841.826 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.841.826 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.569.502 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.569.503 I llama_perf_context_print:        load time =     831.84 ms
0.01.569.504 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.36 tokens per second)
0.01.569.505 I llama_perf_context_print:        eval time =     675.79 ms /    63 runs   (   10.73 ms per token,    93.22 tokens per second)
0.01.569.506 I llama_perf_context_print:       total time =     728.70 ms /    70 tokens
0.01.569.779 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.106s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.925 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.929 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.668 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.418 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.420 I llama_model_loader: - type  f32:  194 tensors
0.00.026.420 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.420 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.421 I print_info: file format = GGUF V3 (latest)
0.00.026.421 I print_info: file type   = Q5_0
0.00.026.425 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.182 I load: special tokens cache size = 25
0.00.040.187 I load: token to piece cache size = 0.2984 MB
0.00.040.190 I print_info: arch             = gptneox
0.00.040.191 I print_info: vocab_only       = 0
0.00.040.191 I print_info: n_ctx_train      = 2048
0.00.040.191 I print_info: n_embd           = 2048
0.00.040.191 I print_info: n_layer          = 24
0.00.040.194 I print_info: n_head           = 16
0.00.040.195 I print_info: n_head_kv        = 16
0.00.040.195 I print_info: n_rot            = 32
0.00.040.195 I print_info: n_swa            = 0
0.00.040.195 I print_info: n_embd_head_k    = 128
0.00.040.196 I print_info: n_embd_head_v    = 128
0.00.040.196 I print_info: n_gqa            = 1
0.00.040.197 I print_info: n_embd_k_gqa     = 2048
0.00.040.198 I print_info: n_embd_v_gqa     = 2048
0.00.040.199 I print_info: f_norm_eps       = 1.0e-05
0.00.040.199 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.199 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.202 I print_info: f_logit_scale    = 0.0e+00
0.00.040.202 I print_info: n_ff             = 8192
0.00.040.202 I print_info: n_expert         = 0
0.00.040.203 I print_info: n_expert_used    = 0
0.00.040.203 I print_info: causal attn      = 1
0.00.040.203 I print_info: pooling type     = 0
0.00.040.203 I print_info: rope type        = 2
0.00.040.203 I print_info: rope scaling     = linear
0.00.040.204 I print_info: freq_base_train  = 10000.0
0.00.040.204 I print_info: freq_scale_train = 1
0.00.040.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.206 I print_info: rope_finetuned   = unknown
0.00.040.206 I print_info: ssm_d_conv       = 0
0.00.040.206 I print_info: ssm_d_inner      = 0
0.00.040.206 I print_info: ssm_d_state      = 0
0.00.040.206 I print_info: ssm_dt_rank      = 0
0.00.040.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.207 I print_info: model type       = 1.4B
0.00.040.207 I print_info: model params     = 1.41 B
0.00.040.207 I print_info: general.name     = 1.4B
0.00.040.208 I print_info: vocab type       = BPE
0.00.040.208 I print_info: n_vocab          = 50304
0.00.040.208 I print_info: n_merges         = 50009
0.00.040.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.219 I print_info: LF token         = 187 'Ċ'
0.00.040.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.221 I print_info: max token length = 1024
0.00.040.221 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.789.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.789.733 I load_tensors: offloading output layer to GPU
0.00.789.734 I load_tensors: offloaded 25/25 layers to GPU
0.00.789.753 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.789.755 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.791.198 I llama_init_from_model: n_seq_max     = 1
0.00.791.202 I llama_init_from_model: n_ctx         = 2048
0.00.791.202 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.791.203 I llama_init_from_model: n_batch       = 2048
0.00.791.203 I llama_init_from_model: n_ubatch      = 512
0.00.791.203 I llama_init_from_model: flash_attn    = 0
0.00.791.205 I llama_init_from_model: freq_base     = 10000.0
0.00.791.206 I llama_init_from_model: freq_scale    = 1
0.00.791.209 I ggml_metal_init: allocating
0.00.791.266 I ggml_metal_init: found device: Apple M4
0.00.791.278 I ggml_metal_init: picking default device: Apple M4
0.00.793.246 I ggml_metal_init: using embedded metal library
0.00.800.096 I ggml_metal_init: GPU name:   Apple M4
0.00.800.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.800.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.800.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.800.103 I ggml_metal_init: simdgroup reduction   = true
0.00.800.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.800.104 I ggml_metal_init: has residency sets    = true
0.00.800.104 I ggml_metal_init: has bfloat            = true
0.00.800.104 I ggml_metal_init: use bfloat            = true
0.00.800.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.800.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.817.789 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.870.950 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.870.958 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.870.994 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.876.799 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.876.801 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.876.801 I llama_init_from_model: graph nodes  = 967
0.00.876.802 I llama_init_from_model: graph splits = 2
0.00.876.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.876.934 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.876.934 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.187 I main: llama threadpool init, n_threads = 4
0.00.935.241 I 
0.00.935.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.266 I 
0.00.935.436 I sampler seed: 1234
0.00.935.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.935.452 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.935.452 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.935.452 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.726.528 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.726.529 I llama_perf_context_print:        load time =     925.67 ms
0.01.726.529 I llama_perf_context_print: prompt eval time =      52.78 ms /     7 tokens (    7.54 ms per token,   132.63 tokens per second)
0.01.726.530 I llama_perf_context_print:        eval time =     735.47 ms /    63 runs   (   11.67 ms per token,    85.66 tokens per second)
0.01.726.530 I llama_perf_context_print:       total time =     792.10 ms /    70 tokens
0.01.726.758 I ggml_metal_free: deallocating

real	0m1.750s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.317 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.807 I llama_model_loader: - type  f32:  194 tensors
0.00.025.807 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.808 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.808 I print_info: file format = GGUF V3 (latest)
0.00.025.809 I print_info: file type   = Q5_1
0.00.025.809 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.908 I load: special tokens cache size = 25
0.00.039.951 I load: token to piece cache size = 0.2984 MB
0.00.039.954 I print_info: arch             = gptneox
0.00.039.955 I print_info: vocab_only       = 0
0.00.039.955 I print_info: n_ctx_train      = 2048
0.00.039.955 I print_info: n_embd           = 2048
0.00.039.955 I print_info: n_layer          = 24
0.00.039.959 I print_info: n_head           = 16
0.00.039.959 I print_info: n_head_kv        = 16
0.00.039.959 I print_info: n_rot            = 32
0.00.039.962 I print_info: n_swa            = 0
0.00.039.962 I print_info: n_embd_head_k    = 128
0.00.039.962 I print_info: n_embd_head_v    = 128
0.00.039.963 I print_info: n_gqa            = 1
0.00.039.964 I print_info: n_embd_k_gqa     = 2048
0.00.039.965 I print_info: n_embd_v_gqa     = 2048
0.00.039.965 I print_info: f_norm_eps       = 1.0e-05
0.00.039.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.966 I print_info: f_logit_scale    = 0.0e+00
0.00.039.967 I print_info: n_ff             = 8192
0.00.039.967 I print_info: n_expert         = 0
0.00.039.968 I print_info: n_expert_used    = 0
0.00.039.968 I print_info: causal attn      = 1
0.00.039.968 I print_info: pooling type     = 0
0.00.039.971 I print_info: rope type        = 2
0.00.039.971 I print_info: rope scaling     = linear
0.00.039.972 I print_info: freq_base_train  = 10000.0
0.00.039.972 I print_info: freq_scale_train = 1
0.00.039.972 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.976 I print_info: rope_finetuned   = unknown
0.00.039.976 I print_info: ssm_d_conv       = 0
0.00.039.977 I print_info: ssm_d_inner      = 0
0.00.039.977 I print_info: ssm_d_state      = 0
0.00.039.977 I print_info: ssm_dt_rank      = 0
0.00.039.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.977 I print_info: model type       = 1.4B
0.00.039.978 I print_info: model params     = 1.41 B
0.00.039.978 I print_info: general.name     = 1.4B
0.00.039.978 I print_info: vocab type       = BPE
0.00.039.978 I print_info: n_vocab          = 50304
0.00.039.979 I print_info: n_merges         = 50009
0.00.039.979 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.980 I print_info: LF token         = 187 'Ċ'
0.00.039.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.980 I print_info: max token length = 1024
0.00.039.980 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.333 I load_tensors: offloading output layer to GPU
0.00.608.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.372 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.375 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.080 I llama_init_from_model: n_seq_max     = 1
0.00.610.083 I llama_init_from_model: n_ctx         = 2048
0.00.610.083 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.084 I llama_init_from_model: n_batch       = 2048
0.00.610.084 I llama_init_from_model: n_ubatch      = 512
0.00.610.085 I llama_init_from_model: flash_attn    = 0
0.00.610.085 I llama_init_from_model: freq_base     = 10000.0
0.00.610.086 I llama_init_from_model: freq_scale    = 1
0.00.610.087 I ggml_metal_init: allocating
0.00.610.109 I ggml_metal_init: found device: Apple M4
0.00.610.117 I ggml_metal_init: picking default device: Apple M4
0.00.611.671 I ggml_metal_init: using embedded metal library
0.00.617.972 I ggml_metal_init: GPU name:   Apple M4
0.00.617.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.978 I ggml_metal_init: simdgroup reduction   = true
0.00.617.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.978 I ggml_metal_init: has residency sets    = true
0.00.617.979 I ggml_metal_init: has bfloat            = true
0.00.617.979 I ggml_metal_init: use bfloat            = true
0.00.617.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.034 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.690.988 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.025 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.059 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.061 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.061 I llama_init_from_model: graph nodes  = 967
0.00.696.061 I llama_init_from_model: graph splits = 2
0.00.696.065 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.192 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.323 I main: llama threadpool init, n_threads = 4
0.00.746.364 I 
0.00.746.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.389 I 
0.00.746.531 I sampler seed: 1234
0.00.746.535 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.569 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.570 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.571 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.586.341 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.01.586.342 I llama_perf_context_print:        load time =     735.73 ms
0.01.586.342 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.80 tokens per second)
0.01.586.346 I llama_perf_context_print:        eval time =     794.54 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.586.347 I llama_perf_context_print:       total time =     840.73 ms /    70 tokens
0.01.586.564 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.189 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.216 I llama_model_loader: - type  f32:  194 tensors
0.00.024.216 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.216 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.217 I print_info: file format = GGUF V3 (latest)
0.00.024.217 I print_info: file type   = Q2_K - Medium
0.00.024.218 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.348 I load: special tokens cache size = 25
0.00.038.257 I load: token to piece cache size = 0.2984 MB
0.00.038.261 I print_info: arch             = gptneox
0.00.038.261 I print_info: vocab_only       = 0
0.00.038.261 I print_info: n_ctx_train      = 2048
0.00.038.261 I print_info: n_embd           = 2048
0.00.038.261 I print_info: n_layer          = 24
0.00.038.264 I print_info: n_head           = 16
0.00.038.265 I print_info: n_head_kv        = 16
0.00.038.265 I print_info: n_rot            = 32
0.00.038.268 I print_info: n_swa            = 0
0.00.038.268 I print_info: n_embd_head_k    = 128
0.00.038.268 I print_info: n_embd_head_v    = 128
0.00.038.269 I print_info: n_gqa            = 1
0.00.038.270 I print_info: n_embd_k_gqa     = 2048
0.00.038.275 I print_info: n_embd_v_gqa     = 2048
0.00.038.275 I print_info: f_norm_eps       = 1.0e-05
0.00.038.276 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.276 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.276 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.276 I print_info: f_logit_scale    = 0.0e+00
0.00.038.277 I print_info: n_ff             = 8192
0.00.038.277 I print_info: n_expert         = 0
0.00.038.278 I print_info: n_expert_used    = 0
0.00.038.278 I print_info: causal attn      = 1
0.00.038.278 I print_info: pooling type     = 0
0.00.038.278 I print_info: rope type        = 2
0.00.038.278 I print_info: rope scaling     = linear
0.00.038.279 I print_info: freq_base_train  = 10000.0
0.00.038.280 I print_info: freq_scale_train = 1
0.00.038.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.280 I print_info: rope_finetuned   = unknown
0.00.038.280 I print_info: ssm_d_conv       = 0
0.00.038.280 I print_info: ssm_d_inner      = 0
0.00.038.280 I print_info: ssm_d_state      = 0
0.00.038.281 I print_info: ssm_dt_rank      = 0
0.00.038.281 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.281 I print_info: model type       = 1.4B
0.00.038.282 I print_info: model params     = 1.41 B
0.00.038.282 I print_info: general.name     = 1.4B
0.00.038.282 I print_info: vocab type       = BPE
0.00.038.282 I print_info: n_vocab          = 50304
0.00.038.283 I print_info: n_merges         = 50009
0.00.038.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.283 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.283 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.283 I print_info: LF token         = 187 'Ċ'
0.00.038.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: max token length = 1024
0.00.038.284 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.361 I load_tensors: offloading output layer to GPU
0.00.345.362 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.394 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.399 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.347.141 I llama_init_from_model: n_seq_max     = 1
0.00.347.147 I llama_init_from_model: n_ctx         = 2048
0.00.347.148 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.347.149 I llama_init_from_model: n_batch       = 2048
0.00.347.149 I llama_init_from_model: n_ubatch      = 512
0.00.347.149 I llama_init_from_model: flash_attn    = 0
0.00.347.151 I llama_init_from_model: freq_base     = 10000.0
0.00.347.152 I llama_init_from_model: freq_scale    = 1
0.00.347.154 I ggml_metal_init: allocating
0.00.347.295 I ggml_metal_init: found device: Apple M4
0.00.347.309 I ggml_metal_init: picking default device: Apple M4
0.00.349.351 I ggml_metal_init: using embedded metal library
0.00.354.869 I ggml_metal_init: GPU name:   Apple M4
0.00.354.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.889 I ggml_metal_init: simdgroup reduction   = true
0.00.354.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.890 I ggml_metal_init: has residency sets    = true
0.00.354.890 I ggml_metal_init: has bfloat            = true
0.00.354.891 I ggml_metal_init: use bfloat            = true
0.00.354.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.436.949 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.436.954 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.436.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.798 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.441.799 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.441.800 I llama_init_from_model: graph nodes  = 967
0.00.441.800 I llama_init_from_model: graph splits = 2
0.00.441.805 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.441.930 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.441.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.741 I main: llama threadpool init, n_threads = 4
0.00.498.782 I 
0.00.498.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.811 I 
0.00.498.992 I sampler seed: 1234
0.00.498.997 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.007 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.008 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.008 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.173.360 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.173.361 I llama_perf_context_print:        load time =     488.81 ms
0.01.173.362 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.64 tokens per second)
0.01.173.363 I llama_perf_context_print:        eval time =     635.76 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.173.364 I llama_perf_context_print:       total time =     675.36 ms /    70 tokens
0.01.173.583 I ggml_metal_free: deallocating

real	0m1.192s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.412 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.800 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.801 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.801 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.802 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.802 I llama_model_loader: - type  f32:  194 tensors
0.00.023.802 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.803 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.803 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.803 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.803 I print_info: file format = GGUF V3 (latest)
0.00.023.804 I print_info: file type   = Q3_K - Medium
0.00.023.804 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.892 I load: special tokens cache size = 25
0.00.037.957 I load: token to piece cache size = 0.2984 MB
0.00.037.959 I print_info: arch             = gptneox
0.00.037.960 I print_info: vocab_only       = 0
0.00.037.960 I print_info: n_ctx_train      = 2048
0.00.037.960 I print_info: n_embd           = 2048
0.00.037.960 I print_info: n_layer          = 24
0.00.037.963 I print_info: n_head           = 16
0.00.037.964 I print_info: n_head_kv        = 16
0.00.037.964 I print_info: n_rot            = 32
0.00.037.965 I print_info: n_swa            = 0
0.00.037.965 I print_info: n_embd_head_k    = 128
0.00.037.965 I print_info: n_embd_head_v    = 128
0.00.037.966 I print_info: n_gqa            = 1
0.00.037.967 I print_info: n_embd_k_gqa     = 2048
0.00.037.967 I print_info: n_embd_v_gqa     = 2048
0.00.037.968 I print_info: f_norm_eps       = 1.0e-05
0.00.037.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.969 I print_info: f_logit_scale    = 0.0e+00
0.00.037.969 I print_info: n_ff             = 8192
0.00.037.970 I print_info: n_expert         = 0
0.00.037.970 I print_info: n_expert_used    = 0
0.00.037.970 I print_info: causal attn      = 1
0.00.037.972 I print_info: pooling type     = 0
0.00.037.972 I print_info: rope type        = 2
0.00.037.973 I print_info: rope scaling     = linear
0.00.037.973 I print_info: freq_base_train  = 10000.0
0.00.037.973 I print_info: freq_scale_train = 1
0.00.037.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.974 I print_info: rope_finetuned   = unknown
0.00.037.974 I print_info: ssm_d_conv       = 0
0.00.037.974 I print_info: ssm_d_inner      = 0
0.00.037.974 I print_info: ssm_d_state      = 0
0.00.037.974 I print_info: ssm_dt_rank      = 0
0.00.037.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.975 I print_info: model type       = 1.4B
0.00.037.975 I print_info: model params     = 1.41 B
0.00.037.975 I print_info: general.name     = 1.4B
0.00.037.976 I print_info: vocab type       = BPE
0.00.037.976 I print_info: n_vocab          = 50304
0.00.037.976 I print_info: n_merges         = 50009
0.00.037.977 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.978 I print_info: LF token         = 187 'Ċ'
0.00.037.978 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.978 I print_info: max token length = 1024
0.00.037.978 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.831 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.841 I load_tensors: offloading output layer to GPU
0.00.442.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.878 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.879 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.555 I llama_init_from_model: n_seq_max     = 1
0.00.444.558 I llama_init_from_model: n_ctx         = 2048
0.00.444.558 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.559 I llama_init_from_model: n_batch       = 2048
0.00.444.559 I llama_init_from_model: n_ubatch      = 512
0.00.444.560 I llama_init_from_model: flash_attn    = 0
0.00.444.561 I llama_init_from_model: freq_base     = 10000.0
0.00.444.562 I llama_init_from_model: freq_scale    = 1
0.00.444.564 I ggml_metal_init: allocating
0.00.444.654 I ggml_metal_init: found device: Apple M4
0.00.444.667 I ggml_metal_init: picking default device: Apple M4
0.00.446.625 I ggml_metal_init: using embedded metal library
0.00.452.215 I ggml_metal_init: GPU name:   Apple M4
0.00.452.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.230 I ggml_metal_init: simdgroup reduction   = true
0.00.452.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.231 I ggml_metal_init: has residency sets    = true
0.00.452.231 I ggml_metal_init: has bfloat            = true
0.00.452.231 I ggml_metal_init: use bfloat            = true
0.00.452.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.619 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.403 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.450 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.677 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.536.679 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.536.679 I llama_init_from_model: graph nodes  = 967
0.00.536.679 I llama_init_from_model: graph splits = 2
0.00.536.684 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.536.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.536.815 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.373 I main: llama threadpool init, n_threads = 4
0.00.590.418 I 
0.00.590.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.441 I 
0.00.590.608 I sampler seed: 1234
0.00.590.612 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.590.633 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.590.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.590.634 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.324.107 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.324.108 I llama_perf_context_print:        load time =     580.81 ms
0.01.324.109 I llama_perf_context_print: prompt eval time =      40.18 ms /     7 tokens (    5.74 ms per token,   174.20 tokens per second)
0.01.324.109 I llama_perf_context_print:        eval time =     690.48 ms /    63 runs   (   10.96 ms per token,    91.24 tokens per second)
0.01.324.110 I llama_perf_context_print:       total time =     734.44 ms /    70 tokens
0.01.324.339 I ggml_metal_free: deallocating

real	0m1.341s
user	0m0.111s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.677 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.284 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.812 I llama_model_loader: - type  f32:  194 tensors
0.00.026.812 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.812 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.813 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.813 I print_info: file format = GGUF V3 (latest)
0.00.026.814 I print_info: file type   = Q4_K - Medium
0.00.026.814 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.677 I load: special tokens cache size = 25
0.00.040.611 I load: token to piece cache size = 0.2984 MB
0.00.040.614 I print_info: arch             = gptneox
0.00.040.614 I print_info: vocab_only       = 0
0.00.040.614 I print_info: n_ctx_train      = 2048
0.00.040.614 I print_info: n_embd           = 2048
0.00.040.614 I print_info: n_layer          = 24
0.00.040.617 I print_info: n_head           = 16
0.00.040.618 I print_info: n_head_kv        = 16
0.00.040.618 I print_info: n_rot            = 32
0.00.040.618 I print_info: n_swa            = 0
0.00.040.618 I print_info: n_embd_head_k    = 128
0.00.040.620 I print_info: n_embd_head_v    = 128
0.00.040.621 I print_info: n_gqa            = 1
0.00.040.621 I print_info: n_embd_k_gqa     = 2048
0.00.040.624 I print_info: n_embd_v_gqa     = 2048
0.00.040.624 I print_info: f_norm_eps       = 1.0e-05
0.00.040.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.625 I print_info: f_logit_scale    = 0.0e+00
0.00.040.626 I print_info: n_ff             = 8192
0.00.040.626 I print_info: n_expert         = 0
0.00.040.628 I print_info: n_expert_used    = 0
0.00.040.628 I print_info: causal attn      = 1
0.00.040.628 I print_info: pooling type     = 0
0.00.040.628 I print_info: rope type        = 2
0.00.040.628 I print_info: rope scaling     = linear
0.00.040.629 I print_info: freq_base_train  = 10000.0
0.00.040.629 I print_info: freq_scale_train = 1
0.00.040.629 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.630 I print_info: rope_finetuned   = unknown
0.00.040.630 I print_info: ssm_d_conv       = 0
0.00.040.630 I print_info: ssm_d_inner      = 0
0.00.040.630 I print_info: ssm_d_state      = 0
0.00.040.630 I print_info: ssm_dt_rank      = 0
0.00.040.630 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.631 I print_info: model type       = 1.4B
0.00.040.631 I print_info: model params     = 1.41 B
0.00.040.631 I print_info: general.name     = 1.4B
0.00.040.632 I print_info: vocab type       = BPE
0.00.040.632 I print_info: n_vocab          = 50304
0.00.040.632 I print_info: n_merges         = 50009
0.00.040.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: LF token         = 187 'Ċ'
0.00.040.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: max token length = 1024
0.00.040.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.820 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.836 I load_tensors: offloading output layer to GPU
0.00.547.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.872 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.877 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.549.534 I llama_init_from_model: n_seq_max     = 1
0.00.549.537 I llama_init_from_model: n_ctx         = 2048
0.00.549.538 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.549.538 I llama_init_from_model: n_batch       = 2048
0.00.549.538 I llama_init_from_model: n_ubatch      = 512
0.00.549.539 I llama_init_from_model: flash_attn    = 0
0.00.549.541 I llama_init_from_model: freq_base     = 10000.0
0.00.549.541 I llama_init_from_model: freq_scale    = 1
0.00.549.544 I ggml_metal_init: allocating
0.00.549.647 I ggml_metal_init: found device: Apple M4
0.00.549.660 I ggml_metal_init: picking default device: Apple M4
0.00.551.688 I ggml_metal_init: using embedded metal library
0.00.558.470 I ggml_metal_init: GPU name:   Apple M4
0.00.558.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.478 I ggml_metal_init: simdgroup reduction   = true
0.00.558.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.478 I ggml_metal_init: has residency sets    = true
0.00.558.478 I ggml_metal_init: has bfloat            = true
0.00.558.479 I ggml_metal_init: use bfloat            = true
0.00.558.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.393 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.470 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.635.479 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.635.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.115 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.640.117 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.640.118 I llama_init_from_model: graph nodes  = 967
0.00.640.118 I llama_init_from_model: graph splits = 2
0.00.640.123 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.640.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.202 I main: llama threadpool init, n_threads = 4
0.00.696.246 I 
0.00.696.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.271 I 
0.00.696.432 I sampler seed: 1234
0.00.696.437 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.455 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.446.923 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.446.924 I llama_perf_context_print:        load time =     683.81 ms
0.01.446.926 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.38 tokens per second)
0.01.446.926 I llama_perf_context_print:        eval time =     700.32 ms /    63 runs   (   11.12 ms per token,    89.96 tokens per second)
0.01.446.927 I llama_perf_context_print:       total time =     751.44 ms /    70 tokens
0.01.447.145 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.487 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.488 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.489 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.490 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.490 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.490 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.491 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.493 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.494 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.019 I llama_model_loader: - type  f32:  194 tensors
0.00.024.019 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.019 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.020 I print_info: file format = GGUF V3 (latest)
0.00.024.020 I print_info: file type   = Q5_K - Medium
0.00.024.021 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.127 I load: special tokens cache size = 25
0.00.038.238 I load: token to piece cache size = 0.2984 MB
0.00.038.242 I print_info: arch             = gptneox
0.00.038.242 I print_info: vocab_only       = 0
0.00.038.242 I print_info: n_ctx_train      = 2048
0.00.038.242 I print_info: n_embd           = 2048
0.00.038.242 I print_info: n_layer          = 24
0.00.038.245 I print_info: n_head           = 16
0.00.038.248 I print_info: n_head_kv        = 16
0.00.038.249 I print_info: n_rot            = 32
0.00.038.249 I print_info: n_swa            = 0
0.00.038.249 I print_info: n_embd_head_k    = 128
0.00.038.249 I print_info: n_embd_head_v    = 128
0.00.038.250 I print_info: n_gqa            = 1
0.00.038.251 I print_info: n_embd_k_gqa     = 2048
0.00.038.251 I print_info: n_embd_v_gqa     = 2048
0.00.038.252 I print_info: f_norm_eps       = 1.0e-05
0.00.038.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.253 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.253 I print_info: f_logit_scale    = 0.0e+00
0.00.038.254 I print_info: n_ff             = 8192
0.00.038.254 I print_info: n_expert         = 0
0.00.038.254 I print_info: n_expert_used    = 0
0.00.038.254 I print_info: causal attn      = 1
0.00.038.254 I print_info: pooling type     = 0
0.00.038.254 I print_info: rope type        = 2
0.00.038.255 I print_info: rope scaling     = linear
0.00.038.255 I print_info: freq_base_train  = 10000.0
0.00.038.256 I print_info: freq_scale_train = 1
0.00.038.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.258 I print_info: rope_finetuned   = unknown
0.00.038.258 I print_info: ssm_d_conv       = 0
0.00.038.258 I print_info: ssm_d_inner      = 0
0.00.038.258 I print_info: ssm_d_state      = 0
0.00.038.259 I print_info: ssm_dt_rank      = 0
0.00.038.259 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.259 I print_info: model type       = 1.4B
0.00.038.259 I print_info: model params     = 1.41 B
0.00.038.259 I print_info: general.name     = 1.4B
0.00.038.260 I print_info: vocab type       = BPE
0.00.038.260 I print_info: n_vocab          = 50304
0.00.038.261 I print_info: n_merges         = 50009
0.00.038.265 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.265 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.265 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.265 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.266 I print_info: LF token         = 187 'Ċ'
0.00.038.266 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.266 I print_info: max token length = 1024
0.00.038.266 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.854 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.873 I load_tensors: offloading output layer to GPU
0.00.615.874 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.913 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.615.914 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.617.508 I llama_init_from_model: n_seq_max     = 1
0.00.617.519 I llama_init_from_model: n_ctx         = 2048
0.00.617.519 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.520 I llama_init_from_model: n_batch       = 2048
0.00.617.520 I llama_init_from_model: n_ubatch      = 512
0.00.617.520 I llama_init_from_model: flash_attn    = 0
0.00.617.522 I llama_init_from_model: freq_base     = 10000.0
0.00.617.522 I llama_init_from_model: freq_scale    = 1
0.00.617.528 I ggml_metal_init: allocating
0.00.617.609 I ggml_metal_init: found device: Apple M4
0.00.617.628 I ggml_metal_init: picking default device: Apple M4
0.00.619.745 I ggml_metal_init: using embedded metal library
0.00.626.458 I ggml_metal_init: GPU name:   Apple M4
0.00.626.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.464 I ggml_metal_init: simdgroup reduction   = true
0.00.626.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.464 I ggml_metal_init: has residency sets    = true
0.00.626.465 I ggml_metal_init: has bfloat            = true
0.00.626.465 I ggml_metal_init: use bfloat            = true
0.00.626.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.235 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.671 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.678 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.711 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.472 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.474 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.474 I llama_init_from_model: graph nodes  = 967
0.00.712.475 I llama_init_from_model: graph splits = 2
0.00.712.480 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.071 I main: llama threadpool init, n_threads = 4
0.00.780.110 I 
0.00.780.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.129 I 
0.00.780.305 I sampler seed: 1234
0.00.780.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.332 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.333 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.333 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.625.272 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.625.273 I llama_perf_context_print:        load time =     770.50 ms
0.01.625.274 I llama_perf_context_print: prompt eval time =      56.87 ms /     7 tokens (    8.12 ms per token,   123.09 tokens per second)
0.01.625.274 I llama_perf_context_print:        eval time =     785.53 ms /    63 runs   (   12.47 ms per token,    80.20 tokens per second)
0.01.625.275 I llama_perf_context_print:       total time =     845.93 ms /    70 tokens
0.01.625.510 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.110s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.686 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.217 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.218 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.650 I llama_model_loader: - type  f32:  194 tensors
0.00.023.650 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.650 I print_info: file format = GGUF V3 (latest)
0.00.023.651 I print_info: file type   = Q6_K
0.00.023.652 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.420 I load: special tokens cache size = 25
0.00.037.430 I load: token to piece cache size = 0.2984 MB
0.00.037.433 I print_info: arch             = gptneox
0.00.037.433 I print_info: vocab_only       = 0
0.00.037.433 I print_info: n_ctx_train      = 2048
0.00.037.433 I print_info: n_embd           = 2048
0.00.037.434 I print_info: n_layer          = 24
0.00.037.436 I print_info: n_head           = 16
0.00.037.437 I print_info: n_head_kv        = 16
0.00.037.437 I print_info: n_rot            = 32
0.00.037.437 I print_info: n_swa            = 0
0.00.037.437 I print_info: n_embd_head_k    = 128
0.00.037.438 I print_info: n_embd_head_v    = 128
0.00.037.439 I print_info: n_gqa            = 1
0.00.037.439 I print_info: n_embd_k_gqa     = 2048
0.00.037.441 I print_info: n_embd_v_gqa     = 2048
0.00.037.441 I print_info: f_norm_eps       = 1.0e-05
0.00.037.442 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.444 I print_info: f_logit_scale    = 0.0e+00
0.00.037.445 I print_info: n_ff             = 8192
0.00.037.445 I print_info: n_expert         = 0
0.00.037.445 I print_info: n_expert_used    = 0
0.00.037.445 I print_info: causal attn      = 1
0.00.037.445 I print_info: pooling type     = 0
0.00.037.446 I print_info: rope type        = 2
0.00.037.446 I print_info: rope scaling     = linear
0.00.037.446 I print_info: freq_base_train  = 10000.0
0.00.037.446 I print_info: freq_scale_train = 1
0.00.037.447 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.447 I print_info: rope_finetuned   = unknown
0.00.037.447 I print_info: ssm_d_conv       = 0
0.00.037.447 I print_info: ssm_d_inner      = 0
0.00.037.447 I print_info: ssm_d_state      = 0
0.00.037.448 I print_info: ssm_dt_rank      = 0
0.00.037.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.448 I print_info: model type       = 1.4B
0.00.037.448 I print_info: model params     = 1.41 B
0.00.037.450 I print_info: general.name     = 1.4B
0.00.037.451 I print_info: vocab type       = BPE
0.00.037.451 I print_info: n_vocab          = 50304
0.00.037.451 I print_info: n_merges         = 50009
0.00.037.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.452 I print_info: LF token         = 187 'Ċ'
0.00.037.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.452 I print_info: max token length = 1024
0.00.037.453 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.218 I load_tensors: offloading output layer to GPU
0.00.640.218 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.242 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.640.243 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.641.745 I llama_init_from_model: n_seq_max     = 1
0.00.641.747 I llama_init_from_model: n_ctx         = 2048
0.00.641.747 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.748 I llama_init_from_model: n_batch       = 2048
0.00.641.748 I llama_init_from_model: n_ubatch      = 512
0.00.641.749 I llama_init_from_model: flash_attn    = 0
0.00.641.750 I llama_init_from_model: freq_base     = 10000.0
0.00.641.750 I llama_init_from_model: freq_scale    = 1
0.00.641.751 I ggml_metal_init: allocating
0.00.641.806 I ggml_metal_init: found device: Apple M4
0.00.641.816 I ggml_metal_init: picking default device: Apple M4
0.00.643.309 I ggml_metal_init: using embedded metal library
0.00.649.332 I ggml_metal_init: GPU name:   Apple M4
0.00.649.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.337 I ggml_metal_init: simdgroup reduction   = true
0.00.649.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.338 I ggml_metal_init: has residency sets    = true
0.00.649.338 I ggml_metal_init: has bfloat            = true
0.00.649.338 I ggml_metal_init: use bfloat            = true
0.00.649.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.139 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.681 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.687 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.723 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.248 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.250 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.250 I llama_init_from_model: graph nodes  = 967
0.00.728.250 I llama_init_from_model: graph splits = 2
0.00.728.256 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.391 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.409 I main: llama threadpool init, n_threads = 4
0.00.795.452 I 
0.00.795.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.478 I 
0.00.795.659 I sampler seed: 1234
0.00.795.663 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.674 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.675 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.987 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.673.988 I llama_perf_context_print:        load time =     786.00 ms
0.01.673.989 I llama_perf_context_print: prompt eval time =      57.67 ms /     7 tokens (    8.24 ms per token,   121.38 tokens per second)
0.01.673.989 I llama_perf_context_print:        eval time =     817.68 ms /    63 runs   (   12.98 ms per token,    77.05 tokens per second)
0.01.673.990 I llama_perf_context_print:       total time =     879.30 ms /    70 tokens
0.01.674.253 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.107s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.708 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.262 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.519 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.523 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.329 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.318 I llama_model_loader: - type  f32:  194 tensors
0.00.050.319 I llama_model_loader: - type  f16:   98 tensors
0.00.050.319 I print_info: file format = GGUF V3 (latest)
0.00.050.320 I print_info: file type   = all F32 (guessed)
0.00.050.323 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.792 I load: special tokens cache size = 25
0.00.069.189 I load: token to piece cache size = 0.2984 MB
0.00.069.192 I print_info: arch             = gptneox
0.00.069.192 I print_info: vocab_only       = 0
0.00.069.193 I print_info: n_ctx_train      = 2048
0.00.069.193 I print_info: n_embd           = 2048
0.00.069.193 I print_info: n_layer          = 24
0.00.069.197 I print_info: n_head           = 16
0.00.069.198 I print_info: n_head_kv        = 16
0.00.069.198 I print_info: n_rot            = 32
0.00.069.198 I print_info: n_swa            = 0
0.00.069.198 I print_info: n_embd_head_k    = 128
0.00.069.198 I print_info: n_embd_head_v    = 128
0.00.069.199 I print_info: n_gqa            = 1
0.00.069.200 I print_info: n_embd_k_gqa     = 2048
0.00.069.200 I print_info: n_embd_v_gqa     = 2048
0.00.069.201 I print_info: f_norm_eps       = 1.0e-05
0.00.069.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.202 I print_info: f_logit_scale    = 0.0e+00
0.00.069.203 I print_info: n_ff             = 8192
0.00.069.203 I print_info: n_expert         = 0
0.00.069.203 I print_info: n_expert_used    = 0
0.00.069.203 I print_info: causal attn      = 1
0.00.069.205 I print_info: pooling type     = 0
0.00.069.205 I print_info: rope type        = 2
0.00.069.206 I print_info: rope scaling     = linear
0.00.069.206 I print_info: freq_base_train  = 10000.0
0.00.069.207 I print_info: freq_scale_train = 1
0.00.069.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.207 I print_info: rope_finetuned   = unknown
0.00.069.207 I print_info: ssm_d_conv       = 0
0.00.069.207 I print_info: ssm_d_inner      = 0
0.00.069.207 I print_info: ssm_d_state      = 0
0.00.069.207 I print_info: ssm_dt_rank      = 0
0.00.069.208 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.208 I print_info: model type       = 1.4B
0.00.069.208 I print_info: model params     = 1.41 B
0.00.069.208 I print_info: general.name     = 1.4B
0.00.069.209 I print_info: vocab type       = BPE
0.00.069.209 I print_info: n_vocab          = 50304
0.00.069.209 I print_info: n_merges         = 50009
0.00.069.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.214 I print_info: LF token         = 187 'Ċ'
0.00.069.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.214 I print_info: max token length = 1024
0.00.069.215 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.451.656 I load_tensors: offloading 24 repeating layers to GPU
0.01.451.661 I load_tensors: offloading output layer to GPU
0.01.451.661 I load_tensors: offloaded 25/25 layers to GPU
0.01.451.692 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.451.693 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.452.885 I llama_init_from_model: n_seq_max     = 1
0.01.452.886 I llama_init_from_model: n_ctx         = 128
0.01.452.886 I llama_init_from_model: n_ctx_per_seq = 128
0.01.452.886 I llama_init_from_model: n_batch       = 128
0.01.452.886 I llama_init_from_model: n_ubatch      = 128
0.01.452.887 I llama_init_from_model: flash_attn    = 0
0.01.452.887 I llama_init_from_model: freq_base     = 10000.0
0.01.452.887 I llama_init_from_model: freq_scale    = 1
0.01.452.888 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.452.892 I ggml_metal_init: allocating
0.01.452.998 I ggml_metal_init: found device: Apple M4
0.01.453.004 I ggml_metal_init: picking default device: Apple M4
0.01.454.222 I ggml_metal_init: using embedded metal library
0.01.458.093 I ggml_metal_init: GPU name:   Apple M4
0.01.458.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.458.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.458.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.458.097 I ggml_metal_init: simdgroup reduction   = true
0.01.458.097 I ggml_metal_init: simdgroup matrix mul. = true
0.01.458.097 I ggml_metal_init: has residency sets    = true
0.01.458.097 I ggml_metal_init: has bfloat            = true
0.01.458.097 I ggml_metal_init: use bfloat            = true
0.01.458.098 I ggml_metal_init: hasUnifiedMemory      = true
0.01.458.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.468.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.470.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.470.324 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.470.349 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.472.026 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.472.028 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.472.028 I llama_init_from_model: graph nodes  = 967
0.01.472.028 I llama_init_from_model: graph splits = 2
0.01.472.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.472.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.506.733 I 
0.01.506.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.506.772 I perplexity: tokenizing the input ..
0.01.512.040 I perplexity: tokenization took 5.266 ms
0.01.512.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.631.085 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.633.959 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.010 I llama_perf_context_print:        load time =    1486.46 ms
0.01.634.012 I llama_perf_context_print: prompt eval time =     118.77 ms /   128 tokens (    0.93 ms per token,  1077.76 tokens per second)
0.01.634.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.014 I llama_perf_context_print:       total time =     127.28 ms /   129 tokens
0.01.634.689 I ggml_metal_free: deallocating

real	0m1.821s
user	0m0.102s
sys	0m0.274s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.295 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.736 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.741 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.424 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.425 I print_info: file format = GGUF V3 (latest)
0.00.025.425 I print_info: file type   = Q8_0
0.00.025.427 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.993 I load: special tokens cache size = 25
0.00.040.229 I load: token to piece cache size = 0.2984 MB
0.00.040.234 I print_info: arch             = gptneox
0.00.040.234 I print_info: vocab_only       = 0
0.00.040.234 I print_info: n_ctx_train      = 2048
0.00.040.234 I print_info: n_embd           = 2048
0.00.040.235 I print_info: n_layer          = 24
0.00.040.239 I print_info: n_head           = 16
0.00.040.240 I print_info: n_head_kv        = 16
0.00.040.240 I print_info: n_rot            = 32
0.00.040.240 I print_info: n_swa            = 0
0.00.040.240 I print_info: n_embd_head_k    = 128
0.00.040.240 I print_info: n_embd_head_v    = 128
0.00.040.241 I print_info: n_gqa            = 1
0.00.040.242 I print_info: n_embd_k_gqa     = 2048
0.00.040.243 I print_info: n_embd_v_gqa     = 2048
0.00.040.243 I print_info: f_norm_eps       = 1.0e-05
0.00.040.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.244 I print_info: f_logit_scale    = 0.0e+00
0.00.040.244 I print_info: n_ff             = 8192
0.00.040.245 I print_info: n_expert         = 0
0.00.040.245 I print_info: n_expert_used    = 0
0.00.040.245 I print_info: causal attn      = 1
0.00.040.245 I print_info: pooling type     = 0
0.00.040.245 I print_info: rope type        = 2
0.00.040.245 I print_info: rope scaling     = linear
0.00.040.246 I print_info: freq_base_train  = 10000.0
0.00.040.246 I print_info: freq_scale_train = 1
0.00.040.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.246 I print_info: rope_finetuned   = unknown
0.00.040.246 I print_info: ssm_d_conv       = 0
0.00.040.246 I print_info: ssm_d_inner      = 0
0.00.040.247 I print_info: ssm_d_state      = 0
0.00.040.247 I print_info: ssm_dt_rank      = 0
0.00.040.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.247 I print_info: model type       = 1.4B
0.00.040.247 I print_info: model params     = 1.41 B
0.00.040.248 I print_info: general.name     = 1.4B
0.00.040.248 I print_info: vocab type       = BPE
0.00.040.248 I print_info: n_vocab          = 50304
0.00.040.248 I print_info: n_merges         = 50009
0.00.040.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: LF token         = 187 'Ċ'
0.00.040.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.252 I print_info: max token length = 1024
0.00.040.252 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.873.657 I load_tensors: offloading 24 repeating layers to GPU
0.00.873.666 I load_tensors: offloading output layer to GPU
0.00.873.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.873.693 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.873.694 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.875.008 I llama_init_from_model: n_seq_max     = 1
0.00.875.009 I llama_init_from_model: n_ctx         = 128
0.00.875.010 I llama_init_from_model: n_ctx_per_seq = 128
0.00.875.010 I llama_init_from_model: n_batch       = 128
0.00.875.010 I llama_init_from_model: n_ubatch      = 128
0.00.875.011 I llama_init_from_model: flash_attn    = 0
0.00.875.011 I llama_init_from_model: freq_base     = 10000.0
0.00.875.012 I llama_init_from_model: freq_scale    = 1
0.00.875.013 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.875.014 I ggml_metal_init: allocating
0.00.875.076 I ggml_metal_init: found device: Apple M4
0.00.875.087 I ggml_metal_init: picking default device: Apple M4
0.00.876.414 I ggml_metal_init: using embedded metal library
0.00.881.775 I ggml_metal_init: GPU name:   Apple M4
0.00.881.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.881.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.881.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.881.781 I ggml_metal_init: simdgroup reduction   = true
0.00.881.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.881.781 I ggml_metal_init: has residency sets    = true
0.00.881.781 I ggml_metal_init: has bfloat            = true
0.00.881.782 I ggml_metal_init: use bfloat            = true
0.00.881.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.881.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.896.810 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.900.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.900.162 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.900.224 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.903.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.903.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.903.350 I llama_init_from_model: graph nodes  = 967
0.00.903.350 I llama_init_from_model: graph splits = 2
0.00.903.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.903.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.932.643 I 
0.00.932.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.932.722 I perplexity: tokenizing the input ..
0.00.939.660 I perplexity: tokenization took 6.937 ms
0.00.939.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.077.828 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.079.325 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.079.351 I llama_perf_context_print:        load time =     923.34 ms
0.01.079.353 I llama_perf_context_print: prompt eval time =     137.94 ms /   128 tokens (    1.08 ms per token,   927.97 tokens per second)
0.01.079.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.079.353 I llama_perf_context_print:       total time =     146.71 ms /   129 tokens
0.01.079.725 I ggml_metal_free: deallocating

real	0m1.095s
user	0m0.076s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.180 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.812 I llama_model_loader: - type  f32:  194 tensors
0.00.025.812 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.813 I print_info: file format = GGUF V3 (latest)
0.00.025.814 I print_info: file type   = Q4_0
0.00.025.817 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.173 I load: special tokens cache size = 25
0.00.040.320 I load: token to piece cache size = 0.2984 MB
0.00.040.325 I print_info: arch             = gptneox
0.00.040.325 I print_info: vocab_only       = 0
0.00.040.325 I print_info: n_ctx_train      = 2048
0.00.040.326 I print_info: n_embd           = 2048
0.00.040.326 I print_info: n_layer          = 24
0.00.040.330 I print_info: n_head           = 16
0.00.040.331 I print_info: n_head_kv        = 16
0.00.040.331 I print_info: n_rot            = 32
0.00.040.331 I print_info: n_swa            = 0
0.00.040.331 I print_info: n_embd_head_k    = 128
0.00.040.331 I print_info: n_embd_head_v    = 128
0.00.040.332 I print_info: n_gqa            = 1
0.00.040.333 I print_info: n_embd_k_gqa     = 2048
0.00.040.334 I print_info: n_embd_v_gqa     = 2048
0.00.040.334 I print_info: f_norm_eps       = 1.0e-05
0.00.040.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.335 I print_info: f_logit_scale    = 0.0e+00
0.00.040.336 I print_info: n_ff             = 8192
0.00.040.336 I print_info: n_expert         = 0
0.00.040.336 I print_info: n_expert_used    = 0
0.00.040.336 I print_info: causal attn      = 1
0.00.040.336 I print_info: pooling type     = 0
0.00.040.336 I print_info: rope type        = 2
0.00.040.337 I print_info: rope scaling     = linear
0.00.040.337 I print_info: freq_base_train  = 10000.0
0.00.040.337 I print_info: freq_scale_train = 1
0.00.040.337 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.338 I print_info: rope_finetuned   = unknown
0.00.040.338 I print_info: ssm_d_conv       = 0
0.00.040.338 I print_info: ssm_d_inner      = 0
0.00.040.338 I print_info: ssm_d_state      = 0
0.00.040.338 I print_info: ssm_dt_rank      = 0
0.00.040.338 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.339 I print_info: model type       = 1.4B
0.00.040.339 I print_info: model params     = 1.41 B
0.00.040.339 I print_info: general.name     = 1.4B
0.00.040.339 I print_info: vocab type       = BPE
0.00.040.340 I print_info: n_vocab          = 50304
0.00.040.340 I print_info: n_merges         = 50009
0.00.040.340 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.340 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.343 I print_info: LF token         = 187 'Ċ'
0.00.040.343 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.343 I print_info: max token length = 1024
0.00.040.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.008 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.025 I load_tensors: offloading output layer to GPU
0.00.619.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.065 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.619.077 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.620.806 I llama_init_from_model: n_seq_max     = 1
0.00.620.808 I llama_init_from_model: n_ctx         = 128
0.00.620.809 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.810 I llama_init_from_model: n_batch       = 128
0.00.620.810 I llama_init_from_model: n_ubatch      = 128
0.00.620.810 I llama_init_from_model: flash_attn    = 0
0.00.620.813 I llama_init_from_model: freq_base     = 10000.0
0.00.620.813 I llama_init_from_model: freq_scale    = 1
0.00.620.814 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.817 I ggml_metal_init: allocating
0.00.620.903 I ggml_metal_init: found device: Apple M4
0.00.620.920 I ggml_metal_init: picking default device: Apple M4
0.00.622.803 I ggml_metal_init: using embedded metal library
0.00.628.185 I ggml_metal_init: GPU name:   Apple M4
0.00.628.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.204 I ggml_metal_init: simdgroup reduction   = true
0.00.628.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.204 I ggml_metal_init: has residency sets    = true
0.00.628.204 I ggml_metal_init: has bfloat            = true
0.00.628.204 I ggml_metal_init: use bfloat            = true
0.00.628.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.888 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.902 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.961 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.240 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.242 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.242 I llama_init_from_model: graph nodes  = 967
0.00.655.243 I llama_init_from_model: graph splits = 2
0.00.655.247 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.828 I 
0.00.678.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.912 I perplexity: tokenizing the input ..
0.00.685.329 I perplexity: tokenization took 6.415 ms
0.00.685.333 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.041 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.808.449 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.808.472 I llama_perf_context_print:        load time =     669.05 ms
0.00.808.473 I llama_perf_context_print: prompt eval time =     121.47 ms /   128 tokens (    0.95 ms per token,  1053.73 tokens per second)
0.00.808.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.474 I llama_perf_context_print:       total time =     129.65 ms /   129 tokens
0.00.808.844 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.078s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.339 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.230 I llama_model_loader: - type  f32:  194 tensors
0.00.025.230 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.231 I print_info: file format = GGUF V3 (latest)
0.00.025.231 I print_info: file type   = Q4_1
0.00.025.232 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.603 I load: special tokens cache size = 25
0.00.039.733 I load: token to piece cache size = 0.2984 MB
0.00.039.738 I print_info: arch             = gptneox
0.00.039.738 I print_info: vocab_only       = 0
0.00.039.739 I print_info: n_ctx_train      = 2048
0.00.039.739 I print_info: n_embd           = 2048
0.00.039.739 I print_info: n_layer          = 24
0.00.039.744 I print_info: n_head           = 16
0.00.039.744 I print_info: n_head_kv        = 16
0.00.039.744 I print_info: n_rot            = 32
0.00.039.745 I print_info: n_swa            = 0
0.00.039.748 I print_info: n_embd_head_k    = 128
0.00.039.748 I print_info: n_embd_head_v    = 128
0.00.039.749 I print_info: n_gqa            = 1
0.00.039.750 I print_info: n_embd_k_gqa     = 2048
0.00.039.750 I print_info: n_embd_v_gqa     = 2048
0.00.039.751 I print_info: f_norm_eps       = 1.0e-05
0.00.039.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.751 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.751 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.751 I print_info: f_logit_scale    = 0.0e+00
0.00.039.752 I print_info: n_ff             = 8192
0.00.039.773 I print_info: n_expert         = 0
0.00.039.781 I print_info: n_expert_used    = 0
0.00.039.781 I print_info: causal attn      = 1
0.00.039.781 I print_info: pooling type     = 0
0.00.039.781 I print_info: rope type        = 2
0.00.039.782 I print_info: rope scaling     = linear
0.00.039.782 I print_info: freq_base_train  = 10000.0
0.00.039.785 I print_info: freq_scale_train = 1
0.00.039.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.786 I print_info: rope_finetuned   = unknown
0.00.039.786 I print_info: ssm_d_conv       = 0
0.00.039.786 I print_info: ssm_d_inner      = 0
0.00.039.787 I print_info: ssm_d_state      = 0
0.00.039.787 I print_info: ssm_dt_rank      = 0
0.00.039.787 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.788 I print_info: model type       = 1.4B
0.00.039.788 I print_info: model params     = 1.41 B
0.00.039.789 I print_info: general.name     = 1.4B
0.00.039.789 I print_info: vocab type       = BPE
0.00.039.789 I print_info: n_vocab          = 50304
0.00.039.790 I print_info: n_merges         = 50009
0.00.039.790 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.791 I print_info: LF token         = 187 'Ċ'
0.00.039.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.793 I print_info: max token length = 1024
0.00.039.793 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.447 I load_tensors: offloading output layer to GPU
0.00.663.448 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.483 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.663.484 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.665.051 I llama_init_from_model: n_seq_max     = 1
0.00.665.053 I llama_init_from_model: n_ctx         = 128
0.00.665.054 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.054 I llama_init_from_model: n_batch       = 128
0.00.665.055 I llama_init_from_model: n_ubatch      = 128
0.00.665.055 I llama_init_from_model: flash_attn    = 0
0.00.665.058 I llama_init_from_model: freq_base     = 10000.0
0.00.665.058 I llama_init_from_model: freq_scale    = 1
0.00.665.059 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.061 I ggml_metal_init: allocating
0.00.665.146 I ggml_metal_init: found device: Apple M4
0.00.665.160 I ggml_metal_init: picking default device: Apple M4
0.00.666.960 I ggml_metal_init: using embedded metal library
0.00.673.895 I ggml_metal_init: GPU name:   Apple M4
0.00.673.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.905 I ggml_metal_init: simdgroup reduction   = true
0.00.673.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.905 I ggml_metal_init: has residency sets    = true
0.00.673.906 I ggml_metal_init: has bfloat            = true
0.00.673.906 I ggml_metal_init: use bfloat            = true
0.00.673.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.566 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.958 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.960 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.960 I llama_init_from_model: graph nodes  = 967
0.00.698.960 I llama_init_from_model: graph splits = 2
0.00.698.964 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.987 I 
0.00.730.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.075 I perplexity: tokenizing the input ..
0.00.737.264 I perplexity: tokenization took 7.186 ms
0.00.737.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.488 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.874.817 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.874.839 I llama_perf_context_print:        load time =     721.07 ms
0.00.874.840 I llama_perf_context_print: prompt eval time =     135.26 ms /   128 tokens (    1.06 ms per token,   946.35 tokens per second)
0.00.874.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.841 I llama_perf_context_print:       total time =     144.86 ms /   129 tokens
0.00.875.216 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.081s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.096 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.480 I llama_model_loader: - type  f32:  194 tensors
0.00.024.481 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.482 I print_info: file format = GGUF V3 (latest)
0.00.024.486 I print_info: file type   = Q5_0
0.00.024.487 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.469 I load: special tokens cache size = 25
0.00.038.523 I load: token to piece cache size = 0.2984 MB
0.00.038.527 I print_info: arch             = gptneox
0.00.038.528 I print_info: vocab_only       = 0
0.00.038.528 I print_info: n_ctx_train      = 2048
0.00.038.528 I print_info: n_embd           = 2048
0.00.038.528 I print_info: n_layer          = 24
0.00.038.532 I print_info: n_head           = 16
0.00.038.533 I print_info: n_head_kv        = 16
0.00.038.533 I print_info: n_rot            = 32
0.00.038.533 I print_info: n_swa            = 0
0.00.038.534 I print_info: n_embd_head_k    = 128
0.00.038.534 I print_info: n_embd_head_v    = 128
0.00.038.535 I print_info: n_gqa            = 1
0.00.038.535 I print_info: n_embd_k_gqa     = 2048
0.00.038.536 I print_info: n_embd_v_gqa     = 2048
0.00.038.537 I print_info: f_norm_eps       = 1.0e-05
0.00.038.537 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.537 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.537 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.538 I print_info: f_logit_scale    = 0.0e+00
0.00.038.538 I print_info: n_ff             = 8192
0.00.038.538 I print_info: n_expert         = 0
0.00.038.538 I print_info: n_expert_used    = 0
0.00.038.539 I print_info: causal attn      = 1
0.00.038.539 I print_info: pooling type     = 0
0.00.038.539 I print_info: rope type        = 2
0.00.038.539 I print_info: rope scaling     = linear
0.00.038.539 I print_info: freq_base_train  = 10000.0
0.00.038.539 I print_info: freq_scale_train = 1
0.00.038.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.540 I print_info: rope_finetuned   = unknown
0.00.038.540 I print_info: ssm_d_conv       = 0
0.00.038.540 I print_info: ssm_d_inner      = 0
0.00.038.540 I print_info: ssm_d_state      = 0
0.00.038.540 I print_info: ssm_dt_rank      = 0
0.00.038.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.541 I print_info: model type       = 1.4B
0.00.038.541 I print_info: model params     = 1.41 B
0.00.038.541 I print_info: general.name     = 1.4B
0.00.038.542 I print_info: vocab type       = BPE
0.00.038.542 I print_info: n_vocab          = 50304
0.00.038.542 I print_info: n_merges         = 50009
0.00.038.542 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.545 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.545 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.545 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.546 I print_info: LF token         = 187 'Ċ'
0.00.038.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.546 I print_info: max token length = 1024
0.00.038.546 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.712.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.712.093 I load_tensors: offloading output layer to GPU
0.00.712.093 I load_tensors: offloaded 25/25 layers to GPU
0.00.712.133 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.712.135 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.713.799 I llama_init_from_model: n_seq_max     = 1
0.00.713.801 I llama_init_from_model: n_ctx         = 128
0.00.713.801 I llama_init_from_model: n_ctx_per_seq = 128
0.00.713.801 I llama_init_from_model: n_batch       = 128
0.00.713.802 I llama_init_from_model: n_ubatch      = 128
0.00.713.802 I llama_init_from_model: flash_attn    = 0
0.00.713.804 I llama_init_from_model: freq_base     = 10000.0
0.00.713.805 I llama_init_from_model: freq_scale    = 1
0.00.713.806 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.713.807 I ggml_metal_init: allocating
0.00.713.893 I ggml_metal_init: found device: Apple M4
0.00.713.906 I ggml_metal_init: picking default device: Apple M4
0.00.715.515 I ggml_metal_init: using embedded metal library
0.00.721.717 I ggml_metal_init: GPU name:   Apple M4
0.00.721.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.724 I ggml_metal_init: simdgroup reduction   = true
0.00.721.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.724 I ggml_metal_init: has residency sets    = true
0.00.721.725 I ggml_metal_init: has bfloat            = true
0.00.721.725 I ggml_metal_init: use bfloat            = true
0.00.721.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.739.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.873 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.742.876 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.742.917 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.171 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.746.172 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.746.173 I llama_init_from_model: graph nodes  = 967
0.00.746.173 I llama_init_from_model: graph splits = 2
0.00.746.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.746.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.706 I 
0.00.777.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.789 I perplexity: tokenizing the input ..
0.00.785.255 I perplexity: tokenization took 7.463 ms
0.00.785.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.933.397 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.934.733 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.934.759 I llama_perf_context_print:        load time =     768.74 ms
0.00.934.760 I llama_perf_context_print: prompt eval time =     147.18 ms /   128 tokens (    1.15 ms per token,   869.67 tokens per second)
0.00.934.761 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.934.761 I llama_perf_context_print:       total time =     157.06 ms /   129 tokens
0.00.935.109 I ggml_metal_free: deallocating

real	0m0.950s
user	0m0.079s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.599 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.601 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.602 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.602 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.603 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.604 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.334 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.069 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.070 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.070 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.071 I llama_model_loader: - type  f32:  194 tensors
0.00.025.072 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.072 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.073 I print_info: file format = GGUF V3 (latest)
0.00.025.073 I print_info: file type   = Q5_1
0.00.025.074 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.374 I load: special tokens cache size = 25
0.00.039.462 I load: token to piece cache size = 0.2984 MB
0.00.039.466 I print_info: arch             = gptneox
0.00.039.466 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.467 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.472 I print_info: n_head_kv        = 16
0.00.039.472 I print_info: n_rot            = 32
0.00.039.472 I print_info: n_swa            = 0
0.00.039.475 I print_info: n_embd_head_k    = 128
0.00.039.475 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.476 I print_info: n_embd_k_gqa     = 2048
0.00.039.477 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.479 I print_info: f_logit_scale    = 0.0e+00
0.00.039.479 I print_info: n_ff             = 8192
0.00.039.480 I print_info: n_expert         = 0
0.00.039.480 I print_info: n_expert_used    = 0
0.00.039.480 I print_info: causal attn      = 1
0.00.039.480 I print_info: pooling type     = 0
0.00.039.480 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.481 I print_info: freq_base_train  = 10000.0
0.00.039.481 I print_info: freq_scale_train = 1
0.00.039.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.482 I print_info: ssm_d_conv       = 0
0.00.039.482 I print_info: ssm_d_inner      = 0
0.00.039.482 I print_info: ssm_d_state      = 0
0.00.039.482 I print_info: ssm_dt_rank      = 0
0.00.039.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.483 I print_info: model params     = 1.41 B
0.00.039.483 I print_info: general.name     = 1.4B
0.00.039.484 I print_info: vocab type       = BPE
0.00.039.484 I print_info: n_vocab          = 50304
0.00.039.484 I print_info: n_merges         = 50009
0.00.039.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: LF token         = 187 'Ċ'
0.00.039.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: max token length = 1024
0.00.039.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.681 I load_tensors: offloading output layer to GPU
0.00.606.681 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.711 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.606.713 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.608.318 I llama_init_from_model: n_seq_max     = 1
0.00.608.322 I llama_init_from_model: n_ctx         = 128
0.00.608.323 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.323 I llama_init_from_model: n_batch       = 128
0.00.608.324 I llama_init_from_model: n_ubatch      = 128
0.00.608.324 I llama_init_from_model: flash_attn    = 0
0.00.608.325 I llama_init_from_model: freq_base     = 10000.0
0.00.608.326 I llama_init_from_model: freq_scale    = 1
0.00.608.326 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.329 I ggml_metal_init: allocating
0.00.608.386 I ggml_metal_init: found device: Apple M4
0.00.608.398 I ggml_metal_init: picking default device: Apple M4
0.00.610.454 I ggml_metal_init: using embedded metal library
0.00.617.530 I ggml_metal_init: GPU name:   Apple M4
0.00.617.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.541 I ggml_metal_init: simdgroup reduction   = true
0.00.617.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.541 I ggml_metal_init: has residency sets    = true
0.00.617.542 I ggml_metal_init: has bfloat            = true
0.00.617.542 I ggml_metal_init: use bfloat            = true
0.00.617.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.107 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.690 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.732 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.894 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.896 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.897 I llama_init_from_model: graph nodes  = 967
0.00.641.897 I llama_init_from_model: graph splits = 2
0.00.641.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.760 I 
0.00.675.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.856 I perplexity: tokenizing the input ..
0.00.682.729 I perplexity: tokenization took 6.87 ms
0.00.682.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.297 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.828.640 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.828.669 I llama_perf_context_print:        load time =     665.83 ms
0.00.828.670 I llama_perf_context_print: prompt eval time =     143.62 ms /   128 tokens (    1.12 ms per token,   891.25 tokens per second)
0.00.828.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.671 I llama_perf_context_print:       total time =     152.91 ms /   129 tokens
0.00.829.055 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.079s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.963 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.458 I llama_model_loader: - type  f32:  194 tensors
0.00.024.458 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.458 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.459 I print_info: file format = GGUF V3 (latest)
0.00.024.459 I print_info: file type   = Q2_K - Medium
0.00.024.461 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.394 I load: special tokens cache size = 25
0.00.038.190 I load: token to piece cache size = 0.2984 MB
0.00.038.193 I print_info: arch             = gptneox
0.00.038.194 I print_info: vocab_only       = 0
0.00.038.194 I print_info: n_ctx_train      = 2048
0.00.038.194 I print_info: n_embd           = 2048
0.00.038.194 I print_info: n_layer          = 24
0.00.038.198 I print_info: n_head           = 16
0.00.038.199 I print_info: n_head_kv        = 16
0.00.038.199 I print_info: n_rot            = 32
0.00.038.199 I print_info: n_swa            = 0
0.00.038.201 I print_info: n_embd_head_k    = 128
0.00.038.202 I print_info: n_embd_head_v    = 128
0.00.038.202 I print_info: n_gqa            = 1
0.00.038.203 I print_info: n_embd_k_gqa     = 2048
0.00.038.204 I print_info: n_embd_v_gqa     = 2048
0.00.038.204 I print_info: f_norm_eps       = 1.0e-05
0.00.038.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.205 I print_info: f_logit_scale    = 0.0e+00
0.00.038.206 I print_info: n_ff             = 8192
0.00.038.206 I print_info: n_expert         = 0
0.00.038.207 I print_info: n_expert_used    = 0
0.00.038.207 I print_info: causal attn      = 1
0.00.038.209 I print_info: pooling type     = 0
0.00.038.209 I print_info: rope type        = 2
0.00.038.209 I print_info: rope scaling     = linear
0.00.038.210 I print_info: freq_base_train  = 10000.0
0.00.038.210 I print_info: freq_scale_train = 1
0.00.038.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.210 I print_info: rope_finetuned   = unknown
0.00.038.211 I print_info: ssm_d_conv       = 0
0.00.038.211 I print_info: ssm_d_inner      = 0
0.00.038.211 I print_info: ssm_d_state      = 0
0.00.038.211 I print_info: ssm_dt_rank      = 0
0.00.038.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.211 I print_info: model type       = 1.4B
0.00.038.212 I print_info: model params     = 1.41 B
0.00.038.212 I print_info: general.name     = 1.4B
0.00.038.212 I print_info: vocab type       = BPE
0.00.038.213 I print_info: n_vocab          = 50304
0.00.038.213 I print_info: n_merges         = 50009
0.00.038.213 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.213 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.214 I print_info: LF token         = 187 'Ċ'
0.00.038.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.216 I print_info: max token length = 1024
0.00.038.216 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.789 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.805 I load_tensors: offloading output layer to GPU
0.00.344.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.838 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.839 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.423 I llama_init_from_model: n_seq_max     = 1
0.00.346.428 I llama_init_from_model: n_ctx         = 128
0.00.346.428 I llama_init_from_model: n_ctx_per_seq = 128
0.00.346.429 I llama_init_from_model: n_batch       = 128
0.00.346.429 I llama_init_from_model: n_ubatch      = 128
0.00.346.429 I llama_init_from_model: flash_attn    = 0
0.00.346.431 I llama_init_from_model: freq_base     = 10000.0
0.00.346.432 I llama_init_from_model: freq_scale    = 1
0.00.346.433 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.346.435 I ggml_metal_init: allocating
0.00.346.518 I ggml_metal_init: found device: Apple M4
0.00.346.531 I ggml_metal_init: picking default device: Apple M4
0.00.348.337 I ggml_metal_init: using embedded metal library
0.00.353.748 I ggml_metal_init: GPU name:   Apple M4
0.00.353.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.763 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.765 I ggml_metal_init: simdgroup reduction   = true
0.00.353.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.766 I ggml_metal_init: has residency sets    = true
0.00.353.766 I ggml_metal_init: has bfloat            = true
0.00.353.766 I ggml_metal_init: use bfloat            = true
0.00.353.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.458 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.379.182 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.379.190 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.379.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.663 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.382.665 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.382.666 I llama_init_from_model: graph nodes  = 967
0.00.382.666 I llama_init_from_model: graph splits = 2
0.00.382.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.382.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.415.691 I 
0.00.415.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.415.783 I perplexity: tokenizing the input ..
0.00.422.301 I perplexity: tokenization took 6.516 ms
0.00.422.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.610 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.956 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.977 I llama_perf_context_print:        load time =     406.79 ms
0.00.563.978 I llama_perf_context_print: prompt eval time =     139.90 ms /   128 tokens (    1.09 ms per token,   914.91 tokens per second)
0.00.563.979 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.979 I llama_perf_context_print:       total time =     148.29 ms /   129 tokens
0.00.564.360 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.283 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.068 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.075 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.838 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.839 I llama_model_loader: - type  f32:  194 tensors
0.00.023.839 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.839 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.839 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.841 I print_info: file format = GGUF V3 (latest)
0.00.023.845 I print_info: file type   = Q3_K - Medium
0.00.023.846 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.731 I load: special tokens cache size = 25
0.00.037.635 I load: token to piece cache size = 0.2984 MB
0.00.037.639 I print_info: arch             = gptneox
0.00.037.640 I print_info: vocab_only       = 0
0.00.037.640 I print_info: n_ctx_train      = 2048
0.00.037.640 I print_info: n_embd           = 2048
0.00.037.640 I print_info: n_layer          = 24
0.00.037.644 I print_info: n_head           = 16
0.00.037.645 I print_info: n_head_kv        = 16
0.00.037.645 I print_info: n_rot            = 32
0.00.037.645 I print_info: n_swa            = 0
0.00.037.646 I print_info: n_embd_head_k    = 128
0.00.037.646 I print_info: n_embd_head_v    = 128
0.00.037.647 I print_info: n_gqa            = 1
0.00.037.647 I print_info: n_embd_k_gqa     = 2048
0.00.037.648 I print_info: n_embd_v_gqa     = 2048
0.00.037.649 I print_info: f_norm_eps       = 1.0e-05
0.00.037.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.650 I print_info: f_logit_scale    = 0.0e+00
0.00.037.650 I print_info: n_ff             = 8192
0.00.037.650 I print_info: n_expert         = 0
0.00.037.651 I print_info: n_expert_used    = 0
0.00.037.651 I print_info: causal attn      = 1
0.00.037.651 I print_info: pooling type     = 0
0.00.037.651 I print_info: rope type        = 2
0.00.037.651 I print_info: rope scaling     = linear
0.00.037.652 I print_info: freq_base_train  = 10000.0
0.00.037.652 I print_info: freq_scale_train = 1
0.00.037.652 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.652 I print_info: rope_finetuned   = unknown
0.00.037.652 I print_info: ssm_d_conv       = 0
0.00.037.653 I print_info: ssm_d_inner      = 0
0.00.037.653 I print_info: ssm_d_state      = 0
0.00.037.653 I print_info: ssm_dt_rank      = 0
0.00.037.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.656 I print_info: model type       = 1.4B
0.00.037.657 I print_info: model params     = 1.41 B
0.00.037.657 I print_info: general.name     = 1.4B
0.00.037.657 I print_info: vocab type       = BPE
0.00.037.658 I print_info: n_vocab          = 50304
0.00.037.658 I print_info: n_merges         = 50009
0.00.037.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.659 I print_info: LF token         = 187 'Ċ'
0.00.037.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.659 I print_info: max token length = 1024
0.00.037.661 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.519 I load_tensors: offloading output layer to GPU
0.00.443.520 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.551 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.552 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.193 I llama_init_from_model: n_seq_max     = 1
0.00.445.200 I llama_init_from_model: n_ctx         = 128
0.00.445.200 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.201 I llama_init_from_model: n_batch       = 128
0.00.445.201 I llama_init_from_model: n_ubatch      = 128
0.00.445.201 I llama_init_from_model: flash_attn    = 0
0.00.445.203 I llama_init_from_model: freq_base     = 10000.0
0.00.445.204 I llama_init_from_model: freq_scale    = 1
0.00.445.204 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.207 I ggml_metal_init: allocating
0.00.445.284 I ggml_metal_init: found device: Apple M4
0.00.445.298 I ggml_metal_init: picking default device: Apple M4
0.00.447.090 I ggml_metal_init: using embedded metal library
0.00.452.503 I ggml_metal_init: GPU name:   Apple M4
0.00.452.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.526 I ggml_metal_init: simdgroup reduction   = true
0.00.452.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.527 I ggml_metal_init: has residency sets    = true
0.00.452.527 I ggml_metal_init: has bfloat            = true
0.00.452.527 I ggml_metal_init: use bfloat            = true
0.00.452.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.486 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.024 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.083 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.479.517 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.479.519 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.479.519 I llama_init_from_model: graph nodes  = 967
0.00.479.520 I llama_init_from_model: graph splits = 2
0.00.479.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.951 I 
0.00.507.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.035 I perplexity: tokenizing the input ..
0.00.514.607 I perplexity: tokenization took 7.568 ms
0.00.514.614 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.824 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.657.166 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.657.189 I llama_perf_context_print:        load time =     498.06 ms
0.00.657.190 I llama_perf_context_print: prompt eval time =     140.25 ms /   128 tokens (    1.10 ms per token,   912.68 tokens per second)
0.00.657.191 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.191 I llama_perf_context_print:       total time =     150.24 ms /   129 tokens
0.00.657.561 I ggml_metal_free: deallocating

real	0m0.672s
user	0m0.080s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.570 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.405 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.262 I llama_model_loader: - type  f32:  194 tensors
0.00.025.262 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.262 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.262 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.263 I print_info: file format = GGUF V3 (latest)
0.00.025.264 I print_info: file type   = Q4_K - Medium
0.00.025.265 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.438 I load: special tokens cache size = 25
0.00.039.588 I load: token to piece cache size = 0.2984 MB
0.00.039.592 I print_info: arch             = gptneox
0.00.039.593 I print_info: vocab_only       = 0
0.00.039.593 I print_info: n_ctx_train      = 2048
0.00.039.593 I print_info: n_embd           = 2048
0.00.039.593 I print_info: n_layer          = 24
0.00.039.597 I print_info: n_head           = 16
0.00.039.598 I print_info: n_head_kv        = 16
0.00.039.598 I print_info: n_rot            = 32
0.00.039.598 I print_info: n_swa            = 0
0.00.039.598 I print_info: n_embd_head_k    = 128
0.00.039.598 I print_info: n_embd_head_v    = 128
0.00.039.599 I print_info: n_gqa            = 1
0.00.039.600 I print_info: n_embd_k_gqa     = 2048
0.00.039.601 I print_info: n_embd_v_gqa     = 2048
0.00.039.601 I print_info: f_norm_eps       = 1.0e-05
0.00.039.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.605 I print_info: f_logit_scale    = 0.0e+00
0.00.039.605 I print_info: n_ff             = 8192
0.00.039.607 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.607 I print_info: causal attn      = 1
0.00.039.607 I print_info: pooling type     = 0
0.00.039.607 I print_info: rope type        = 2
0.00.039.607 I print_info: rope scaling     = linear
0.00.039.608 I print_info: freq_base_train  = 10000.0
0.00.039.608 I print_info: freq_scale_train = 1
0.00.039.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.608 I print_info: rope_finetuned   = unknown
0.00.039.609 I print_info: ssm_d_conv       = 0
0.00.039.609 I print_info: ssm_d_inner      = 0
0.00.039.609 I print_info: ssm_d_state      = 0
0.00.039.610 I print_info: ssm_dt_rank      = 0
0.00.039.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.610 I print_info: model type       = 1.4B
0.00.039.611 I print_info: model params     = 1.41 B
0.00.039.611 I print_info: general.name     = 1.4B
0.00.039.611 I print_info: vocab type       = BPE
0.00.039.612 I print_info: n_vocab          = 50304
0.00.039.612 I print_info: n_merges         = 50009
0.00.039.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: LF token         = 187 'Ċ'
0.00.039.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: max token length = 1024
0.00.039.614 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.549.926 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.940 I load_tensors: offloading output layer to GPU
0.00.549.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.971 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.549.972 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.551.644 I llama_init_from_model: n_seq_max     = 1
0.00.551.649 I llama_init_from_model: n_ctx         = 128
0.00.551.649 I llama_init_from_model: n_ctx_per_seq = 128
0.00.551.650 I llama_init_from_model: n_batch       = 128
0.00.551.650 I llama_init_from_model: n_ubatch      = 128
0.00.551.651 I llama_init_from_model: flash_attn    = 0
0.00.551.652 I llama_init_from_model: freq_base     = 10000.0
0.00.551.653 I llama_init_from_model: freq_scale    = 1
0.00.551.654 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.551.673 I ggml_metal_init: allocating
0.00.551.748 I ggml_metal_init: found device: Apple M4
0.00.551.773 I ggml_metal_init: picking default device: Apple M4
0.00.553.551 I ggml_metal_init: using embedded metal library
0.00.559.078 I ggml_metal_init: GPU name:   Apple M4
0.00.559.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.087 I ggml_metal_init: simdgroup reduction   = true
0.00.559.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.088 I ggml_metal_init: has residency sets    = true
0.00.559.088 I ggml_metal_init: has bfloat            = true
0.00.559.089 I ggml_metal_init: use bfloat            = true
0.00.559.090 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.445 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.582.116 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.585.411 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.585.413 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.585.413 I llama_init_from_model: graph nodes  = 967
0.00.585.414 I llama_init_from_model: graph splits = 2
0.00.585.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.585.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.273 I 
0.00.615.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.354 I perplexity: tokenizing the input ..
0.00.622.779 I perplexity: tokenization took 7.42 ms
0.00.622.785 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.151 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.770.492 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.770.520 I llama_perf_context_print:        load time =     605.47 ms
0.00.770.521 I llama_perf_context_print: prompt eval time =     145.42 ms /   128 tokens (    1.14 ms per token,   880.23 tokens per second)
0.00.770.522 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.522 I llama_perf_context_print:       total time =     155.25 ms /   129 tokens
0.00.770.924 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.080s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.773 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.381 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.100 I llama_model_loader: - type  f32:  194 tensors
0.00.024.102 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.106 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.107 I print_info: file format = GGUF V3 (latest)
0.00.024.107 I print_info: file type   = Q5_K - Medium
0.00.024.108 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.147 I load: special tokens cache size = 25
0.00.038.163 I load: token to piece cache size = 0.2984 MB
0.00.038.168 I print_info: arch             = gptneox
0.00.038.168 I print_info: vocab_only       = 0
0.00.038.168 I print_info: n_ctx_train      = 2048
0.00.038.168 I print_info: n_embd           = 2048
0.00.038.169 I print_info: n_layer          = 24
0.00.038.173 I print_info: n_head           = 16
0.00.038.174 I print_info: n_head_kv        = 16
0.00.038.174 I print_info: n_rot            = 32
0.00.038.174 I print_info: n_swa            = 0
0.00.038.174 I print_info: n_embd_head_k    = 128
0.00.038.175 I print_info: n_embd_head_v    = 128
0.00.038.176 I print_info: n_gqa            = 1
0.00.038.176 I print_info: n_embd_k_gqa     = 2048
0.00.038.177 I print_info: n_embd_v_gqa     = 2048
0.00.038.177 I print_info: f_norm_eps       = 1.0e-05
0.00.038.178 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.178 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.178 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.178 I print_info: f_logit_scale    = 0.0e+00
0.00.038.179 I print_info: n_ff             = 8192
0.00.038.179 I print_info: n_expert         = 0
0.00.038.179 I print_info: n_expert_used    = 0
0.00.038.179 I print_info: causal attn      = 1
0.00.038.179 I print_info: pooling type     = 0
0.00.038.180 I print_info: rope type        = 2
0.00.038.180 I print_info: rope scaling     = linear
0.00.038.180 I print_info: freq_base_train  = 10000.0
0.00.038.180 I print_info: freq_scale_train = 1
0.00.038.180 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.181 I print_info: rope_finetuned   = unknown
0.00.038.183 I print_info: ssm_d_conv       = 0
0.00.038.183 I print_info: ssm_d_inner      = 0
0.00.038.183 I print_info: ssm_d_state      = 0
0.00.038.183 I print_info: ssm_dt_rank      = 0
0.00.038.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.183 I print_info: model type       = 1.4B
0.00.038.184 I print_info: model params     = 1.41 B
0.00.038.184 I print_info: general.name     = 1.4B
0.00.038.184 I print_info: vocab type       = BPE
0.00.038.184 I print_info: n_vocab          = 50304
0.00.038.185 I print_info: n_merges         = 50009
0.00.038.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.185 I print_info: LF token         = 187 'Ċ'
0.00.038.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.186 I print_info: max token length = 1024
0.00.038.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.959 I load_tensors: offloading output layer to GPU
0.00.589.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.991 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.992 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.505 I llama_init_from_model: n_seq_max     = 1
0.00.591.508 I llama_init_from_model: n_ctx         = 128
0.00.591.508 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.508 I llama_init_from_model: n_batch       = 128
0.00.591.509 I llama_init_from_model: n_ubatch      = 128
0.00.591.509 I llama_init_from_model: flash_attn    = 0
0.00.591.511 I llama_init_from_model: freq_base     = 10000.0
0.00.591.512 I llama_init_from_model: freq_scale    = 1
0.00.591.512 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.515 I ggml_metal_init: allocating
0.00.591.569 I ggml_metal_init: found device: Apple M4
0.00.591.588 I ggml_metal_init: picking default device: Apple M4
0.00.593.253 I ggml_metal_init: using embedded metal library
0.00.599.550 I ggml_metal_init: GPU name:   Apple M4
0.00.599.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.558 I ggml_metal_init: simdgroup reduction   = true
0.00.599.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.558 I ggml_metal_init: has residency sets    = true
0.00.599.558 I ggml_metal_init: has bfloat            = true
0.00.599.559 I ggml_metal_init: use bfloat            = true
0.00.599.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.781 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.237 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.569 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.570 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.571 I llama_init_from_model: graph nodes  = 967
0.00.623.571 I llama_init_from_model: graph splits = 2
0.00.623.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.445 I 
0.00.653.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.544 I perplexity: tokenizing the input ..
0.00.660.057 I perplexity: tokenization took 6.509 ms
0.00.660.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.642 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.971 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.995 I llama_perf_context_print:        load time =     644.66 ms
0.00.797.996 I llama_perf_context_print: prompt eval time =     135.83 ms /   128 tokens (    1.06 ms per token,   942.37 tokens per second)
0.00.797.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.997 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.798.370 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.442 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.445 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.445 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.176 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.934 I llama_model_loader: - type  f32:  194 tensors
0.00.023.934 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.935 I print_info: file format = GGUF V3 (latest)
0.00.023.936 I print_info: file type   = Q6_K
0.00.023.936 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.847 I load: special tokens cache size = 25
0.00.037.856 I load: token to piece cache size = 0.2984 MB
0.00.037.860 I print_info: arch             = gptneox
0.00.037.860 I print_info: vocab_only       = 0
0.00.037.861 I print_info: n_ctx_train      = 2048
0.00.037.861 I print_info: n_embd           = 2048
0.00.037.861 I print_info: n_layer          = 24
0.00.037.865 I print_info: n_head           = 16
0.00.037.866 I print_info: n_head_kv        = 16
0.00.037.866 I print_info: n_rot            = 32
0.00.037.868 I print_info: n_swa            = 0
0.00.037.868 I print_info: n_embd_head_k    = 128
0.00.037.870 I print_info: n_embd_head_v    = 128
0.00.037.870 I print_info: n_gqa            = 1
0.00.037.873 I print_info: n_embd_k_gqa     = 2048
0.00.037.874 I print_info: n_embd_v_gqa     = 2048
0.00.037.875 I print_info: f_norm_eps       = 1.0e-05
0.00.037.875 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.875 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.875 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.875 I print_info: f_logit_scale    = 0.0e+00
0.00.037.876 I print_info: n_ff             = 8192
0.00.037.876 I print_info: n_expert         = 0
0.00.037.876 I print_info: n_expert_used    = 0
0.00.037.876 I print_info: causal attn      = 1
0.00.037.878 I print_info: pooling type     = 0
0.00.037.878 I print_info: rope type        = 2
0.00.037.878 I print_info: rope scaling     = linear
0.00.037.878 I print_info: freq_base_train  = 10000.0
0.00.037.879 I print_info: freq_scale_train = 1
0.00.037.879 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.879 I print_info: rope_finetuned   = unknown
0.00.037.879 I print_info: ssm_d_conv       = 0
0.00.037.879 I print_info: ssm_d_inner      = 0
0.00.037.879 I print_info: ssm_d_state      = 0
0.00.037.879 I print_info: ssm_dt_rank      = 0
0.00.037.879 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.880 I print_info: model type       = 1.4B
0.00.037.880 I print_info: model params     = 1.41 B
0.00.037.880 I print_info: general.name     = 1.4B
0.00.037.881 I print_info: vocab type       = BPE
0.00.037.881 I print_info: n_vocab          = 50304
0.00.037.881 I print_info: n_merges         = 50009
0.00.037.885 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.885 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.885 I print_info: LF token         = 187 'Ċ'
0.00.037.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.886 I print_info: max token length = 1024
0.00.037.886 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.578.260 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.267 I load_tensors: offloading output layer to GPU
0.00.578.268 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.298 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.578.301 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.579.845 I llama_init_from_model: n_seq_max     = 1
0.00.579.848 I llama_init_from_model: n_ctx         = 128
0.00.579.848 I llama_init_from_model: n_ctx_per_seq = 128
0.00.579.849 I llama_init_from_model: n_batch       = 128
0.00.579.850 I llama_init_from_model: n_ubatch      = 128
0.00.579.850 I llama_init_from_model: flash_attn    = 0
0.00.579.851 I llama_init_from_model: freq_base     = 10000.0
0.00.579.851 I llama_init_from_model: freq_scale    = 1
0.00.579.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.579.854 I ggml_metal_init: allocating
0.00.579.903 I ggml_metal_init: found device: Apple M4
0.00.579.915 I ggml_metal_init: picking default device: Apple M4
0.00.581.391 I ggml_metal_init: using embedded metal library
0.00.587.416 I ggml_metal_init: GPU name:   Apple M4
0.00.587.419 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.587.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.587.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.587.421 I ggml_metal_init: simdgroup reduction   = true
0.00.587.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.587.422 I ggml_metal_init: has residency sets    = true
0.00.587.422 I ggml_metal_init: has bfloat            = true
0.00.587.422 I ggml_metal_init: use bfloat            = true
0.00.587.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.587.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.483 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.606.906 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.606.910 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.606.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.294 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.610.295 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.610.296 I llama_init_from_model: graph nodes  = 967
0.00.610.296 I llama_init_from_model: graph splits = 2
0.00.610.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.610.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.738 I 
0.00.641.824 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.832 I perplexity: tokenizing the input ..
0.00.649.028 I perplexity: tokenization took 7.193 ms
0.00.649.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.819 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.782.167 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.782.189 I llama_perf_context_print:        load time =     632.79 ms
0.00.782.190 I llama_perf_context_print: prompt eval time =     131.23 ms /   128 tokens (    1.03 ms per token,   975.36 tokens per second)
0.00.782.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.191 I llama_perf_context_print:       total time =     140.46 ms /   129 tokens
0.00.782.525 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.077s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.300 I build: 4762 (af7747c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.981 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.994 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.999 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.999 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.003 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.006 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.217 I llama_model_loader: - type  f32:  194 tensors
0.00.052.217 I llama_model_loader: - type  f16:   98 tensors
0.00.052.218 I print_info: file format = GGUF V3 (latest)
0.00.052.219 I print_info: file type   = all F32 (guessed)
0.00.052.220 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.155 I load: special tokens cache size = 25
0.00.072.042 I load: token to piece cache size = 0.2984 MB
0.00.072.045 I print_info: arch             = gptneox
0.00.072.045 I print_info: vocab_only       = 0
0.00.072.045 I print_info: n_ctx_train      = 2048
0.00.072.045 I print_info: n_embd           = 2048
0.00.072.046 I print_info: n_layer          = 24
0.00.072.048 I print_info: n_head           = 16
0.00.072.049 I print_info: n_head_kv        = 16
0.00.072.049 I print_info: n_rot            = 32
0.00.072.050 I print_info: n_swa            = 0
0.00.072.050 I print_info: n_embd_head_k    = 128
0.00.072.050 I print_info: n_embd_head_v    = 128
0.00.072.051 I print_info: n_gqa            = 1
0.00.072.054 I print_info: n_embd_k_gqa     = 2048
0.00.072.054 I print_info: n_embd_v_gqa     = 2048
0.00.072.055 I print_info: f_norm_eps       = 1.0e-05
0.00.072.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.055 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.055 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.055 I print_info: f_logit_scale    = 0.0e+00
0.00.072.056 I print_info: n_ff             = 8192
0.00.072.056 I print_info: n_expert         = 0
0.00.072.056 I print_info: n_expert_used    = 0
0.00.072.057 I print_info: causal attn      = 1
0.00.072.057 I print_info: pooling type     = 0
0.00.072.057 I print_info: rope type        = 2
0.00.072.059 I print_info: rope scaling     = linear
0.00.072.059 I print_info: freq_base_train  = 10000.0
0.00.072.060 I print_info: freq_scale_train = 1
0.00.072.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.060 I print_info: rope_finetuned   = unknown
0.00.072.060 I print_info: ssm_d_conv       = 0
0.00.072.060 I print_info: ssm_d_inner      = 0
0.00.072.060 I print_info: ssm_d_state      = 0
0.00.072.060 I print_info: ssm_dt_rank      = 0
0.00.072.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.061 I print_info: model type       = 1.4B
0.00.072.061 I print_info: model params     = 1.41 B
0.00.072.061 I print_info: general.name     = 1.4B
0.00.072.062 I print_info: vocab type       = BPE
0.00.072.062 I print_info: n_vocab          = 50304
0.00.072.062 I print_info: n_merges         = 50009
0.00.072.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.063 I print_info: LF token         = 187 'Ċ'
0.00.072.067 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.067 I print_info: max token length = 1024
0.00.072.067 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.274.950 I load_tensors: offloading 24 repeating layers to GPU
0.01.274.954 I load_tensors: offloading output layer to GPU
0.01.274.955 I load_tensors: offloaded 25/25 layers to GPU
0.01.274.986 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.274.987 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.275.921 I llama_init_from_model: n_seq_max     = 1
0.01.275.922 I llama_init_from_model: n_ctx         = 128
0.01.275.922 I llama_init_from_model: n_ctx_per_seq = 128
0.01.275.922 I llama_init_from_model: n_batch       = 128
0.01.275.922 I llama_init_from_model: n_ubatch      = 128
0.01.275.923 I llama_init_from_model: flash_attn    = 0
0.01.275.923 I llama_init_from_model: freq_base     = 10000.0
0.01.275.924 I llama_init_from_model: freq_scale    = 1
0.01.275.924 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.275.929 I ggml_metal_init: allocating
0.01.276.010 I ggml_metal_init: found device: Apple M4
0.01.276.017 I ggml_metal_init: picking default device: Apple M4
0.01.277.256 I ggml_metal_init: using embedded metal library
0.01.281.091 I ggml_metal_init: GPU name:   Apple M4
0.01.281.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.281.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.281.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.281.095 I ggml_metal_init: simdgroup reduction   = true
0.01.281.096 I ggml_metal_init: simdgroup matrix mul. = true
0.01.281.096 I ggml_metal_init: has residency sets    = true
0.01.281.096 I ggml_metal_init: has bfloat            = true
0.01.281.096 I ggml_metal_init: use bfloat            = true
0.01.281.097 I ggml_metal_init: hasUnifiedMemory      = true
0.01.281.098 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.291.643 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.293.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.293.322 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.293.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.294.983 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.294.984 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.294.985 I llama_init_from_model: graph nodes  = 967
0.01.294.985 I llama_init_from_model: graph splits = 2
0.01.294.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.294.986 I 
0.01.295.021 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.295.022 I compute_imatrix: tokenizing the input ..
0.01.299.152 I compute_imatrix: tokenization took 4.129 ms
0.01.299.154 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.564.604 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.567.003 I llama_perf_context_print:        load time =    1543.72 ms
0.01.567.004 I llama_perf_context_print: prompt eval time =     263.71 ms /   128 tokens (    2.06 ms per token,   485.38 tokens per second)
0.01.567.005 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.567.005 I llama_perf_context_print:       total time =    1546.11 ms /   129 tokens
0.01.567.484 I ggml_metal_free: deallocating

real	0m1.749s
user	0m0.124s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4762 (af7747c9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156b06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156b06be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156b09c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156b0a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156b0a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156b0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156b0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156b0b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156b0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156b0c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156b0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156b0c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156b0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156b0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156b0e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156b0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156b0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156b0f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156b0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156b10760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156b10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156b115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156b11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156b123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156b12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156b12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156b133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156b14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156b14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156b14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156b14ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156b14fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156b15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156b15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156b16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156b164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156b16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156b16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156b172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156b17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156b17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156b18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156b18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156b189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156b18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156b192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156b198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156b1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156b1a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156b1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156b1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156b1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156b1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156b1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156b1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156b1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156b1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156b1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156b1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156b1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156b1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156b1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156b1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156b1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156b1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156b20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156b206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156b20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156b20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156b21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156b21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156b21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156b22260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156b227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156b22d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156b23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156b237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156b23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156b24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156b24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156b24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156b25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156b25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156b25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156b26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156b26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156b26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156b27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156b27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156b27cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156b28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156b28750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156b28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156b291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156b29740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156b29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156b2a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156b19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156b2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156b2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156b2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156b2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156b2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156b2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156b2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156b2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156b2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156b2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156b2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156b2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156b2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156b2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156b2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156b2f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156b2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156b300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156b30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156b30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156b30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156b31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156b31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156b31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156b32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156b325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156b32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156b32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156b333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156b33870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156b33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156b341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156b34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156b34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156b34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156b35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156b358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156b35d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156b36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156b366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156b36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156b36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156b37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156b37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156b37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156b38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156b38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156b38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156b39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156b394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156b39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156b39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156b3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156b3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156b3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156b3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156b3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156b3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156b3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156b3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156b3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156b3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156b3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156b3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156b3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156b3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156b3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156b3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156b3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156b3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156b3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156b3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156b3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156b403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156b40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156b40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156b411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156b41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156b41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156b41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156b42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156b428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156b42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156b43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156b436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156b43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156b44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156b444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156b44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156b44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156b45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156b45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156b45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156b46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156b46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156b46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156b46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156b47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156b47a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156b47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156b48320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156b48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156b48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156b49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156b49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156b49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156b4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156b4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156b4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156b4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156b4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156b4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156b4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156b4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156b4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156b4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156b4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156b4e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156b4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156b4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156b4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156b4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156b4fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156b502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156b507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156b50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156b51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156b517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156b51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156b52280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156b527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156b52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156b53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156b537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156b53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156b54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156b547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156b54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156b55250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156b557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156b55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156b56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156b56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156b56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156b57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156b57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156b57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156b58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156b58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156b58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156b59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156b59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156b59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156b5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156b5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156b5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156b5b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156b5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156b5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156b5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156b5c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156b5cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156b5d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156b5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156b5dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156b5e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156b5e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156b5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156b5f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156b5f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156b5faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156b5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156b60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156b608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156b60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156b61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156b616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156b61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156b61ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156b62490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156b62930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156b62dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156b63270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156b63710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156b63c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156b64380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156b64aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156b651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156b658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156b65ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156b66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156b66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156b66c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.733.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145504b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145504f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145505400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145505870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145505ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145506150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1455065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145506a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145506ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145507310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145507780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145507e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145508990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145509140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145509950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14550a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14550a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14550aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14550b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14550bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14550c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14550cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14550d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14550d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14550e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14550e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14550e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14550ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14550ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14550f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14550f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14550fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145510180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145510440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1455108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145510d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145511190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145511600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145511a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145511ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145512350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1455127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145512c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1455130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145513510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145513980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145513df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145514260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1455146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145514b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145514fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145515420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145515890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145515d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145516170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1455165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145516b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145517050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1455174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145517930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145517da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145518210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145518680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145518af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145518f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1455193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145519840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145519cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14551a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14551a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14551aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14551ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14551b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14551b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14551bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14551c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14551c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14551c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14551cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14551d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14551d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14551dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14551df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14551e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14551e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14551ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14551f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14551f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14551f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14551fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1455202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145520730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145520ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145521010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145521480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1455218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145521d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1455221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145522640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145522ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145522f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145523390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145523800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145523c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1455240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145524550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1455249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145524e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1455252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145525710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145525b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145525ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145526460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1455268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145526d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1455271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145527620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145527a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145527f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145528370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1455287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145528c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1455290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145529530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1455299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145529e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14552a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14552a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14552ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14552afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14552b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14552b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14552bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14552c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14552c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14552ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14552cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14552d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14552d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14552dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14552e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14552e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14552e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14552edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14552f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14552f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14552fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14552ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145530420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145530890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145530d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145531170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1455315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145531a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145531ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145532330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1455327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145532c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145533080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1455334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145533960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145533dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145534240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1455346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145534b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145534f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145535bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145535e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145536140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1455365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145536a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145536e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145537300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145537770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145537be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145538050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1455384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145538930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145538da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145539210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145539680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145539af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145539f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14553a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14553a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14553acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14553b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14553b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14553ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14553be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14553c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14553c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14553cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14553d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14553d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14553d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14553dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14553e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14553e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14553ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14553ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14553f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14553f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14553fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145540290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145540700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145540b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145540fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145541500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145541a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145542580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145542840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145542e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1455433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145543980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145543f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145544500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145544ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145545080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145545640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145545c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1455461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145546780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145546d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145547300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1455478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145547e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145548440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145548a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145548fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145549580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145549b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14554a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14554a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14554ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14554b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14554b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14554bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14554c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14554c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14554cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14554d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14554da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14554e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14554e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14554ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14554f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14554f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14554fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1455502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145550880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145550e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145551400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1455519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145551f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145552540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145552b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1455530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145553680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145553c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145554200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1455547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145554d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145555340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145555900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145555ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145556480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145556a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145556f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145557440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145557940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145557e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145558340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145558840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145558d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145559240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145559740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145559c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14555a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14555a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14555ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14555b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14555b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14555bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14555c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14555cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14555d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14555d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14555df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14555e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14555e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14555b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14554c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14554b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145548140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145545900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145555040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145552800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145550580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14554e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145546480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145543c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145548cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145549e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14554f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14554c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145553f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145547b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145551100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14554a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14554cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1455475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145555600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1455447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1455430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145545340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145555bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14554af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145553380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145549280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14554bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14554fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145547000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14554ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1455516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145545ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1455544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145551c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14554d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145556740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145544d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145556180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145544200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145554a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14554e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145550b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145553940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145552240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14554a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145541cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145504680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14555da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14550b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14555ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14555f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14555f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14555f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14555fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14555fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14555ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145560250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145560510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1455607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145560a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145560d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145561010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1455612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145561590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145561850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145561b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145561dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145562090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145562350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145562610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1455628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145562b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145562e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145563110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1455633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145563690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145563950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145563c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145563ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145564190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145564450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145564710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1455649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145564c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145564f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145565210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1455654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145565790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145565a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145565d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145565fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145566290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145566550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145566810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145566ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145566d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145567050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145567310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1455675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145567890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145567b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145567e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1455680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145568390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145568650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145568910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145568bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145568e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145569150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145569410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1455696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145569990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145569c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145569f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14556a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14556a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14556a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14556aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14556acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14556af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14556b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14556b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14556b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14556ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14556bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14556c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14556c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14556c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14556c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14556cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14556cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14556d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14556d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14556d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14556d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14556db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14556de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14556e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14556e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14556e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14556e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14556ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14556eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14556f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14556f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14556f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14556f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14556fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14556ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145570210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1455704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145570790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145570a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145570d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145570fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145571290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145571550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145571810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145571ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145571d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145572050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145572310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1455725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145572890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145572b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145572e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1455730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145573390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145573650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145573910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145573bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145573e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145574150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145574410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1455746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145574990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145574c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145574f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1455751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145575490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145575750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145575a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145575cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145575f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145576250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145576510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1455767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145576a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145576d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145577010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1455772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145577590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145577850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145577b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145577dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145578090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145578350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145578610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1455788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145578b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145578e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145579110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1455793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145579690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145579950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145579c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145579ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14557a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14557a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14557aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14557ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14557afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14557b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14557b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14557b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14557baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14557bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14557c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14557ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14557cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14557d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14557da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14557dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14557e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14557ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14557efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14557f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14557fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14557ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145580500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145580a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145580fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1455814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145581a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145581f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1455824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145582a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145582f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1455834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145583a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145583f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1455844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145584a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145584f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1455854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145585a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145585f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1455864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1455869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145586f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145587490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1455879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145587f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145588480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1455889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145588f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145589470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1455899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145589f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14558a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14558a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14558af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14558b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14558b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14558bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14558bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14558c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14558c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14558cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14558cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14558d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14558d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14558dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14558e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14558e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14558e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14558ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14558f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14558f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14558fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145590000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145590cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145591410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145591b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145591df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145592260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145592860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145592e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.808s
user	0m0.282s
sys	0m0.332s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4762 (af7747c9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15aa0a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15aa0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15aa0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15aa0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15aa0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15aa0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15aa0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15aa0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15aa0d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15aa0db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15aa0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15aa0e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15aa0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15aa0f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15aa10020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15aa10740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15aa10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15aa11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15aa11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15aa12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15aa12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15aa132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15aa139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15aa14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15aa14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15aa14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15aa15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15aa15ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15aa16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15aa166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15aa16b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15aa16e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15aa176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15aa17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15aa17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15aa18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15aa18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15aa18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15aa19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15aa195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15aa19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15aa19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15aa1a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15aa1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15aa1ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15aa1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15aa1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15aa1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15aa1c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15aa1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15aa1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15aa1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15aa1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15aa1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15aa1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15aa1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15aa1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15aa1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15aa1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15aa206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15aa20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15aa20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15aa212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15aa21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15aa21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15aa22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15aa22530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15aa229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15aa22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15aa23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15aa237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15aa23c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15aa240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15aa24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15aa24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15aa250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15aa25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15aa25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15aa260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15aa26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15aa26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15aa270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15aa27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15aa27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15aa280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15aa28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15aa28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15aa290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15aa295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15aa29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15aa2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15aa2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15aa2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15aa2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15aa2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15aa2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15aa2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15aa1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15aa2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15aa2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15aa2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15aa2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15aa2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15aa2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15aa2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15aa2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15aa2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15aa2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15aa2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15aa301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15aa30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15aa30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15aa311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15aa31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15aa31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15aa31f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15aa32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15aa328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15aa32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15aa33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15aa336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15aa33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15aa33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15aa34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15aa34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15aa34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15aa35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15aa35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15aa35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15aa36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15aa364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15aa36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15aa36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15aa372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15aa37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15aa37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15aa380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15aa38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15aa389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15aa38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15aa39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15aa397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15aa39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15aa3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15aa3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15aa3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15aa3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15aa3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15aa3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15aa3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15aa3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15aa3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15aa3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15aa3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15aa3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15aa3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15aa3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15aa3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15aa3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15aa3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15aa3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15aa3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15aa3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15aa3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15aa40220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15aa406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15aa40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15aa41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15aa414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15aa41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15aa41de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15aa42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15aa42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15aa42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15aa43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15aa43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15aa439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15aa43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15aa442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15aa44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15aa44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15aa450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15aa45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15aa45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15aa45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15aa46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15aa467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15aa46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15aa47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15aa475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15aa47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15aa47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15aa483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15aa488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15aa48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15aa49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15aa498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15aa49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15aa4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15aa4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15aa4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15aa4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15aa4ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15aa4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15aa4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15aa4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15aa4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15aa4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15aa4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15aa4df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15aa4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15aa4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15aa4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15aa4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15aa4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15aa50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15aa506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15aa50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15aa51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15aa51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15aa51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15aa52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15aa52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15aa52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15aa53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15aa53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15aa53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15aa54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15aa54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15aa54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15aa55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15aa55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15aa55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15aa560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15aa56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15aa56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15aa570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15aa57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15aa57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15aa580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15aa58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15aa58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15aa590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15aa59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15aa59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15aa5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15aa5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15aa5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15aa5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15aa5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15aa5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15aa5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15aa5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15aa5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15aa5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15aa5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15aa5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15aa5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15aa5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15aa5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15aa5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15aa5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15aa5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15aa60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15aa605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15aa60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15aa61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15aa614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15aa61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15aa61e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15aa622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15aa62760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15aa62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15aa630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15aa63540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15aa639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15aa63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15aa64320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15aa647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15aa64c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15aa65100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15aa655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15aa65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15aa66210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15aa66930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15aa67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15aa67770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15aa67a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15aa68220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15aa684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15aa68af0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.106.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159706610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159706a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159706ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159707360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1597077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159707c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1597080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159708520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159708990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159708ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159709360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1597099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15970a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15970acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15970b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15970bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15970c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15970ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15970d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15970d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15970e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15970e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15970ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15970f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15970fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15970ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159710230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1597106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159710b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1597113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159711920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159711d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159712050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1597124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159712930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159712da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159713210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159713680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159713af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159713f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1597143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159714840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159714cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159715120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159715590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159715a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159715e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1597162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159716750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159716bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159717030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1597174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159717910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159717d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1597181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159718760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159718c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1597190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159719540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1597199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159719e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15971a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15971a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15971ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15971afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15971b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15971b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15971bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15971c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15aa4a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15aa4bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15aa687a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15aa49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15aa4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15aa1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15aa1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15aa1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15aa4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15aa14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15aa1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15aa1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15aa1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15aa1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15aa1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15aa1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15aa1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15aa13f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15aa0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15aa2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15aa67cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15aa170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15aa173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15aa4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15aa4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15aa15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15aa157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15aa15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15aa68f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15aa69210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15aa694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15aa69790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15aa69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15aa69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15aa69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15aa6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15aa6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15aa6a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15aa6aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15aa6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15aa6b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15aa6b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15aa6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15aa6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15aa6bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15aa6be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15aa6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15aa6c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15aa6c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15aa6c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15aa6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15aa6ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15aa6d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15aa6d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15aa6d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15aa6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15aa6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15aa6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15aa6e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15aa6e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15aa6e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15aa6ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15aa6ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15aa6ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15aa6f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15aa6f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15aa6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15aa6fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15aa6fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15aa70010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15aa702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15aa70590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15aa70850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15aa70b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15aa70dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15aa71090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15aa71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15aa71610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15aa718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15aa71b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15aa71e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15aa72110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15aa723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15aa72690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15aa72950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15aa72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15aa72ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15aa73190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15aa73450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15aa73710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15aa739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15aa73c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15aa73f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15aa74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15aa744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15aa74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15aa74a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15aa74d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15aa74fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15aa75290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15aa75550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15aa75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15aa75ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15aa75d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15aa76050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15aa76310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15aa765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15aa76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15aa76b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15aa76e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15aa770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15aa77390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15aa77650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15aa77910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15aa77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15aa77e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15aa78150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15aa78410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15aa786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15aa78990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15aa78c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15aa78f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15aa791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15aa79490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15aa79750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15aa79a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15aa79cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15aa79f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15aa7a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15aa7a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15aa7a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15aa7aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15aa7ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15aa7b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15aa7b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15aa7b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15aa7b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15aa7bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15aa7bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15aa7c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15aa7c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15aa7c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15aa7cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15aa7cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15aa7d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15aa7d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15aa7d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15aa7d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15aa7dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15aa7df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15aa7e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15aa7e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15aa7e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15aa7ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15aa7ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15aa7efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15aa7f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15aa7f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15aa7f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1596080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159608380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1596087f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159608c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1596090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159609540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1596099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159609e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15960a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15960a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15960ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15960afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15960b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15960b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15960bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15960c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15960c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15960ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15960cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15960d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15960d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15960dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15960e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15960e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15960e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15960ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15960f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15960f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15960fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15960ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1596108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159610d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159611180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1596115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159611a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159611ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1596127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159612d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159613190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159613600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159613a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159613ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159614350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1596147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159614c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1596150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159615510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159615df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159616260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1596166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159616b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159617db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1596184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159619170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1596195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159619a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159619fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15961a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15961a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15961a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15961af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15961b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15961b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15961b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15961ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15961bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15961bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15961c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15961cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15961d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15961d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15961d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15961de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15961e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15961eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15961f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15961f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15961fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1596200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1596205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159620b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1596210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159621620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1596218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159621e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159622120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1596223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1596226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159622960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159622c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1596231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159623460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1596239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159623ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159623f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159624220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1596244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1596247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159624a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159624fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1596252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159625820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159625ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159625da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159626060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159626320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1596265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159626f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159627480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159627ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1596283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1596288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159628dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1596292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1596297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15962a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15962a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15962ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15962b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15962b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15962bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15962c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15962c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15962cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15962d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15962d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15962df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15962e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15962eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15962f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15962f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15962fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1596301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159630760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159630d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1596312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1596318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159631e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159632420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1596329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159632fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159633560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1596340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1596346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159634c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159635220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1596357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159635da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159636920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159636ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1596374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159637a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159638020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1596385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159638ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159639160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159639720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15963a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15963a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15963ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15963b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15963b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15963ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15963bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15963c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15963c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15963ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15963d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15963d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15963de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15963e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15963e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15963ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15963f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15963f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15963fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159640680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159640b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1596410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1596415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159641ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159641fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1596424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1596429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159642f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159643410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159643920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159644340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159644850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159645260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159645760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159646180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159646690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159646ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1596470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1596475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159647fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1596484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159648a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159649420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159649930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159649e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15964a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15964a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15964ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15964b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15964b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15964bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15964c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15964c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15964cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15964d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15964d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15964db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15964e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15964e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15964ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15964ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15964f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15964f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15964fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159650380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159650890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159650da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1596512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1596517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159651cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1596521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1596526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159652c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1596531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159653760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159653d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1596542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1596548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159654ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1596554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159655ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159656180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159656a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159657850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159657cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159658630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159658de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159659330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159659880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159659dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15965a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15965a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15965adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15965b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15965b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15965bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15965c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15965c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15965cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15965d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15965d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15965dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15965e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15965e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15965ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15965f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15965f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15965fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1596602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159660d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1596612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159661800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159661d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1596622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1596627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159662d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159663290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1596637e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159663d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159664280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1596647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159664d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159665270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1596657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159665d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159666260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1596667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159666d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159667250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1596677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159667cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159668240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159668790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159668ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159669780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159669cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15966a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15966a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15966acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15966b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15966b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15966bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15966c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15966c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15966c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15966ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15966d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15966d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15966dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15966e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15966e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15966ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15966eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15966f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15966f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15966fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159670210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159670930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159671050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159671770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159671e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159672150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159672940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159672c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159673210 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.972s
user	0m0.234s
sys	0m0.202s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
