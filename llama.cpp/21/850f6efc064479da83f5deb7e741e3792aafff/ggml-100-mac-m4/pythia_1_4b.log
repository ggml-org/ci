Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.596s
user	0m0.876s
sys	0m1.262s
++ nproc
+ make -j10
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking C executable ../bin/test-c
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Built target test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Built target test-barrier
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-idle
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-idle
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Built target llama-infill
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-bench
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Built target llama-lookahead
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-retrieval
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Built target llama-perplexity
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-quantize
[ 92%] Linking CXX executable ../../bin/llama-run
[ 93%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 94%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 94%] Built target llama-save-load-state
[ 94%] Built target llama-retrieval
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-speculative
[ 94%] Built target llama-speculative-simple
[ 94%] Linking CXX executable ../../bin/llama-gen-docs
[ 94%] Built target llama-tts
[ 95%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Built target llama-run
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[100%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[100%] Linking CXX executable ../../bin/llama-vdot
[100%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[100%] Built target llama-gen-docs
[100%] Built target llama-convert-llama2c-to-ggml
[100%] Linking CXX executable ../../bin/llama-q8dot
[100%] Built target llama-cvector-generator
[100%] Built target llama-export-lora
[100%] Built target llama-llava-cli
[100%] Built target llama-minicpmv-cli
[100%] Built target llama-vdot
[100%] Built target llama-qwen2vl-cli
[100%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.086s
user	0m6.295s
sys	0m9.795s

main: quantize time =  2979.21 ms
main:    total time =  2979.21 ms

main: quantize time =  1620.35 ms
main:    total time =  1620.35 ms

main: quantize time =  1517.39 ms
main:    total time =  1517.39 ms

main: quantize time =  3297.59 ms
main:    total time =  3297.59 ms

main: quantize time =  1889.51 ms
main:    total time =  1889.51 ms

main: quantize time =  5134.40 ms
main:    total time =  5134.40 ms

main: quantize time =  5656.44 ms
main:    total time =  5656.44 ms

main: quantize time =  7319.65 ms
main:    total time =  7319.65 ms

main: quantize time =  5895.52 ms
main:    total time =  5895.52 ms

main: quantize time =  4516.80 ms
main:    total time =  4516.80 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.123 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.297 I main: llama backend init
0.00.000.303 I main: load the model and apply lora adapter, if any
0.00.031.164 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.702 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.729 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.730 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.731 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.731 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.739 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.740 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.876 I llama_model_loader: - type  f32:  194 tensors
0.00.062.876 I llama_model_loader: - type  f16:   98 tensors
0.00.062.878 I print_info: file format = GGUF V3 (latest)
0.00.062.881 I print_info: file type   = all F32 (guessed)
0.00.062.883 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.092.087 I load: special tokens cache size = 25
0.00.099.251 I load: token to piece cache size = 0.2984 MB
0.00.099.254 I print_info: arch             = gptneox
0.00.099.255 I print_info: vocab_only       = 0
0.00.099.255 I print_info: n_ctx_train      = 2048
0.00.099.255 I print_info: n_embd           = 2048
0.00.099.255 I print_info: n_layer          = 24
0.00.099.258 I print_info: n_head           = 16
0.00.099.259 I print_info: n_head_kv        = 16
0.00.099.259 I print_info: n_rot            = 32
0.00.099.259 I print_info: n_swa            = 0
0.00.099.260 I print_info: n_embd_head_k    = 128
0.00.099.260 I print_info: n_embd_head_v    = 128
0.00.099.261 I print_info: n_gqa            = 1
0.00.099.261 I print_info: n_embd_k_gqa     = 2048
0.00.099.262 I print_info: n_embd_v_gqa     = 2048
0.00.099.262 I print_info: f_norm_eps       = 1.0e-05
0.00.099.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.263 I print_info: f_logit_scale    = 0.0e+00
0.00.099.264 I print_info: n_ff             = 8192
0.00.099.264 I print_info: n_expert         = 0
0.00.099.264 I print_info: n_expert_used    = 0
0.00.099.264 I print_info: causal attn      = 1
0.00.099.264 I print_info: pooling type     = 0
0.00.099.264 I print_info: rope type        = 2
0.00.099.265 I print_info: rope scaling     = linear
0.00.099.265 I print_info: freq_base_train  = 10000.0
0.00.099.265 I print_info: freq_scale_train = 1
0.00.099.266 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.266 I print_info: rope_finetuned   = unknown
0.00.099.266 I print_info: ssm_d_conv       = 0
0.00.099.266 I print_info: ssm_d_inner      = 0
0.00.099.266 I print_info: ssm_d_state      = 0
0.00.099.266 I print_info: ssm_dt_rank      = 0
0.00.099.266 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.268 I print_info: model type       = 1.4B
0.00.099.268 I print_info: model params     = 1.41 B
0.00.099.268 I print_info: general.name     = 1.4B
0.00.099.269 I print_info: vocab type       = BPE
0.00.099.271 I print_info: n_vocab          = 50304
0.00.099.271 I print_info: n_merges         = 50009
0.00.099.271 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.272 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.272 I print_info: LF token         = 128 'Ä'
0.00.099.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.272 I print_info: max token length = 1024
0.00.136.552 I load_tensors: offloading 24 repeating layers to GPU
0.00.136.557 I load_tensors: offloading output layer to GPU
0.00.136.557 I load_tensors: offloaded 25/25 layers to GPU
0.00.136.579 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.136.580 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.136.874 I llama_init_from_model: n_seq_max     = 1
0.00.136.875 I llama_init_from_model: n_ctx         = 2048
0.00.136.876 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.136.876 I llama_init_from_model: n_batch       = 2048
0.00.136.876 I llama_init_from_model: n_ubatch      = 512
0.00.136.876 I llama_init_from_model: flash_attn    = 0
0.00.136.876 I llama_init_from_model: freq_base     = 10000.0
0.00.136.877 I llama_init_from_model: freq_scale    = 1
0.00.136.878 I ggml_metal_init: allocating
0.00.136.894 I ggml_metal_init: found device: Apple M4
0.00.136.896 I ggml_metal_init: picking default device: Apple M4
0.00.137.466 I ggml_metal_init: using embedded metal library
0.00.156.320 I ggml_metal_init: GPU name:   Apple M4
0.00.156.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.323 I ggml_metal_init: simdgroup reduction   = true
0.00.156.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.323 I ggml_metal_init: has residency sets    = true
0.00.156.323 I ggml_metal_init: has bfloat            = true
0.00.156.323 I ggml_metal_init: use bfloat            = true
0.00.156.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.201.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.231.381 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.231.386 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.231.407 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.235.891 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.235.893 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.235.894 I llama_init_from_model: graph nodes  = 967
0.00.235.894 I llama_init_from_model: graph splits = 2
0.00.235.897 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.236.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.236.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.302.717 I main: llama threadpool init, n_threads = 4
0.00.302.756 I 
0.00.302.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.302.789 I 
0.00.302.855 I sampler seed: 1234
0.00.302.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.302.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.302.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.302.886 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.137.490 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.02.137.491 I llama_perf_context_print:        load time =     270.51 ms
0.02.137.492 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.30 tokens per second)
0.02.137.494 I llama_perf_context_print:        eval time =    1788.05 ms /    63 runs   (   28.38 ms per token,    35.23 tokens per second)
0.02.137.494 I llama_perf_context_print:       total time =    1835.81 ms /    70 tokens
0.02.137.735 I ggml_metal_free: deallocating

real	0m2.416s
user	0m0.147s
sys	0m0.131s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.285 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.203 I llama_model_loader: - type  f32:  194 tensors
0.00.032.203 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.204 I print_info: file format = GGUF V3 (latest)
0.00.032.205 I print_info: file type   = Q8_0
0.00.032.206 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.866 I load: special tokens cache size = 25
0.00.058.066 I load: token to piece cache size = 0.2984 MB
0.00.058.070 I print_info: arch             = gptneox
0.00.058.071 I print_info: vocab_only       = 0
0.00.058.071 I print_info: n_ctx_train      = 2048
0.00.058.071 I print_info: n_embd           = 2048
0.00.058.071 I print_info: n_layer          = 24
0.00.058.077 I print_info: n_head           = 16
0.00.058.080 I print_info: n_head_kv        = 16
0.00.058.080 I print_info: n_rot            = 32
0.00.058.081 I print_info: n_swa            = 0
0.00.058.081 I print_info: n_embd_head_k    = 128
0.00.058.081 I print_info: n_embd_head_v    = 128
0.00.058.082 I print_info: n_gqa            = 1
0.00.058.083 I print_info: n_embd_k_gqa     = 2048
0.00.058.084 I print_info: n_embd_v_gqa     = 2048
0.00.058.084 I print_info: f_norm_eps       = 1.0e-05
0.00.058.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.085 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.085 I print_info: f_logit_scale    = 0.0e+00
0.00.058.086 I print_info: n_ff             = 8192
0.00.058.086 I print_info: n_expert         = 0
0.00.058.086 I print_info: n_expert_used    = 0
0.00.058.087 I print_info: causal attn      = 1
0.00.058.087 I print_info: pooling type     = 0
0.00.058.087 I print_info: rope type        = 2
0.00.058.087 I print_info: rope scaling     = linear
0.00.058.088 I print_info: freq_base_train  = 10000.0
0.00.058.088 I print_info: freq_scale_train = 1
0.00.058.088 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.089 I print_info: rope_finetuned   = unknown
0.00.058.089 I print_info: ssm_d_conv       = 0
0.00.058.089 I print_info: ssm_d_inner      = 0
0.00.058.089 I print_info: ssm_d_state      = 0
0.00.058.089 I print_info: ssm_dt_rank      = 0
0.00.058.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.090 I print_info: model type       = 1.4B
0.00.058.090 I print_info: model params     = 1.41 B
0.00.058.090 I print_info: general.name     = 1.4B
0.00.058.091 I print_info: vocab type       = BPE
0.00.058.091 I print_info: n_vocab          = 50304
0.00.058.091 I print_info: n_merges         = 50009
0.00.058.092 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.092 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.092 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.092 I print_info: LF token         = 128 'Ä'
0.00.058.093 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.093 I print_info: max token length = 1024
0.01.087.407 I load_tensors: offloading 24 repeating layers to GPU
0.01.087.413 I load_tensors: offloading output layer to GPU
0.01.087.414 I load_tensors: offloaded 25/25 layers to GPU
0.01.087.439 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.087.441 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.088.160 I llama_init_from_model: n_seq_max     = 1
0.01.088.163 I llama_init_from_model: n_ctx         = 2048
0.01.088.163 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.088.164 I llama_init_from_model: n_batch       = 2048
0.01.088.164 I llama_init_from_model: n_ubatch      = 512
0.01.088.165 I llama_init_from_model: flash_attn    = 0
0.01.088.166 I llama_init_from_model: freq_base     = 10000.0
0.01.088.166 I llama_init_from_model: freq_scale    = 1
0.01.088.167 I ggml_metal_init: allocating
0.01.088.185 I ggml_metal_init: found device: Apple M4
0.01.088.190 I ggml_metal_init: picking default device: Apple M4
0.01.089.468 I ggml_metal_init: using embedded metal library
0.01.094.877 I ggml_metal_init: GPU name:   Apple M4
0.01.094.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.094.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.094.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.094.883 I ggml_metal_init: simdgroup reduction   = true
0.01.094.883 I ggml_metal_init: simdgroup matrix mul. = true
0.01.094.883 I ggml_metal_init: has residency sets    = true
0.01.094.884 I ggml_metal_init: has bfloat            = true
0.01.094.884 I ggml_metal_init: use bfloat            = true
0.01.094.885 I ggml_metal_init: hasUnifiedMemory      = true
0.01.094.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.111.736 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.169.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.169.123 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.169.193 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.173.583 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.173.585 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.173.585 I llama_init_from_model: graph nodes  = 967
0.01.173.585 I llama_init_from_model: graph splits = 2
0.01.173.589 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.173.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.173.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.234.079 I main: llama threadpool init, n_threads = 4
0.01.234.122 I 
0.01.234.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.234.145 I 
0.01.234.366 I sampler seed: 1234
0.01.234.371 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.234.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.234.411 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.234.411 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.319.123 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.02.319.124 I llama_perf_context_print:        load time =    1223.34 ms
0.02.319.125 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.68 tokens per second)
0.02.319.126 I llama_perf_context_print:        eval time =    1037.41 ms /    63 runs   (   16.47 ms per token,    60.73 tokens per second)
0.02.319.126 I llama_perf_context_print:       total time =    1085.96 ms /    70 tokens
0.02.319.450 I ggml_metal_free: deallocating

real	0m2.339s
user	0m0.122s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.017.195 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.330 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.988 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.484 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.486 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.486 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.486 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.487 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.487 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.488 I llama_model_loader: - type  f32:  194 tensors
0.00.043.488 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.488 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.489 I print_info: file format = GGUF V3 (latest)
0.00.043.490 I print_info: file type   = Q4_0
0.00.043.498 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.067.560 I load: special tokens cache size = 25
0.00.075.776 I load: token to piece cache size = 0.2984 MB
0.00.075.780 I print_info: arch             = gptneox
0.00.075.780 I print_info: vocab_only       = 0
0.00.075.780 I print_info: n_ctx_train      = 2048
0.00.075.780 I print_info: n_embd           = 2048
0.00.075.780 I print_info: n_layer          = 24
0.00.075.785 I print_info: n_head           = 16
0.00.075.786 I print_info: n_head_kv        = 16
0.00.075.786 I print_info: n_rot            = 32
0.00.075.786 I print_info: n_swa            = 0
0.00.075.786 I print_info: n_embd_head_k    = 128
0.00.075.786 I print_info: n_embd_head_v    = 128
0.00.075.787 I print_info: n_gqa            = 1
0.00.075.788 I print_info: n_embd_k_gqa     = 2048
0.00.075.789 I print_info: n_embd_v_gqa     = 2048
0.00.075.789 I print_info: f_norm_eps       = 1.0e-05
0.00.075.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.790 I print_info: f_logit_scale    = 0.0e+00
0.00.075.791 I print_info: n_ff             = 8192
0.00.075.791 I print_info: n_expert         = 0
0.00.075.791 I print_info: n_expert_used    = 0
0.00.075.791 I print_info: causal attn      = 1
0.00.075.791 I print_info: pooling type     = 0
0.00.075.793 I print_info: rope type        = 2
0.00.075.795 I print_info: rope scaling     = linear
0.00.075.796 I print_info: freq_base_train  = 10000.0
0.00.075.796 I print_info: freq_scale_train = 1
0.00.075.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.797 I print_info: rope_finetuned   = unknown
0.00.075.797 I print_info: ssm_d_conv       = 0
0.00.075.797 I print_info: ssm_d_inner      = 0
0.00.075.797 I print_info: ssm_d_state      = 0
0.00.075.797 I print_info: ssm_dt_rank      = 0
0.00.075.797 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.798 I print_info: model type       = 1.4B
0.00.075.798 I print_info: model params     = 1.41 B
0.00.075.798 I print_info: general.name     = 1.4B
0.00.075.799 I print_info: vocab type       = BPE
0.00.075.799 I print_info: n_vocab          = 50304
0.00.075.799 I print_info: n_merges         = 50009
0.00.075.799 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.800 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.800 I print_info: LF token         = 128 'Ä'
0.00.075.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.801 I print_info: max token length = 1024
0.00.663.442 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.461 I load_tensors: offloading output layer to GPU
0.00.663.462 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.497 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.663.499 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.664.823 I llama_init_from_model: n_seq_max     = 1
0.00.664.828 I llama_init_from_model: n_ctx         = 2048
0.00.664.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.664.829 I llama_init_from_model: n_batch       = 2048
0.00.664.829 I llama_init_from_model: n_ubatch      = 512
0.00.664.830 I llama_init_from_model: flash_attn    = 0
0.00.664.832 I llama_init_from_model: freq_base     = 10000.0
0.00.664.832 I llama_init_from_model: freq_scale    = 1
0.00.664.835 I ggml_metal_init: allocating
0.00.664.918 I ggml_metal_init: found device: Apple M4
0.00.664.928 I ggml_metal_init: picking default device: Apple M4
0.00.666.717 I ggml_metal_init: using embedded metal library
0.00.672.254 I ggml_metal_init: GPU name:   Apple M4
0.00.672.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.269 I ggml_metal_init: simdgroup reduction   = true
0.00.672.269 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.270 I ggml_metal_init: has residency sets    = true
0.00.672.270 I ggml_metal_init: has bfloat            = true
0.00.672.270 I ggml_metal_init: use bfloat            = true
0.00.672.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.619 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.753.200 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.753.223 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.757.357 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.757.360 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.757.360 I llama_init_from_model: graph nodes  = 967
0.00.757.360 I llama_init_from_model: graph splits = 2
0.00.757.365 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.757.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.757.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.459 I main: llama threadpool init, n_threads = 4
0.00.811.505 I 
0.00.811.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.529 I 
0.00.811.763 I sampler seed: 1234
0.00.811.768 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.779 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.779 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.491.694 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.491.695 I llama_perf_context_print:        load time =     793.35 ms
0.01.491.695 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.01.491.696 I llama_perf_context_print:        eval time =     633.34 ms /    63 runs   (   10.05 ms per token,    99.47 tokens per second)
0.01.491.696 I llama_perf_context_print:       total time =     681.14 ms /    70 tokens
0.01.491.920 I ggml_metal_free: deallocating

real	0m1.512s
user	0m0.133s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.904 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.032.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.487 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.633 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.633 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.041.634 I llama_model_loader: - type  f32:  194 tensors
0.00.041.634 I llama_model_loader: - type q4_1:   97 tensors
0.00.041.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.635 I print_info: file format = GGUF V3 (latest)
0.00.041.636 I print_info: file type   = Q4_1
0.00.041.636 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.063.695 I load: special tokens cache size = 25
0.00.071.751 I load: token to piece cache size = 0.2984 MB
0.00.071.755 I print_info: arch             = gptneox
0.00.071.755 I print_info: vocab_only       = 0
0.00.071.755 I print_info: n_ctx_train      = 2048
0.00.071.755 I print_info: n_embd           = 2048
0.00.071.756 I print_info: n_layer          = 24
0.00.071.759 I print_info: n_head           = 16
0.00.071.760 I print_info: n_head_kv        = 16
0.00.071.760 I print_info: n_rot            = 32
0.00.071.761 I print_info: n_swa            = 0
0.00.071.761 I print_info: n_embd_head_k    = 128
0.00.071.761 I print_info: n_embd_head_v    = 128
0.00.071.762 I print_info: n_gqa            = 1
0.00.071.763 I print_info: n_embd_k_gqa     = 2048
0.00.071.763 I print_info: n_embd_v_gqa     = 2048
0.00.071.764 I print_info: f_norm_eps       = 1.0e-05
0.00.071.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.766 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.766 I print_info: f_logit_scale    = 0.0e+00
0.00.071.767 I print_info: n_ff             = 8192
0.00.071.767 I print_info: n_expert         = 0
0.00.071.767 I print_info: n_expert_used    = 0
0.00.071.768 I print_info: causal attn      = 1
0.00.071.768 I print_info: pooling type     = 0
0.00.071.769 I print_info: rope type        = 2
0.00.071.771 I print_info: rope scaling     = linear
0.00.071.771 I print_info: freq_base_train  = 10000.0
0.00.071.771 I print_info: freq_scale_train = 1
0.00.071.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.772 I print_info: rope_finetuned   = unknown
0.00.071.772 I print_info: ssm_d_conv       = 0
0.00.071.772 I print_info: ssm_d_inner      = 0
0.00.071.772 I print_info: ssm_d_state      = 0
0.00.071.772 I print_info: ssm_dt_rank      = 0
0.00.071.773 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.773 I print_info: model type       = 1.4B
0.00.071.773 I print_info: model params     = 1.41 B
0.00.071.773 I print_info: general.name     = 1.4B
0.00.071.774 I print_info: vocab type       = BPE
0.00.071.774 I print_info: n_vocab          = 50304
0.00.071.775 I print_info: n_merges         = 50009
0.00.071.775 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.776 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.781 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.782 I print_info: LF token         = 128 'Ä'
0.00.071.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.783 I print_info: max token length = 1024
0.00.702.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.895 I load_tensors: offloading output layer to GPU
0.00.702.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.930 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.702.931 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.704.435 I llama_init_from_model: n_seq_max     = 1
0.00.704.439 I llama_init_from_model: n_ctx         = 2048
0.00.704.440 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.704.440 I llama_init_from_model: n_batch       = 2048
0.00.704.440 I llama_init_from_model: n_ubatch      = 512
0.00.704.441 I llama_init_from_model: flash_attn    = 0
0.00.704.443 I llama_init_from_model: freq_base     = 10000.0
0.00.704.443 I llama_init_from_model: freq_scale    = 1
0.00.704.445 I ggml_metal_init: allocating
0.00.704.532 I ggml_metal_init: found device: Apple M4
0.00.704.541 I ggml_metal_init: picking default device: Apple M4
0.00.706.405 I ggml_metal_init: using embedded metal library
0.00.712.966 I ggml_metal_init: GPU name:   Apple M4
0.00.712.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.712.971 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.712.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.712.972 I ggml_metal_init: simdgroup reduction   = true
0.00.712.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.712.972 I ggml_metal_init: has residency sets    = true
0.00.712.973 I ggml_metal_init: has bfloat            = true
0.00.712.973 I ggml_metal_init: use bfloat            = true
0.00.712.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.712.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.424 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.933 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.787.939 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.787.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.792.156 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.792.159 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.792.159 I llama_init_from_model: graph nodes  = 967
0.00.792.159 I llama_init_from_model: graph splits = 2
0.00.792.166 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.792.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.792.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.849 I main: llama threadpool init, n_threads = 4
0.00.847.896 I 
0.00.847.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.921 I 
0.00.848.152 I sampler seed: 1234
0.00.848.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.182 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.182 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.575.053 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.575.054 I llama_perf_context_print:        load time =     836.04 ms
0.01.575.055 I llama_perf_context_print: prompt eval time =      44.49 ms /     7 tokens (    6.36 ms per token,   157.34 tokens per second)
0.01.575.057 I llama_perf_context_print:        eval time =     679.66 ms /    63 runs   (   10.79 ms per token,    92.69 tokens per second)
0.01.575.057 I llama_perf_context_print:       total time =     728.11 ms /    70 tokens
0.01.575.293 I ggml_metal_free: deallocating

real	0m1.594s
user	0m0.128s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.016.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.037.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.491 I llama_model_loader: - type  f32:  194 tensors
0.00.048.492 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.492 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.493 I print_info: file format = GGUF V3 (latest)
0.00.048.493 I print_info: file type   = Q5_0
0.00.048.494 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.076.224 I load: special tokens cache size = 25
0.00.087.289 I load: token to piece cache size = 0.2984 MB
0.00.087.293 I print_info: arch             = gptneox
0.00.087.294 I print_info: vocab_only       = 0
0.00.087.294 I print_info: n_ctx_train      = 2048
0.00.087.294 I print_info: n_embd           = 2048
0.00.087.295 I print_info: n_layer          = 24
0.00.087.298 I print_info: n_head           = 16
0.00.087.299 I print_info: n_head_kv        = 16
0.00.087.300 I print_info: n_rot            = 32
0.00.087.300 I print_info: n_swa            = 0
0.00.087.300 I print_info: n_embd_head_k    = 128
0.00.087.300 I print_info: n_embd_head_v    = 128
0.00.087.301 I print_info: n_gqa            = 1
0.00.087.302 I print_info: n_embd_k_gqa     = 2048
0.00.087.303 I print_info: n_embd_v_gqa     = 2048
0.00.087.304 I print_info: f_norm_eps       = 1.0e-05
0.00.087.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.305 I print_info: f_logit_scale    = 0.0e+00
0.00.087.306 I print_info: n_ff             = 8192
0.00.087.306 I print_info: n_expert         = 0
0.00.087.307 I print_info: n_expert_used    = 0
0.00.087.307 I print_info: causal attn      = 1
0.00.087.307 I print_info: pooling type     = 0
0.00.087.308 I print_info: rope type        = 2
0.00.087.313 I print_info: rope scaling     = linear
0.00.087.314 I print_info: freq_base_train  = 10000.0
0.00.087.314 I print_info: freq_scale_train = 1
0.00.087.314 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.315 I print_info: rope_finetuned   = unknown
0.00.087.315 I print_info: ssm_d_conv       = 0
0.00.087.315 I print_info: ssm_d_inner      = 0
0.00.087.315 I print_info: ssm_d_state      = 0
0.00.087.315 I print_info: ssm_dt_rank      = 0
0.00.087.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.316 I print_info: model type       = 1.4B
0.00.087.317 I print_info: model params     = 1.41 B
0.00.087.317 I print_info: general.name     = 1.4B
0.00.087.317 I print_info: vocab type       = BPE
0.00.087.318 I print_info: n_vocab          = 50304
0.00.087.318 I print_info: n_merges         = 50009
0.00.087.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.320 I print_info: LF token         = 128 'Ä'
0.00.087.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.320 I print_info: max token length = 1024
0.00.818.713 I load_tensors: offloading 24 repeating layers to GPU
0.00.818.729 I load_tensors: offloading output layer to GPU
0.00.818.729 I load_tensors: offloaded 25/25 layers to GPU
0.00.818.764 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.818.766 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.820.354 I llama_init_from_model: n_seq_max     = 1
0.00.820.358 I llama_init_from_model: n_ctx         = 2048
0.00.820.358 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.820.359 I llama_init_from_model: n_batch       = 2048
0.00.820.359 I llama_init_from_model: n_ubatch      = 512
0.00.820.360 I llama_init_from_model: flash_attn    = 0
0.00.820.361 I llama_init_from_model: freq_base     = 10000.0
0.00.820.361 I llama_init_from_model: freq_scale    = 1
0.00.820.363 I ggml_metal_init: allocating
0.00.820.379 I ggml_metal_init: found device: Apple M4
0.00.820.385 I ggml_metal_init: picking default device: Apple M4
0.00.821.838 I ggml_metal_init: using embedded metal library
0.00.828.026 I ggml_metal_init: GPU name:   Apple M4
0.00.828.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.828.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.828.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.828.032 I ggml_metal_init: simdgroup reduction   = true
0.00.828.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.828.032 I ggml_metal_init: has residency sets    = true
0.00.828.033 I ggml_metal_init: has bfloat            = true
0.00.828.033 I ggml_metal_init: use bfloat            = true
0.00.828.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.828.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.845.080 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.901.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.901.387 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.901.410 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.905.922 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.905.925 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.905.925 I llama_init_from_model: graph nodes  = 967
0.00.905.925 I llama_init_from_model: graph splits = 2
0.00.905.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.906.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.906.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.506 I main: llama threadpool init, n_threads = 4
0.00.965.552 I 
0.00.965.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.579 I 
0.00.965.812 I sampler seed: 1234
0.00.965.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.965.860 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.965.864 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.965.864 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.750.282 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.750.283 I llama_perf_context_print:        load time =     947.69 ms
0.01.750.283 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.43 tokens per second)
0.01.750.284 I llama_perf_context_print:        eval time =     734.44 ms /    63 runs   (   11.66 ms per token,    85.78 tokens per second)
0.01.750.284 I llama_perf_context_print:       total time =     785.70 ms /    70 tokens
0.01.750.497 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.139s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.923 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.153 I llama_model_loader: - type  f32:  194 tensors
0.00.026.154 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.155 I print_info: file format = GGUF V3 (latest)
0.00.026.155 I print_info: file type   = Q5_1
0.00.026.156 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.961 I load: special tokens cache size = 25
0.00.052.085 I load: token to piece cache size = 0.2984 MB
0.00.052.088 I print_info: arch             = gptneox
0.00.052.088 I print_info: vocab_only       = 0
0.00.052.088 I print_info: n_ctx_train      = 2048
0.00.052.088 I print_info: n_embd           = 2048
0.00.052.089 I print_info: n_layer          = 24
0.00.052.091 I print_info: n_head           = 16
0.00.052.092 I print_info: n_head_kv        = 16
0.00.052.092 I print_info: n_rot            = 32
0.00.052.092 I print_info: n_swa            = 0
0.00.052.093 I print_info: n_embd_head_k    = 128
0.00.052.093 I print_info: n_embd_head_v    = 128
0.00.052.093 I print_info: n_gqa            = 1
0.00.052.094 I print_info: n_embd_k_gqa     = 2048
0.00.052.095 I print_info: n_embd_v_gqa     = 2048
0.00.052.096 I print_info: f_norm_eps       = 1.0e-05
0.00.052.096 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.096 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.096 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.096 I print_info: f_logit_scale    = 0.0e+00
0.00.052.097 I print_info: n_ff             = 8192
0.00.052.097 I print_info: n_expert         = 0
0.00.052.097 I print_info: n_expert_used    = 0
0.00.052.097 I print_info: causal attn      = 1
0.00.052.098 I print_info: pooling type     = 0
0.00.052.098 I print_info: rope type        = 2
0.00.052.098 I print_info: rope scaling     = linear
0.00.052.098 I print_info: freq_base_train  = 10000.0
0.00.052.099 I print_info: freq_scale_train = 1
0.00.052.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.101 I print_info: rope_finetuned   = unknown
0.00.052.101 I print_info: ssm_d_conv       = 0
0.00.052.102 I print_info: ssm_d_inner      = 0
0.00.052.102 I print_info: ssm_d_state      = 0
0.00.052.102 I print_info: ssm_dt_rank      = 0
0.00.052.103 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.103 I print_info: model type       = 1.4B
0.00.052.104 I print_info: model params     = 1.41 B
0.00.052.104 I print_info: general.name     = 1.4B
0.00.052.104 I print_info: vocab type       = BPE
0.00.052.105 I print_info: n_vocab          = 50304
0.00.052.105 I print_info: n_merges         = 50009
0.00.052.105 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.105 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.105 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.106 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.106 I print_info: LF token         = 128 'Ä'
0.00.052.106 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.106 I print_info: max token length = 1024
0.00.632.536 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.551 I load_tensors: offloading output layer to GPU
0.00.632.552 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.586 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.632.588 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.633.900 I llama_init_from_model: n_seq_max     = 1
0.00.633.905 I llama_init_from_model: n_ctx         = 2048
0.00.633.906 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.633.906 I llama_init_from_model: n_batch       = 2048
0.00.633.906 I llama_init_from_model: n_ubatch      = 512
0.00.633.907 I llama_init_from_model: flash_attn    = 0
0.00.633.909 I llama_init_from_model: freq_base     = 10000.0
0.00.633.910 I llama_init_from_model: freq_scale    = 1
0.00.633.912 I ggml_metal_init: allocating
0.00.633.980 I ggml_metal_init: found device: Apple M4
0.00.633.990 I ggml_metal_init: picking default device: Apple M4
0.00.635.770 I ggml_metal_init: using embedded metal library
0.00.642.348 I ggml_metal_init: GPU name:   Apple M4
0.00.642.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.353 I ggml_metal_init: simdgroup reduction   = true
0.00.642.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.354 I ggml_metal_init: has residency sets    = true
0.00.642.354 I ggml_metal_init: has bfloat            = true
0.00.642.355 I ggml_metal_init: use bfloat            = true
0.00.642.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.357 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.254 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.638 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.648 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.841 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.843 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.844 I llama_init_from_model: graph nodes  = 967
0.00.720.844 I llama_init_from_model: graph splits = 2
0.00.720.851 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.428 I main: llama threadpool init, n_threads = 4
0.00.780.475 I 
0.00.780.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.513 I 
0.00.780.729 I sampler seed: 1234
0.00.780.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.752 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.753 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.753 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.617.952 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.617.953 I llama_perf_context_print:        load time =     770.18 ms
0.01.617.954 I llama_perf_context_print: prompt eval time =      49.64 ms /     7 tokens (    7.09 ms per token,   141.03 tokens per second)
0.01.617.955 I llama_perf_context_print:        eval time =     784.75 ms /    63 runs   (   12.46 ms per token,    80.28 tokens per second)
0.01.617.956 I llama_perf_context_print:       total time =     838.44 ms /    70 tokens
0.01.618.224 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.124s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.336 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.336 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.339 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.340 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.497 I llama_model_loader: - type  f32:  194 tensors
0.00.025.498 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.498 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.498 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.499 I print_info: file format = GGUF V3 (latest)
0.00.025.499 I print_info: file type   = Q2_K - Medium
0.00.025.500 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.482 I load: special tokens cache size = 25
0.00.050.622 I load: token to piece cache size = 0.2984 MB
0.00.050.625 I print_info: arch             = gptneox
0.00.050.625 I print_info: vocab_only       = 0
0.00.050.625 I print_info: n_ctx_train      = 2048
0.00.050.626 I print_info: n_embd           = 2048
0.00.050.626 I print_info: n_layer          = 24
0.00.050.629 I print_info: n_head           = 16
0.00.050.630 I print_info: n_head_kv        = 16
0.00.050.630 I print_info: n_rot            = 32
0.00.050.631 I print_info: n_swa            = 0
0.00.050.631 I print_info: n_embd_head_k    = 128
0.00.050.631 I print_info: n_embd_head_v    = 128
0.00.050.632 I print_info: n_gqa            = 1
0.00.050.633 I print_info: n_embd_k_gqa     = 2048
0.00.050.633 I print_info: n_embd_v_gqa     = 2048
0.00.050.634 I print_info: f_norm_eps       = 1.0e-05
0.00.050.634 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.634 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.635 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.635 I print_info: f_logit_scale    = 0.0e+00
0.00.050.636 I print_info: n_ff             = 8192
0.00.050.636 I print_info: n_expert         = 0
0.00.050.636 I print_info: n_expert_used    = 0
0.00.050.636 I print_info: causal attn      = 1
0.00.050.636 I print_info: pooling type     = 0
0.00.050.636 I print_info: rope type        = 2
0.00.050.637 I print_info: rope scaling     = linear
0.00.050.640 I print_info: freq_base_train  = 10000.0
0.00.050.640 I print_info: freq_scale_train = 1
0.00.050.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.640 I print_info: rope_finetuned   = unknown
0.00.050.640 I print_info: ssm_d_conv       = 0
0.00.050.641 I print_info: ssm_d_inner      = 0
0.00.050.641 I print_info: ssm_d_state      = 0
0.00.050.641 I print_info: ssm_dt_rank      = 0
0.00.050.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.641 I print_info: model type       = 1.4B
0.00.050.643 I print_info: model params     = 1.41 B
0.00.050.643 I print_info: general.name     = 1.4B
0.00.050.644 I print_info: vocab type       = BPE
0.00.050.644 I print_info: n_vocab          = 50304
0.00.050.645 I print_info: n_merges         = 50009
0.00.050.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.646 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.647 I print_info: LF token         = 128 'Ä'
0.00.050.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.648 I print_info: max token length = 1024
0.00.364.619 I load_tensors: offloading 24 repeating layers to GPU
0.00.364.632 I load_tensors: offloading output layer to GPU
0.00.364.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.364.668 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.364.670 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.365.933 I llama_init_from_model: n_seq_max     = 1
0.00.365.939 I llama_init_from_model: n_ctx         = 2048
0.00.365.939 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.365.940 I llama_init_from_model: n_batch       = 2048
0.00.365.940 I llama_init_from_model: n_ubatch      = 512
0.00.365.940 I llama_init_from_model: flash_attn    = 0
0.00.365.942 I llama_init_from_model: freq_base     = 10000.0
0.00.365.943 I llama_init_from_model: freq_scale    = 1
0.00.365.945 I ggml_metal_init: allocating
0.00.366.018 I ggml_metal_init: found device: Apple M4
0.00.366.030 I ggml_metal_init: picking default device: Apple M4
0.00.367.852 I ggml_metal_init: using embedded metal library
0.00.373.412 I ggml_metal_init: GPU name:   Apple M4
0.00.373.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.373.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.373.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.373.429 I ggml_metal_init: simdgroup reduction   = true
0.00.373.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.373.430 I ggml_metal_init: has residency sets    = true
0.00.373.430 I ggml_metal_init: has bfloat            = true
0.00.373.431 I ggml_metal_init: use bfloat            = true
0.00.373.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.373.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.395.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.456.883 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.456.891 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.456.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.461.303 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.461.305 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.461.305 I llama_init_from_model: graph nodes  = 967
0.00.461.305 I llama_init_from_model: graph splits = 2
0.00.461.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.461.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.461.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.523.618 I main: llama threadpool init, n_threads = 4
0.00.523.666 I 
0.00.523.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.523.692 I 
0.00.523.917 I sampler seed: 1234
0.00.523.922 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.523.933 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.523.939 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.523.939 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.200.630 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.200.631 I llama_perf_context_print:        load time =     513.11 ms
0.01.200.632 I llama_perf_context_print: prompt eval time =      40.33 ms /     7 tokens (    5.76 ms per token,   173.59 tokens per second)
0.01.200.633 I llama_perf_context_print:        eval time =     633.58 ms /    63 runs   (   10.06 ms per token,    99.44 tokens per second)
0.01.200.633 I llama_perf_context_print:       total time =     677.90 ms /    70 tokens
0.01.200.855 I ggml_metal_free: deallocating

real	0m1.218s
user	0m0.125s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.668 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.740 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.741 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.742 I llama_model_loader: - type  f32:  194 tensors
0.00.025.743 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.743 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.743 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.743 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.744 I print_info: file format = GGUF V3 (latest)
0.00.025.744 I print_info: file type   = Q3_K - Medium
0.00.025.745 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.517 I load: special tokens cache size = 25
0.00.051.617 I load: token to piece cache size = 0.2984 MB
0.00.051.620 I print_info: arch             = gptneox
0.00.051.621 I print_info: vocab_only       = 0
0.00.051.621 I print_info: n_ctx_train      = 2048
0.00.051.621 I print_info: n_embd           = 2048
0.00.051.621 I print_info: n_layer          = 24
0.00.051.624 I print_info: n_head           = 16
0.00.051.625 I print_info: n_head_kv        = 16
0.00.051.625 I print_info: n_rot            = 32
0.00.051.625 I print_info: n_swa            = 0
0.00.051.626 I print_info: n_embd_head_k    = 128
0.00.051.626 I print_info: n_embd_head_v    = 128
0.00.051.626 I print_info: n_gqa            = 1
0.00.051.627 I print_info: n_embd_k_gqa     = 2048
0.00.051.628 I print_info: n_embd_v_gqa     = 2048
0.00.051.629 I print_info: f_norm_eps       = 1.0e-05
0.00.051.629 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.629 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.629 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.629 I print_info: f_logit_scale    = 0.0e+00
0.00.051.630 I print_info: n_ff             = 8192
0.00.051.630 I print_info: n_expert         = 0
0.00.051.630 I print_info: n_expert_used    = 0
0.00.051.631 I print_info: causal attn      = 1
0.00.051.633 I print_info: pooling type     = 0
0.00.051.633 I print_info: rope type        = 2
0.00.051.634 I print_info: rope scaling     = linear
0.00.051.634 I print_info: freq_base_train  = 10000.0
0.00.051.634 I print_info: freq_scale_train = 1
0.00.051.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.635 I print_info: rope_finetuned   = unknown
0.00.051.635 I print_info: ssm_d_conv       = 0
0.00.051.635 I print_info: ssm_d_inner      = 0
0.00.051.639 I print_info: ssm_d_state      = 0
0.00.051.639 I print_info: ssm_dt_rank      = 0
0.00.051.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.640 I print_info: model type       = 1.4B
0.00.051.641 I print_info: model params     = 1.41 B
0.00.051.641 I print_info: general.name     = 1.4B
0.00.051.641 I print_info: vocab type       = BPE
0.00.051.641 I print_info: n_vocab          = 50304
0.00.051.642 I print_info: n_merges         = 50009
0.00.051.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: LF token         = 128 'Ä'
0.00.051.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: max token length = 1024
0.00.478.580 I load_tensors: offloading 24 repeating layers to GPU
0.00.478.593 I load_tensors: offloading output layer to GPU
0.00.478.593 I load_tensors: offloaded 25/25 layers to GPU
0.00.478.626 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.478.627 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.480.132 I llama_init_from_model: n_seq_max     = 1
0.00.480.137 I llama_init_from_model: n_ctx         = 2048
0.00.480.137 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.480.138 I llama_init_from_model: n_batch       = 2048
0.00.480.138 I llama_init_from_model: n_ubatch      = 512
0.00.480.139 I llama_init_from_model: flash_attn    = 0
0.00.480.141 I llama_init_from_model: freq_base     = 10000.0
0.00.480.141 I llama_init_from_model: freq_scale    = 1
0.00.480.144 I ggml_metal_init: allocating
0.00.480.205 I ggml_metal_init: found device: Apple M4
0.00.480.214 I ggml_metal_init: picking default device: Apple M4
0.00.481.968 I ggml_metal_init: using embedded metal library
0.00.487.411 I ggml_metal_init: GPU name:   Apple M4
0.00.487.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.487.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.487.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.487.428 I ggml_metal_init: simdgroup reduction   = true
0.00.487.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.487.428 I ggml_metal_init: has residency sets    = true
0.00.487.429 I ggml_metal_init: has bfloat            = true
0.00.487.429 I ggml_metal_init: use bfloat            = true
0.00.487.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.487.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.508.058 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.566.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.566.539 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.566.568 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.571.094 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.571.096 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.571.096 I llama_init_from_model: graph nodes  = 967
0.00.571.097 I llama_init_from_model: graph splits = 2
0.00.571.102 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.571.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.571.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.417 I main: llama threadpool init, n_threads = 4
0.00.627.461 I 
0.00.627.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.489 I 
0.00.627.723 I sampler seed: 1234
0.00.627.727 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.627.738 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.627.738 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.627.738 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.373.340 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.373.340 I llama_perf_context_print:        load time =     617.63 ms
0.01.373.341 I llama_perf_context_print: prompt eval time =      49.70 ms /     7 tokens (    7.10 ms per token,   140.85 tokens per second)
0.01.373.342 I llama_perf_context_print:        eval time =     693.05 ms /    63 runs   (   11.00 ms per token,    90.90 tokens per second)
0.01.373.342 I llama_perf_context_print:       total time =     746.84 ms /    70 tokens
0.01.373.545 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.124s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.362 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.604 I llama_model_loader: - type  f32:  194 tensors
0.00.025.604 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.605 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.605 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.605 I print_info: file format = GGUF V3 (latest)
0.00.025.606 I print_info: file type   = Q4_K - Medium
0.00.025.607 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.505 I load: special tokens cache size = 25
0.00.050.576 I load: token to piece cache size = 0.2984 MB
0.00.050.579 I print_info: arch             = gptneox
0.00.050.579 I print_info: vocab_only       = 0
0.00.050.579 I print_info: n_ctx_train      = 2048
0.00.050.579 I print_info: n_embd           = 2048
0.00.050.579 I print_info: n_layer          = 24
0.00.050.582 I print_info: n_head           = 16
0.00.050.583 I print_info: n_head_kv        = 16
0.00.050.583 I print_info: n_rot            = 32
0.00.050.583 I print_info: n_swa            = 0
0.00.050.583 I print_info: n_embd_head_k    = 128
0.00.050.584 I print_info: n_embd_head_v    = 128
0.00.050.584 I print_info: n_gqa            = 1
0.00.050.585 I print_info: n_embd_k_gqa     = 2048
0.00.050.586 I print_info: n_embd_v_gqa     = 2048
0.00.050.586 I print_info: f_norm_eps       = 1.0e-05
0.00.050.587 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.587 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.587 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.587 I print_info: f_logit_scale    = 0.0e+00
0.00.050.588 I print_info: n_ff             = 8192
0.00.050.588 I print_info: n_expert         = 0
0.00.050.588 I print_info: n_expert_used    = 0
0.00.050.588 I print_info: causal attn      = 1
0.00.050.590 I print_info: pooling type     = 0
0.00.050.591 I print_info: rope type        = 2
0.00.050.591 I print_info: rope scaling     = linear
0.00.050.592 I print_info: freq_base_train  = 10000.0
0.00.050.593 I print_info: freq_scale_train = 1
0.00.050.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.594 I print_info: rope_finetuned   = unknown
0.00.050.594 I print_info: ssm_d_conv       = 0
0.00.050.594 I print_info: ssm_d_inner      = 0
0.00.050.594 I print_info: ssm_d_state      = 0
0.00.050.594 I print_info: ssm_dt_rank      = 0
0.00.050.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.595 I print_info: model type       = 1.4B
0.00.050.595 I print_info: model params     = 1.41 B
0.00.050.595 I print_info: general.name     = 1.4B
0.00.050.596 I print_info: vocab type       = BPE
0.00.050.596 I print_info: n_vocab          = 50304
0.00.050.596 I print_info: n_merges         = 50009
0.00.050.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.597 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.602 I print_info: LF token         = 128 'Ä'
0.00.050.602 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.602 I print_info: max token length = 1024
0.00.552.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.552.599 I load_tensors: offloading output layer to GPU
0.00.552.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.552.627 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.552.628 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.554.146 I llama_init_from_model: n_seq_max     = 1
0.00.554.153 I llama_init_from_model: n_ctx         = 2048
0.00.554.154 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.554.154 I llama_init_from_model: n_batch       = 2048
0.00.554.154 I llama_init_from_model: n_ubatch      = 512
0.00.554.155 I llama_init_from_model: flash_attn    = 0
0.00.554.157 I llama_init_from_model: freq_base     = 10000.0
0.00.554.157 I llama_init_from_model: freq_scale    = 1
0.00.554.165 I ggml_metal_init: allocating
0.00.554.225 I ggml_metal_init: found device: Apple M4
0.00.554.233 I ggml_metal_init: picking default device: Apple M4
0.00.556.092 I ggml_metal_init: using embedded metal library
0.00.561.718 I ggml_metal_init: GPU name:   Apple M4
0.00.561.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.561.740 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.561.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.561.741 I ggml_metal_init: simdgroup reduction   = true
0.00.561.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.561.742 I ggml_metal_init: has residency sets    = true
0.00.561.742 I ggml_metal_init: has bfloat            = true
0.00.561.742 I ggml_metal_init: use bfloat            = true
0.00.561.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.561.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.582.741 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.785 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.631.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.631.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.636.620 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.636.620 I llama_init_from_model: graph nodes  = 967
0.00.636.621 I llama_init_from_model: graph splits = 2
0.00.636.625 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.636.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.636.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.526 I main: llama threadpool init, n_threads = 4
0.00.696.577 I 
0.00.696.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.605 I 
0.00.696.834 I sampler seed: 1234
0.00.696.839 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.850 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.850 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.850 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.450.347 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47779.27 tokens per second)
0.01.450.348 I llama_perf_context_print:        load time =     686.28 ms
0.01.450.349 I llama_perf_context_print: prompt eval time =      50.66 ms /     7 tokens (    7.24 ms per token,   138.18 tokens per second)
0.01.450.349 I llama_perf_context_print:        eval time =     700.35 ms /    63 runs   (   11.12 ms per token,    89.96 tokens per second)
0.01.450.350 I llama_perf_context_print:       total time =     754.70 ms /    70 tokens
0.01.450.574 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.122s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.616 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.620 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.621 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.621 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.622 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.622 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.623 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.624 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.626 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.626 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.965 I llama_model_loader: - type  f32:  194 tensors
0.00.026.965 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.965 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.966 I print_info: file format = GGUF V3 (latest)
0.00.026.967 I print_info: file type   = Q5_K - Medium
0.00.026.968 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.215 I load: special tokens cache size = 25
0.00.053.304 I load: token to piece cache size = 0.2984 MB
0.00.053.309 I print_info: arch             = gptneox
0.00.053.309 I print_info: vocab_only       = 0
0.00.053.309 I print_info: n_ctx_train      = 2048
0.00.053.310 I print_info: n_embd           = 2048
0.00.053.310 I print_info: n_layer          = 24
0.00.053.314 I print_info: n_head           = 16
0.00.053.315 I print_info: n_head_kv        = 16
0.00.053.315 I print_info: n_rot            = 32
0.00.053.315 I print_info: n_swa            = 0
0.00.053.315 I print_info: n_embd_head_k    = 128
0.00.053.315 I print_info: n_embd_head_v    = 128
0.00.053.316 I print_info: n_gqa            = 1
0.00.053.317 I print_info: n_embd_k_gqa     = 2048
0.00.053.318 I print_info: n_embd_v_gqa     = 2048
0.00.053.318 I print_info: f_norm_eps       = 1.0e-05
0.00.053.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.319 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.319 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.319 I print_info: f_logit_scale    = 0.0e+00
0.00.053.320 I print_info: n_ff             = 8192
0.00.053.320 I print_info: n_expert         = 0
0.00.053.320 I print_info: n_expert_used    = 0
0.00.053.320 I print_info: causal attn      = 1
0.00.053.320 I print_info: pooling type     = 0
0.00.053.320 I print_info: rope type        = 2
0.00.053.320 I print_info: rope scaling     = linear
0.00.053.321 I print_info: freq_base_train  = 10000.0
0.00.053.321 I print_info: freq_scale_train = 1
0.00.053.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.321 I print_info: rope_finetuned   = unknown
0.00.053.322 I print_info: ssm_d_conv       = 0
0.00.053.322 I print_info: ssm_d_inner      = 0
0.00.053.322 I print_info: ssm_d_state      = 0
0.00.053.322 I print_info: ssm_dt_rank      = 0
0.00.053.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.322 I print_info: model type       = 1.4B
0.00.053.323 I print_info: model params     = 1.41 B
0.00.053.323 I print_info: general.name     = 1.4B
0.00.053.323 I print_info: vocab type       = BPE
0.00.053.323 I print_info: n_vocab          = 50304
0.00.053.324 I print_info: n_merges         = 50009
0.00.053.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.325 I print_info: LF token         = 128 'Ä'
0.00.053.325 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.325 I print_info: max token length = 1024
0.00.620.184 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.200 I load_tensors: offloading output layer to GPU
0.00.620.201 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.243 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.620.245 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.621.262 I llama_init_from_model: n_seq_max     = 1
0.00.621.266 I llama_init_from_model: n_ctx         = 2048
0.00.621.267 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.268 I llama_init_from_model: n_batch       = 2048
0.00.621.268 I llama_init_from_model: n_ubatch      = 512
0.00.621.268 I llama_init_from_model: flash_attn    = 0
0.00.621.271 I llama_init_from_model: freq_base     = 10000.0
0.00.621.271 I llama_init_from_model: freq_scale    = 1
0.00.621.274 I ggml_metal_init: allocating
0.00.621.368 I ggml_metal_init: found device: Apple M4
0.00.621.378 I ggml_metal_init: picking default device: Apple M4
0.00.623.184 I ggml_metal_init: using embedded metal library
0.00.629.723 I ggml_metal_init: GPU name:   Apple M4
0.00.629.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.729 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.730 I ggml_metal_init: simdgroup reduction   = true
0.00.629.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.730 I ggml_metal_init: has residency sets    = true
0.00.629.731 I ggml_metal_init: has bfloat            = true
0.00.629.731 I ggml_metal_init: use bfloat            = true
0.00.629.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.849 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.450 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.477 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.705.646 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.705.648 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.705.649 I llama_init_from_model: graph nodes  = 967
0.00.705.649 I llama_init_from_model: graph splits = 2
0.00.705.654 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.705.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.705.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.886 I main: llama threadpool init, n_threads = 4
0.00.768.929 I 
0.00.768.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.956 I 
0.00.769.178 I sampler seed: 1234
0.00.769.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.192 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.193 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.193 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.778 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.610.779 I llama_perf_context_print:        load time =     758.08 ms
0.01.610.780 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.62 tokens per second)
0.01.610.781 I llama_perf_context_print:        eval time =     787.61 ms /    63 runs   (   12.50 ms per token,    79.99 tokens per second)
0.01.610.781 I llama_perf_context_print:       total time =     842.78 ms /    70 tokens
0.01.611.019 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.123s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.471 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.397 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.398 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.398 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.398 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.617 I llama_model_loader: - type  f32:  194 tensors
0.00.026.617 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.618 I print_info: file format = GGUF V3 (latest)
0.00.026.618 I print_info: file type   = Q6_K
0.00.026.619 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.407 I load: special tokens cache size = 25
0.00.052.486 I load: token to piece cache size = 0.2984 MB
0.00.052.490 I print_info: arch             = gptneox
0.00.052.490 I print_info: vocab_only       = 0
0.00.052.490 I print_info: n_ctx_train      = 2048
0.00.052.490 I print_info: n_embd           = 2048
0.00.052.490 I print_info: n_layer          = 24
0.00.052.493 I print_info: n_head           = 16
0.00.052.494 I print_info: n_head_kv        = 16
0.00.052.494 I print_info: n_rot            = 32
0.00.052.494 I print_info: n_swa            = 0
0.00.052.494 I print_info: n_embd_head_k    = 128
0.00.052.495 I print_info: n_embd_head_v    = 128
0.00.052.496 I print_info: n_gqa            = 1
0.00.052.496 I print_info: n_embd_k_gqa     = 2048
0.00.052.498 I print_info: n_embd_v_gqa     = 2048
0.00.052.498 I print_info: f_norm_eps       = 1.0e-05
0.00.052.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.499 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.499 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.499 I print_info: f_logit_scale    = 0.0e+00
0.00.052.500 I print_info: n_ff             = 8192
0.00.052.500 I print_info: n_expert         = 0
0.00.052.502 I print_info: n_expert_used    = 0
0.00.052.502 I print_info: causal attn      = 1
0.00.052.502 I print_info: pooling type     = 0
0.00.052.502 I print_info: rope type        = 2
0.00.052.503 I print_info: rope scaling     = linear
0.00.052.503 I print_info: freq_base_train  = 10000.0
0.00.052.503 I print_info: freq_scale_train = 1
0.00.052.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.505 I print_info: rope_finetuned   = unknown
0.00.052.505 I print_info: ssm_d_conv       = 0
0.00.052.505 I print_info: ssm_d_inner      = 0
0.00.052.506 I print_info: ssm_d_state      = 0
0.00.052.506 I print_info: ssm_dt_rank      = 0
0.00.052.506 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.506 I print_info: model type       = 1.4B
0.00.052.506 I print_info: model params     = 1.41 B
0.00.052.507 I print_info: general.name     = 1.4B
0.00.052.507 I print_info: vocab type       = BPE
0.00.052.507 I print_info: n_vocab          = 50304
0.00.052.507 I print_info: n_merges         = 50009
0.00.052.508 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.508 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.508 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.508 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.510 I print_info: LF token         = 128 'Ä'
0.00.052.510 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.510 I print_info: max token length = 1024
0.00.682.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.236 I load_tensors: offloading output layer to GPU
0.00.682.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.272 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.682.273 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.683.481 I llama_init_from_model: n_seq_max     = 1
0.00.683.484 I llama_init_from_model: n_ctx         = 2048
0.00.683.484 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.683.485 I llama_init_from_model: n_batch       = 2048
0.00.683.486 I llama_init_from_model: n_ubatch      = 512
0.00.683.486 I llama_init_from_model: flash_attn    = 0
0.00.683.487 I llama_init_from_model: freq_base     = 10000.0
0.00.683.487 I llama_init_from_model: freq_scale    = 1
0.00.683.488 I ggml_metal_init: allocating
0.00.683.506 I ggml_metal_init: found device: Apple M4
0.00.683.511 I ggml_metal_init: picking default device: Apple M4
0.00.685.012 I ggml_metal_init: using embedded metal library
0.00.691.347 I ggml_metal_init: GPU name:   Apple M4
0.00.691.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.353 I ggml_metal_init: simdgroup reduction   = true
0.00.691.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.354 I ggml_metal_init: has residency sets    = true
0.00.691.354 I ggml_metal_init: has bfloat            = true
0.00.691.355 I ggml_metal_init: use bfloat            = true
0.00.691.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.196 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.761.497 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.761.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.765.796 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.765.798 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.765.799 I llama_init_from_model: graph nodes  = 967
0.00.765.799 I llama_init_from_model: graph splits = 2
0.00.765.805 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.765.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.765.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.505 I main: llama threadpool init, n_threads = 4
0.00.829.554 I 
0.00.829.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.580 I 
0.00.829.800 I sampler seed: 1234
0.00.829.806 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.857 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.701.807 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54996.13 tokens per second)
0.01.701.808 I llama_perf_context_print:        load time =     819.13 ms
0.01.701.808 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.701.810 I llama_perf_context_print:        eval time =     814.68 ms /    63 runs   (   12.93 ms per token,    77.33 tokens per second)
0.01.701.810 I llama_perf_context_print:       total time =     873.20 ms /    70 tokens
0.01.702.066 I ggml_metal_free: deallocating

real	0m1.722s
user	0m0.123s
sys	0m0.235s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.693 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.030.520 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.862 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.887 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.889 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.628 I llama_model_loader: - type  f32:  194 tensors
0.00.062.629 I llama_model_loader: - type  f16:   98 tensors
0.00.062.630 I print_info: file format = GGUF V3 (latest)
0.00.062.631 I print_info: file type   = all F32 (guessed)
0.00.062.632 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.987 I load: special tokens cache size = 25
0.00.097.080 I load: token to piece cache size = 0.2984 MB
0.00.097.083 I print_info: arch             = gptneox
0.00.097.084 I print_info: vocab_only       = 0
0.00.097.084 I print_info: n_ctx_train      = 2048
0.00.097.084 I print_info: n_embd           = 2048
0.00.097.084 I print_info: n_layer          = 24
0.00.097.087 I print_info: n_head           = 16
0.00.097.088 I print_info: n_head_kv        = 16
0.00.097.088 I print_info: n_rot            = 32
0.00.097.088 I print_info: n_swa            = 0
0.00.097.088 I print_info: n_embd_head_k    = 128
0.00.097.089 I print_info: n_embd_head_v    = 128
0.00.097.089 I print_info: n_gqa            = 1
0.00.097.090 I print_info: n_embd_k_gqa     = 2048
0.00.097.091 I print_info: n_embd_v_gqa     = 2048
0.00.097.091 I print_info: f_norm_eps       = 1.0e-05
0.00.097.091 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.097.091 I print_info: f_clamp_kqv      = 0.0e+00
0.00.097.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.097.092 I print_info: f_logit_scale    = 0.0e+00
0.00.097.092 I print_info: n_ff             = 8192
0.00.097.093 I print_info: n_expert         = 0
0.00.097.093 I print_info: n_expert_used    = 0
0.00.097.093 I print_info: causal attn      = 1
0.00.097.093 I print_info: pooling type     = 0
0.00.097.096 I print_info: rope type        = 2
0.00.097.096 I print_info: rope scaling     = linear
0.00.097.096 I print_info: freq_base_train  = 10000.0
0.00.097.096 I print_info: freq_scale_train = 1
0.00.097.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.097.097 I print_info: rope_finetuned   = unknown
0.00.097.097 I print_info: ssm_d_conv       = 0
0.00.097.097 I print_info: ssm_d_inner      = 0
0.00.097.097 I print_info: ssm_d_state      = 0
0.00.097.097 I print_info: ssm_dt_rank      = 0
0.00.097.097 I print_info: ssm_dt_b_c_rms   = 0
0.00.097.098 I print_info: model type       = 1.4B
0.00.097.098 I print_info: model params     = 1.41 B
0.00.097.098 I print_info: general.name     = 1.4B
0.00.097.099 I print_info: vocab type       = BPE
0.00.097.099 I print_info: n_vocab          = 50304
0.00.097.100 I print_info: n_merges         = 50009
0.00.097.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.097.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.097.100 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.097.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.097.101 I print_info: LF token         = 128 'Ä'
0.00.097.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.097.101 I print_info: max token length = 1024
0.00.901.714 I load_tensors: offloading 24 repeating layers to GPU
0.00.901.721 I load_tensors: offloading output layer to GPU
0.00.901.721 I load_tensors: offloaded 25/25 layers to GPU
0.00.901.748 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.901.750 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.902.518 I llama_init_from_model: n_seq_max     = 1
0.00.902.521 I llama_init_from_model: n_ctx         = 128
0.00.902.522 I llama_init_from_model: n_ctx_per_seq = 128
0.00.902.522 I llama_init_from_model: n_batch       = 128
0.00.902.522 I llama_init_from_model: n_ubatch      = 128
0.00.902.522 I llama_init_from_model: flash_attn    = 0
0.00.902.523 I llama_init_from_model: freq_base     = 10000.0
0.00.902.524 I llama_init_from_model: freq_scale    = 1
0.00.902.524 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.902.526 I ggml_metal_init: allocating
0.00.902.599 I ggml_metal_init: found device: Apple M4
0.00.902.604 I ggml_metal_init: picking default device: Apple M4
0.00.903.734 I ggml_metal_init: using embedded metal library
0.00.907.735 I ggml_metal_init: GPU name:   Apple M4
0.00.907.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.907.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.907.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.907.740 I ggml_metal_init: simdgroup reduction   = true
0.00.907.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.907.740 I ggml_metal_init: has residency sets    = true
0.00.907.741 I ggml_metal_init: has bfloat            = true
0.00.907.741 I ggml_metal_init: use bfloat            = true
0.00.907.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.907.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.918.578 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.920.288 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.920.290 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.920.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.921.964 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.921.966 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.921.966 I llama_init_from_model: graph nodes  = 967
0.00.921.966 I llama_init_from_model: graph splits = 2
0.00.921.967 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.921.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.354 I 
0.00.957.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.409 I perplexity: tokenizing the input ..
0.00.967.506 I perplexity: tokenization took 10.095 ms
0.00.967.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.086.161 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.087.504 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.087.516 I llama_perf_context_print:        load time =     926.82 ms
0.01.087.517 I llama_perf_context_print: prompt eval time =     118.29 ms /   128 tokens (    0.92 ms per token,  1082.11 tokens per second)
0.01.087.518 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.087.518 I llama_perf_context_print:       total time =     130.16 ms /   129 tokens
0.01.087.897 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.119s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.548 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.549 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.550 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.601 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.603 I llama_model_loader: - type  f32:  194 tensors
0.00.025.604 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.605 I print_info: file format = GGUF V3 (latest)
0.00.025.605 I print_info: file type   = Q8_0
0.00.025.606 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.920 I load: special tokens cache size = 25
0.00.053.119 I load: token to piece cache size = 0.2984 MB
0.00.053.122 I print_info: arch             = gptneox
0.00.053.122 I print_info: vocab_only       = 0
0.00.053.122 I print_info: n_ctx_train      = 2048
0.00.053.122 I print_info: n_embd           = 2048
0.00.053.123 I print_info: n_layer          = 24
0.00.053.126 I print_info: n_head           = 16
0.00.053.127 I print_info: n_head_kv        = 16
0.00.053.127 I print_info: n_rot            = 32
0.00.053.128 I print_info: n_swa            = 0
0.00.053.128 I print_info: n_embd_head_k    = 128
0.00.053.128 I print_info: n_embd_head_v    = 128
0.00.053.129 I print_info: n_gqa            = 1
0.00.053.129 I print_info: n_embd_k_gqa     = 2048
0.00.053.130 I print_info: n_embd_v_gqa     = 2048
0.00.053.131 I print_info: f_norm_eps       = 1.0e-05
0.00.053.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.131 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.132 I print_info: f_logit_scale    = 0.0e+00
0.00.053.132 I print_info: n_ff             = 8192
0.00.053.132 I print_info: n_expert         = 0
0.00.053.132 I print_info: n_expert_used    = 0
0.00.053.133 I print_info: causal attn      = 1
0.00.053.133 I print_info: pooling type     = 0
0.00.053.133 I print_info: rope type        = 2
0.00.053.133 I print_info: rope scaling     = linear
0.00.053.133 I print_info: freq_base_train  = 10000.0
0.00.053.134 I print_info: freq_scale_train = 1
0.00.053.134 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.134 I print_info: rope_finetuned   = unknown
0.00.053.134 I print_info: ssm_d_conv       = 0
0.00.053.135 I print_info: ssm_d_inner      = 0
0.00.053.135 I print_info: ssm_d_state      = 0
0.00.053.135 I print_info: ssm_dt_rank      = 0
0.00.053.135 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.135 I print_info: model type       = 1.4B
0.00.053.136 I print_info: model params     = 1.41 B
0.00.053.136 I print_info: general.name     = 1.4B
0.00.053.136 I print_info: vocab type       = BPE
0.00.053.136 I print_info: n_vocab          = 50304
0.00.053.137 I print_info: n_merges         = 50009
0.00.053.137 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.137 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.137 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.137 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.138 I print_info: LF token         = 128 'Ä'
0.00.053.138 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.138 I print_info: max token length = 1024
0.00.888.345 I load_tensors: offloading 24 repeating layers to GPU
0.00.888.349 I load_tensors: offloading output layer to GPU
0.00.888.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.888.375 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.888.378 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.889.572 I llama_init_from_model: n_seq_max     = 1
0.00.889.574 I llama_init_from_model: n_ctx         = 128
0.00.889.574 I llama_init_from_model: n_ctx_per_seq = 128
0.00.889.575 I llama_init_from_model: n_batch       = 128
0.00.889.575 I llama_init_from_model: n_ubatch      = 128
0.00.889.575 I llama_init_from_model: flash_attn    = 0
0.00.889.576 I llama_init_from_model: freq_base     = 10000.0
0.00.889.577 I llama_init_from_model: freq_scale    = 1
0.00.889.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.889.578 I ggml_metal_init: allocating
0.00.889.629 I ggml_metal_init: found device: Apple M4
0.00.889.635 I ggml_metal_init: picking default device: Apple M4
0.00.890.854 I ggml_metal_init: using embedded metal library
0.00.896.022 I ggml_metal_init: GPU name:   Apple M4
0.00.896.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.896.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.896.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.896.027 I ggml_metal_init: simdgroup reduction   = true
0.00.896.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.896.028 I ggml_metal_init: has residency sets    = true
0.00.896.028 I ggml_metal_init: has bfloat            = true
0.00.896.028 I ggml_metal_init: use bfloat            = true
0.00.896.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.896.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.911.509 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.914.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.914.922 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.914.971 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.918.089 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.918.091 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.918.091 I llama_init_from_model: graph nodes  = 967
0.00.918.091 I llama_init_from_model: graph splits = 2
0.00.918.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.918.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.946.780 I 
0.00.946.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.946.870 I perplexity: tokenizing the input ..
0.00.958.261 I perplexity: tokenization took 11.389 ms
0.00.958.274 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.694 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.095.115 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.095.145 I llama_perf_context_print:        load time =     937.65 ms
0.01.095.146 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.14 tokens per second)
0.01.095.147 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.095.148 I llama_perf_context_print:       total time =     148.37 ms /   129 tokens
0.01.095.573 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.094s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.719 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.609 I llama_model_loader: - type  f32:  194 tensors
0.00.025.609 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.613 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.614 I print_info: file format = GGUF V3 (latest)
0.00.025.614 I print_info: file type   = Q4_0
0.00.025.615 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.613 I load: special tokens cache size = 25
0.00.050.561 I load: token to piece cache size = 0.2984 MB
0.00.050.564 I print_info: arch             = gptneox
0.00.050.565 I print_info: vocab_only       = 0
0.00.050.565 I print_info: n_ctx_train      = 2048
0.00.050.565 I print_info: n_embd           = 2048
0.00.050.565 I print_info: n_layer          = 24
0.00.050.569 I print_info: n_head           = 16
0.00.050.569 I print_info: n_head_kv        = 16
0.00.050.570 I print_info: n_rot            = 32
0.00.050.570 I print_info: n_swa            = 0
0.00.050.572 I print_info: n_embd_head_k    = 128
0.00.050.573 I print_info: n_embd_head_v    = 128
0.00.050.573 I print_info: n_gqa            = 1
0.00.050.574 I print_info: n_embd_k_gqa     = 2048
0.00.050.576 I print_info: n_embd_v_gqa     = 2048
0.00.050.576 I print_info: f_norm_eps       = 1.0e-05
0.00.050.577 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.577 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.577 I print_info: f_logit_scale    = 0.0e+00
0.00.050.578 I print_info: n_ff             = 8192
0.00.050.578 I print_info: n_expert         = 0
0.00.050.578 I print_info: n_expert_used    = 0
0.00.050.578 I print_info: causal attn      = 1
0.00.050.578 I print_info: pooling type     = 0
0.00.050.579 I print_info: rope type        = 2
0.00.050.579 I print_info: rope scaling     = linear
0.00.050.579 I print_info: freq_base_train  = 10000.0
0.00.050.580 I print_info: freq_scale_train = 1
0.00.050.580 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.580 I print_info: rope_finetuned   = unknown
0.00.050.580 I print_info: ssm_d_conv       = 0
0.00.050.580 I print_info: ssm_d_inner      = 0
0.00.050.580 I print_info: ssm_d_state      = 0
0.00.050.581 I print_info: ssm_dt_rank      = 0
0.00.050.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.582 I print_info: model type       = 1.4B
0.00.050.583 I print_info: model params     = 1.41 B
0.00.050.583 I print_info: general.name     = 1.4B
0.00.050.584 I print_info: vocab type       = BPE
0.00.050.584 I print_info: n_vocab          = 50304
0.00.050.584 I print_info: n_merges         = 50009
0.00.050.584 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.584 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: LF token         = 128 'Ä'
0.00.050.585 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.586 I print_info: max token length = 1024
0.00.588.083 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.098 I load_tensors: offloading output layer to GPU
0.00.588.099 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.131 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.132 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.589.638 I llama_init_from_model: n_seq_max     = 1
0.00.589.643 I llama_init_from_model: n_ctx         = 128
0.00.589.644 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.644 I llama_init_from_model: n_batch       = 128
0.00.589.644 I llama_init_from_model: n_ubatch      = 128
0.00.589.645 I llama_init_from_model: flash_attn    = 0
0.00.589.647 I llama_init_from_model: freq_base     = 10000.0
0.00.589.647 I llama_init_from_model: freq_scale    = 1
0.00.589.648 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.650 I ggml_metal_init: allocating
0.00.589.737 I ggml_metal_init: found device: Apple M4
0.00.589.746 I ggml_metal_init: picking default device: Apple M4
0.00.591.516 I ggml_metal_init: using embedded metal library
0.00.597.741 I ggml_metal_init: GPU name:   Apple M4
0.00.597.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.752 I ggml_metal_init: simdgroup reduction   = true
0.00.597.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.753 I ggml_metal_init: has residency sets    = true
0.00.597.753 I ggml_metal_init: has bfloat            = true
0.00.597.753 I ggml_metal_init: use bfloat            = true
0.00.597.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.522 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.033 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.039 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.233 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.235 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.236 I llama_init_from_model: graph nodes  = 967
0.00.623.236 I llama_init_from_model: graph splits = 2
0.00.623.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.705 I 
0.00.648.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.805 I perplexity: tokenizing the input ..
0.00.657.123 I perplexity: tokenization took 8.316 ms
0.00.657.137 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.764 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.780.117 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.780.132 I llama_perf_context_print:        load time =     638.98 ms
0.00.780.133 I llama_perf_context_print: prompt eval time =     121.40 ms /   128 tokens (    0.95 ms per token,  1054.39 tokens per second)
0.00.780.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.134 I llama_perf_context_print:       total time =     131.43 ms /   129 tokens
0.00.780.504 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.091s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.532 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.519 I llama_model_loader: - type  f32:  194 tensors
0.00.024.520 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.520 I print_info: file format = GGUF V3 (latest)
0.00.024.521 I print_info: file type   = Q4_1
0.00.024.522 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.395 I load: special tokens cache size = 25
0.00.049.482 I load: token to piece cache size = 0.2984 MB
0.00.049.485 I print_info: arch             = gptneox
0.00.049.486 I print_info: vocab_only       = 0
0.00.049.486 I print_info: n_ctx_train      = 2048
0.00.049.486 I print_info: n_embd           = 2048
0.00.049.486 I print_info: n_layer          = 24
0.00.049.490 I print_info: n_head           = 16
0.00.049.490 I print_info: n_head_kv        = 16
0.00.049.490 I print_info: n_rot            = 32
0.00.049.491 I print_info: n_swa            = 0
0.00.049.491 I print_info: n_embd_head_k    = 128
0.00.049.491 I print_info: n_embd_head_v    = 128
0.00.049.492 I print_info: n_gqa            = 1
0.00.049.492 I print_info: n_embd_k_gqa     = 2048
0.00.049.493 I print_info: n_embd_v_gqa     = 2048
0.00.049.494 I print_info: f_norm_eps       = 1.0e-05
0.00.049.494 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.494 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.494 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.494 I print_info: f_logit_scale    = 0.0e+00
0.00.049.495 I print_info: n_ff             = 8192
0.00.049.495 I print_info: n_expert         = 0
0.00.049.495 I print_info: n_expert_used    = 0
0.00.049.495 I print_info: causal attn      = 1
0.00.049.495 I print_info: pooling type     = 0
0.00.049.496 I print_info: rope type        = 2
0.00.049.496 I print_info: rope scaling     = linear
0.00.049.496 I print_info: freq_base_train  = 10000.0
0.00.049.497 I print_info: freq_scale_train = 1
0.00.049.497 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.497 I print_info: rope_finetuned   = unknown
0.00.049.497 I print_info: ssm_d_conv       = 0
0.00.049.497 I print_info: ssm_d_inner      = 0
0.00.049.498 I print_info: ssm_d_state      = 0
0.00.049.498 I print_info: ssm_dt_rank      = 0
0.00.049.498 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.498 I print_info: model type       = 1.4B
0.00.049.498 I print_info: model params     = 1.41 B
0.00.049.499 I print_info: general.name     = 1.4B
0.00.049.499 I print_info: vocab type       = BPE
0.00.049.499 I print_info: n_vocab          = 50304
0.00.049.499 I print_info: n_merges         = 50009
0.00.049.500 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.500 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.500 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.500 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.501 I print_info: LF token         = 128 'Ä'
0.00.049.501 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.501 I print_info: max token length = 1024
0.00.631.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.881 I load_tensors: offloading output layer to GPU
0.00.631.882 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.914 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.631.915 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.633.511 I llama_init_from_model: n_seq_max     = 1
0.00.633.515 I llama_init_from_model: n_ctx         = 128
0.00.633.516 I llama_init_from_model: n_ctx_per_seq = 128
0.00.633.516 I llama_init_from_model: n_batch       = 128
0.00.633.517 I llama_init_from_model: n_ubatch      = 128
0.00.633.517 I llama_init_from_model: flash_attn    = 0
0.00.633.519 I llama_init_from_model: freq_base     = 10000.0
0.00.633.520 I llama_init_from_model: freq_scale    = 1
0.00.633.520 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.633.522 I ggml_metal_init: allocating
0.00.633.591 I ggml_metal_init: found device: Apple M4
0.00.633.604 I ggml_metal_init: picking default device: Apple M4
0.00.635.242 I ggml_metal_init: using embedded metal library
0.00.641.523 I ggml_metal_init: GPU name:   Apple M4
0.00.641.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.531 I ggml_metal_init: simdgroup reduction   = true
0.00.641.532 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.532 I ggml_metal_init: has residency sets    = true
0.00.641.532 I ggml_metal_init: has bfloat            = true
0.00.641.533 I ggml_metal_init: use bfloat            = true
0.00.641.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.625 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.188 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.191 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.223 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.349 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.351 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.351 I llama_init_from_model: graph nodes  = 967
0.00.666.352 I llama_init_from_model: graph splits = 2
0.00.666.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.897 I 
0.00.695.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.002 I perplexity: tokenizing the input ..
0.00.707.088 I perplexity: tokenization took 11.085 ms
0.00.707.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.008 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.837.338 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.837.355 I llama_perf_context_print:        load time =     687.06 ms
0.00.837.356 I llama_perf_context_print: prompt eval time =     128.68 ms /   128 tokens (    1.01 ms per token,   994.75 tokens per second)
0.00.837.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.357 I llama_perf_context_print:       total time =     141.46 ms /   129 tokens
0.00.837.806 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.094s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.259 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.604 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.605 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.606 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.606 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.607 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.543 I llama_model_loader: - type  f32:  194 tensors
0.00.026.544 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.545 I print_info: file format = GGUF V3 (latest)
0.00.026.545 I print_info: file type   = Q5_0
0.00.026.546 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.639 I load: special tokens cache size = 25
0.00.051.729 I load: token to piece cache size = 0.2984 MB
0.00.051.732 I print_info: arch             = gptneox
0.00.051.732 I print_info: vocab_only       = 0
0.00.051.732 I print_info: n_ctx_train      = 2048
0.00.051.733 I print_info: n_embd           = 2048
0.00.051.733 I print_info: n_layer          = 24
0.00.051.736 I print_info: n_head           = 16
0.00.051.737 I print_info: n_head_kv        = 16
0.00.051.737 I print_info: n_rot            = 32
0.00.051.737 I print_info: n_swa            = 0
0.00.051.737 I print_info: n_embd_head_k    = 128
0.00.051.737 I print_info: n_embd_head_v    = 128
0.00.051.740 I print_info: n_gqa            = 1
0.00.051.740 I print_info: n_embd_k_gqa     = 2048
0.00.051.741 I print_info: n_embd_v_gqa     = 2048
0.00.051.742 I print_info: f_norm_eps       = 1.0e-05
0.00.051.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.744 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.745 I print_info: f_logit_scale    = 0.0e+00
0.00.051.745 I print_info: n_ff             = 8192
0.00.051.746 I print_info: n_expert         = 0
0.00.051.746 I print_info: n_expert_used    = 0
0.00.051.746 I print_info: causal attn      = 1
0.00.051.746 I print_info: pooling type     = 0
0.00.051.746 I print_info: rope type        = 2
0.00.051.746 I print_info: rope scaling     = linear
0.00.051.747 I print_info: freq_base_train  = 10000.0
0.00.051.747 I print_info: freq_scale_train = 1
0.00.051.747 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.747 I print_info: rope_finetuned   = unknown
0.00.051.747 I print_info: ssm_d_conv       = 0
0.00.051.747 I print_info: ssm_d_inner      = 0
0.00.051.748 I print_info: ssm_d_state      = 0
0.00.051.748 I print_info: ssm_dt_rank      = 0
0.00.051.748 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.748 I print_info: model type       = 1.4B
0.00.051.748 I print_info: model params     = 1.41 B
0.00.051.749 I print_info: general.name     = 1.4B
0.00.051.749 I print_info: vocab type       = BPE
0.00.051.749 I print_info: n_vocab          = 50304
0.00.051.750 I print_info: n_merges         = 50009
0.00.051.750 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.751 I print_info: LF token         = 128 'Ä'
0.00.051.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.751 I print_info: max token length = 1024
0.00.715.470 I load_tensors: offloading 24 repeating layers to GPU
0.00.715.489 I load_tensors: offloading output layer to GPU
0.00.715.490 I load_tensors: offloaded 25/25 layers to GPU
0.00.715.525 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.715.526 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.716.764 I llama_init_from_model: n_seq_max     = 1
0.00.716.770 I llama_init_from_model: n_ctx         = 128
0.00.716.771 I llama_init_from_model: n_ctx_per_seq = 128
0.00.716.771 I llama_init_from_model: n_batch       = 128
0.00.716.772 I llama_init_from_model: n_ubatch      = 128
0.00.716.772 I llama_init_from_model: flash_attn    = 0
0.00.716.774 I llama_init_from_model: freq_base     = 10000.0
0.00.716.775 I llama_init_from_model: freq_scale    = 1
0.00.716.776 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.716.778 I ggml_metal_init: allocating
0.00.716.857 I ggml_metal_init: found device: Apple M4
0.00.716.867 I ggml_metal_init: picking default device: Apple M4
0.00.718.839 I ggml_metal_init: using embedded metal library
0.00.725.322 I ggml_metal_init: GPU name:   Apple M4
0.00.725.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.725.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.725.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.725.331 I ggml_metal_init: simdgroup reduction   = true
0.00.725.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.725.331 I ggml_metal_init: has residency sets    = true
0.00.725.332 I ggml_metal_init: has bfloat            = true
0.00.725.332 I ggml_metal_init: use bfloat            = true
0.00.725.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.725.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.747.162 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.747.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.595 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.596 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.597 I llama_init_from_model: graph nodes  = 967
0.00.750.597 I llama_init_from_model: graph splits = 2
0.00.750.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.261 I 
0.00.780.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.338 I perplexity: tokenizing the input ..
0.00.791.957 I perplexity: tokenization took 11.617 ms
0.00.791.976 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.936.576 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.937.923 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.937.940 I llama_perf_context_print:        load time =     770.00 ms
0.00.937.940 I llama_perf_context_print: prompt eval time =     144.15 ms /   128 tokens (    1.13 ms per token,   887.97 tokens per second)
0.00.937.941 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.942 I llama_perf_context_print:       total time =     157.68 ms /   129 tokens
0.00.938.355 I ggml_metal_free: deallocating

real	0m0.954s
user	0m0.096s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.563 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.679 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.680 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.680 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.681 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.681 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.681 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.682 I llama_model_loader: - type  f32:  194 tensors
0.00.025.682 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.683 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.683 I print_info: file format = GGUF V3 (latest)
0.00.025.684 I print_info: file type   = Q5_1
0.00.025.685 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.170 I load: special tokens cache size = 25
0.00.051.166 I load: token to piece cache size = 0.2984 MB
0.00.051.169 I print_info: arch             = gptneox
0.00.051.169 I print_info: vocab_only       = 0
0.00.051.169 I print_info: n_ctx_train      = 2048
0.00.051.170 I print_info: n_embd           = 2048
0.00.051.170 I print_info: n_layer          = 24
0.00.051.172 I print_info: n_head           = 16
0.00.051.173 I print_info: n_head_kv        = 16
0.00.051.173 I print_info: n_rot            = 32
0.00.051.174 I print_info: n_swa            = 0
0.00.051.174 I print_info: n_embd_head_k    = 128
0.00.051.174 I print_info: n_embd_head_v    = 128
0.00.051.175 I print_info: n_gqa            = 1
0.00.051.175 I print_info: n_embd_k_gqa     = 2048
0.00.051.176 I print_info: n_embd_v_gqa     = 2048
0.00.051.177 I print_info: f_norm_eps       = 1.0e-05
0.00.051.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.182 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.182 I print_info: f_logit_scale    = 0.0e+00
0.00.051.184 I print_info: n_ff             = 8192
0.00.051.184 I print_info: n_expert         = 0
0.00.051.184 I print_info: n_expert_used    = 0
0.00.051.185 I print_info: causal attn      = 1
0.00.051.185 I print_info: pooling type     = 0
0.00.051.185 I print_info: rope type        = 2
0.00.051.185 I print_info: rope scaling     = linear
0.00.051.185 I print_info: freq_base_train  = 10000.0
0.00.051.186 I print_info: freq_scale_train = 1
0.00.051.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.186 I print_info: rope_finetuned   = unknown
0.00.051.189 I print_info: ssm_d_conv       = 0
0.00.051.189 I print_info: ssm_d_inner      = 0
0.00.051.190 I print_info: ssm_d_state      = 0
0.00.051.190 I print_info: ssm_dt_rank      = 0
0.00.051.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.190 I print_info: model type       = 1.4B
0.00.051.190 I print_info: model params     = 1.41 B
0.00.051.191 I print_info: general.name     = 1.4B
0.00.051.191 I print_info: vocab type       = BPE
0.00.051.191 I print_info: n_vocab          = 50304
0.00.051.191 I print_info: n_merges         = 50009
0.00.051.192 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.192 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.192 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.192 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.192 I print_info: LF token         = 128 'Ä'
0.00.051.193 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: max token length = 1024
0.00.628.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.310 I load_tensors: offloading output layer to GPU
0.00.628.311 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.347 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.628.348 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.629.575 I llama_init_from_model: n_seq_max     = 1
0.00.629.581 I llama_init_from_model: n_ctx         = 128
0.00.629.581 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.582 I llama_init_from_model: n_batch       = 128
0.00.629.582 I llama_init_from_model: n_ubatch      = 128
0.00.629.582 I llama_init_from_model: flash_attn    = 0
0.00.629.585 I llama_init_from_model: freq_base     = 10000.0
0.00.629.585 I llama_init_from_model: freq_scale    = 1
0.00.629.586 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.595 I ggml_metal_init: allocating
0.00.629.672 I ggml_metal_init: found device: Apple M4
0.00.629.683 I ggml_metal_init: picking default device: Apple M4
0.00.631.604 I ggml_metal_init: using embedded metal library
0.00.638.696 I ggml_metal_init: GPU name:   Apple M4
0.00.638.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.704 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.704 I ggml_metal_init: simdgroup reduction   = true
0.00.638.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.705 I ggml_metal_init: has residency sets    = true
0.00.638.705 I ggml_metal_init: has bfloat            = true
0.00.638.705 I ggml_metal_init: use bfloat            = true
0.00.638.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.468 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.761 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.763 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.763 I llama_init_from_model: graph nodes  = 967
0.00.662.764 I llama_init_from_model: graph splits = 2
0.00.662.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.767 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.216 I 
0.00.692.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.294 I perplexity: tokenizing the input ..
0.00.701.348 I perplexity: tokenization took 9.052 ms
0.00.701.363 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.021 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.837.334 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.837.348 I llama_perf_context_print:        load time =     682.76 ms
0.00.837.349 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.40 tokens per second)
0.00.837.350 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.350 I llama_perf_context_print:       total time =     145.14 ms /   129 tokens
0.00.837.707 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.094s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.522 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.522 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.523 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.527 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.529 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.579 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.564 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.564 I llama_model_loader: - type  f32:  194 tensors
0.00.026.565 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.565 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.565 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.566 I print_info: file format = GGUF V3 (latest)
0.00.026.566 I print_info: file type   = Q2_K - Medium
0.00.026.567 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.250 I load: special tokens cache size = 25
0.00.052.311 I load: token to piece cache size = 0.2984 MB
0.00.052.314 I print_info: arch             = gptneox
0.00.052.314 I print_info: vocab_only       = 0
0.00.052.315 I print_info: n_ctx_train      = 2048
0.00.052.315 I print_info: n_embd           = 2048
0.00.052.315 I print_info: n_layer          = 24
0.00.052.318 I print_info: n_head           = 16
0.00.052.319 I print_info: n_head_kv        = 16
0.00.052.319 I print_info: n_rot            = 32
0.00.052.319 I print_info: n_swa            = 0
0.00.052.319 I print_info: n_embd_head_k    = 128
0.00.052.320 I print_info: n_embd_head_v    = 128
0.00.052.322 I print_info: n_gqa            = 1
0.00.052.323 I print_info: n_embd_k_gqa     = 2048
0.00.052.324 I print_info: n_embd_v_gqa     = 2048
0.00.052.325 I print_info: f_norm_eps       = 1.0e-05
0.00.052.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.326 I print_info: f_logit_scale    = 0.0e+00
0.00.052.326 I print_info: n_ff             = 8192
0.00.052.327 I print_info: n_expert         = 0
0.00.052.327 I print_info: n_expert_used    = 0
0.00.052.327 I print_info: causal attn      = 1
0.00.052.327 I print_info: pooling type     = 0
0.00.052.327 I print_info: rope type        = 2
0.00.052.327 I print_info: rope scaling     = linear
0.00.052.328 I print_info: freq_base_train  = 10000.0
0.00.052.328 I print_info: freq_scale_train = 1
0.00.052.328 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.329 I print_info: rope_finetuned   = unknown
0.00.052.329 I print_info: ssm_d_conv       = 0
0.00.052.329 I print_info: ssm_d_inner      = 0
0.00.052.329 I print_info: ssm_d_state      = 0
0.00.052.329 I print_info: ssm_dt_rank      = 0
0.00.052.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.330 I print_info: model type       = 1.4B
0.00.052.330 I print_info: model params     = 1.41 B
0.00.052.330 I print_info: general.name     = 1.4B
0.00.052.331 I print_info: vocab type       = BPE
0.00.052.331 I print_info: n_vocab          = 50304
0.00.052.331 I print_info: n_merges         = 50009
0.00.052.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.332 I print_info: LF token         = 128 'Ä'
0.00.052.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.333 I print_info: max token length = 1024
0.00.362.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.789 I load_tensors: offloading output layer to GPU
0.00.362.790 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.836 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.362.837 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.143 I llama_init_from_model: n_seq_max     = 1
0.00.364.150 I llama_init_from_model: n_ctx         = 128
0.00.364.150 I llama_init_from_model: n_ctx_per_seq = 128
0.00.364.151 I llama_init_from_model: n_batch       = 128
0.00.364.151 I llama_init_from_model: n_ubatch      = 128
0.00.364.151 I llama_init_from_model: flash_attn    = 0
0.00.364.154 I llama_init_from_model: freq_base     = 10000.0
0.00.364.155 I llama_init_from_model: freq_scale    = 1
0.00.364.155 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.158 I ggml_metal_init: allocating
0.00.364.244 I ggml_metal_init: found device: Apple M4
0.00.364.255 I ggml_metal_init: picking default device: Apple M4
0.00.366.208 I ggml_metal_init: using embedded metal library
0.00.371.938 I ggml_metal_init: GPU name:   Apple M4
0.00.371.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.371.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.371.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.371.963 I ggml_metal_init: simdgroup reduction   = true
0.00.371.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.371.964 I ggml_metal_init: has residency sets    = true
0.00.371.964 I ggml_metal_init: has bfloat            = true
0.00.371.964 I ggml_metal_init: use bfloat            = true
0.00.371.966 I ggml_metal_init: hasUnifiedMemory      = true
0.00.371.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.394.470 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.398.166 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.398.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.398.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.401.653 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.401.656 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.401.656 I llama_init_from_model: graph nodes  = 967
0.00.401.656 I llama_init_from_model: graph splits = 2
0.00.401.660 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.401.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.427.879 I 
0.00.427.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.427.926 I perplexity: tokenizing the input ..
0.00.435.914 I perplexity: tokenization took 7.986 ms
0.00.435.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.567.269 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.568.593 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.568.609 I llama_perf_context_print:        load time =     417.25 ms
0.00.568.610 I llama_perf_context_print: prompt eval time =     131.08 ms /   128 tokens (    1.02 ms per token,   976.47 tokens per second)
0.00.568.610 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.568.611 I llama_perf_context_print:       total time =     140.73 ms /   129 tokens
0.00.569.035 I ggml_metal_free: deallocating

real	0m0.585s
user	0m0.095s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.195 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.503 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.505 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.674 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.675 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.675 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.676 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.677 I llama_model_loader: - type  f32:  194 tensors
0.00.025.677 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.677 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.678 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.678 I print_info: file format = GGUF V3 (latest)
0.00.025.679 I print_info: file type   = Q3_K - Medium
0.00.025.680 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.276 I load: special tokens cache size = 25
0.00.051.334 I load: token to piece cache size = 0.2984 MB
0.00.051.337 I print_info: arch             = gptneox
0.00.051.337 I print_info: vocab_only       = 0
0.00.051.337 I print_info: n_ctx_train      = 2048
0.00.051.337 I print_info: n_embd           = 2048
0.00.051.338 I print_info: n_layer          = 24
0.00.051.341 I print_info: n_head           = 16
0.00.051.342 I print_info: n_head_kv        = 16
0.00.051.343 I print_info: n_rot            = 32
0.00.051.343 I print_info: n_swa            = 0
0.00.051.344 I print_info: n_embd_head_k    = 128
0.00.051.345 I print_info: n_embd_head_v    = 128
0.00.051.346 I print_info: n_gqa            = 1
0.00.051.347 I print_info: n_embd_k_gqa     = 2048
0.00.051.348 I print_info: n_embd_v_gqa     = 2048
0.00.051.348 I print_info: f_norm_eps       = 1.0e-05
0.00.051.350 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.350 I print_info: f_logit_scale    = 0.0e+00
0.00.051.351 I print_info: n_ff             = 8192
0.00.051.351 I print_info: n_expert         = 0
0.00.051.351 I print_info: n_expert_used    = 0
0.00.051.352 I print_info: causal attn      = 1
0.00.051.352 I print_info: pooling type     = 0
0.00.051.352 I print_info: rope type        = 2
0.00.051.352 I print_info: rope scaling     = linear
0.00.051.353 I print_info: freq_base_train  = 10000.0
0.00.051.353 I print_info: freq_scale_train = 1
0.00.051.357 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.357 I print_info: rope_finetuned   = unknown
0.00.051.357 I print_info: ssm_d_conv       = 0
0.00.051.357 I print_info: ssm_d_inner      = 0
0.00.051.359 I print_info: ssm_d_state      = 0
0.00.051.359 I print_info: ssm_dt_rank      = 0
0.00.051.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.359 I print_info: model type       = 1.4B
0.00.051.359 I print_info: model params     = 1.41 B
0.00.051.359 I print_info: general.name     = 1.4B
0.00.051.360 I print_info: vocab type       = BPE
0.00.051.360 I print_info: n_vocab          = 50304
0.00.051.360 I print_info: n_merges         = 50009
0.00.051.361 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.362 I print_info: LF token         = 128 'Ä'
0.00.051.362 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: max token length = 1024
0.00.469.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.469.627 I load_tensors: offloading output layer to GPU
0.00.469.627 I load_tensors: offloaded 25/25 layers to GPU
0.00.469.660 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.469.662 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.470.921 I llama_init_from_model: n_seq_max     = 1
0.00.470.933 I llama_init_from_model: n_ctx         = 128
0.00.470.934 I llama_init_from_model: n_ctx_per_seq = 128
0.00.470.934 I llama_init_from_model: n_batch       = 128
0.00.470.935 I llama_init_from_model: n_ubatch      = 128
0.00.470.935 I llama_init_from_model: flash_attn    = 0
0.00.470.938 I llama_init_from_model: freq_base     = 10000.0
0.00.470.939 I llama_init_from_model: freq_scale    = 1
0.00.470.939 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.470.945 I ggml_metal_init: allocating
0.00.471.022 I ggml_metal_init: found device: Apple M4
0.00.471.033 I ggml_metal_init: picking default device: Apple M4
0.00.472.851 I ggml_metal_init: using embedded metal library
0.00.478.672 I ggml_metal_init: GPU name:   Apple M4
0.00.478.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.478.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.478.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.478.697 I ggml_metal_init: simdgroup reduction   = true
0.00.478.697 I ggml_metal_init: simdgroup matrix mul. = true
0.00.478.697 I ggml_metal_init: has residency sets    = true
0.00.478.697 I ggml_metal_init: has bfloat            = true
0.00.478.698 I ggml_metal_init: use bfloat            = true
0.00.478.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.478.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.498.374 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.502.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.502.053 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.502.083 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.505.108 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.505.110 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.505.110 I llama_init_from_model: graph nodes  = 967
0.00.505.111 I llama_init_from_model: graph splits = 2
0.00.505.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.505.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.014 I 
0.00.535.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.087 I perplexity: tokenizing the input ..
0.00.546.345 I perplexity: tokenization took 11.256 ms
0.00.546.359 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.686.296 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.687.613 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.687.628 I llama_perf_context_print:        load time =     525.81 ms
0.00.687.629 I llama_perf_context_print: prompt eval time =     139.68 ms /   128 tokens (    1.09 ms per token,   916.39 tokens per second)
0.00.687.630 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.687.631 I llama_perf_context_print:       total time =     152.62 ms /   129 tokens
0.00.688.037 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.097s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.514 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.272 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.387 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.380 I llama_model_loader: - type  f32:  194 tensors
0.00.026.380 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.381 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.381 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.381 I print_info: file format = GGUF V3 (latest)
0.00.026.382 I print_info: file type   = Q4_K - Medium
0.00.026.383 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.213 I load: special tokens cache size = 25
0.00.051.431 I load: token to piece cache size = 0.2984 MB
0.00.051.434 I print_info: arch             = gptneox
0.00.051.434 I print_info: vocab_only       = 0
0.00.051.434 I print_info: n_ctx_train      = 2048
0.00.051.434 I print_info: n_embd           = 2048
0.00.051.434 I print_info: n_layer          = 24
0.00.051.438 I print_info: n_head           = 16
0.00.051.439 I print_info: n_head_kv        = 16
0.00.051.439 I print_info: n_rot            = 32
0.00.051.439 I print_info: n_swa            = 0
0.00.051.439 I print_info: n_embd_head_k    = 128
0.00.051.439 I print_info: n_embd_head_v    = 128
0.00.051.441 I print_info: n_gqa            = 1
0.00.051.442 I print_info: n_embd_k_gqa     = 2048
0.00.051.443 I print_info: n_embd_v_gqa     = 2048
0.00.051.443 I print_info: f_norm_eps       = 1.0e-05
0.00.051.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.448 I print_info: f_logit_scale    = 0.0e+00
0.00.051.449 I print_info: n_ff             = 8192
0.00.051.449 I print_info: n_expert         = 0
0.00.051.449 I print_info: n_expert_used    = 0
0.00.051.449 I print_info: causal attn      = 1
0.00.051.450 I print_info: pooling type     = 0
0.00.051.450 I print_info: rope type        = 2
0.00.051.450 I print_info: rope scaling     = linear
0.00.051.450 I print_info: freq_base_train  = 10000.0
0.00.051.451 I print_info: freq_scale_train = 1
0.00.051.451 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.451 I print_info: rope_finetuned   = unknown
0.00.051.451 I print_info: ssm_d_conv       = 0
0.00.051.451 I print_info: ssm_d_inner      = 0
0.00.051.452 I print_info: ssm_d_state      = 0
0.00.051.452 I print_info: ssm_dt_rank      = 0
0.00.051.452 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.452 I print_info: model type       = 1.4B
0.00.051.453 I print_info: model params     = 1.41 B
0.00.051.453 I print_info: general.name     = 1.4B
0.00.051.453 I print_info: vocab type       = BPE
0.00.051.453 I print_info: n_vocab          = 50304
0.00.051.454 I print_info: n_merges         = 50009
0.00.051.454 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.454 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.454 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.455 I print_info: LF token         = 128 'Ä'
0.00.051.455 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.455 I print_info: max token length = 1024
0.00.541.379 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.391 I load_tensors: offloading output layer to GPU
0.00.541.392 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.423 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.541.424 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.542.646 I llama_init_from_model: n_seq_max     = 1
0.00.542.654 I llama_init_from_model: n_ctx         = 128
0.00.542.654 I llama_init_from_model: n_ctx_per_seq = 128
0.00.542.655 I llama_init_from_model: n_batch       = 128
0.00.542.655 I llama_init_from_model: n_ubatch      = 128
0.00.542.656 I llama_init_from_model: flash_attn    = 0
0.00.542.659 I llama_init_from_model: freq_base     = 10000.0
0.00.542.659 I llama_init_from_model: freq_scale    = 1
0.00.542.660 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.542.663 I ggml_metal_init: allocating
0.00.542.738 I ggml_metal_init: found device: Apple M4
0.00.542.747 I ggml_metal_init: picking default device: Apple M4
0.00.544.537 I ggml_metal_init: using embedded metal library
0.00.550.368 I ggml_metal_init: GPU name:   Apple M4
0.00.550.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.389 I ggml_metal_init: simdgroup reduction   = true
0.00.550.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.389 I ggml_metal_init: has residency sets    = true
0.00.550.390 I ggml_metal_init: has bfloat            = true
0.00.550.390 I ggml_metal_init: use bfloat            = true
0.00.550.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.569.888 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.573.614 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.573.624 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.573.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.576.890 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.576.892 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.576.893 I llama_init_from_model: graph nodes  = 967
0.00.576.893 I llama_init_from_model: graph splits = 2
0.00.576.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.576.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.699 I 
0.00.602.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.778 I perplexity: tokenizing the input ..
0.00.611.463 I perplexity: tokenization took 8.683 ms
0.00.611.477 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.856 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.746.155 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.746.168 I llama_perf_context_print:        load time =     592.18 ms
0.00.746.169 I llama_perf_context_print: prompt eval time =     133.12 ms /   128 tokens (    1.04 ms per token,   961.56 tokens per second)
0.00.746.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.170 I llama_perf_context_print:       total time =     143.47 ms /   129 tokens
0.00.746.543 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.093s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.102 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.365 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.376 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.376 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.383 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.569 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.569 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.570 I print_info: file format = GGUF V3 (latest)
0.00.025.573 I print_info: file type   = Q5_K - Medium
0.00.025.574 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.177 I load: special tokens cache size = 25
0.00.051.293 I load: token to piece cache size = 0.2984 MB
0.00.051.296 I print_info: arch             = gptneox
0.00.051.296 I print_info: vocab_only       = 0
0.00.051.296 I print_info: n_ctx_train      = 2048
0.00.051.297 I print_info: n_embd           = 2048
0.00.051.297 I print_info: n_layer          = 24
0.00.051.300 I print_info: n_head           = 16
0.00.051.301 I print_info: n_head_kv        = 16
0.00.051.301 I print_info: n_rot            = 32
0.00.051.301 I print_info: n_swa            = 0
0.00.051.301 I print_info: n_embd_head_k    = 128
0.00.051.302 I print_info: n_embd_head_v    = 128
0.00.051.302 I print_info: n_gqa            = 1
0.00.051.303 I print_info: n_embd_k_gqa     = 2048
0.00.051.304 I print_info: n_embd_v_gqa     = 2048
0.00.051.305 I print_info: f_norm_eps       = 1.0e-05
0.00.051.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.306 I print_info: f_logit_scale    = 0.0e+00
0.00.051.306 I print_info: n_ff             = 8192
0.00.051.307 I print_info: n_expert         = 0
0.00.051.309 I print_info: n_expert_used    = 0
0.00.051.309 I print_info: causal attn      = 1
0.00.051.309 I print_info: pooling type     = 0
0.00.051.309 I print_info: rope type        = 2
0.00.051.310 I print_info: rope scaling     = linear
0.00.051.310 I print_info: freq_base_train  = 10000.0
0.00.051.312 I print_info: freq_scale_train = 1
0.00.051.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.312 I print_info: rope_finetuned   = unknown
0.00.051.312 I print_info: ssm_d_conv       = 0
0.00.051.313 I print_info: ssm_d_inner      = 0
0.00.051.313 I print_info: ssm_d_state      = 0
0.00.051.313 I print_info: ssm_dt_rank      = 0
0.00.051.313 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.313 I print_info: model type       = 1.4B
0.00.051.314 I print_info: model params     = 1.41 B
0.00.051.314 I print_info: general.name     = 1.4B
0.00.051.318 I print_info: vocab type       = BPE
0.00.051.318 I print_info: n_vocab          = 50304
0.00.051.318 I print_info: n_merges         = 50009
0.00.051.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.319 I print_info: LF token         = 128 'Ä'
0.00.051.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: max token length = 1024
0.00.616.820 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.841 I load_tensors: offloading output layer to GPU
0.00.616.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.874 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.616.875 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.618.175 I llama_init_from_model: n_seq_max     = 1
0.00.618.181 I llama_init_from_model: n_ctx         = 128
0.00.618.181 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.182 I llama_init_from_model: n_batch       = 128
0.00.618.182 I llama_init_from_model: n_ubatch      = 128
0.00.618.182 I llama_init_from_model: flash_attn    = 0
0.00.618.184 I llama_init_from_model: freq_base     = 10000.0
0.00.618.185 I llama_init_from_model: freq_scale    = 1
0.00.618.186 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.188 I ggml_metal_init: allocating
0.00.618.269 I ggml_metal_init: found device: Apple M4
0.00.618.279 I ggml_metal_init: picking default device: Apple M4
0.00.620.119 I ggml_metal_init: using embedded metal library
0.00.626.534 I ggml_metal_init: GPU name:   Apple M4
0.00.626.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.540 I ggml_metal_init: simdgroup reduction   = true
0.00.626.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.541 I ggml_metal_init: has residency sets    = true
0.00.626.541 I ggml_metal_init: has bfloat            = true
0.00.626.541 I ggml_metal_init: use bfloat            = true
0.00.626.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.162 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.811 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.815 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.841 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.010 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.012 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.013 I llama_init_from_model: graph nodes  = 967
0.00.651.013 I llama_init_from_model: graph splits = 2
0.00.651.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.274 I 
0.00.680.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.346 I perplexity: tokenizing the input ..
0.00.691.610 I perplexity: tokenization took 11.262 ms
0.00.691.623 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.862 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.833.164 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.833.181 I llama_perf_context_print:        load time =     671.16 ms
0.00.833.182 I llama_perf_context_print: prompt eval time =     139.98 ms /   128 tokens (    1.09 ms per token,   914.44 tokens per second)
0.00.833.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.185 I llama_perf_context_print:       total time =     152.91 ms /   129 tokens
0.00.833.650 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.096s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.427 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.341 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.503 I llama_model_loader: - type  f32:  194 tensors
0.00.026.504 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.504 I print_info: file format = GGUF V3 (latest)
0.00.026.505 I print_info: file type   = Q6_K
0.00.026.507 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.547 I load: special tokens cache size = 25
0.00.051.796 I load: token to piece cache size = 0.2984 MB
0.00.051.799 I print_info: arch             = gptneox
0.00.051.799 I print_info: vocab_only       = 0
0.00.051.799 I print_info: n_ctx_train      = 2048
0.00.051.799 I print_info: n_embd           = 2048
0.00.051.800 I print_info: n_layer          = 24
0.00.051.803 I print_info: n_head           = 16
0.00.051.803 I print_info: n_head_kv        = 16
0.00.051.804 I print_info: n_rot            = 32
0.00.051.804 I print_info: n_swa            = 0
0.00.051.804 I print_info: n_embd_head_k    = 128
0.00.051.804 I print_info: n_embd_head_v    = 128
0.00.051.805 I print_info: n_gqa            = 1
0.00.051.806 I print_info: n_embd_k_gqa     = 2048
0.00.051.806 I print_info: n_embd_v_gqa     = 2048
0.00.051.807 I print_info: f_norm_eps       = 1.0e-05
0.00.051.807 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.808 I print_info: f_logit_scale    = 0.0e+00
0.00.051.809 I print_info: n_ff             = 8192
0.00.051.809 I print_info: n_expert         = 0
0.00.051.809 I print_info: n_expert_used    = 0
0.00.051.809 I print_info: causal attn      = 1
0.00.051.809 I print_info: pooling type     = 0
0.00.051.809 I print_info: rope type        = 2
0.00.051.810 I print_info: rope scaling     = linear
0.00.051.810 I print_info: freq_base_train  = 10000.0
0.00.051.810 I print_info: freq_scale_train = 1
0.00.051.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.811 I print_info: rope_finetuned   = unknown
0.00.051.811 I print_info: ssm_d_conv       = 0
0.00.051.811 I print_info: ssm_d_inner      = 0
0.00.051.811 I print_info: ssm_d_state      = 0
0.00.051.812 I print_info: ssm_dt_rank      = 0
0.00.051.813 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.813 I print_info: model type       = 1.4B
0.00.051.815 I print_info: model params     = 1.41 B
0.00.051.815 I print_info: general.name     = 1.4B
0.00.051.816 I print_info: vocab type       = BPE
0.00.051.816 I print_info: n_vocab          = 50304
0.00.051.816 I print_info: n_merges         = 50009
0.00.051.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.817 I print_info: LF token         = 128 'Ä'
0.00.051.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: max token length = 1024
0.00.192.670 I load_tensors: offloading 24 repeating layers to GPU
0.00.192.676 I load_tensors: offloading output layer to GPU
0.00.192.676 I load_tensors: offloaded 25/25 layers to GPU
0.00.192.705 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.192.707 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.193.409 I llama_init_from_model: n_seq_max     = 1
0.00.193.411 I llama_init_from_model: n_ctx         = 128
0.00.193.411 I llama_init_from_model: n_ctx_per_seq = 128
0.00.193.411 I llama_init_from_model: n_batch       = 128
0.00.193.411 I llama_init_from_model: n_ubatch      = 128
0.00.193.412 I llama_init_from_model: flash_attn    = 0
0.00.193.412 I llama_init_from_model: freq_base     = 10000.0
0.00.193.413 I llama_init_from_model: freq_scale    = 1
0.00.193.413 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.193.414 I ggml_metal_init: allocating
0.00.193.453 I ggml_metal_init: found device: Apple M4
0.00.193.459 I ggml_metal_init: picking default device: Apple M4
0.00.194.372 I ggml_metal_init: using embedded metal library
0.00.198.447 I ggml_metal_init: GPU name:   Apple M4
0.00.198.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.198.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.198.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.198.451 I ggml_metal_init: simdgroup reduction   = true
0.00.198.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.198.452 I ggml_metal_init: has residency sets    = true
0.00.198.452 I ggml_metal_init: has bfloat            = true
0.00.198.452 I ggml_metal_init: use bfloat            = true
0.00.198.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.198.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.210.926 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.212.897 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.212.900 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.212.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.214.849 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.214.850 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.214.851 I llama_init_from_model: graph nodes  = 967
0.00.214.851 I llama_init_from_model: graph splits = 2
0.00.214.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.214.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.244.870 I 
0.00.244.909 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.244.919 I perplexity: tokenizing the input ..
0.00.252.173 I perplexity: tokenization took 7.252 ms
0.00.252.186 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.391.419 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.392.754 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.392.767 I llama_perf_context_print:        load time =     234.44 ms
0.00.392.767 I llama_perf_context_print: prompt eval time =     138.97 ms /   128 tokens (    1.09 ms per token,   921.04 tokens per second)
0.00.392.768 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.392.769 I llama_perf_context_print:       total time =     147.90 ms /   129 tokens
0.00.393.201 I ggml_metal_free: deallocating

real	0m0.409s
user	0m0.084s
sys	0m0.070s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.322 I build: 4564 (21850f6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.666 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.675 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.675 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.676 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.724 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.792 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.796 I llama_model_loader: - type  f32:  194 tensors
0.00.052.797 I llama_model_loader: - type  f16:   98 tensors
0.00.052.797 I print_info: file format = GGUF V3 (latest)
0.00.052.798 I print_info: file type   = all F32 (guessed)
0.00.052.799 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.502 I load: special tokens cache size = 25
0.00.083.998 I load: token to piece cache size = 0.2984 MB
0.00.084.001 I print_info: arch             = gptneox
0.00.084.002 I print_info: vocab_only       = 0
0.00.084.002 I print_info: n_ctx_train      = 2048
0.00.084.002 I print_info: n_embd           = 2048
0.00.084.002 I print_info: n_layer          = 24
0.00.084.006 I print_info: n_head           = 16
0.00.084.006 I print_info: n_head_kv        = 16
0.00.084.006 I print_info: n_rot            = 32
0.00.084.007 I print_info: n_swa            = 0
0.00.084.007 I print_info: n_embd_head_k    = 128
0.00.084.007 I print_info: n_embd_head_v    = 128
0.00.084.007 I print_info: n_gqa            = 1
0.00.084.008 I print_info: n_embd_k_gqa     = 2048
0.00.084.009 I print_info: n_embd_v_gqa     = 2048
0.00.084.009 I print_info: f_norm_eps       = 1.0e-05
0.00.084.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.012 I print_info: f_logit_scale    = 0.0e+00
0.00.084.012 I print_info: n_ff             = 8192
0.00.084.013 I print_info: n_expert         = 0
0.00.084.013 I print_info: n_expert_used    = 0
0.00.084.013 I print_info: causal attn      = 1
0.00.084.013 I print_info: pooling type     = 0
0.00.084.013 I print_info: rope type        = 2
0.00.084.013 I print_info: rope scaling     = linear
0.00.084.014 I print_info: freq_base_train  = 10000.0
0.00.084.014 I print_info: freq_scale_train = 1
0.00.084.014 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.015 I print_info: rope_finetuned   = unknown
0.00.084.015 I print_info: ssm_d_conv       = 0
0.00.084.015 I print_info: ssm_d_inner      = 0
0.00.084.015 I print_info: ssm_d_state      = 0
0.00.084.016 I print_info: ssm_dt_rank      = 0
0.00.084.018 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.018 I print_info: model type       = 1.4B
0.00.084.018 I print_info: model params     = 1.41 B
0.00.084.018 I print_info: general.name     = 1.4B
0.00.084.019 I print_info: vocab type       = BPE
0.00.084.019 I print_info: n_vocab          = 50304
0.00.084.019 I print_info: n_merges         = 50009
0.00.084.020 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.020 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.020 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.020 I print_info: LF token         = 128 'Ä'
0.00.084.021 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.021 I print_info: max token length = 1024
0.01.372.213 I load_tensors: offloading 24 repeating layers to GPU
0.01.372.218 I load_tensors: offloading output layer to GPU
0.01.372.218 I load_tensors: offloaded 25/25 layers to GPU
0.01.372.246 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.372.248 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.373.044 I llama_init_from_model: n_seq_max     = 1
0.01.373.045 I llama_init_from_model: n_ctx         = 128
0.01.373.045 I llama_init_from_model: n_ctx_per_seq = 128
0.01.373.045 I llama_init_from_model: n_batch       = 128
0.01.373.046 I llama_init_from_model: n_ubatch      = 128
0.01.373.046 I llama_init_from_model: flash_attn    = 0
0.01.373.046 I llama_init_from_model: freq_base     = 10000.0
0.01.373.047 I llama_init_from_model: freq_scale    = 1
0.01.373.047 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.373.048 I ggml_metal_init: allocating
0.01.373.130 I ggml_metal_init: found device: Apple M4
0.01.373.134 I ggml_metal_init: picking default device: Apple M4
0.01.374.303 I ggml_metal_init: using embedded metal library
0.01.378.020 I ggml_metal_init: GPU name:   Apple M4
0.01.378.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.378.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.378.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.378.024 I ggml_metal_init: simdgroup reduction   = true
0.01.378.024 I ggml_metal_init: simdgroup matrix mul. = true
0.01.378.024 I ggml_metal_init: has residency sets    = true
0.01.378.024 I ggml_metal_init: has bfloat            = true
0.01.378.024 I ggml_metal_init: use bfloat            = true
0.01.378.025 I ggml_metal_init: hasUnifiedMemory      = true
0.01.378.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.388.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.390.214 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.390.216 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.390.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.391.837 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.391.838 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.391.838 I llama_init_from_model: graph nodes  = 967
0.01.391.839 I llama_init_from_model: graph splits = 2
0.01.391.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.391.840 I 
0.01.391.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.391.880 I compute_imatrix: tokenizing the input ..
0.01.399.411 I compute_imatrix: tokenization took 7.531 ms
0.01.399.412 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.662.569 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.665.129 I llama_perf_context_print:        load time =    1639.71 ms
0.01.665.130 I llama_perf_context_print: prompt eval time =     261.42 ms /   128 tokens (    2.04 ms per token,   489.63 tokens per second)
0.01.665.130 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.665.131 I llama_perf_context_print:       total time =    1642.27 ms /   129 tokens
0.01.665.629 I ggml_metal_free: deallocating

real	0m1.848s
user	0m0.141s
sys	0m0.258s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4564 (21850f6e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x110e05140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x110e05850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x110e05e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x110e063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x110e06960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x110e06f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x110e074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x110e07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x110e08020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x110e08520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x110e08a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x110e08f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x110e09a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x110e0a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x110e0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x110e0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x110e0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x110e0bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x110e0c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x110e0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x110e0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x110e0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x110e0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x110e0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x110e0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x110e0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x110e0fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x110e108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x110e10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x110e110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x110e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x110e11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x110e120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x110e125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x110e128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x110e12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x110e131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x110e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x110e13b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x110e13fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x110e14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x110e14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x110e14da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x110e15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x110e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x110e15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x110e16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x110e16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x110e17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x110e17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x110e17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x110e18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x110e18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x110e18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x110e19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x110e19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x110e19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x110e1a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x110e1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x110e1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x110e1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x110e1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x110e1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x110e1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x110e1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x110e1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x110e1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x110e1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x110e1d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x110e1dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x110e1e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x110e1e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x110e1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x110e1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x110e1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x110e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x110e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x110e20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x110e20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x110e21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x110e21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x110e21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x110e21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x110e22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x110e22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x110e22fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x110e23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x110e23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x110e23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x110e24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x110e24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x110e24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x110e25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x110e25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x110e25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x110e26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x110e26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x110e16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x110e26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x110e27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x110e27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x110e28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x110e28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x110e28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x110e29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x110e29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x110e29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x110e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x110e2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x110e2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x110e2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x110e2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x110e2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x110e2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x110e2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x110e2c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x110e2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x110e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x110e2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x110e2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x110e2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x110e2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x110e2e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x110e2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x110e2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x110e2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x110e2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x110e300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x110e30580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x110e30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x110e30ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x110e31360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x110e31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x110e31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x110e32140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x110e325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x110e32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x110e32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x110e333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x110e33860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x110e33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x110e341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x110e34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x110e34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x110e34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x110e35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x110e358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x110e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x110e36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x110e366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x110e36b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x110e36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x110e37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x110e37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x110e37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x110e38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x110e38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x110e38ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x110e39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110e394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x110e39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x110e39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x110e3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x110e3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x110e3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x110e3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110e3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x110e3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110e3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110e3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110e3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110e3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x110e3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110e3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x110e3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x110e3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x110e3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x110e3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x110e3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110e3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x110e3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x110e3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x110e40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110e40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x110e411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110e41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x110e41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x110e41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x110e42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x110e428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x110e42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x110e432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x110e43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x110e43d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x110e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x110e44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x110e44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x110e451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x110e457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x110e45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x110e46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x110e46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x110e46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x110e47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x110e47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x110e47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x110e48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x110e488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x110e490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x110e495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x110e49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x110e4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x110e4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x110e4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x110e4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x110e4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x110e4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x110e4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x110e4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x110e4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x110e4d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x110e4d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x110e4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x110e4e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x110e4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x110e4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x110e4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x110e4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x110e4fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x110e50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x110e50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x110e50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x110e51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x110e51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x110e51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x110e52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x110e52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x110e52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x110e53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x110e53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x110e53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x110e53ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x110e54540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x110e54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x110e54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x110e55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x110e55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x110e55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x110e56520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x110e56a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x110e56fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x110e57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x110e57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x110e57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x110e58500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x110e58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x110e58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x110e594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110e59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x110e59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x110e5a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x110e5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x110e5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x110e5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x110e5ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x110e5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x110e5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x110e5c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x110e5cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x110e5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x110e5d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x110e5da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x110e5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x110e5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x110e5e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x110e5ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x110e5f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x110e5f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x110e5fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x110e5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x110e604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x110e60bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x110e61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110e61a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110e62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110e62410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110e62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110e62ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110e634d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.725.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107204d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107205190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107205600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107205a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107205ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107206350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1072067c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107206c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1072070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107207510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107207980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107208070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107208b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107209340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107209b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10720a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10720a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10720b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10720b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10720bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10720c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10720cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10720d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10720db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10720e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10720e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10720e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10720ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10720f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10720f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10720f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10720ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107210380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107210640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107210ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107210f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107211390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107211800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107211c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1072120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107212550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1072129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107212e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1072132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107213710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107213b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107213ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107214460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1072148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107214d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1072151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107215620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107215a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107215f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107216370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1072167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107216d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107217250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1072176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107217b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107217fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107218410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107218880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107218cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120e06e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120e072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120e07720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120e07b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120e08000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120e08470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120e088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120e08d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120e091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120e09630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120e09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120e09f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120e0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120e0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107219160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1072195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107219a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107219eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10721a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10721a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10721ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10721b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10721b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10721b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10721bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10721c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10721c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10721cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10721cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10721d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10721d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10721dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10721e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10721e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10721ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10721ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10721f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10721f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10721fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107220050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1072204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107220930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107220da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107221210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107221680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107221af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107221f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1072223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107222840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107222cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107223120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107223590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107223a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107223e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1072242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107224750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107224bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107225030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1072254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107225910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107225d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1072261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107226660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107226ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107226f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1072273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107227820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107227c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107228100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107228570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1072289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107228e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1072292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107229730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107229ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10722a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10722a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10722a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10722ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10722b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10722b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10722bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10722bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10722c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10722c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10722cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10722d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10722d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10722d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10722de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10722e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10722e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10722eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10722eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10722f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10722f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10722fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1072301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107230620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107230a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107230f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107231370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107231fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107232280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107232540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1072329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107232e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107233290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107233700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107233b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107233fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107234450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1072348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107234d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1072351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107235610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107235a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107235ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107236360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1072367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107236c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1072370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107237520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107237990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107237e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107238270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1072386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107238b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107238fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107239430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1072398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107239d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10723a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10723a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10723aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10723aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10723b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10723b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10723bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10723c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10723c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10723cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10723cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10723d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10723d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10723de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10723e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10723ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10723f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10723f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10723fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107240340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107240900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107240ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107241480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107241a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107242000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1072425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107242b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107243140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107243700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107243cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107244280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107244840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107244e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1072453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107245980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107245f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107246500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107246ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107247080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107247640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107247c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1072481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107248780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107248d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107249300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1072498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107249e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10724a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10724aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10724afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10724b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10724bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10724c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10724c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10724cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10724d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10724d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10724ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10724e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10724e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10724ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10724f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10724fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107250040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107250600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107250bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107251180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107251740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107251d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1072522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107252880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107252e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107253340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107253840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107253d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107254240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107254740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107254c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107255140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107255640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107255b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107256040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107256540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107256a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107256f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107257440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107257940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107258350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107258a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107259190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1072598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107259b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10725a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10725a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10725ac30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1319044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131904950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131904dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131905230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1319056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131905b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131905f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1319063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131906860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131906d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1319071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131907860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131908380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131908b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131909340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131909a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13190a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13190a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13190afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13190b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13190beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13190c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13190ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13190d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13190db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13190ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13190e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13190e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13190e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13190ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13190f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13190f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13190fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13190fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131910340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1319107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131910c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131911090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131911500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131911de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131912250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1319126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131912b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131912fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131913410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131913880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131913cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131914160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1319145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131914a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131914eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131915320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131915790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131915c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131916070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1319165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131916ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131916f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1319173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131917830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131917ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131918110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131918580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1319189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131918e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1319192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131919740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131919bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13191a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13191a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13191a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13191ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13191b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13191b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13191bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13191bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13191c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13191c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13191cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13191d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13191d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13191d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13191de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13191e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13191e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13191eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13191f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13191f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13191f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13191fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1319201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131920630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131920aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131920f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131921380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1319217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131921c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1319220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131922540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1319229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131922e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131923290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131923b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131923de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131924250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1319246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131924b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131924fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131925410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131925880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131925cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131926160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1319265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131926a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131926eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131927320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131927790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131927c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131928070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120e0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120e0ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120e0b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120e0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120e0b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120e0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120e0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120e0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120e0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120e0c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120e0c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120e0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120e0cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120e0d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120e0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120e0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120e0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120e0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120e0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120e0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120e0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120e0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120e0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120e100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120e10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120e109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1319284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131928950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131928dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131929230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1319296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131929b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131929f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13192a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13192a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13192acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13192b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13192b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13192ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13192be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13192c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13192c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13192cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13192d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13192d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13192d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13192dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13192e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13192e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13192eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13192ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13192f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13192f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13192fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131930120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131930590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131930a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131930e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1319312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131931750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131931bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131932030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1319324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131932910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131932d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1319331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131933660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131933ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131933f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1319343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131934820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131934c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131935100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131935570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1319359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131935e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1319362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131936730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131936ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131937010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131937480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1319378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131937d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1319381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131938d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131939010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1319392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131939740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131939bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13193a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13193a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13193a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13193ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13193b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13193b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13193bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13193bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13193c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13193c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13193cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13193d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13193d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13193d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13193de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13193e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13193e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13193eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13193f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13193f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13193f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13193fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1319401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131940630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131940aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131940f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131941380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1319417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131941c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1319420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131942540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1319429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131942e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131943290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131943700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131943b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131943fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131944450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1319448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131944d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1319451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131945610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131945a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131945ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131946360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1319467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131946c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1319470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131947520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131947990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131947e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131948270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1319486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131948b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131948fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131949430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1319498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131949d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13194a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13194a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13194aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13194aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13194b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13194b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13194bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13194c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13194c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13194c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13194d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13194db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110e469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110e44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110e63180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110e44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110e45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110e18540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.790s
user	0m0.300s
sys	0m0.319s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4564 (21850f6e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133f0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133f0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133f0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133f0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133f0f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133f0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133f0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133f10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133f10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133f10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133f11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133f11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133f12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133f12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133f13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133f13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133f14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133f14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133f14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133f15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133f15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133f164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133f16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133f17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133f17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133f17d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133f189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133f18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133f191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133f19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133f19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133f1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133f1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133f1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133f1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133f1b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133f1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133f1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133f1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133f1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133f1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f1eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f24b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f34130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f3b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f3cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f3e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f40cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f49c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f4c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f4c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f63090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f63fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f66030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f66970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f68090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f69420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f69b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f6a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f6a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f6ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f6afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f6b5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.111.995 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.112.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133e083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133e08820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133e08c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133e09100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133e09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133e099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133e09e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133e0a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133e0a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133e0aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133e0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133e0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133e0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133e0c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133e0d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133e0d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133e0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133e0e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133e0ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133e0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133e0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133e103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133e10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133e11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133e11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133e11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133e11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133e12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133e12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133e12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133e13070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133e135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133e13a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133e13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133e14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133e145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133e14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133e14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133e15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133e15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133e15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133e16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133e164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133e16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133e16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133e17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133e17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133e17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133e17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133e183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133e18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133e18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133e19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133e19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133e19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133e19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133e1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133e1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133e1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133e1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133e1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133e1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133e1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133e1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133e1cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133e1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133e1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133e1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133e1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133e1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133e1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133e1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133e1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133e1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133e1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133e201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133e20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133e20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133e20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133e21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133e217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133e21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133e220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133e22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133e22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133e22e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133e23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133e236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133e23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133e23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133e24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133e248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133e24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133e25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133e255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133e25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133e25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133e26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133e267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133e26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133e27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133e27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133e27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133e28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133e286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133e28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133e28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133e29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133e29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133e29cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133e2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133e2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133e2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133e2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133e2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133e2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133e2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133e2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133e2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133e2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133e2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133e2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133e2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133e2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133e2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133e2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133e2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133e2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133e2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133e2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133e2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133e2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133e30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133e30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133e30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133e31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133e314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133e31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133e31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133e32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133e32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133e32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133e32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133e333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133e33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133e33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133e34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133e34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133e34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133e352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133e35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133e35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133e36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133e364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133e36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133e36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133e371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133e37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133e37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133e37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133e383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133e38820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133e39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133e399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133e39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133e3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133e3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133e3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133e3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133e3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133e3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133e3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133e3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133e3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133e3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133e3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133e3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133e3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133e3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133e3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133e3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133e3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133e3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133e3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133e3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133e3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133e40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133e408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133e40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133e411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133e41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133e41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133e41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133e42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133e427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133e42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133e431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133e436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133e43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133e43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133e44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133e44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133e44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133e452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133e45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133e460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133e46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133e46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133e47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133e477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133e47d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133e48350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133e48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133e48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133e49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133e49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133e4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133e4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133e4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133e4b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133e4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133e4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133e4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133e4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133e4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133e4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133e4d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133e4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133e4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133e4ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133e4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133e4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133e4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133e501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133e50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133e50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133e51310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133e518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133e51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133e52450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133e52a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133e52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133e53590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133e53b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133e54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133e54c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133e55250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133e55810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133e55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133e56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133e56950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133e56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133e574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133e57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133e58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133e58610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133e58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133e59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133e59750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133e59d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133e5a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133e5a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133e5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133e5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133e5b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133e5bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133e5c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133e5c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133e5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133e5cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133e5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133e5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133e5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133e5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133e5e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133e5edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133e5f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133e5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133e60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133e60d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133e61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133e617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133e61ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133e620c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135904f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1359053a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135905810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135905c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1359060f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135906560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1359069d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135906e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1359072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135907720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135907b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135908270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135908d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135909540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135909d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13590a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13590ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13590b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13590b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13590c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13590c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13590cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13590d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13590de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13590e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13590e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13590eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13590ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13590f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13590f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13590fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1359101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135910620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1359108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135910d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1359111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135911630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135911aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135911f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135912380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1359127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135912c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1359130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135913540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1359139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135913e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135914290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135914700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135914b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135914fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135915450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1359158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135915d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1359161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135916610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135916a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135916ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1359174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135917960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135917dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135918240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1359186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135918b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135918f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135919400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135919870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135919ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13591a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13591a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13591aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13591aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13591b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13591b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13591bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13591c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13591c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13591c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13591cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13591d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13591d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13591db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13591df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13591e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13591e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13591ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13591f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13591f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13591fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13591fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1359202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135920760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135920bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135921040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1359214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135921920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135921d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135922200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135922670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135922ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135922f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1359233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135923830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135923ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135924530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1359247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135924c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1359250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135925540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1359259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135925e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135926290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135926700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135926b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135926fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135927450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1359278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135927d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1359281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135928610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135928a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135928ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135929360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1359297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135929c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13592a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13592a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13592a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13592ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13592b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13592b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13592bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13592bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13592c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13592c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13592cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13592d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13592d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13592da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13592ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13592e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13592e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13592ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13592f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13592f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13592f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13592fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135930250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1359306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135930b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135930fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135931410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135931880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135931cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135932160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1359325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135932a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135932eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135933320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135933790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135933c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135934070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1359344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135934950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135934dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135935230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1359356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135935b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135935f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1359363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135936860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135936cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135937140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1359375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135937a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135937e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135938300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135938770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135938be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135939050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1359394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135939930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135939da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13593a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13593a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13593aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13593af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13593b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13593b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13593bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13593c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13593c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13593ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13593ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13593d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13593d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13593dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13593e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13593e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13593e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13593ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13593f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13593f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13593fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13593ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1359403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135940820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135940c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135941100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135941570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1359419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135942560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135942820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135942ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135942f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1359433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135943830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135943ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135944110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135944580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1359449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135944e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1359452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135945740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135945bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135946020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135946490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135946900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135946d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1359471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135947650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135947ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135947f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1359483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135948810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135948c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1359490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135949560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1359499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135949e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13594a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13594a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13594ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13594b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13594b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13594b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13594bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13594c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13594c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13594caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13594cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13594d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13594d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13594dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13594e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13594e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13594e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13594ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13594f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13594f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13594fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13594ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135950450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1359508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135950d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1359511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135951610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135951a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135951ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135952360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1359527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135952c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1359530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135953520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135953990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135953e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135954270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1359546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135954b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135954fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135955430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1359558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135955d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135956180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135956bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135957310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135957a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135958150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135958410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135958880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135958e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135959490 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.978s
user	0m0.250s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
